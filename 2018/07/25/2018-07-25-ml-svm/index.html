<!-- build time:Wed Dec 12 2018 18:25:52 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="svm,"><link rel="alternate" href="/atom.xml" title="Eve-Seven" type="application/atom+xml"><meta name="description" content="关于支持向量机SVM的理解和推导总结"><meta name="keywords" content="svm"><meta property="og:type" content="article"><meta property="og:title" content="机器学习-支持向量机SVM"><meta property="og:url" content="http://yoursite.com/2018/07/25/2018-07-25-ml-svm/index.html"><meta property="og:site_name" content="Eve-Seven"><meta property="og:description" content="关于支持向量机SVM的理解和推导总结"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://yoursite.com/images/ml/18.png"><meta property="og:image" content="http://yoursite.com/images/ml/19.png"><meta property="og:image" content="http://yoursite.com/images/ml/20.png"><meta property="og:image" content="http://yoursite.com/images/ml/16.png"><meta property="og:image" content="http://yoursite.com/images/ml/17.png"><meta property="og:image" content="http://yoursite.com/images/ml/21.png"><meta property="og:image" content="http://yoursite.com/images/ml/22.png"><meta property="og:image" content="http://yoursite.com/images/ml/58.png"><meta property="og:image" content="http://yoursite.com/images/ml/53.png"><meta property="og:image" content="http://yoursite.com/images/ml/54.png"><meta property="og:image" content="http://yoursite.com/images/ml/55.png"><meta property="og:image" content="http://yoursite.com/images/ml/56.png"><meta property="og:image" content="http://yoursite.com/images/ml/57.png"><meta property="og:updated_time" content="2018-12-11T13:59:12.973Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="机器学习-支持向量机SVM"><meta name="twitter:description" content="关于支持向量机SVM的理解和推导总结"><meta name="twitter:image" content="http://yoursite.com/images/ml/18.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",version:"5.1.4",sidebar:{position:"left",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2018/07/25/2018-07-25-ml-svm/"><title>机器学习-支持向量机SVM | Eve-Seven</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Eve-Seven</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description"></h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/25/2018-07-25-ml-svm/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Seven"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Eve-Seven"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">机器学习-支持向量机SVM</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-25T00:00:00+08:00">2018-07-25 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">3.9k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">15</span></div><div class="post-description">关于支持向量机SVM的理解和推导总结</div></div></header><div class="post-body" itemprop="articleBody"><h2 id="支持向量机SVM的概念及起源"><a href="#支持向量机SVM的概念及起源" class="headerlink" title="支持向量机SVM的概念及起源"></a>支持向量机SVM的概念及起源</h2><h3 id="什么是支持向量机SVM"><a href="#什么是支持向量机SVM" class="headerlink" title="什么是支持向量机SVM"></a>什么是支持向量机SVM</h3><p>支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。</p><h3 id="分类标准的起源：Logistic回归"><a href="#分类标准的起源：Logistic回归" class="headerlink" title="分类标准的起源：Logistic回归"></a>分类标准的起源：Logistic回归</h3><h4 id="我们先看看什么是线性分类器"><a href="#我们先看看什么是线性分类器" class="headerlink" title="我们先看看什么是线性分类器"></a>我们先看看什么是线性分类器</h4><p>给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用$x$表示数据点，用$y$表示类别（$y=1$或者$y=-1$，分别代表两个不同的类），一个线性分类器的学习目标便是要在$n$维的数据空间中找到一个超平面（hyper plane），这个超平面的方程可以表示为（$w^T$中的T代表转置）：</p><h2 id="w-Tx-b-0"><a href="#w-Tx-b-0" class="headerlink" title="$w^Tx+b=0$"></a>$w^Tx+b=0$</h2><p><strong>Logistic回归</strong>目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于$y=1$的概率。</p><p>根据我们前面<a href="https://sevenold.github.io/2018/07/ml-logisticRegression/" target="_blank" rel="noopener">Logistic回归的推导</a> ，假设函数：</p><h2 id="h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx"><a href="#h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx" class="headerlink" title="$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$"></a>$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$</h2><p>其中x是n维特征向量，函数g就是Logistic函数。</p><h4 id="同样-g-z-frac-1-1-e-z-的图像："><a href="#同样-g-z-frac-1-1-e-z-的图像：" class="headerlink" title="同样$g(z)=\frac{1}{1+e^{-z}}$的图像："></a>同样$g(z)=\frac{1}{1+e^{-z}}$的图像：</h4><p><img src="/images/ml/18.png" alt="images"></p><p>从图像中，我们就可以看出将无穷映射到了（0,1）。</p><p>而假设函数就是特征属于y=1的概率。</p><h2 id="P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x"><a href="#P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x" class="headerlink" title="$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$"></a>$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$</h2><p>所以，我们在判别一个新的特征属于哪个类别的时候，就只需要求$h_\theta(x)$就可以了，由上面的图中可以看出如果$h_\theta(x)$ 大于0.5就是y=1的类，否则就是属于y=1的类。</p><h4 id="然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y-0和y-1替换为-y-1-y-1-然后把-theta-0-替换成b，把-w-替换-theta-i-所以就变换为："><a href="#然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y-0和y-1替换为-y-1-y-1-然后把-theta-0-替换成b，把-w-替换-theta-i-所以就变换为：" class="headerlink" title="然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y=0和y=1替换为$y=-1,y=1 $,然后把$\theta_0$替换成b，把$w$替换$\theta_i$,所以就变换为："></a>然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y=0和y=1替换为$y=-1,y=1 $,然后把$\theta_0$替换成b，把$w$替换$\theta_i$,所以就变换为：</h4><h2 id="h-theta-x-g-w-Tx-b"><a href="#h-theta-x-g-w-Tx-b" class="headerlink" title="$h_\theta(x)=g(w^Tx+b)$"></a>$h_\theta(x)=g(w^Tx+b)$</h2><h4 id="所以我们就可以假设函数：-h-w-b-x-g-w-Tx-b-中的g-z-做一个简化，将其简单映射到y-1和y-1上。映射关系如下："><a href="#所以我们就可以假设函数：-h-w-b-x-g-w-Tx-b-中的g-z-做一个简化，将其简单映射到y-1和y-1上。映射关系如下：" class="headerlink" title="所以我们就可以假设函数：$h_{w,b}(x)=g(w^Tx+b)$,中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下："></a>所以我们就可以假设函数：$h_{w,b}(x)=g(w^Tx+b)$,中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下：</h4><h2 id="g-z-left-lbrace-begin-aligned-1-amp-amp-z-ge0-1-amp-amp-z-lt-0-end-aligned-right"><a href="#g-z-left-lbrace-begin-aligned-1-amp-amp-z-ge0-1-amp-amp-z-lt-0-end-aligned-right" class="headerlink" title="$ g(z)=\left\lbrace  \begin{aligned} 1 &amp;&amp; z \ge0 \\ -1 &amp;&amp; z&lt;0 \end{aligned} \right. $"></a>$ g(z)=\left\lbrace \begin{aligned} 1 &amp;&amp; z \ge0 \\ -1 &amp;&amp; z&lt;0 \end{aligned} \right. $</h2><h4 id="然后我们举个线性分类的例子来看看"><a href="#然后我们举个线性分类的例子来看看" class="headerlink" title="然后我们举个线性分类的例子来看看"></a>然后我们举个线性分类的例子来看看</h4><p>如下图所示，现在有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示。由于这些数据是线性可分的，所以可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是-1，另一边所对应的y全是1。</p><p><img src="/images/ml/19.png" alt="images"></p><p>这个超平面可以用分类函数$f(x)=w^Tx+b$来表示，当f(x)等于0时候，x便是位于超平面上的点，而f(x)大于0的点对应y=1的数据点，f(x)小于0的点对应y=-1的点，如下图所示：</p><p><img src="/images/ml/20.png" alt="images"></p><p>接下来的问题是，如何确定这个超平面呢？从直观上而言，这个超平面应该是最适合分开两类数据的直线。而判定“最适合”的标准就是这条直线离直线两边的数据的间隔最大。所以，得寻找有着最大间隔的超平面。</p><h3 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h3><p>SVM支持向量机（英文全称：support vector machine）是一个分类算法， 通过找到一个分类平面， 将数据分隔在平面两侧， 从而达到分类的目的。如下图所示， 直线表示的是训练出的一个分类平面， 将数据有效的分隔开。</p><p><img src="/images/ml/16.png" alt="images"></p><p>根据上面的逻辑，我们在做数据分隔的时候，有很多个分类平面，这时我们就需要找出“最优”的那一个平面模型，根据【超平面】【数据点】【分开】这几个词，我们可以想到最优的模型必然是最大程度地将数据点划分开的模型，不能靠近负样本也不能靠近正样本，要不偏不倚，并且与所有Support Vector的距离尽量大才可以。</p><p><img src="/images/ml/17.png" alt="images"></p><h4 id="上图中-x-0-是-x-在超平面上的投影，-ω-是超平面的法向量"><a href="#上图中-x-0-是-x-在超平面上的投影，-ω-是超平面的法向量" class="headerlink" title="上图中 $x_0$ 是 $x $在超平面上的投影，$ω$是超平面的法向量."></a>上图中 $x_0$ 是 $x $在超平面上的投影，$ω$是超平面的法向量.</h4><h4 id="超平面用线性方程来描述："><a href="#超平面用线性方程来描述：" class="headerlink" title="超平面用线性方程来描述："></a>超平面用线性方程来描述：</h4><h2 id="w-T-b-0"><a href="#w-T-b-0" class="headerlink" title="$w^T+b=0$"></a>$w^T+b=0$</h2><h3 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h3><p>在超平面$w ^T x + b = 0$确定的情况下，$|w^Tx+b|$表示点距离超平面的距离，而超平面作为二分类器，如果$w^Tx+b&gt;0$， 判断类别y为1, 否则判定为-1。从而引出函数间隔的定义：</p><h2 id="r-y-w-Tx-b-yf-x"><a href="#r-y-w-Tx-b-yf-x" class="headerlink" title="$r=y(w^Tx+b)=yf(x)$"></a>$r=y(w^Tx+b)=yf(x)$</h2><p>其中y是训练数据的类标记值， 如果$y(w^T x + b) &gt;0$说明，预测的值和标记的值相同， 分类正确，而且值越大，说明点离平面越远，分类的可靠程度更高。这是对单个样本的函数定义， 对整个样本集来说，要找到所有样本中间隔值最小的作为整个集合的函数间隔：</p><h2 id="r-min-r-i-i-1-2-cdot-cdot-cdot-n"><a href="#r-min-r-i-i-1-2-cdot-cdot-cdot-n" class="headerlink" title="$r=min \   r_i  , i=1,2 \cdot \cdot \cdot n$"></a>$r=min \ r_i , i=1,2 \cdot \cdot \cdot n$</h2><p>即w和b同时缩小或放大M倍后，超平面并没有变化，但是函数间隔跟着w和b变化。所以，需要加入约束条件使得函数间隔固定, 也就是几何间隔。</p><h3 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h3><h4 id="样本空间-x-到超平面-x-0-的距离："><a href="#样本空间-x-到超平面-x-0-的距离：" class="headerlink" title="样本空间$x$到超平面$x_0$的距离："></a>样本空间$x$到超平面$x_0$的距离：</h4><h2 id="r-frac-w-Tx-b-w"><a href="#r-frac-w-Tx-b-w" class="headerlink" title="$r=\frac{|w^Tx+b|}{||w||}$"></a>$r=\frac{|w^Tx+b|}{||w||}$</h2><h4 id="如果超平面将样本成功分类，若-y-i-1-则有-w-Tx-b-gt-0-若-y-i-1-则有-w-Tx-b-lt-0-则下式成立"><a href="#如果超平面将样本成功分类，若-y-i-1-则有-w-Tx-b-gt-0-若-y-i-1-则有-w-Tx-b-lt-0-则下式成立" class="headerlink" title="如果超平面将样本成功分类，若$y_i=+1$,则有$w^Tx+b&gt;0$;若$y_i=-1$,则有$w^Tx+b&lt;0$;则下式成立"></a>如果超平面将样本成功分类，若$y_i=+1$,则有$w^Tx+b&gt;0$;若$y_i=-1$,则有$w^Tx+b&lt;0$;则下式成立</h4><h2 id="left-lbrace-begin-aligned-w-Tx-b-ge-1-amp-amp-y-i-1-w-Tx-b-le-1-amp-amp-y-i-1-end-aligned-right"><a href="#left-lbrace-begin-aligned-w-Tx-b-ge-1-amp-amp-y-i-1-w-Tx-b-le-1-amp-amp-y-i-1-end-aligned-right" class="headerlink" title="$ \left \lbrace \begin{aligned} w^Tx+b \ge+1&amp;&amp; y_i=+1 \\ w^Tx+b\le-1 &amp;&amp; y_i=-1 \end{aligned} \right.$"></a>$ \left \lbrace \begin{aligned} w^Tx+b \ge+1&amp;&amp; y_i=+1 \\ w^Tx+b\le-1 &amp;&amp; y_i=-1 \end{aligned} \right.$</h2><p><img src="/images/ml/21.png" alt="images"></p><h4 id="从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support-vector）"><a href="#从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support-vector）" class="headerlink" title="从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support vector）"></a>从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为<strong>“支持向量”</strong>（support vector）</h4><h4 id="两个异类支持向量到超平面的距离之和（被称为间隔）："><a href="#两个异类支持向量到超平面的距离之和（被称为间隔）：" class="headerlink" title="两个异类支持向量到超平面的距离之和（被称为间隔）："></a>两个异类支持向量到超平面的距离之和（被称为间隔）：</h4><h2 id="r-frac-2-w"><a href="#r-frac-2-w" class="headerlink" title="$r=\frac{2}{||w||}$"></a>$r=\frac{2}{||w||}$</h2><h3 id="”最大间隔“的超平面"><a href="#”最大间隔“的超平面" class="headerlink" title="”最大间隔“的超平面"></a>”最大间隔“的超平面</h3><h4 id="我们要找的”最大间隔“的超平面，即："><a href="#我们要找的”最大间隔“的超平面，即：" class="headerlink" title="我们要找的”最大间隔“的超平面，即："></a>我们要找的”最大间隔“的超平面，即：</h4><h2 id="max-w-b-frac-2-w"><a href="#max-w-b-frac-2-w" class="headerlink" title="$max_{w,b}\frac{2}{||w||}$"></a>$max_{w,b}\frac{2}{||w||}$</h2><h2 id="s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m"><a href="#s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m" class="headerlink" title="$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h2><h3 id="SVM的二次凸函数和约束条件"><a href="#SVM的二次凸函数和约束条件" class="headerlink" title="SVM的二次凸函数和约束条件"></a>SVM的二次凸函数和约束条件</h3><p>最大间隔分类器的求解， 可以转换为上面的一个最优化问题， 即在满足约束条件：</p><h2 id="y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m"><a href="#y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m" class="headerlink" title="$y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h2><h4 id="求出就最大的-frac1-w-。"><a href="#求出就最大的-frac1-w-。" class="headerlink" title="求出就最大的$\frac1{||w||}$。"></a>求出就最大的$\frac1{||w||}$。</h4><h4 id="为更好的利用现有的理论和计算方法，-可以将求解-frac1-w-最大值，-转换为一个二次凸函数优化问题：求解-min-w-b-frac-1-2-w-2-，-两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。"><a href="#为更好的利用现有的理论和计算方法，-可以将求解-frac1-w-最大值，-转换为一个二次凸函数优化问题：求解-min-w-b-frac-1-2-w-2-，-两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。" class="headerlink" title="为更好的利用现有的理论和计算方法， 可以将求解$\frac1{||w||}$最大值， 转换为一个二次凸函数优化问题：求解 $min_{w,b}\frac{1}{2}{||w||}^2$， 两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。"></a>为更好的利用现有的理论和计算方法， 可以将求解$\frac1{||w||}$最大值， 转换为一个二次凸函数优化问题：求解 $min_{w,b}\frac{1}{2}{||w||}^2$， 两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。</h4><h3 id="SVM的基本型"><a href="#SVM的基本型" class="headerlink" title="SVM的基本型"></a>SVM的基本型</h3><h4 id="我们前面已经知道，最大化-w-1-等价于最小化-w-2-所以："><a href="#我们前面已经知道，最大化-w-1-等价于最小化-w-2-所以：" class="headerlink" title="我们前面已经知道，最大化$||w||^{-1}$等价于最小化$||w||^2$,所以："></a>我们前面已经知道，最大化$||w||^{-1}$等价于最小化$||w||^2$,所以：</h4><h2 id="min-w-b-frac-1-2-w-2"><a href="#min-w-b-frac-1-2-w-2" class="headerlink" title="$min_{w,b}\frac{1}{2}{||w||}^2$"></a>$min_{w,b}\frac{1}{2}{||w||}^2$</h2><h2 id="s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m-1"><a href="#s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m-1" class="headerlink" title="$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h2><h3 id="拉格朗日构建方程"><a href="#拉格朗日构建方程" class="headerlink" title="拉格朗日构建方程"></a>拉格朗日构建方程</h3><p>由于这个问题的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality）变换到对偶变量(dual variable)的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法.</p><h4 id="这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，"><a href="#这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，" class="headerlink" title="这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，"></a>这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，</h4><p>进而推广到非线性分类问题。 具体来说就是对svm基本型的每条约束添加拉格朗日乘子$a_i\ge0$,则该问题的拉格朗日函数可写为：</p><h4 id="L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b-a-i-a-1-a-2-cdot-cdot-cdot-a-m"><a href="#L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b-a-i-a-1-a-2-cdot-cdot-cdot-a-m" class="headerlink" title="$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$                   $a_i=(a_1;a_2; \cdot \cdot \cdot a_m)$."></a>$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$ $a_i=(a_1;a_2; \cdot \cdot \cdot a_m)$.</h4><p>我们的目标是让拉格朗如函数$ L(ω,b,α)$ 针对 $α$ 达到最大值。为什么能够这么写呢，我们可以这样想，哪怕有一个 $y_i(ω^Tx_i+b)⩾1$不满足，只要让对应的 $α_i$ 是正无穷就好了。所以，如果$L(ω,b,α)$有有限的最大值，那么那些不等式条件是自然满足的。 之后，我们再让 $L(ω,b,α)$ 针对 $ω,b$ 达到最小值，就可以了。 从而，我们的目标函数变成：</p><h4 id="原问题是极小极大的问题"><a href="#原问题是极小极大的问题" class="headerlink" title="原问题是极小极大的问题"></a>原问题是极小极大的问题</h4><h2 id="min-w-b-max-aL-w-b-a-p"><a href="#min-w-b-max-aL-w-b-a-p" class="headerlink" title="$min_{w,b}max_aL(w,b,a)=p^*$"></a>$min_{w,b}max_aL(w,b,a)=p^*$</h2><h4 id="原始问题的对偶问题，是极大极小问题"><a href="#原始问题的对偶问题，是极大极小问题" class="headerlink" title="原始问题的对偶问题，是极大极小问题"></a>原始问题的对偶问题，是极大极小问题</h4><h2 id="max-amin-w-b-L-w-b-a-b"><a href="#max-amin-w-b-L-w-b-a-b" class="headerlink" title="$max_amin_{w,b}L(w,b,a)=b^*$"></a>$max_amin_{w,b}L(w,b,a)=b^*$</h2><p>交换以后的新问题是原始问题的对偶问题，这个新问题的最优值用$d^*$来表示。而且有$d^∗⩽p^∗$，在满足某些条件的情况下，这两者相等，这个时候就可以通过求解对偶问题来间接地求解原始问题。</p><p>这所谓的“满足某些条件”就是要满足KKT条件。</p><h2 id="left-lbrace-begin-aligned-a-i-ge0-y-i-w-Tx-i-b-1-ge-0-a-i-y-i-w-Tx-i-b-1-0-end-aligned-right"><a href="#left-lbrace-begin-aligned-a-i-ge0-y-i-w-Tx-i-b-1-ge-0-a-i-y-i-w-Tx-i-b-1-0-end-aligned-right" class="headerlink" title="$ \left\lbrace\ \begin{aligned} a_i \ge0 \\ y_i(w^Tx_i+b) -1\ge 0 \\ a_i(y_i(w^Tx_i+b)-1)=0 \end{aligned} \right. $"></a>$ \left\lbrace\ \begin{aligned} a_i \ge0 \\ y_i(w^Tx_i+b) -1\ge 0 \\ a_i(y_i(w^Tx_i+b)-1)=0 \end{aligned} \right. $</h2><h3 id="KKT条件的意义"><a href="#KKT条件的意义" class="headerlink" title="KKT条件的意义"></a>KKT条件的意义</h3><h5 id="一般地，一个最优化数学模型能够表示成下列标准形式："><a href="#一般地，一个最优化数学模型能够表示成下列标准形式：" class="headerlink" title="一般地，一个最优化数学模型能够表示成下列标准形式："></a>一般地，一个最优化数学模型能够表示成下列标准形式：</h5><h2 id="min-f-x"><a href="#min-f-x" class="headerlink" title="$min.f(x)$"></a>$min.f(x)$</h2><h2 id="s-t-h-j-x-0-j-1-cdot-cdot-cdot-n"><a href="#s-t-h-j-x-0-j-1-cdot-cdot-cdot-n" class="headerlink" title="$s.t.  h_j(x)=0,j=1,\cdot \cdot \cdot n$"></a>$s.t. h_j(x)=0,j=1,\cdot \cdot \cdot n$</h2><h2 id="g-k-x-le0-k-1-cdot-cdot-cdot-m"><a href="#g-k-x-le0-k-1-cdot-cdot-cdot-m" class="headerlink" title="$g_k(x)\le0,k=1,\cdot \cdot \cdot m$"></a>$g_k(x)\le0,k=1,\cdot \cdot \cdot m$</h2><h2 id="x-in-X-subset-R-n"><a href="#x-in-X-subset-R-n" class="headerlink" title="$x\in X \subset R^n$"></a>$x\in X \subset R^n$</h2><h5 id="其中，f-x-是需要最小化的函数，h-x-是等式约束，g-x-是不等式约束，p和q分别为等式约束和不等式约束的数量。"><a href="#其中，f-x-是需要最小化的函数，h-x-是等式约束，g-x-是不等式约束，p和q分别为等式约束和不等式约束的数量。" class="headerlink" title="其中，f(x)是需要最小化的函数，h(x)是等式约束，g(x)是不等式约束，p和q分别为等式约束和不等式约束的数量。"></a>其中，f(x)是需要最小化的函数，h(x)是等式约束，g(x)是不等式约束，p和q分别为等式约束和不等式约束的数量。</h5><ul><li><h4 id="凸优化的概念：-x-subset-R-n-为一凸集，-f-x-to-R-为一凸函数，凸优化就是要找出一点-x-in-X-使得每一-x-in-X-满足-f-x-le-f-x"><a href="#凸优化的概念：-x-subset-R-n-为一凸集，-f-x-to-R-为一凸函数，凸优化就是要找出一点-x-in-X-使得每一-x-in-X-满足-f-x-le-f-x" class="headerlink" title="凸优化的概念：$x\subset R^n$为一凸集，$f:x\to R$为一凸函数，凸优化就是要找出一点$x^\in X$,使得每一$x\in X$满足$f(x^)\le f(x)$"></a>凸优化的概念：$x\subset R^n$为一凸集，$f:x\to R$为一凸函数，凸优化就是要找出一点$x^<em>\in X$,使得每一$x\in X$满足$f(x^</em>)\le f(x)$</h4></li><li><h4 id="KKT条件的意义：它是一个非线性规划（Nonlinear-Programming）问题能有最优化解法的必要和充分条件。"><a href="#KKT条件的意义：它是一个非线性规划（Nonlinear-Programming）问题能有最优化解法的必要和充分条件。" class="headerlink" title="KKT条件的意义：它是一个非线性规划（Nonlinear Programming）问题能有最优化解法的必要和充分条件。"></a>KKT条件的意义：它是一个非线性规划（Nonlinear Programming）问题能有最优化解法的必要和充分条件。</h4></li></ul><p>而KKT条件就是指上面最优化数学模型的标准形式中的最小点$ x*$ 必须满足下面的条件：</p><p><img src="/images/ml/22.png" alt="images"></p><p>经过论证，我们这里的问题是满足KKT条件的（首先已经满足Slater condition，再者$f(x)$和$g(x)$也都是可微的，即$L$对$w$和$b$都可导），因此现在我们便转化为求解第二个问题。</p><p>也就是说，原始问题通过满足KKT条件，已经转化成了对偶问题。而求解这个对偶学习问题，分为3个步骤：首先要让$L(w，b，a)$关于$w$和$b$最小化，然后求对$a$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。</p><h3 id="对偶问题求解"><a href="#对偶问题求解" class="headerlink" title="对偶问题求解"></a>对偶问题求解</h3><h4 id="首先固定-a-先求出-min-w-b-L-w-b-a-所以分别对-w-b-进行求偏导并令其等于0。"><a href="#首先固定-a-先求出-min-w-b-L-w-b-a-所以分别对-w-b-进行求偏导并令其等于0。" class="headerlink" title="首先固定$a$,先求出$min_{w,b}L(w,b,a)$,所以分别对$w,b$进行求偏导并令其等于0。"></a>首先固定$a$,先求出$min_{w,b}L(w,b,a)$,所以分别对$w,b$进行求偏导并令其等于0。</h4><h2 id="frac-partial-L-w-b-a-partial-w-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-w-0"><a href="#frac-partial-L-w-b-a-partial-w-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-w-0" class="headerlink" title="$\frac{\partial L(w,b,a)}{\partial w}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial w}  =0$"></a>$\frac{\partial L(w,b,a)}{\partial w}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial w} =0$</h2><h2 id="frac-partial-L-w-b-a-partial-b-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-b-0"><a href="#frac-partial-L-w-b-a-partial-b-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-b-0" class="headerlink" title="$\frac{\partial L(w,b,a)}{\partial b}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial b}  =0$"></a>$\frac{\partial L(w,b,a)}{\partial b}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial b} =0$</h2><h2 id="w-sum-i-1-na-ix-iy-i"><a href="#w-sum-i-1-na-ix-iy-i" class="headerlink" title="$w=\sum_{i=1}^na_ix_iy_i$"></a>$w=\sum_{i=1}^na_ix_iy_i$</h2><h2 id="sum-i-1-na-iy-i-0"><a href="#sum-i-1-na-iy-i-0" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h2><p>然后我们将以上结果带入原式<strong>$L(w,b,a)$</strong>:</p><h2 id="L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b"><a href="#L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b" class="headerlink" title="$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$"></a>$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$</h2><h2 id="frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx"><a href="#frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx" class="headerlink" title="$=\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx$"></a>$=\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx$</h2><h4 id="导入-w-sum-i-1-na-ix-iy-i"><a href="#导入-w-sum-i-1-na-ix-iy-i" class="headerlink" title="导入$w=\sum_{i=1}^na_ix_iy_i$:"></a><strong>导入$w=\sum_{i=1}^na_ix_iy_i$:</strong></h4><h2 id="frac12w-T-sum-i-1-na-ix-iy-i-sum-i-1-na-i-sum-i-1-n-a-iby-i-w-T-sum-i-1-n-a-iy-ix"><a href="#frac12w-T-sum-i-1-na-ix-iy-i-sum-i-1-na-i-sum-i-1-n-a-iby-i-w-T-sum-i-1-n-a-iy-ix" class="headerlink" title="$=\frac12w^T(\sum_{i=1}^na_ix_iy_i)+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-w^T(\sum_{i=1}^n a_iy_ix)$"></a>$=\frac12w^T(\sum_{i=1}^na_ix_iy_i)+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-w^T(\sum_{i=1}^n a_iy_ix)$</h2><h4 id="导入-sum-i-1-na-iy-i-0"><a href="#导入-sum-i-1-na-iy-i-0" class="headerlink" title="导入$\sum_{i=1}^na_iy_i=0$:"></a><strong>导入$\sum_{i=1}^na_iy_i=0$:</strong></h4><h2 id="sum-i-1-na-i-frac12w-T-sum-i-1-na-ix-iy-i"><a href="#sum-i-1-na-i-frac12w-T-sum-i-1-na-ix-iy-i" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12w^T(\sum_{i=1}^na_ix_iy_i)$"></a>$=\sum_{i=1}^na_i-\frac12w^T(\sum_{i=1}^na_ix_iy_i)$</h2><h2 id="sum-i-1-na-i-frac12-sum-i-1-na-ix-iy-i-T-sum-i-1-na-ix-iy-i"><a href="#sum-i-1-na-i-frac12-sum-i-1-na-ix-iy-i-T-sum-i-1-na-ix-iy-i" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12{(\sum_{i=1}^na_ix_iy_i)}^T(\sum_{i=1}^na_ix_iy_i)$"></a>$=\sum_{i=1}^na_i-\frac12{(\sum_{i=1}^na_ix_iy_i)}^T(\sum_{i=1}^na_ix_iy_i)$</h2><h2 id="sum-i-1-na-i-frac12-sum-i-1-na-i-x-i-Ty-i-sum-i-1-na-ix-iy-i"><a href="#sum-i-1-na-i-frac12-sum-i-1-na-i-x-i-Ty-i-sum-i-1-na-ix-iy-i" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12(\sum_{i=1}^na_i{x_i}^Ty_i)(\sum_{i=1}^na_ix_iy_i)$"></a>$=\sum_{i=1}^na_i-\frac12(\sum_{i=1}^na_i{x_i}^Ty_i)(\sum_{i=1}^na_ix_iy_i)$</h2><h2 id="sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><a href="#sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$=\sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h2><p>从上面的最后一个式子，我们可以看出，此时的拉格朗日函数只包含了一个变量，那就是$a_i$(求出了$a_i$便能求出$w,b$,然后我们的分类函数$f(x)=w^Tx+b$就非常容易的求出来了)。</p><h4 id="然后求对-a-的极大值："><a href="#然后求对-a-的极大值：" class="headerlink" title="然后求对$a$的极大值："></a>然后求对$a$的极大值：</h4><p>即是关于对偶问题的最优化问题。经过上面第一个步骤的求w和b，得到的拉格朗日函数式子已经没有了变量w，b，只有从上面的式子得到：</p><h2 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h2><h2 id="s-t-a-i-ge0-i-1-cdot-cdot-cdot-n"><a href="#s-t-a-i-ge0-i-1-cdot-cdot-cdot-n" class="headerlink" title="$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$</h2><h2 id="sum-i-1-na-iy-i-0-1"><a href="#sum-i-1-na-iy-i-0-1" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h2><p>我们一般用SMO算法来求解$a$</p><h3 id="SMO优化算法"><a href="#SMO优化算法" class="headerlink" title="SMO优化算法"></a>SMO优化算法</h3><p>SMO算法由Microsoft Research的John C. Platt在1998年提出，并成为最快的二次规划优化算法，特别针对线性SVM和数据稀疏时性能更优。</p><p>SMO的基本思路是先固定$a_i$之外的所有参数，然后求$a_i$上的极值。由于存在约束$\sum_{i=1}^na_iy_i=0$，若固定$a_i$之外的其他变量，则$a_i$可由其他变量导出。于是，SMO每次选择两个变量$a_i和a_j$,并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p><ul><li><h4 id="选取一对需更新的变量-a-i和-a-j"><a href="#选取一对需更新的变量-a-i和-a-j" class="headerlink" title="选取一对需更新的变量$a_i和 a_j$."></a>选取一对需更新的变量$a_i和 a_j$.</h4></li><li><h4 id="固定-a-i和-a-j-以外的参数，求解-max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-获得更新后的-a-i和-a-j"><a href="#固定-a-i和-a-j-以外的参数，求解-max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-获得更新后的-a-i和-a-j" class="headerlink" title="固定$a_i和 a_j$以外的参数，求解$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$,获得更新后的$a_i和 a_j$"></a>固定$a_i和 a_j$以外的参数，求解$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$,获得更新后的$a_i和 a_j$</h4></li></ul><h4 id="那如何做才能做到不断收敛呢？"><a href="#那如何做才能做到不断收敛呢？" class="headerlink" title="那如何做才能做到不断收敛呢？"></a>那如何做才能做到不断收敛呢？</h4><p>注意只需选取的$a_i和 a_j$ 中有一个不满足KKT的条件，目标函数就会在不断迭代后减小。直观来说<strong>KKT条件的违背的程度越大，则变量更新后可能导致的目标函数值减幅越大</strong>，</p><h4 id="那如何选取变量呢？"><a href="#那如何选取变量呢？" class="headerlink" title="那如何选取变量呢？"></a>那如何选取变量呢？</h4><p>第一个变量SMO 先选取违背KKT条件程度最大的变量。</p><p>第二个变量应该选择一个使目标函数值减小最快的变量。</p><p><strong>但是</strong>由于比较各变量所对应的目标函数值减幅的复杂度过高，因此SMO就采用了一个启发式：<strong>使选取的变量所对应的样本之间的间隔最大</strong>。</p><p><strong>总结</strong>：这样选取的两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。SMO之所以高效，就是在于固定其他参数后，只优化两个参数的过程能做到非常高效。</p><h4 id="所以：只考虑-a-i和-a-j-时，约束条件就改变为："><a href="#所以：只考虑-a-i和-a-j-时，约束条件就改变为：" class="headerlink" title="所以：只考虑$a_i和 a_j$时，约束条件就改变为："></a>所以：只考虑$a_i和 a_j$时，约束条件就改变为：</h4><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-1"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-1" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h4><h4 id="s-t-a-iy-i-a-jy-j-c-a-i-ge-0-a-j-ge-0"><a href="#s-t-a-iy-i-a-jy-j-c-a-i-ge-0-a-j-ge-0" class="headerlink" title="$s.t. ,a_iy_i+a_jy_j=c, a_i \ge 0, a_j \ge 0$"></a>$s.t. ,a_iy_i+a_jy_j=c, a_i \ge 0, a_j \ge 0$</h4><h4 id="sum-i-1-na-iy-i-0-2"><a href="#sum-i-1-na-iy-i-0-2" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><h4 id="其中：-c-sum-k-ne-i-j-a-ky-k-是使-sum-i-1-na-iy-i-0-成立的常数。"><a href="#其中：-c-sum-k-ne-i-j-a-ky-k-是使-sum-i-1-na-iy-i-0-成立的常数。" class="headerlink" title="其中：$c= -\sum_{k \ne i,j}a_ky_k$是使$\sum_{i=1}^na_iy_i=0$ 成立的常数。"></a>其中：$c= -\sum_{k \ne i,j}a_ky_k$是使$\sum_{i=1}^na_iy_i=0$ 成立的常数。</h4><h4 id="然后用-a-iy-i-a-jy-j-c-消去变量-a-j-，就可以得到一个关于-a-j-的单变量的二次规划问题，通过约束条件-a-i-ge0-就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的-a-i-和-a-j-。"><a href="#然后用-a-iy-i-a-jy-j-c-消去变量-a-j-，就可以得到一个关于-a-j-的单变量的二次规划问题，通过约束条件-a-i-ge0-就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的-a-i-和-a-j-。" class="headerlink" title="然后用$a_iy_i+a_jy_j=c$ 消去变量$a_j$ ，就可以得到一个关于$a_j$ 的单变量的二次规划问题，通过约束条件$a_i \ge0$,就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的$a_i 和 a_j$。"></a>然后用$a_iy_i+a_jy_j=c$ 消去变量$a_j$ ，就可以得到一个关于$a_j$ 的单变量的二次规划问题，通过约束条件$a_i \ge0$,就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的$a_i 和 a_j$。</h4><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><h4 id="用SMO-解出-a-后，我们前面以前算出-w-的值，所以我们还需要在前面的基础上算出b的值"><a href="#用SMO-解出-a-后，我们前面以前算出-w-的值，所以我们还需要在前面的基础上算出b的值" class="headerlink" title="用SMO 解出$a$后，我们前面以前算出$w$的值，所以我们还需要在前面的基础上算出b的值"></a>用SMO 解出$a$后，我们前面以前算出$w$的值，所以我们还需要在前面的基础上算出b的值</h4><h2 id="w-sum-i-1-na-ix-iy-i-1"><a href="#w-sum-i-1-na-ix-iy-i-1" class="headerlink" title="$w=\sum_{i=1}^na_ix_iy_i$"></a>$w=\sum_{i=1}^na_ix_iy_i$</h2><h2 id="b-y-j-sum-i-1-n-a-iy-i-x-i-Tx-j"><a href="#b-y-j-sum-i-1-n-a-iy-i-x-i-Tx-j" class="headerlink" title="$b=y_j-\sum_{i=1}^n a_iy_i{x_i}^Tx_j$"></a>$b=y_j-\sum_{i=1}^n a_iy_i{x_i}^Tx_j$</h2><h4 id="所以就可以得到模型"><a href="#所以就可以得到模型" class="headerlink" title="所以就可以得到模型"></a>所以就可以得到模型</h4><h2 id="f-x-w-Tx-b-sum-i-1-n-a-iy-i-x-i-Tx-b"><a href="#f-x-w-Tx-b-sum-i-1-n-a-iy-i-x-i-Tx-b" class="headerlink" title="$f(x)=w^Tx+b=\sum_{i=1}^n a_iy_i{x_i}^Tx+b$"></a>$f(x)=w^Tx+b=\sum_{i=1}^n a_iy_i{x_i}^Tx+b$</h2><h3 id="实战一下"><a href="#实战一下" class="headerlink" title="实战一下"></a>实战一下</h3><p><img src="/images/ml/58.png" alt="image"></p><p><img src="/images/ml/53.png" alt="image"></p><p><img src="/images/ml/54.png" alt="image"></p><p><img src="/images/ml/55.png" alt="image"></p><p><img src="/images/ml/56.png" alt="image"></p><p><img src="/images/ml/57.png" alt="image"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/ml-svm/" target="_blank" rel="noopener">点击阅读原文</a></p></div><div><div class="my_post_copyright"><script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script><script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script><script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script><link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css"><p><span>本文标题:</span>机器学习-支持向量机SVM</p><p><span>文章作者:</span>Seven</p><p><span>发布时间:</span>2018年07月25日 - 00:00:00</p><p><span>最后更新:</span>2018年12月11日 - 21:59:12</p><p><span>原始链接:</span><a href="/2018/07/25/2018-07-25-ml-svm/" title="机器学习-支持向量机SVM">http://yoursite.com/2018/07/25/2018-07-25-ml-svm/</a> <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://yoursite.com/2018/07/25/2018-07-25-ml-svm/" aria-label="复制成功！"></i></span></p><p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p></div><script>var clipboard=new Clipboard(".fa-clipboard");clipboard.on("success",$(function(){$(".fa-clipboard").click(function(){swal({title:"",text:"复制成功",html:!1,timer:500,showConfirmButton:!1})})}))</script></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>打赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/wechatpay.jpg" alt="Seven 微信支付"><p>微信支付</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="/images/alipay.jpg" alt="Seven 支付宝"><p>支付宝</p></div></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong> Seven</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="http://yoursite.com/2018/07/25/2018-07-25-ml-svm/" title="机器学习-支持向量机SVM">http://yoursite.com/2018/07/25/2018-07-25-ml-svm/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/svm/" rel="tag">✐ svm</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/07/24/2018-07-24-arithmetic-gradientDescent/" rel="next" title="优化算法-梯度下降"><i class="fa fa-chevron-left"></i> 优化算法-梯度下降</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/07/26/2018-07-26-ml-svm-kernel/" rel="prev" title="机器学习-SVM-核函数">机器学习-SVM-核函数 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Seven"><p class="site-author-name" itemprop="name">Seven</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">103</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">12</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">42</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div id="days"></div><script>function show_date_time(){window.setTimeout("show_date_time()",1e3),BirthDay=new Date("12/12/2018 15:13:14"),today=new Date,timeold=today.getTime()-BirthDay.getTime(),sectimeold=timeold/1e3,secondsold=Math.floor(sectimeold),msPerDay=864e5,e_daysold=timeold/msPerDay,daysold=Math.floor(e_daysold),e_hrsold=24*(e_daysold-daysold),hrsold=setzero(Math.floor(e_hrsold)),e_minsold=60*(e_hrsold-hrsold),minsold=setzero(Math.floor(60*(e_hrsold-hrsold))),seconds=setzero(Math.floor(60*(e_minsold-minsold))),document.getElementById("days").innerHTML="已运行 "+daysold+" 天 "+hrsold+" 小时 "+minsold+" 分 "+seconds+" 秒"}function setzero(e){return 10>e&&(e="0"+e),e}show_date_time()</script></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量机SVM的概念及起源"><span class="nav-number">1.</span> <span class="nav-text">支持向量机SVM的概念及起源</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是支持向量机SVM"><span class="nav-number">1.1.</span> <span class="nav-text">什么是支持向量机SVM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类标准的起源：Logistic回归"><span class="nav-number">1.2.</span> <span class="nav-text">分类标准的起源：Logistic回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#我们先看看什么是线性分类器"><span class="nav-number">1.2.1.</span> <span class="nav-text">我们先看看什么是线性分类器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#w-Tx-b-0"><span class="nav-number">2.</span> <span class="nav-text">$w^Tx+b=0$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx"><span class="nav-number">3.</span> <span class="nav-text">$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#同样-g-z-frac-1-1-e-z-的图像："><span class="nav-number">3.0.1.</span> <span class="nav-text">同样$g(z)=\frac{1}{1+e^{-z}}$的图像：</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x"><span class="nav-number">4.</span> <span class="nav-text">$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y-0和y-1替换为-y-1-y-1-然后把-theta-0-替换成b，把-w-替换-theta-i-所以就变换为："><span class="nav-number">4.0.1.</span> <span class="nav-text">然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y=0和y=1替换为$y=-1,y=1 $,然后把$\theta_0$替换成b，把$w$替换$\theta_i$,所以就变换为：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#h-theta-x-g-w-Tx-b"><span class="nav-number">5.</span> <span class="nav-text">$h_\theta(x)=g(w^Tx+b)$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#所以我们就可以假设函数：-h-w-b-x-g-w-Tx-b-中的g-z-做一个简化，将其简单映射到y-1和y-1上。映射关系如下："><span class="nav-number">5.0.1.</span> <span class="nav-text">所以我们就可以假设函数：$h_{w,b}(x)=g(w^Tx+b)$,中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#g-z-left-lbrace-begin-aligned-1-amp-amp-z-ge0-1-amp-amp-z-lt-0-end-aligned-right"><span class="nav-number">6.</span> <span class="nav-text">$ g(z)=\left\lbrace \begin{aligned} 1 &amp;&amp; z \ge0 \\ -1 &amp;&amp; z&lt;0 \end{aligned} \right. $</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#然后我们举个线性分类的例子来看看"><span class="nav-number">6.0.1.</span> <span class="nav-text">然后我们举个线性分类的例子来看看</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#间隔与支持向量"><span class="nav-number">6.1.</span> <span class="nav-text">间隔与支持向量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#上图中-x-0-是-x-在超平面上的投影，-ω-是超平面的法向量"><span class="nav-number">6.1.1.</span> <span class="nav-text">上图中 $x_0$ 是 $x $在超平面上的投影，$ω$是超平面的法向量.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#超平面用线性方程来描述："><span class="nav-number">6.1.2.</span> <span class="nav-text">超平面用线性方程来描述：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#w-T-b-0"><span class="nav-number">7.</span> <span class="nav-text">$w^T+b=0$</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#函数间隔"><span class="nav-number">7.1.</span> <span class="nav-text">函数间隔</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#r-y-w-Tx-b-yf-x"><span class="nav-number">8.</span> <span class="nav-text">$r=y(w^Tx+b)=yf(x)$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#r-min-r-i-i-1-2-cdot-cdot-cdot-n"><span class="nav-number">9.</span> <span class="nav-text">$r=min \ r_i , i=1,2 \cdot \cdot \cdot n$</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#几何间隔"><span class="nav-number">9.1.</span> <span class="nav-text">几何间隔</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#样本空间-x-到超平面-x-0-的距离："><span class="nav-number">9.1.1.</span> <span class="nav-text">样本空间$x$到超平面$x_0$的距离：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#r-frac-w-Tx-b-w"><span class="nav-number">10.</span> <span class="nav-text">$r=\frac{|w^Tx+b|}{||w||}$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#如果超平面将样本成功分类，若-y-i-1-则有-w-Tx-b-gt-0-若-y-i-1-则有-w-Tx-b-lt-0-则下式成立"><span class="nav-number">10.0.1.</span> <span class="nav-text">如果超平面将样本成功分类，若$y_i=+1$,则有$w^Tx+b&gt;0$;若$y_i=-1$,则有$w^Tx+b&lt;0$;则下式成立</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#left-lbrace-begin-aligned-w-Tx-b-ge-1-amp-amp-y-i-1-w-Tx-b-le-1-amp-amp-y-i-1-end-aligned-right"><span class="nav-number">11.</span> <span class="nav-text">$ \left \lbrace \begin{aligned} w^Tx+b \ge+1&amp;&amp; y_i=+1 \\ w^Tx+b\le-1 &amp;&amp; y_i=-1 \end{aligned} \right.$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support-vector）"><span class="nav-number">11.0.1.</span> <span class="nav-text">从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support vector）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#两个异类支持向量到超平面的距离之和（被称为间隔）："><span class="nav-number">11.0.2.</span> <span class="nav-text">两个异类支持向量到超平面的距离之和（被称为间隔）：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#r-frac-2-w"><span class="nav-number">12.</span> <span class="nav-text">$r=\frac{2}{||w||}$</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#”最大间隔“的超平面"><span class="nav-number">12.1.</span> <span class="nav-text">”最大间隔“的超平面</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#我们要找的”最大间隔“的超平面，即："><span class="nav-number">12.1.1.</span> <span class="nav-text">我们要找的”最大间隔“的超平面，即：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#max-w-b-frac-2-w"><span class="nav-number">13.</span> <span class="nav-text">$max_{w,b}\frac{2}{||w||}$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m"><span class="nav-number">14.</span> <span class="nav-text">$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM的二次凸函数和约束条件"><span class="nav-number">14.1.</span> <span class="nav-text">SVM的二次凸函数和约束条件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m"><span class="nav-number">15.</span> <span class="nav-text">$y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#求出就最大的-frac1-w-。"><span class="nav-number">15.0.1.</span> <span class="nav-text">求出就最大的$\frac1{||w||}$。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为更好的利用现有的理论和计算方法，-可以将求解-frac1-w-最大值，-转换为一个二次凸函数优化问题：求解-min-w-b-frac-1-2-w-2-，-两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。"><span class="nav-number">15.0.2.</span> <span class="nav-text">为更好的利用现有的理论和计算方法， 可以将求解$\frac1{||w||}$最大值， 转换为一个二次凸函数优化问题：求解 $min_{w,b}\frac{1}{2}{||w||}^2$， 两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM的基本型"><span class="nav-number">15.1.</span> <span class="nav-text">SVM的基本型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#我们前面已经知道，最大化-w-1-等价于最小化-w-2-所以："><span class="nav-number">15.1.1.</span> <span class="nav-text">我们前面已经知道，最大化$||w||^{-1}$等价于最小化$||w||^2$,所以：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#min-w-b-frac-1-2-w-2"><span class="nav-number">16.</span> <span class="nav-text">$min_{w,b}\frac{1}{2}{||w||}^2$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m-1"><span class="nav-number">17.</span> <span class="nav-text">$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#拉格朗日构建方程"><span class="nav-number">17.1.</span> <span class="nav-text">拉格朗日构建方程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，"><span class="nav-number">17.1.1.</span> <span class="nav-text">这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b-a-i-a-1-a-2-cdot-cdot-cdot-a-m"><span class="nav-number">17.1.2.</span> <span class="nav-text">$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$ $a_i=(a_1;a_2; \cdot \cdot \cdot a_m)$.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#原问题是极小极大的问题"><span class="nav-number">17.1.3.</span> <span class="nav-text">原问题是极小极大的问题</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#min-w-b-max-aL-w-b-a-p"><span class="nav-number">18.</span> <span class="nav-text">$min_{w,b}max_aL(w,b,a)=p^*$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#原始问题的对偶问题，是极大极小问题"><span class="nav-number">18.0.1.</span> <span class="nav-text">原始问题的对偶问题，是极大极小问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#max-amin-w-b-L-w-b-a-b"><span class="nav-number">19.</span> <span class="nav-text">$max_amin_{w,b}L(w,b,a)=b^*$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#left-lbrace-begin-aligned-a-i-ge0-y-i-w-Tx-i-b-1-ge-0-a-i-y-i-w-Tx-i-b-1-0-end-aligned-right"><span class="nav-number">20.</span> <span class="nav-text">$ \left\lbrace\ \begin{aligned} a_i \ge0 \\ y_i(w^Tx_i+b) -1\ge 0 \\ a_i(y_i(w^Tx_i+b)-1)=0 \end{aligned} \right. $</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#KKT条件的意义"><span class="nav-number">20.1.</span> <span class="nav-text">KKT条件的意义</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#一般地，一个最优化数学模型能够表示成下列标准形式："><span class="nav-number">20.1.0.1.</span> <span class="nav-text">一般地，一个最优化数学模型能够表示成下列标准形式：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#min-f-x"><span class="nav-number">21.</span> <span class="nav-text">$min.f(x)$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#s-t-h-j-x-0-j-1-cdot-cdot-cdot-n"><span class="nav-number">22.</span> <span class="nav-text">$s.t. h_j(x)=0,j=1,\cdot \cdot \cdot n$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#g-k-x-le0-k-1-cdot-cdot-cdot-m"><span class="nav-number">23.</span> <span class="nav-text">$g_k(x)\le0,k=1,\cdot \cdot \cdot m$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#x-in-X-subset-R-n"><span class="nav-number">24.</span> <span class="nav-text">$x\in X \subset R^n$</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#其中，f-x-是需要最小化的函数，h-x-是等式约束，g-x-是不等式约束，p和q分别为等式约束和不等式约束的数量。"><span class="nav-number">24.0.0.1.</span> <span class="nav-text">其中，f(x)是需要最小化的函数，h(x)是等式约束，g(x)是不等式约束，p和q分别为等式约束和不等式约束的数量。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#凸优化的概念：-x-subset-R-n-为一凸集，-f-x-to-R-为一凸函数，凸优化就是要找出一点-x-in-X-使得每一-x-in-X-满足-f-x-le-f-x"><span class="nav-number">24.0.1.</span> <span class="nav-text">凸优化的概念：$x\subset R^n$为一凸集，$f:x\to R$为一凸函数，凸优化就是要找出一点$x^\in X$,使得每一$x\in X$满足$f(x^)\le f(x)$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KKT条件的意义：它是一个非线性规划（Nonlinear-Programming）问题能有最优化解法的必要和充分条件。"><span class="nav-number">24.0.2.</span> <span class="nav-text">KKT条件的意义：它是一个非线性规划（Nonlinear Programming）问题能有最优化解法的必要和充分条件。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对偶问题求解"><span class="nav-number">24.1.</span> <span class="nav-text">对偶问题求解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#首先固定-a-先求出-min-w-b-L-w-b-a-所以分别对-w-b-进行求偏导并令其等于0。"><span class="nav-number">24.1.1.</span> <span class="nav-text">首先固定$a$,先求出$min_{w,b}L(w,b,a)$,所以分别对$w,b$进行求偏导并令其等于0。</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#frac-partial-L-w-b-a-partial-w-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-w-0"><span class="nav-number">25.</span> <span class="nav-text">$\frac{\partial L(w,b,a)}{\partial w}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial w} =0$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#frac-partial-L-w-b-a-partial-b-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-b-0"><span class="nav-number">26.</span> <span class="nav-text">$\frac{\partial L(w,b,a)}{\partial b}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial b} =0$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#w-sum-i-1-na-ix-iy-i"><span class="nav-number">27.</span> <span class="nav-text">$w=\sum_{i=1}^na_ix_iy_i$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sum-i-1-na-iy-i-0"><span class="nav-number">28.</span> <span class="nav-text">$\sum_{i=1}^na_iy_i=0$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b"><span class="nav-number">29.</span> <span class="nav-text">$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx"><span class="nav-number">30.</span> <span class="nav-text">$=\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#导入-w-sum-i-1-na-ix-iy-i"><span class="nav-number">30.0.1.</span> <span class="nav-text">导入$w=\sum_{i=1}^na_ix_iy_i$:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#frac12w-T-sum-i-1-na-ix-iy-i-sum-i-1-na-i-sum-i-1-n-a-iby-i-w-T-sum-i-1-n-a-iy-ix"><span class="nav-number">31.</span> <span class="nav-text">$=\frac12w^T(\sum_{i=1}^na_ix_iy_i)+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-w^T(\sum_{i=1}^n a_iy_ix)$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#导入-sum-i-1-na-iy-i-0"><span class="nav-number">31.0.1.</span> <span class="nav-text">导入$\sum_{i=1}^na_iy_i=0$:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sum-i-1-na-i-frac12w-T-sum-i-1-na-ix-iy-i"><span class="nav-number">32.</span> <span class="nav-text">$=\sum_{i=1}^na_i-\frac12w^T(\sum_{i=1}^na_ix_iy_i)$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sum-i-1-na-i-frac12-sum-i-1-na-ix-iy-i-T-sum-i-1-na-ix-iy-i"><span class="nav-number">33.</span> <span class="nav-text">$=\sum_{i=1}^na_i-\frac12{(\sum_{i=1}^na_ix_iy_i)}^T(\sum_{i=1}^na_ix_iy_i)$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sum-i-1-na-i-frac12-sum-i-1-na-i-x-i-Ty-i-sum-i-1-na-ix-iy-i"><span class="nav-number">34.</span> <span class="nav-text">$=\sum_{i=1}^na_i-\frac12(\sum_{i=1}^na_i{x_i}^Ty_i)(\sum_{i=1}^na_ix_iy_i)$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><span class="nav-number">35.</span> <span class="nav-text">$=\sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#然后求对-a-的极大值："><span class="nav-number">35.0.1.</span> <span class="nav-text">然后求对$a$的极大值：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><span class="nav-number">36.</span> <span class="nav-text">$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#s-t-a-i-ge0-i-1-cdot-cdot-cdot-n"><span class="nav-number">37.</span> <span class="nav-text">$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sum-i-1-na-iy-i-0-1"><span class="nav-number">38.</span> <span class="nav-text">$\sum_{i=1}^na_iy_i=0$</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SMO优化算法"><span class="nav-number">38.1.</span> <span class="nav-text">SMO优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#选取一对需更新的变量-a-i和-a-j"><span class="nav-number">38.1.1.</span> <span class="nav-text">选取一对需更新的变量$a_i和 a_j$.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#固定-a-i和-a-j-以外的参数，求解-max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-获得更新后的-a-i和-a-j"><span class="nav-number">38.1.2.</span> <span class="nav-text">固定$a_i和 a_j$以外的参数，求解$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$,获得更新后的$a_i和 a_j$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#那如何做才能做到不断收敛呢？"><span class="nav-number">38.1.3.</span> <span class="nav-text">那如何做才能做到不断收敛呢？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#那如何选取变量呢？"><span class="nav-number">38.1.4.</span> <span class="nav-text">那如何选取变量呢？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#所以：只考虑-a-i和-a-j-时，约束条件就改变为："><span class="nav-number">38.1.5.</span> <span class="nav-text">所以：只考虑$a_i和 a_j$时，约束条件就改变为：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-1"><span class="nav-number">38.1.6.</span> <span class="nav-text">$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#s-t-a-iy-i-a-jy-j-c-a-i-ge-0-a-j-ge-0"><span class="nav-number">38.1.7.</span> <span class="nav-text">$s.t. ,a_iy_i+a_jy_j=c, a_i \ge 0, a_j \ge 0$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sum-i-1-na-iy-i-0-2"><span class="nav-number">38.1.8.</span> <span class="nav-text">$\sum_{i=1}^na_iy_i=0$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其中：-c-sum-k-ne-i-j-a-ky-k-是使-sum-i-1-na-iy-i-0-成立的常数。"><span class="nav-number">38.1.9.</span> <span class="nav-text">其中：$c= -\sum_{k \ne i,j}a_ky_k$是使$\sum_{i=1}^na_iy_i=0$ 成立的常数。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#然后用-a-iy-i-a-jy-j-c-消去变量-a-j-，就可以得到一个关于-a-j-的单变量的二次规划问题，通过约束条件-a-i-ge0-就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的-a-i-和-a-j-。"><span class="nav-number">38.1.10.</span> <span class="nav-text">然后用$a_iy_i+a_jy_j=c$ 消去变量$a_j$ ，就可以得到一个关于$a_j$ 的单变量的二次规划问题，通过约束条件$a_i \ge0$,就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的$a_i 和 a_j$。</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性模型"><span class="nav-number">38.2.</span> <span class="nav-text">线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#用SMO-解出-a-后，我们前面以前算出-w-的值，所以我们还需要在前面的基础上算出b的值"><span class="nav-number">38.2.1.</span> <span class="nav-text">用SMO 解出$a$后，我们前面以前算出$w$的值，所以我们还需要在前面的基础上算出b的值</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#w-sum-i-1-na-ix-iy-i-1"><span class="nav-number">39.</span> <span class="nav-text">$w=\sum_{i=1}^na_ix_iy_i$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#b-y-j-sum-i-1-n-a-iy-i-x-i-Tx-j"><span class="nav-number">40.</span> <span class="nav-text">$b=y_j-\sum_{i=1}^n a_iy_i{x_i}^Tx_j$</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#所以就可以得到模型"><span class="nav-number">40.0.1.</span> <span class="nav-text">所以就可以得到模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#f-x-w-Tx-b-sum-i-1-n-a-iy-i-x-i-Tx-b"><span class="nav-number">41.</span> <span class="nav-text">$f(x)=w^Tx+b=\sum_{i=1}^n a_iy_i{x_i}^Tx+b$</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实战一下"><span class="nav-number">41.1.</span> <span class="nav-text">实战一下</span></a></li></ol></li></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Seven</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">144.8k</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="/js/src/instantclick.min.js" data-no-instant=""></script><script data-no-instant="">InstantClick.init()</script></body></html><!-- rebuild by neat -->