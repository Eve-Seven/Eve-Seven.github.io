<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>python第六话之函数基础和函数参数</title>
      <link href="/2018/12/12/2018-12-13-python-main/"/>
      <url>/2018/12/12/2018-12-13-python-main/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>[TOC]</p><h3 id="函数基础和函数参数"><a href="#函数基础和函数参数" class="headerlink" title="函数基础和函数参数"></a>函数基础和函数参数</h3><p>函数是组织好的，可重复使用的，用来实现单一，或相关联功能的代码段。</p><p>函数能提高应用的模块性，和代码的重复利用率。你已经知道Python提供了许多内建函数，比如print()。但你也可以自己创建函数，这被叫做用户自定义函数。</p><h3 id="函数基础"><a href="#函数基础" class="headerlink" title="函数基础"></a>函数基础</h3><h4 id="定义一个函数"><a href="#定义一个函数" class="headerlink" title="定义一个函数"></a>定义一个函数</h4><blockquote><p>你可以定义一个由自己想要功能的函数，以下是简单的规则：</p><ul><li>函数代码块以 <strong>def</strong> 关键词开头，后接函数标识符名称和圆括号 <strong>()</strong>。</li><li>任何传入参数和自变量必须放在圆括号中间，圆括号之间可以用于定义参数。</li><li>函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。</li><li>函数内容以冒号起始，并且缩进。</li><li><strong>return [表达式]</strong> 结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回 None。</li></ul></blockquote><p><strong>演示</strong>：</p><blockquote><p>我们上节课实现了打印列表，如果我们打印几个列表呢？</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> li:</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'---------'</span>)</span><br><span class="line"></span><br><span class="line">li = [<span class="number">1</span>, <span class="string">'A'</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> li:</span><br><span class="line">    print(i)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'---------'</span>)</span><br><span class="line">li = [<span class="number">1</span>, <span class="number">3</span>, <span class="string">'s'</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> li:</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><p><strong>输出结果：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line">---------</span><br><span class="line"><span class="number">1</span></span><br><span class="line">A</span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line">---------</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">s</span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">9</span></span><br></pre></td></tr></table></figure><blockquote><p>以我们上节所讲的知识点，如果要打印三个列表的话，就是上述这种方法，那还有没有更简单的呢？</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">l1 = [1, 0, 5, 7, 9]</span><br><span class="line">l2 = [1, &apos;A&apos;, 5, 7, 9]</span><br><span class="line">l3 = [1, 0, &apos;S&apos;, 7, 9]</span><br><span class="line"></span><br><span class="line">def demo(li):</span><br><span class="line">for i in li:</span><br><span class="line">print(i)</span><br><span class="line"></span><br><span class="line">demo(l1)</span><br><span class="line">print(&apos;---------&apos;)</span><br><span class="line">demo(l2)</span><br><span class="line">print(&apos;---------&apos;)</span><br><span class="line">demo(l3)</span><br></pre></td></tr></table></figure><p><strong>输出结果</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line">---------</span><br><span class="line"><span class="number">1</span></span><br><span class="line">A</span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line">---------</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line">S</span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">9</span></span><br></pre></td></tr></table></figure><blockquote><p>上述就是使用函数的形式来实现多个列表的打印，是不是比前面的更简单。</p></blockquote><h4 id="函数的定义"><a href="#函数的定义" class="headerlink" title="函数的定义"></a>函数的定义</h4><blockquote><p>def 函数名(参数)：</p><p>​ pass</p><p>​ return 表达式</p><p>函数名命名规则： 字母、数字和下划线组成，和变量命名规则一致</p><p>return 后面可以返回任意表达式，但不能是赋值语句</p><p>注意：函数名定义和变量名的定义是一样的，只能使用字母、数字和下划线定义，不能以数字开头。</p></blockquote><h4 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h4><blockquote><p>关键字是不能拿来做变量定义的。</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">3</span>]: a</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">NameError                                 Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-3</span><span class="number">-3</span>f786850e387&gt; <span class="keyword">in</span> &lt;module&gt;()</span><br><span class="line">----&gt; 1 a</span><br><span class="line"></span><br><span class="line">NameError: name <span class="string">'a'</span> <span class="keyword">is</span> <span class="keyword">not</span> defined</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: <span class="function"><span class="keyword">def</span></span></span><br><span class="line">  File "&lt;ipython-input-4-7b18d017f89f&gt;", line 1</span><br><span class="line">    <span class="function"><span class="keyword">def</span></span></span><br><span class="line"><span class="function">       ^</span></span><br><span class="line"><span class="function"><span class="title">SyntaxError</span>:</span> invalid syntax</span><br></pre></td></tr></table></figure><blockquote><p>如果把关键字拿来定义，是会报语法错误的。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> keyword</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: print(keyword.kwlist)</span><br><span class="line">[<span class="string">'False'</span>, <span class="string">'None'</span>, <span class="string">'True'</span>, <span class="string">'and'</span>, <span class="string">'as'</span>, <span class="string">'assert'</span>, <span class="string">'break'</span>, <span class="string">'class'</span>, <span class="string">'continue'</span>, <span class="string">'def'</span>, <span class="string">'del'</span>, <span class="string">'elif'</span>, <span class="string">'else'</span>, <span class="string">'except'</span>, <span class="string">'finally'</span>, <span class="string">'for'</span>, <span class="string">'from'</span>, <span class="string">'global'</span>, <span class="string">'if'</span>, <span class="string">'import'</span>, <span class="string">'in'</span>, <span class="string">'is'</span>, <span class="string">'lambda'</span>, <span class="string">'nonlocal'</span>, <span class="string">'not'</span>, <span class="string">'or'</span>, <span class="string">'pass'</span>, <span class="string">'raise'</span>, <span class="string">'return'</span>, <span class="string">'try'</span>, <span class="string">'while'</span>, <span class="string">'with'</span>, <span class="string">'yield'</span>]</span><br></pre></td></tr></table></figure><blockquote><p>上述就是整个Python编程语言的全部关键字，在基础阶段都会提到的。</p></blockquote><h4 id="函数调用"><a href="#函数调用" class="headerlink" title="函数调用"></a>函数调用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">l1 = [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'S'</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(li)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> li:</span><br><span class="line">        print(i)</span><br><span class="line"></span><br><span class="line">demo(l1)</span><br></pre></td></tr></table></figure><blockquote><p>调用方式：函数名（参数）</p></blockquote><h4 id="函数返回"><a href="#函数返回" class="headerlink" title="函数返回"></a>函数返回</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">l1 = [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'S'</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(li)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> li:</span><br><span class="line">        print(i)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'ok'</span></span><br><span class="line"></span><br><span class="line">print(demo(l1))</span><br></pre></td></tr></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line">S</span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line">ok</span><br></pre></td></tr></table></figure><blockquote><p>return：</p><p>注意 return 和 print 的区别，return是函数的返回值，返回值可以赋值给变量，而print只是打印出来</p></blockquote><h3 id="函数参数"><a href="#函数参数" class="headerlink" title="函数参数"></a>函数参数</h3><blockquote><p>刚才讲到了函数的定义，那函数里面可以传入哪些对象呢？</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(x)</span>:</span></span><br><span class="line">    print(x)</span><br><span class="line"></span><br><span class="line">demo(<span class="string">'demo'</span>)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">demo</span><br></pre></td></tr></table></figure><blockquote><p>如果我们不传值呢？</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(x)</span>:</span></span><br><span class="line">    print(x)</span><br><span class="line"></span><br><span class="line">demo()</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: demo() missing <span class="number">1</span> required positional argument: <span class="string">'x'</span></span><br></pre></td></tr></table></figure><blockquote><p>TypeError：demo()缺少一个必需的位置参数：’x’。</p></blockquote><blockquote><p>传入几个参数呢？</p></blockquote><h4 id="必备参数"><a href="#必备参数" class="headerlink" title="必备参数"></a>必备参数</h4><blockquote><p>def func(x):</p><p>pass</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(x)</span>:</span></span><br><span class="line">    print(x)</span><br><span class="line"></span><br><span class="line">demo(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: demo() takes <span class="number">1</span> positional argument but <span class="number">2</span> were given</span><br></pre></td></tr></table></figure><blockquote><p>一个参数对应一个数值</p></blockquote><h4 id="默认参数"><a href="#默认参数" class="headerlink" title="默认参数"></a>默认参数</h4><blockquote><p>def func(x, y=None):</p><p>pass</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(x, y=<span class="number">1</span>)</span>:</span></span><br><span class="line">    print(x, y)</span><br><span class="line"></span><br><span class="line">demo(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">demo(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="number">2</span></span><br><span class="line"><span class="number">3</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><blockquote><p>y=1.就是默认参数，没有传入新参数的时候，就使用默认参数。</p></blockquote><h4 id="关键字参数"><a href="#关键字参数" class="headerlink" title="关键字参数"></a>关键字参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(x, y=<span class="number">1</span>)</span>:</span></span><br><span class="line">    print(x, y)</span><br><span class="line"></span><br><span class="line">demo(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">demo(y=<span class="string">"q"</span>, x=<span class="string">'s'</span>)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="number">2</span></span><br><span class="line">s q</span><br></pre></td></tr></table></figure><blockquote><p>关键字参数，调用的时候带上参数名。</p></blockquote><h4 id="不定长参数"><a href="#不定长参数" class="headerlink" title="不定长参数"></a>不定长参数</h4><blockquote><p>def func(*args, **kwargs):</p><p>pass</p><p>注意：*+参数名</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(*args)</span>:</span></span><br><span class="line">    print(args)</span><br><span class="line"></span><br><span class="line">demo(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">demo(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="number">1</span>,)</span><br></pre></td></tr></table></figure><blockquote><p>参数名前面加<code>*号</code>是不定长参数，输出是一个元组。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(*a)</span>:</span></span><br><span class="line">    print(*a)  <span class="comment"># 加*：去除括号</span></span><br><span class="line">    print(a)</span><br><span class="line"></span><br><span class="line">demo(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(<span class="string">'-------'</span>)</span><br><span class="line">demo((<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">print(<span class="string">'-------'</span>)</span><br><span class="line">demo(*(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">-------</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">((<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>),)</span><br><span class="line">-------</span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span></span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><blockquote><p>加*：去除括号</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(**a)</span>:</span></span><br><span class="line">    print(a)</span><br><span class="line"></span><br><span class="line">demo(x=<span class="number">1</span>, y=<span class="number">2</span>, s=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'x'</span>: <span class="number">1</span>, <span class="string">'y'</span>: <span class="number">2</span>, <span class="string">'s'</span>: <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>参数名前面加<code>**号</code>是不定长参数，输出是一个字典。</p><p>注意：传入的参数是键值对。</p></blockquote><h4 id="演示："><a href="#演示：" class="headerlink" title="演示："></a><strong>演示：</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    print(args)</span><br><span class="line">    print(kwargs)</span><br><span class="line"></span><br><span class="line">demo(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, x=<span class="number">1</span>, y=<span class="number">2</span>, s=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">&#123;<span class="string">'x'</span>: <span class="number">1</span>, <span class="string">'y'</span>: <span class="number">2</span>, <span class="string">'s'</span>: <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>传入的键值对，只能放在最后。</p></blockquote><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><blockquote><p>必备参数：在函数调用的时候，必备参数必须要传入</p><p>默认参数： 在函数调用的时候，默认参数可以不传入值，不传入值时，会使用默认参数</p><p>不定长参数：在函数调用的时候，不定长参数可以不传入，也可以传入任意长度。其中定义时，元组形式可以放到参数最前面，字典形式只能放到最后面</p></blockquote><h3 id="常见的内置函数"><a href="#常见的内置函数" class="headerlink" title="常见的内置函数"></a>常见的内置函数</h3><p>常见内置函数提供了一些处理的数据的方法，可以帮助我们提高开发速度</p><h4 id="常见函数"><a href="#常见函数" class="headerlink" title="常见函数"></a>常见函数</h4><h5 id="len"><a href="#len" class="headerlink" title="len"></a><code>len</code></h5><blockquote><p>求长度</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">2</span>,<span class="number">8</span>,<span class="number">5</span>]</span><br><span class="line">In [<span class="number">6</span>]: len(li)</span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">3</span></span><br></pre></td></tr></table></figure><h5 id="min"><a href="#min" class="headerlink" title="min"></a><code>min</code></h5><blockquote><p>求最小值</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">2</span>,<span class="number">8</span>,<span class="number">5</span>]</span><br><span class="line">In [<span class="number">6</span>]: len(li)</span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">3</span></span><br></pre></td></tr></table></figure><h5 id="max"><a href="#max" class="headerlink" title="max"></a><code>max</code></h5><blockquote><p>求最大值</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">2</span>,<span class="number">8</span>,<span class="number">5</span>]</span><br><span class="line">In [<span class="number">8</span>]: max(li)</span><br><span class="line">Out[<span class="number">8</span>]: <span class="number">8</span></span><br></pre></td></tr></table></figure><h5 id="sorted"><a href="#sorted" class="headerlink" title="sorted"></a><code>sorted</code></h5><blockquote><p>排序</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">2</span>,<span class="number">8</span>,<span class="number">5</span>]</span><br><span class="line">In [<span class="number">9</span>]: sorted(li)</span><br><span class="line">Out[<span class="number">9</span>]: [<span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>]</span><br></pre></td></tr></table></figure><h5 id="reversed"><a href="#reversed" class="headerlink" title="reversed"></a><code>reversed</code></h5><blockquote><p>反向</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">2</span>,<span class="number">8</span>,<span class="number">5</span>]</span><br><span class="line">In [<span class="number">10</span>]: reversed(li)</span><br><span class="line">Out[<span class="number">10</span>]: &lt;list_reverseiterator at <span class="number">0x7f68aa81af98</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: list(reversed(li))</span><br><span class="line">Out[<span class="number">11</span>]: [<span class="number">5</span>, <span class="number">8</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure><h5 id="sum"><a href="#sum" class="headerlink" title="sum"></a><code>sum</code></h5><blockquote><p>求和</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">2</span>,<span class="number">8</span>,<span class="number">5</span>]</span><br><span class="line">In [<span class="number">12</span>]: sum(li)</span><br><span class="line">Out[<span class="number">12</span>]: <span class="number">15</span></span><br></pre></td></tr></table></figure><h4 id="进制转换函数"><a href="#进制转换函数" class="headerlink" title="进制转换函数"></a>进制转换函数</h4><h5 id="bin"><a href="#bin" class="headerlink" title="bin"></a><code>bin</code></h5><blockquote><p>二进制</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: bin(<span class="number">12</span>)</span><br><span class="line">Out[<span class="number">13</span>]: <span class="string">'0b1100'</span></span><br></pre></td></tr></table></figure><h5 id="oct"><a href="#oct" class="headerlink" title="oct"></a><code>oct</code></h5><blockquote><p>八进制</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">16</span>]: oct(<span class="number">18</span>)</span><br><span class="line">Out[<span class="number">16</span>]: <span class="string">'0o22</span></span><br></pre></td></tr></table></figure><h5 id="hex"><a href="#hex" class="headerlink" title="hex"></a><code>hex</code></h5><blockquote><p>十六进制</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">17</span>]: hex(<span class="number">12</span>)</span><br><span class="line">Out[<span class="number">17</span>]: <span class="string">'0xc'</span></span><br></pre></td></tr></table></figure><h5 id="ord"><a href="#ord" class="headerlink" title="ord"></a><code>ord</code></h5><blockquote><p>字符转ASCII码</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">19</span>]: ord(<span class="string">'a'</span>)</span><br><span class="line">Out[<span class="number">19</span>]: <span class="number">97</span></span><br></pre></td></tr></table></figure><h5 id="chr"><a href="#chr" class="headerlink" title="chr"></a><code>chr</code></h5><blockquote><p>ASCII码转字符</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">20</span>]: chr(<span class="number">97</span>)</span><br><span class="line">Out[<span class="number">20</span>]: <span class="string">'a'</span></span><br></pre></td></tr></table></figure><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><h5 id="enumerate"><a href="#enumerate" class="headerlink" title="enumerate"></a><code>enumerate</code></h5><blockquote><p>返回一个可以枚举的对象</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">21</span>]: li = [<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: enumerate(li)</span><br><span class="line">Out[<span class="number">22</span>]: &lt;enumerate at <span class="number">0x7f68aa877d80</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: list(enumerate(li))</span><br><span class="line">Out[<span class="number">23</span>]: [(<span class="number">0</span>, <span class="string">'a'</span>), (<span class="number">1</span>, <span class="string">'b'</span>), (<span class="number">2</span>, <span class="string">'c'</span>), (<span class="number">3</span>, <span class="string">'d'</span>)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: dict(enumerate(li))</span><br><span class="line">Out[<span class="number">24</span>]: &#123;<span class="number">0</span>: <span class="string">'a'</span>, <span class="number">1</span>: <span class="string">'b'</span>, <span class="number">2</span>: <span class="string">'c'</span>, <span class="number">3</span>: <span class="string">'d'</span>&#125;</span><br></pre></td></tr></table></figure><h5 id="eval"><a href="#eval" class="headerlink" title="eval"></a><code>eval</code></h5><blockquote><p>取出字符串中内容</p><p>将字符串str当成有效的表达式来求值并返回计算结果</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">25</span>]: a = <span class="string">"&#123;'a':1&#125;"</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: eval(a)</span><br><span class="line">Out[<span class="number">26</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: b = <span class="string">'1 + 2 + 3'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: eval(b)</span><br><span class="line">Out[<span class="number">28</span>]: <span class="number">6</span></span><br></pre></td></tr></table></figure><h5 id="exec"><a href="#exec" class="headerlink" title="exec"></a><code>exec</code></h5><blockquote><p>执行字符串或complie方法编译过的字符串，没有返回值</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">29</span>]: s = <span class="string">'''</span></span><br><span class="line"><span class="string">    ...: z = 10</span></span><br><span class="line"><span class="string">    ...: su = x + y + z</span></span><br><span class="line"><span class="string">    ...: print(su)</span></span><br><span class="line"><span class="string">    ...: print('OK')</span></span><br><span class="line"><span class="string">    ...: '''</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">30</span>]: x = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">31</span>]: y = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: exec(s)</span><br><span class="line"><span class="number">13</span></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">In [<span class="number">33</span>]: exec(s,&#123;<span class="string">'x'</span>:<span class="number">0</span>,<span class="string">'y'</span>:<span class="number">0</span>&#125;)</span><br><span class="line"><span class="number">10</span></span><br><span class="line">OK</span><br><span class="line"></span><br><span class="line">In [<span class="number">34</span>]: exec(s,&#123;<span class="string">'x'</span>:<span class="number">0</span>,<span class="string">'y'</span>:<span class="number">0</span>&#125;,&#123;<span class="string">'y'</span>:<span class="number">10</span>,<span class="string">'z'</span>:<span class="number">0</span>&#125;)  <span class="comment">#以字符串为主,以最后的为主</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line">OK</span><br></pre></td></tr></table></figure><blockquote><p>注意：eval 和 exec 是炸弹 能不能就不用，就好像你从不知道这东西一样，除非你足够的熟悉</p></blockquote><h5 id="filter"><a href="#filter" class="headerlink" title="filter"></a><code>filter</code></h5><blockquote><p>过滤器</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">38</span>]: <span class="function"><span class="keyword">def</span> <span class="title">test1</span><span class="params">(x)</span>:</span></span><br><span class="line">    ...:     <span class="keyword">return</span> x&gt;<span class="number">10</span></span><br><span class="line">    ...: l1 = [<span class="number">10</span>,<span class="number">2</span>,<span class="number">20</span>,<span class="number">13</span>,<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">39</span>]: filter(test1, l1)</span><br><span class="line">Out[<span class="number">39</span>]: &lt;filter at <span class="number">0x7f68aa7ecb70</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">40</span>]: list(filter(test1, l1))</span><br><span class="line">Out[<span class="number">40</span>]: [<span class="number">20</span>, <span class="number">13</span>]</span><br></pre></td></tr></table></figure><h5 id="map"><a href="#map" class="headerlink" title="map"></a><code>map</code></h5><blockquote><p>对于参数iterable中的每个元素都应用fuction函数，并将结果作为列表返回</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">41</span>]: l2 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">42</span>]: map(str,l2)</span><br><span class="line">Out[<span class="number">42</span>]: &lt;map at <span class="number">0x7f68aa7ecba8</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">43</span>]: list(map(str,l2))</span><br><span class="line">Out[<span class="number">43</span>]: [<span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>]</span><br></pre></td></tr></table></figure><h5 id="zip"><a href="#zip" class="headerlink" title="zip"></a><code>zip</code></h5><blockquote><p>将对象逐一配对</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">44</span>]: l3 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">45</span>]: t1 = (<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">46</span>]: zip(t1,l3)</span><br><span class="line">Out[<span class="number">46</span>]: &lt;zip at <span class="number">0x7f68abb3ec48</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: list(zip(t1,l3))</span><br><span class="line">Out[<span class="number">47</span>]: [(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">2</span>), (<span class="string">'c'</span>, <span class="number">3</span>)]</span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: dict(zip(t1,l3))</span><br><span class="line">Out[<span class="number">48</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python基础 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python第五话之控制流程</title>
      <link href="/2018/12/12/2018-12-12-python-if-while/"/>
      <url>/2018/12/12/2018-12-12-python-if-while/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>[TOC]</p><h3 id="控制流程"><a href="#控制流程" class="headerlink" title="控制流程"></a>控制流程</h3><p>逻辑值包含了两个值：<br><code>True</code>：表示非空的量(比如：string,tuple.list.set,dictonary等) ，所有非零数 。<br><code>False</code>：表示0,None,空的量等<br><code>作用</code>：主要用于判断语句中，用来判断</p><ul><li>一个字符串是否为空</li><li>一个运算结果是否为零</li><li>一个表达式是否可用</li></ul><h3 id="条件判断"><a href="#条件判断" class="headerlink" title="条件判断"></a>条件判断</h3><p>条件语句是根据条件来设置程序接下来的走向。</p><p>条件语句的关键字有<code>if，elif，else</code>。</p><h4 id="基本形式："><a href="#基本形式：" class="headerlink" title="基本形式："></a>基本形式：</h4><blockquote><p>if 判断条件:</p><p>执行语句</p><p>else:</p><p>执行语句</p></blockquote><p>判断条件后面和else这个关键字后面都必须加冒号，冒号后面缩进的语句是子语句，多个子语句组成了语句块，如果是单个语句可以与条件写在同一行直接跟在冒号的后面，如果是语句块则一行一条语句，每一行都必须缩进。注意冒号和缩进都是语法的一部分，缩进一般为四个空格。</p><h4 id="单个条件"><a href="#单个条件" class="headerlink" title="单个条件"></a>单个条件</h4><p>这个是针对只有一个判断条件时的，条件满足时就执行缩进的子语句，else就是表示其余的情况，只要条件不满足则执行else后面子语句。判断语句一般是返回值为bool类型的表达式，值为True则是条件满足，值为False则是条件不满足。</p><h4 id="演示"><a href="#演示" class="headerlink" title="演示"></a><strong>演示</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="string">'天晴'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> a==<span class="string">'天晴'</span>:</span><br><span class="line">print(<span class="string">'天气好，出去玩吧！'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">'天气不好，呆在家吧。。'</span>)</span><br><span class="line"></span><br><span class="line">天气好，出去玩吧！</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="string">'下雨'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> a==<span class="string">'天晴'</span>:</span><br><span class="line">print(<span class="string">'天气好，出去玩吧！'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">print(<span class="string">'天气不好，呆在家吧。。'</span>)</span><br><span class="line"></span><br><span class="line">天气不好，呆在家吧。。</span><br></pre></td></tr></table></figure><h4 id="多个条件"><a href="#多个条件" class="headerlink" title="多个条件"></a>多个条件</h4><p>如果判断需要多个条件需同时判断时，可以使用 or （或），表示两个条件有一个成立时判断条件成功；使用 and （与）时，表示只有两个条件同时成立的情况下，判断条件才成功。</p><h4 id="演示："><a href="#演示：" class="headerlink" title="演示："></a>演示：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="string">"天晴"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = <span class="string">"有空"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> a==<span class="string">"天晴"</span> <span class="keyword">and</span> t==<span class="string">"有空"</span>:</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">"天气真好，咱们出去玩！！"</span>)</span><br><span class="line"><span class="meta">... </span><span class="keyword">else</span>:</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">"天气不好，呆在家吧！！"</span>)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line">天气真好，咱们出去玩！！</span><br></pre></td></tr></table></figure><p>对于多条件分支的判断使用elif关键字来用来条件分支的.</p><h4 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h4><blockquote><p>if 判断条件1:</p><p>执行语句1</p><p>elif 判断条件2:</p><p>执行语句2</p><p>elif 判断条件n:</p><p>执行语句n</p><p>else:</p><p>执行语句x</p></blockquote><p>写多条件分支时，同一个条件中只能有一个if一个else，对elif的个数没有限制但必须是写在if后面，else放在最后表示以上条件都不满足的情况。满足哪个判断条件就执行这个判断条件对应的执行语句，如果列出的条件都不满足则执行else的子语句，语句的执行顺序是从上到下，遇到满足的条件则直接进入它的子语句块，其他剩余判断条件和子语句将不再进行判断和执行。</p><h4 id="演示-1"><a href="#演示-1" class="headerlink" title="演示"></a>演示</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'分数等级测试'</span>)</span><br><span class="line">score = input(<span class="string">'请输入你的分数'</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="number">90</span>&lt;=int(score)&lt;=<span class="number">100</span>:</span><br><span class="line">    print(<span class="string">'你的等级是A'</span>)</span><br><span class="line"><span class="keyword">elif</span> <span class="number">75</span>&lt;=int(score)&lt;<span class="number">90</span>:</span><br><span class="line">    print(<span class="string">'你的等级是B'</span>)</span><br><span class="line"><span class="keyword">elif</span> <span class="number">60</span>&lt;=int(score)&lt;<span class="number">75</span>:</span><br><span class="line">    print(<span class="string">'你的等级是C'</span>)</span><br><span class="line"><span class="keyword">elif</span> <span class="number">0</span>&lt;=int(score)&lt;<span class="number">60</span>:</span><br><span class="line">    print(<span class="string">'你的等级是D'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'输入有误!'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">运行结果（python shell中显示）：</span><br><span class="line">分数等级测试</span><br><span class="line">请输入你的分数<span class="number">98</span></span><br><span class="line">你的等级是A</span><br></pre></td></tr></table></figure><p>这里使用了内置的函数input()获取键盘的输入，这里会把键盘的输入以字符串的形式赋值给score这个名字，同类型的才可以进行比较，所以后面在进行条件判断时要把score转换成int类型再进行比较。</p><h3 id="三目运算"><a href="#三目运算" class="headerlink" title="三目运算"></a>三目运算</h3><h4 id="演示：-1"><a href="#演示：-1" class="headerlink" title="演示："></a>演示：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">3</span> </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> a&gt;<span class="number">5</span>:</span><br><span class="line"><span class="meta">... </span>    print(<span class="keyword">True</span>)</span><br><span class="line"><span class="meta">... </span><span class="keyword">else</span>:</span><br><span class="line"><span class="meta">... </span>    print(<span class="keyword">False</span>)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>更简单的写法呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="keyword">True</span>) <span class="keyword">if</span> a&gt;<span class="number">5</span> <span class="keyword">else</span> print(<span class="keyword">False</span>) </span><br><span class="line"><span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>####</p><h3 id="条件循环"><a href="#条件循环" class="headerlink" title="条件循环"></a>条件循环</h3><h4 id="while"><a href="#while" class="headerlink" title="while"></a><code>while</code></h4><blockquote><p>语法规则：</p><p>while 判断语句：</p><p>​ 循环体</p><p>注意：注意缩进</p></blockquote><h4 id="演示：-2"><a href="#演示：-2" class="headerlink" title="演示："></a>演示：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>li = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>i = <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">while</span> i &lt; len(li):</span><br><span class="line"><span class="meta">... </span>    print(<span class="keyword">True</span>) <span class="keyword">if</span> li[i]&gt;<span class="number">5</span> <span class="keyword">else</span> print(<span class="keyword">False</span>) </span><br><span class="line"><span class="meta">... </span>    i += <span class="number">1</span></span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="keyword">False</span></span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>对于刚才值大于5的三目运算，如果是判断一个列表中数字该怎么做呢？</p><h4 id="break"><a href="#break" class="headerlink" title="break"></a>break</h4><blockquote><p>跳出循环</p></blockquote><p><strong>演示</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt; len(li):</span><br><span class="line"><span class="keyword">if</span> li[i] &gt; <span class="number">5</span>:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">print(li[i])</span><br><span class="line">i += <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure><h4 id="continue"><a href="#continue" class="headerlink" title="continue"></a>continue</h4><blockquote><p>跳过此次循环</p></blockquote><p><strong>演示</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> i &lt; len(li):</span><br><span class="line">    print(li[i])</span><br><span class="line"><span class="keyword">if</span> li[i] == <span class="number">5</span>:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">i += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>大家猜测下执行结果。。。。。</p><p>解决上面的问题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line">i = <span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> i &lt; len(li)<span class="number">-1</span>:</span><br><span class="line">    print(li[i])</span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> li[i] == <span class="number">5</span>:</span><br><span class="line">    <span class="keyword">continue</span></span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">9</span></span><br></pre></td></tr></table></figure><h4 id="else"><a href="#else" class="headerlink" title="else"></a>else</h4><blockquote><p>当while的条件不满足时，运行。</p><p>注意：break时，不运行</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line">i = <span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> i &lt; len(li)<span class="number">-1</span>:</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> li[i] == <span class="number">5</span>:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">print(li[i])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'ok'</span>)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line">ok</span><br></pre></td></tr></table></figure><p><strong>演示</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line">i = <span class="number">-1</span></span><br><span class="line"><span class="keyword">while</span> i &lt; len(li)<span class="number">-1</span>:</span><br><span class="line">i += <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> li[i] == <span class="number">5</span>:</span><br><span class="line">        <span class="comment"># continue</span></span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">print(li[i])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'ok'</span>)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h4><blockquote><p>循环可以被终止：</p><ul><li>判断语句可以返回 False</li><li>通过break终止循环</li></ul><p>else的执行条件：</p><p>只有在循环不是被break终止的情况下才会执行else中的内容</p></blockquote><h3 id="迭代循环"><a href="#迭代循环" class="headerlink" title="迭代循环"></a>迭代循环</h3><h4 id="for"><a href="#for" class="headerlink" title="for"></a><strong>for</strong></h4><blockquote><p>只要是可迭代对象，都可以使用for循环遍历。</p></blockquote><h4 id="语法规则"><a href="#语法规则" class="headerlink" title="语法规则"></a>语法规则</h4><blockquote><p>for i in obj：</p><p>​ 循环体</p><p>注意：注意缩进</p></blockquote><p><strong>演示</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">li = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> li:</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">9</span></span><br></pre></td></tr></table></figure><h4 id="range"><a href="#range" class="headerlink" title="range"></a><strong>range</strong></h4><p><strong>演示</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">21</span>):</span><br><span class="line">print(i)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">11</span></span><br><span class="line"><span class="number">12</span></span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="number">14</span></span><br><span class="line"><span class="number">15</span></span><br><span class="line"><span class="number">16</span></span><br><span class="line"><span class="number">17</span></span><br><span class="line"><span class="number">18</span></span><br><span class="line"><span class="number">19</span></span><br><span class="line"><span class="number">20</span></span><br></pre></td></tr></table></figure><blockquote><p>内置函数，表示一个范围，不包含结尾值。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">3</span>]: list(range(<span class="number">21</span>))</span><br><span class="line">Out[<span class="number">3</span>]: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>]</span><br><span class="line">    </span><br><span class="line">In [<span class="number">4</span>]: list(range(<span class="number">2</span>, <span class="number">21</span>))</span><br><span class="line">Out[<span class="number">4</span>]: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: list(range(<span class="number">1</span>, <span class="number">21</span>, <span class="number">2</span>))</span><br><span class="line">Out[<span class="number">5</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">15</span>, <span class="number">17</span>, <span class="number">19</span>]</span><br></pre></td></tr></table></figure><h4 id="continue-1"><a href="#continue-1" class="headerlink" title="continue"></a><strong>continue</strong></h4><blockquote><p>跳出当前循环</p></blockquote><p><strong>演示</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">21</span>):</span><br><span class="line"><span class="keyword">if</span> i%<span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">print(i)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">11</span></span><br><span class="line"><span class="number">12</span></span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="number">14</span></span><br><span class="line"><span class="number">16</span></span><br><span class="line"><span class="number">17</span></span><br><span class="line"><span class="number">18</span></span><br><span class="line"><span class="number">19</span></span><br></pre></td></tr></table></figure><h4 id="break-1"><a href="#break-1" class="headerlink" title="break"></a>break</h4><blockquote><p>跳出循环</p></blockquote><p><strong>演示</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">21</span>):</span><br><span class="line"><span class="keyword">if</span> i%<span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">print(i)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><h4 id="else-1"><a href="#else-1" class="headerlink" title="else"></a><strong>else</strong></h4><blockquote><p>当for循环结束时，运行。</p><p>注意：break时，不运行</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">21</span>):</span><br><span class="line"><span class="keyword">if</span> i%<span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line">print(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'end...'</span>)</span><br></pre></td></tr></table></figure><p><strong>输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">11</span></span><br><span class="line"><span class="number">12</span></span><br><span class="line"><span class="number">13</span></span><br><span class="line"><span class="number">14</span></span><br><span class="line"><span class="number">16</span></span><br><span class="line"><span class="number">17</span></span><br><span class="line"><span class="number">18</span></span><br><span class="line"><span class="number">19</span></span><br><span class="line">end...</span><br></pre></td></tr></table></figure><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h4><blockquote><p>for 后面需要接上可迭代对象</p><p>for会依次取出可迭代对象中的元素</p><p>continue的用法：</p><p>continue和break类似，但是continue不会终止循环，而是结束本次循环，跳到下次循环</p></blockquote><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python基础 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python第四话之散列类型、运算优先级和逻辑运算</title>
      <link href="/2018/12/11/2018-12-11-python-dict-set/"/>
      <url>/2018/12/11/2018-12-11-python-dict-set/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>[TOC]</p><h3 id="散列类型、运算优先级和逻辑运算"><a href="#散列类型、运算优先级和逻辑运算" class="headerlink" title="散列类型、运算优先级和逻辑运算"></a>散列类型、运算优先级和逻辑运算</h3><p><code>散列类型</code>也就是我们所熟知的<code>字典</code>和<code>集合</code>，我们今天来看看散列类型的相关逻辑运算。</p><h3 id="集合（set）"><a href="#集合（set）" class="headerlink" title="集合（set）"></a>集合（set）</h3><h4 id="集合的特点："><a href="#集合的特点：" class="headerlink" title="集合的特点："></a>集合的特点：</h4><p>无序、元素是唯一的。</p><h4 id="集合的创建："><a href="#集合的创建：" class="headerlink" title="集合的创建："></a>集合的创建：</h4><p>用大括号“{}”，各元素之间用逗号隔开；也可以通过类型转换的方式使用set()内置函数将列表或元祖转换为集合类型。在创建的过程中会自动过滤掉重复的元素，保证元素的唯一性。</p><h4 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]                                              </span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: s                                                              </span><br><span class="line">Out[<span class="number">2</span>]: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: se = set(s)                                                    </span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: se                                                             </span><br><span class="line">Out[<span class="number">4</span>]: &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: type(se)                                                       </span><br><span class="line">Out[<span class="number">5</span>]: set</span><br><span class="line">    </span><br><span class="line">In [<span class="number">6</span>]: &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;                                                  </span><br><span class="line">Out[<span class="number">6</span>]: &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意：列表是允许元素重复的，但是当我们把列表转成集合后，里面重复的元素就去掉了。</p></blockquote><h4 id="集合的运算"><a href="#集合的运算" class="headerlink" title="集合的运算"></a>集合的运算</h4><p>交集：&amp;</p><p>并集：|</p><p>差集：-</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-11/79094336.jpg" alt=""></p><h5 id="交集"><a href="#交集" class="headerlink" title="交集"></a><code>交集</code></h5><p>两个集合(s 和t)的差补或相对补集是指一个集合C，该集合中的元素，只属于集合s，而不属于集合t。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">7</span>]: s1 = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;                                    </span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: s2 = &#123;<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="string">'b'</span>,<span class="string">'c'</span>&#125;                                           </span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: s1 &amp; s2                                                        </span><br><span class="line">Out[<span class="number">9</span>]: &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="string">'b'</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>两个集合取交集，最后输出的元素是属于两个集合所共有的元素。</p></blockquote><h5 id="并集"><a href="#并集" class="headerlink" title="并集"></a><code>并集</code></h5><p>联合(union)操作和集合的OR(又称可兼析取(inclusive disjunction))其实是等价的，两个集合的联合是一个新集合，该集合中的每个元素都至少是其中一个集合的成员，即：属于两个集合其中之一的成员。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">10</span>]: s1 = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: s2 = &#123;<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="string">'b'</span>,<span class="string">'c'</span>&#125;                                          </span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: s1 | s2                                                       </span><br><span class="line">Out[<span class="number">12</span>]: &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>并集指的是两个集合的元素进行一个整合，最后生成的元素都是属于原来两个集合之中的某一个。</p></blockquote><h5 id="差集"><a href="#差集" class="headerlink" title="差集"></a><code>差集</code></h5><p>和其他的布尔集合操作相似，对称差分是集合的XOR(又称”异 或” (exclusive disjunction)).两个集合(s 和t)的对称差分是指另外一个集合C,该集合中的元素，只能是属于集合s 或者集合t的成员，不能同时属于两个集合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: s1 = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: s2 = &#123;<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="string">'b'</span>,<span class="string">'c'</span>&#125;                                          </span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: s1 - s2                                                       </span><br><span class="line">Out[<span class="number">15</span>]: &#123;<span class="number">1</span>, <span class="number">4</span>, <span class="string">'a'</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>差集也叫被减集合的补集。</p></blockquote><h5 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a><code>扩展</code></h5><p><strong>add</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">16</span>]: s1 = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: s1.add(<span class="number">8</span>)                                                     </span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: s1                                                            </span><br><span class="line">Out[<span class="number">18</span>]: &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: s1.add(<span class="string">'w'</span>)                                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: s1                                                            </span><br><span class="line">Out[<span class="number">20</span>]: &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'w'</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>往集合里添加元素。</p></blockquote><p><strong>pop</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">23</span>]: s1                                                            </span><br><span class="line">Out[<span class="number">23</span>]: &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'w'</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: s1.pop()                                                      </span><br><span class="line">Out[<span class="number">24</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">25</span>]: s1.pop()                                                      </span><br><span class="line">Out[<span class="number">25</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: s1.pop()                                                      </span><br><span class="line">Out[<span class="number">26</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: s1.pop()                                                      </span><br><span class="line">Out[<span class="number">27</span>]: <span class="number">3</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: s1.pop()                                                      </span><br><span class="line">Out[<span class="number">28</span>]: <span class="number">4</span></span><br></pre></td></tr></table></figure><blockquote><p>pop方法是没有参数的，因为集合是无序的，所以在移除的时候是随机移除的。</p></blockquote><p><strong>remove</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">30</span>]: s1 = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">31</span>]: s1.remove(<span class="number">1</span>)                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: s1                                                            </span><br><span class="line">Out[<span class="number">32</span>]: &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">33</span>]: s1.remove(<span class="string">'a'</span>)                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">34</span>]: s1                                                            </span><br><span class="line">Out[<span class="number">34</span>]: &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'b'</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>remove方法是指定元素进行删除。</p></blockquote><p><strong>update</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">35</span>]: s1                                                            </span><br><span class="line">Out[<span class="number">35</span>]: &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'b'</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">36</span>]: s1.update(&#123;<span class="string">'w'</span>, <span class="string">'c'</span>&#125;)                                         </span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: s1                                                            </span><br><span class="line">Out[<span class="number">37</span>]: &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'w'</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>update方法是往集合里面添加集合。</p></blockquote><p><strong>isdisjoint</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">38</span>]: s1 = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">39</span>]: s2 = &#123;<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="string">'b'</span>,<span class="string">'c'</span>&#125;                                          </span><br><span class="line"></span><br><span class="line">In [<span class="number">40</span>]: s1.isdisjoint(s2)                                             </span><br><span class="line">Out[<span class="number">40</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">41</span>]: s1.isdisjoint(&#123;<span class="number">6</span>, <span class="number">8</span>, <span class="number">7</span>&#125;)                                      </span><br><span class="line">Out[<span class="number">41</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure><blockquote><p>isdisjoint方法是判断两个集合有没有交集，有返回False，没有则返回True</p></blockquote><p><strong>issubset</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">42</span>]: s1 = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">43</span>]: s2 = &#123;<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="string">'b'</span>,<span class="string">'c'</span>&#125;                                          </span><br><span class="line"></span><br><span class="line">In [<span class="number">44</span>]: s1.issubset(s2)                                               </span><br><span class="line">Out[<span class="number">44</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">45</span>]: s2.issubset(s1)                                               </span><br><span class="line">Out[<span class="number">45</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">46</span>]: s1.issubset(s1)                                               </span><br><span class="line">Out[<span class="number">46</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: &#123;<span class="number">1</span>, <span class="number">2</span>&#125;.issubset(s1)                                           </span><br><span class="line">Out[<span class="number">47</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: s1                                                            </span><br><span class="line">Out[<span class="number">48</span>]: &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>判断前面的集合是不是后面的集合的子集。</p></blockquote><p><strong>issuperset</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">51</span>]: s1 = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="string">'a'</span>, <span class="string">'b'</span>&#125;                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">52</span>]: s2 = &#123;<span class="number">2</span>,<span class="number">3</span>,<span class="string">'b'</span>&#125;                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">53</span>]: s2.issubset(s1)                                               </span><br><span class="line">Out[<span class="number">53</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">54</span>]: s1.issuperset(s2)                                             </span><br><span class="line">Out[<span class="number">54</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure><blockquote><p>判断后面的集合是前面集合的子集。</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><blockquote><ul><li>集合唯一性：集合中的元素具有唯一性，不存在两个相同的元素。</li><li>集合可变性：集合中的元素是可变的，集合是可变对象。</li><li>集合无序性：集合中的元素是无序的，所以没有存在索引。</li></ul></blockquote><h3 id="字典（dict）"><a href="#字典（dict）" class="headerlink" title="字典（dict）"></a>字典（dict）</h3><p>字典是除了列表外的另一种<code>可变类型</code>，字典的元素是以键值对的形式存在，字典的键必须是唯一，可以是数字、字符串或者是元组，键可以为任何不可变类型，列表和集合不能作为字典的键。</p><h4 id="字典的创建"><a href="#字典的创建" class="headerlink" title="字典的创建"></a>字典的创建</h4><p>第一种 { key :value } ，字典里的键和值用“：”隔开，一对键和值组成一个项，项和项之间用“，”隔开。</p><p>第二种使用内置函数dict(key=value)，要注意的是这里使用的是“=”赋值的方式，键是以名字的形式所以这种方法的键就必须符合名字的要求，且不能使用关键字作为键。</p><p>如果你要使用关键字作为键名那么就只能用第一种方法，关键字以字符串的形式来创建。</p><p>通过字典的键可以访问这个键所对应的值，字典是可变类型，所以可以直接对字典的项进行修改，使用dictname[key] = value，如果这个键存在于字典中，则是修改这个键所对应的值，如果这个键不存在则是往字典中添加这个项。</p><h4 id="演示-1"><a href="#演示-1" class="headerlink" title="演示"></a><strong>演示</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">56</span>]: &#123;<span class="string">'a'</span>:<span class="number">1</span>, <span class="string">'b'</span>:<span class="number">2</span>&#125;                                                </span><br><span class="line">Out[<span class="number">56</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">57</span>]: s = &#123;<span class="string">'a'</span>:<span class="number">1</span>, <span class="string">'b'</span>:<span class="number">2</span>&#125;                                            </span><br><span class="line"></span><br><span class="line">In [<span class="number">58</span>]: s,type(s)                                                     </span><br><span class="line">Out[<span class="number">58</span>]: (&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;, dict)</span><br><span class="line"></span><br><span class="line">In [<span class="number">56</span>]: &#123;<span class="string">'a'</span>:<span class="number">1</span>, <span class="string">'b'</span>:<span class="number">2</span>&#125;                                                </span><br><span class="line">Out[<span class="number">56</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">57</span>]: s = &#123;<span class="string">'a'</span>:<span class="number">1</span>, <span class="string">'b'</span>:<span class="number">2</span>&#125;                                            </span><br><span class="line"></span><br><span class="line">In [<span class="number">58</span>]: s,type(s)                                                     </span><br><span class="line">Out[<span class="number">58</span>]: (&#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;, dict)</span><br></pre></td></tr></table></figure><blockquote><p>字典形式：{key:value}</p></blockquote><h4 id="字典的运用"><a href="#字典的运用" class="headerlink" title="字典的运用"></a>字典的运用</h4><h5 id="查看"><a href="#查看" class="headerlink" title="查看"></a><code>查看</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">67</span>]: a = dict(a=<span class="number">1</span>, b=<span class="number">2</span>)                                            </span><br><span class="line"></span><br><span class="line">In [<span class="number">68</span>]: a                                                             </span><br><span class="line">Out[<span class="number">68</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">69</span>]: a[<span class="string">'a'</span>]                                                        </span><br><span class="line">Out[<span class="number">69</span>]: <span class="number">1</span></span><br></pre></td></tr></table></figure><blockquote><p>由于字典也是无序的，所以我们在取值的时候，是根据key来取出对应的value的。</p></blockquote><h5 id="增加"><a href="#增加" class="headerlink" title="增加"></a><code>增加</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">70</span>]: a                                                             </span><br><span class="line">Out[<span class="number">70</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">71</span>]: a[<span class="string">'c'</span>] = <span class="number">3</span>                                                    </span><br><span class="line"></span><br><span class="line">In [<span class="number">72</span>]: a                                                             </span><br><span class="line">Out[<span class="number">72</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>往字典里添加元素时，是key和value对应增加的。</p></blockquote><h5 id="修改"><a href="#修改" class="headerlink" title="修改"></a><code>修改</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">73</span>]: a                                                             </span><br><span class="line">Out[<span class="number">73</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">74</span>]: a[<span class="string">'a'</span>] = <span class="string">'w'</span>                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">75</span>]: a                                                             </span><br><span class="line">Out[<span class="number">75</span>]: &#123;<span class="string">'a'</span>: <span class="string">'w'</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>修改字典是通过key取出value，然后对应的去重新赋值。</p></blockquote><h4 id="字典的增删改查"><a href="#字典的增删改查" class="headerlink" title="字典的增删改查"></a>字典的增删改查</h4><h5 id="增加-1"><a href="#增加-1" class="headerlink" title="增加"></a>增加</h5><p><code>copy</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">76</span>]: a                                                             </span><br><span class="line">Out[<span class="number">76</span>]: &#123;<span class="string">'a'</span>: <span class="string">'w'</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">77</span>]: b = a.copy()                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">78</span>]: b                                                             </span><br><span class="line">Out[<span class="number">78</span>]: &#123;<span class="string">'a'</span>: <span class="string">'w'</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>复制成一个新字典。</p></blockquote><p><code>fromkeys</code></p><p>查看fromkeys的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">79</span>]: help(a.fromkeys)</span><br></pre></td></tr></table></figure><blockquote><p>fromkeys(iterable, value=None, /) method of builtins.type instance Returns a new dict with keys from iterable and values equal to value.</p><p>注意： 返回一个新的dict，其中包含来自iterable的键，值等于value。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">93</span>]: a                                                             </span><br><span class="line">Out[<span class="number">93</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">94</span>]: s = a.fromkeys([<span class="string">'c'</span>, <span class="string">'d'</span>])                                    </span><br><span class="line"></span><br><span class="line">In [<span class="number">95</span>]: s                                                             </span><br><span class="line">Out[<span class="number">95</span>]: &#123;<span class="string">'c'</span>: <span class="keyword">None</span>, <span class="string">'d'</span>: <span class="keyword">None</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">96</span>]: s = a.fromkeys([<span class="string">'c'</span>, <span class="string">'d'</span>], <span class="number">7</span>)                                 </span><br><span class="line"></span><br><span class="line">In [<span class="number">97</span>]: s                                                             </span><br><span class="line">Out[<span class="number">97</span>]: &#123;<span class="string">'c'</span>: <span class="number">7</span>, <span class="string">'d'</span>: <span class="number">7</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>使用fromkey方法的时候，原字典是不变的，会返回一个新的字典。</p></blockquote><p><code>setfefault</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">103</span>]: a                                                            </span><br><span class="line">Out[<span class="number">103</span>]: &#123;<span class="string">'a'</span>: <span class="string">'w'</span>, <span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">104</span>]: a.setdefault(<span class="string">'a'</span>)                                            </span><br><span class="line">Out[<span class="number">104</span>]: <span class="string">'w'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">105</span>]: a.setdefault(<span class="string">'b'</span>)                                            </span><br><span class="line">Out[<span class="number">105</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">106</span>]: a.setdefault(<span class="string">'c'</span>)                                            </span><br><span class="line"></span><br><span class="line">In [<span class="number">107</span>]: a                                                            </span><br><span class="line">Out[<span class="number">107</span>]: &#123;<span class="string">'a'</span>: <span class="string">'w'</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="keyword">None</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">108</span>]: a.setdefault(<span class="string">'d'</span>, <span class="number">4</span>)                                         </span><br><span class="line">Out[<span class="number">108</span>]: <span class="number">4</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">109</span>]: a                                                            </span><br><span class="line">Out[<span class="number">109</span>]: &#123;<span class="string">'a'</span>: <span class="string">'w'</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="keyword">None</span>, <span class="string">'d'</span>: <span class="number">4</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>查询并返回key所对应的值，如果没有这个key,则会新建。有则查，无则增。</p></blockquote><h5 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h5><p><code>clear</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">110</span>]: a                                                            </span><br><span class="line">Out[<span class="number">110</span>]: &#123;<span class="string">'a'</span>: <span class="string">'w'</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="keyword">None</span>, <span class="string">'d'</span>: <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">111</span>]: a.clear()                                                    </span><br><span class="line"></span><br><span class="line">In [<span class="number">112</span>]: a                                                            </span><br><span class="line">Out[<span class="number">112</span>]: &#123;&#125;</span><br></pre></td></tr></table></figure><blockquote><p>删除所有键值对</p></blockquote><p><code>pop</code></p><p>查看pop 的方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">121</span>]: help(a.pop)</span><br></pre></td></tr></table></figure><blockquote><p>pop(…) method of builtins.dict instance<br>​ D.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value.<br>​ If key is not found, d is returned if given, otherwise KeyError is raised</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">116</span>]: a                                                            </span><br><span class="line">Out[<span class="number">116</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'d'</span>: <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">117</span>]: a.pop(<span class="string">'a'</span>)                                                   </span><br><span class="line">Out[<span class="number">117</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">118</span>]: a                                                            </span><br><span class="line">Out[<span class="number">118</span>]: &#123;<span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'d'</span>: <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">119</span>]: a.pop(<span class="string">'d'</span>)                                                   </span><br><span class="line">Out[<span class="number">119</span>]: <span class="number">4</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">120</span>]: a.pop()                                                      </span><br><span class="line">-----------------------------------------------------------------------</span><br><span class="line">TypeError                             Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-120</span><span class="number">-9</span>c070c907602&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">----&gt; 1 a.pop()</span><br><span class="line"></span><br><span class="line">TypeError: pop expected at least <span class="number">1</span> arguments, got <span class="number">0</span></span><br></pre></td></tr></table></figure><blockquote><p>pop方法是删除指定的键并返回相应的值。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">134</span>]: a                                                            </span><br><span class="line">Out[<span class="number">134</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'d'</span>: <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">135</span>]: a.pop(<span class="string">'c'</span>, <span class="string">'b'</span>)                                              </span><br><span class="line">Out[<span class="number">135</span>]: <span class="string">'b'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">136</span>]: a.pop(<span class="string">'a'</span>, <span class="string">'b'</span>)                                              </span><br><span class="line">Out[<span class="number">136</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">137</span>]: a                                                            </span><br><span class="line">Out[<span class="number">137</span>]: &#123;<span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'d'</span>: <span class="number">4</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>如果传入两个值，第一个是key，第二个是一个值，如果找到key, 就删除对应键值对，并返回该值，如果没有找到key,就返回你所传入的第二个值。</p></blockquote><p><code>popitem</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">140</span>]: a                                                            </span><br><span class="line">Out[<span class="number">140</span>]: &#123;<span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'d'</span>: <span class="number">4</span>, <span class="string">'s'</span>: <span class="number">6</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">141</span>]: a.popitem()                                                  </span><br><span class="line">Out[<span class="number">141</span>]: (<span class="string">'s'</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">142</span>]: a.popitem()                                                  </span><br><span class="line">Out[<span class="number">142</span>]: (<span class="string">'d'</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">143</span>]: a                                                            </span><br><span class="line">Out[<span class="number">143</span>]: &#123;<span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>由于字典也是无序的，多以popitem是随机删除一个键值对。</p></blockquote><h5 id="修改-1"><a href="#修改-1" class="headerlink" title="修改"></a>修改</h5><p><code>update</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">145</span>]: a                                                            </span><br><span class="line">Out[<span class="number">145</span>]: &#123;<span class="string">'b'</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">146</span>]: a.update(&#123;<span class="string">'a'</span>:<span class="number">1</span>, <span class="string">'c'</span>:<span class="number">4</span>, <span class="string">'s'</span>: <span class="number">9</span>&#125;)                             </span><br><span class="line"></span><br><span class="line">In [<span class="number">147</span>]: a                                                            </span><br><span class="line">Out[<span class="number">147</span>]: &#123;<span class="string">'a'</span>: <span class="number">1</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">4</span>, <span class="string">'s'</span>: <span class="number">9</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">148</span>]: a.update(&#123;<span class="string">'a'</span>:<span class="number">0</span>&#125;)                                            </span><br><span class="line"></span><br><span class="line">In [<span class="number">149</span>]: a                                                            </span><br><span class="line">Out[<span class="number">149</span>]: &#123;<span class="string">'a'</span>: <span class="number">0</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">4</span>, <span class="string">'s'</span>: <span class="number">9</span>&#125;</span><br></pre></td></tr></table></figure><blockquote><p>update方法，对于键值对的处理是，有则改，无则增。</p></blockquote><h5 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h5><p><code>get</code></p><p>查看get的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">155</span>]: help(a.get)</span><br></pre></td></tr></table></figure><blockquote><p>get(…) method of builtins.dict instance<br>​ D.get(k[,d]) -&gt; D[k] if k in D, else d. d defaults to None.</p><p>注意：默认返回None</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">151</span>]: a                                                            </span><br><span class="line">Out[<span class="number">151</span>]: &#123;<span class="string">'a'</span>: <span class="number">0</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">4</span>, <span class="string">'s'</span>: <span class="number">9</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">152</span>]: a.get(<span class="string">'a'</span>)                                                   </span><br><span class="line">Out[<span class="number">152</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">153</span>]: a.get(<span class="string">'f'</span>)                                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">154</span>]: a.get(<span class="string">'f'</span>, <span class="string">"没有"</span>)                                           </span><br><span class="line">Out[<span class="number">154</span>]: <span class="string">'没有'</span></span><br></pre></td></tr></table></figure><blockquote><p>get方法是如果查询到key就返回对应的value，如果没有，就返回你给定的提示值。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">156</span>]: c,d = a.get(<span class="string">'f'</span>, (<span class="number">2</span>,<span class="number">3</span>))                                      </span><br><span class="line"></span><br><span class="line">In [<span class="number">157</span>]: c                                                            </span><br><span class="line">Out[<span class="number">157</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">158</span>]: d                                                            </span><br><span class="line">Out[<span class="number">158</span>]: <span class="number">3</span></span><br></pre></td></tr></table></figure><blockquote><p>也可以通过这个功能，做些操作。</p></blockquote><p><code>keys</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">159</span>]: a                                                            </span><br><span class="line">Out[<span class="number">159</span>]: &#123;<span class="string">'a'</span>: <span class="number">0</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">4</span>, <span class="string">'s'</span>: <span class="number">9</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">160</span>]: a.keys()                                                     </span><br><span class="line">Out[<span class="number">160</span>]: dict_keys([<span class="string">'s'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'b'</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">161</span>]: list(a.keys())                                               </span><br><span class="line">Out[<span class="number">161</span>]: [<span class="string">'s'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>, <span class="string">'b'</span>]</span><br></pre></td></tr></table></figure><blockquote><p>获取字典里所有的key。</p></blockquote><p><code>value</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">162</span>]: a                                                            </span><br><span class="line">Out[<span class="number">162</span>]: &#123;<span class="string">'a'</span>: <span class="number">0</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">4</span>, <span class="string">'s'</span>: <span class="number">9</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">163</span>]: a.values()                                                   </span><br><span class="line">Out[<span class="number">163</span>]: dict_values([<span class="number">9</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">164</span>]: list(a.values())                                             </span><br><span class="line">Out[<span class="number">164</span>]: [<span class="number">9</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure><blockquote><p>获取所有的value。</p></blockquote><p><code>items</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">165</span>]: a                                                            </span><br><span class="line">Out[<span class="number">165</span>]: &#123;<span class="string">'a'</span>: <span class="number">0</span>, <span class="string">'b'</span>: <span class="number">2</span>, <span class="string">'c'</span>: <span class="number">4</span>, <span class="string">'s'</span>: <span class="number">9</span>&#125;</span><br><span class="line"></span><br><span class="line">In [<span class="number">166</span>]: a.items()                                                    </span><br><span class="line">Out[<span class="number">166</span>]: dict_items([(<span class="string">'s'</span>, <span class="number">9</span>), (<span class="string">'a'</span>, <span class="number">0</span>), (<span class="string">'c'</span>, <span class="number">4</span>), (<span class="string">'b'</span>, <span class="number">2</span>)])</span><br><span class="line"></span><br><span class="line">In [<span class="number">167</span>]: list(a.items())                                              </span><br><span class="line">Out[<span class="number">167</span>]: [(<span class="string">'s'</span>, <span class="number">9</span>), (<span class="string">'a'</span>, <span class="number">0</span>), (<span class="string">'c'</span>, <span class="number">4</span>), (<span class="string">'b'</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure><blockquote><p>获取所有的键值对。</p></blockquote><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><blockquote><ul><li>键（key）唯一性： 字典中的键（key）具有唯一性，不存在两个相同的键（key）</li><li>可变性： 字典是可变对象，但是自动减的键（key）必须是不可变对象</li><li>无序性：字典中的键也是无序的，所以不能通过索引取值。</li></ul></blockquote><h3 id="运算符及优先级"><a href="#运算符及优先级" class="headerlink" title="运算符及优先级"></a>运算符及优先级</h3><h4 id="Python中的运算符"><a href="#Python中的运算符" class="headerlink" title="Python中的运算符"></a>Python中的运算符</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-11/41589190.jpg" alt="1"></p><h4 id="演示："><a href="#演示：" class="headerlink" title="演示："></a>演示：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">168</span>]: <span class="number">2</span> **<span class="number">3</span>                                                        </span><br><span class="line">Out[<span class="number">168</span>]: <span class="number">8</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">169</span>]: <span class="number">2</span>+<span class="number">2</span>                                                          </span><br><span class="line">Out[<span class="number">169</span>]: <span class="number">4</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">170</span>]: <span class="number">2</span><span class="number">-1</span>                                                          </span><br><span class="line">Out[<span class="number">170</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">171</span>]: <span class="number">2</span>&lt;<span class="number">2</span>                                                          </span><br><span class="line">Out[<span class="number">171</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">172</span>]: <span class="number">2</span>&gt;<span class="number">20</span>                                                         </span><br><span class="line">Out[<span class="number">172</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">173</span>]: <span class="number">3</span>&lt;=(<span class="number">1</span>+<span class="number">2</span>)                                                     </span><br><span class="line">Out[<span class="number">173</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">174</span>]: <span class="number">5</span>&gt;=<span class="number">1</span>                                                         </span><br><span class="line">Out[<span class="number">174</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">175</span>]: <span class="number">2</span>==<span class="number">2</span>                                                         </span><br><span class="line">Out[<span class="number">175</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">176</span>]: <span class="number">2</span>!=<span class="number">2</span>                                                         </span><br><span class="line">Out[<span class="number">176</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">177</span>]: a =<span class="number">1</span> </span><br><span class="line">In [<span class="number">181</span>]: <span class="number">8</span> %<span class="number">2</span>                                                         </span><br><span class="line">Out[<span class="number">181</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">182</span>]: a                                                            </span><br><span class="line">Out[<span class="number">182</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">183</span>]: a += <span class="number">1</span>                                                       </span><br><span class="line"></span><br><span class="line">In [<span class="number">184</span>]: a                                                            </span><br><span class="line">Out[<span class="number">184</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">185</span>]: a /= <span class="number">1.2</span>                                                     </span><br><span class="line"></span><br><span class="line">In [<span class="number">186</span>]: a                                                            </span><br><span class="line">Out[<span class="number">186</span>]: <span class="number">1.6666666666666667</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">187</span>]: a %= <span class="number">1</span>                                                       </span><br><span class="line"></span><br><span class="line">In [<span class="number">188</span>]: a                                                            </span><br><span class="line">Out[<span class="number">188</span>]: <span class="number">0.6666666666666667</span></span><br><span class="line">    </span><br><span class="line">In [<span class="number">189</span>]: a = <span class="number">1</span>                                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">190</span>]: b = a                                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">191</span>]: a <span class="keyword">is</span> b  <span class="comment"># 判断是否是id一致                                                      </span></span><br><span class="line">Out[<span class="number">191</span>]: <span class="keyword">True</span></span><br><span class="line">    </span><br><span class="line">In [<span class="number">192</span>]: <span class="number">1</span> <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>]                                                  </span><br><span class="line">Out[<span class="number">192</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">193</span>]: <span class="number">1</span> <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>]                                              </span><br><span class="line">Out[<span class="number">193</span>]: <span class="keyword">False</span></span><br></pre></td></tr></table></figure><h4 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h4><h5 id="查看对象类型"><a href="#查看对象类型" class="headerlink" title="查看对象类型"></a>查看对象类型</h5><p><code>type</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">204</span>]: a = <span class="number">1</span>                                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">205</span>]: b = <span class="string">'s'</span>                                                      </span><br><span class="line"></span><br><span class="line">In [<span class="number">206</span>]: c = [<span class="number">1</span>, <span class="number">2</span>]                                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">207</span>]: type(a), type(b), type(c)                                    </span><br><span class="line">Out[<span class="number">207</span>]: (int, str, list)</span><br></pre></td></tr></table></figure><blockquote><p>直接返回对象的类型</p></blockquote><p><code>isinstance</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">208</span>]: a                                                            </span><br><span class="line">Out[<span class="number">208</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">209</span>]: b                                                            </span><br><span class="line">Out[<span class="number">209</span>]: <span class="string">'s'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">210</span>]: c                                                            </span><br><span class="line">Out[<span class="number">210</span>]: [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">211</span>]: isinstance(a, int)                                           </span><br><span class="line">Out[<span class="number">211</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">212</span>]: isinstance(a, str)                                           </span><br><span class="line">Out[<span class="number">212</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">213</span>]: isinstance(b, str)                                           </span><br><span class="line">Out[<span class="number">213</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure><blockquote><p>判断对象的类型</p></blockquote><h5 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">216</span>]: a = <span class="number">1</span>                                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">217</span>]: b = <span class="number">2</span>                                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">218</span>]: c = <span class="number">1</span>                                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">219</span>]: a == b                                                       </span><br><span class="line">Out[<span class="number">219</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">220</span>]: a == c                                                       </span><br><span class="line">Out[<span class="number">220</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">221</span>]: b != c                                                       </span><br><span class="line">Out[<span class="number">221</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure><h5 id="如果有多个条件"><a href="#如果有多个条件" class="headerlink" title="如果有多个条件"></a>如果有多个条件</h5><blockquote><ul><li>判断语句1 and 判断语句2</li><li>判断语句1 or 判断语句2</li><li>not 判断语句1</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">227</span>]: a==b <span class="keyword">and</span> b!=c                                                </span><br><span class="line">Out[<span class="number">227</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">228</span>]: a==b <span class="keyword">or</span> b!=c                                                 </span><br><span class="line">Out[<span class="number">228</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">229</span>]: <span class="keyword">not</span> a==b                                                     </span><br><span class="line">Out[<span class="number">229</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python基础 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python第三话之格式化输出和深浅复制</title>
      <link href="/2018/12/09/2018-12-8-python-output/"/>
      <url>/2018/12/09/2018-12-8-python-output/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>[TOC]</p><h3 id="格式化输出和深浅拷贝"><a href="#格式化输出和深浅拷贝" class="headerlink" title="格式化输出和深浅拷贝"></a>格式化输出和深浅拷贝</h3><p>前面我熟悉了<code>列表</code>、<code>字符串</code>、<code>元组</code>的常用操作以及对应的<code>增删改查</code>，今天我们来深入了解<code>格式化输出</code>和<code>深浅复制</code>相关的知识点。</p><h3 id="格式化输出"><a href="#格式化输出" class="headerlink" title="格式化输出"></a>格式化输出</h3><h4 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h4><h5 id="s"><a href="#s" class="headerlink" title="%s"></a><code>%s</code></h5><blockquote><p>%字符串</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: a = <span class="string">'hello'</span>                                                    </span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: b = <span class="string">'python '</span>                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="string">"%s %s"</span> %(a, b)                                                </span><br><span class="line">Out[<span class="number">3</span>]: <span class="string">'hello python '</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: c = <span class="number">123</span>                                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="string">"%s %s"</span> %(a, c)                                                </span><br><span class="line">Out[<span class="number">5</span>]: <span class="string">'hello 123'</span></span><br></pre></td></tr></table></figure><h5 id="d"><a href="#d" class="headerlink" title="%d"></a><code>%d</code></h5><blockquote><p>%数字</p><p>注意：只能传入数字</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">6</span>]: a = <span class="string">'hello'</span>                                                    </span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: b = <span class="string">'python '</span>                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: c = <span class="number">123</span>                                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: <span class="string">"%d %d"</span> %(a, c)                                                </span><br><span class="line">-----------------------------------------------------------------------</span><br><span class="line">TypeError                             Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-9</span><span class="number">-2</span>d6d5e198328&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">----&gt; 1 "%d %d" %(a, c)</span><br><span class="line"></span><br><span class="line">TypeError: %d format: a number <span class="keyword">is</span> required, <span class="keyword">not</span> str</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: <span class="string">"%d %d"</span> %(c, c)                                               </span><br><span class="line">Out[<span class="number">10</span>]: <span class="string">'123 123'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: <span class="string">"%d"</span>%<span class="number">123.34</span>                                                   </span><br><span class="line">Out[<span class="number">11</span>]: <span class="string">'123'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: <span class="string">"%d %d"</span> %(<span class="number">123</span>, <span class="number">123.34</span>)                                        </span><br><span class="line">Out[<span class="number">12</span>]: <span class="string">'123 123'</span></span><br></pre></td></tr></table></figure><h5 id="f"><a href="#f" class="headerlink" title="%f"></a><code>%f</code></h5><blockquote><p>%浮点数</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">14</span>]: <span class="string">'%f'</span>%<span class="number">2.3</span>                                                      </span><br><span class="line">Out[<span class="number">14</span>]: <span class="string">'2.300000'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: <span class="string">'%.2f'</span>%<span class="number">2.3</span>                                                    </span><br><span class="line">Out[<span class="number">15</span>]: <span class="string">'2.30'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: <span class="string">'%.2f'</span>%<span class="number">2.333434</span>                                               </span><br><span class="line">Out[<span class="number">16</span>]: <span class="string">'2.33'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: <span class="string">'%.2f'</span>%<span class="number">2.35</span>                                                   </span><br><span class="line">Out[<span class="number">17</span>]: <span class="string">'2.35'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: <span class="string">'%.2f'</span>%<span class="number">2.36</span>                                                   </span><br><span class="line">Out[<span class="number">18</span>]: <span class="string">'2.36'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: <span class="string">'%.3f'</span>%<span class="number">2.36</span>                                                   </span><br><span class="line">Out[<span class="number">19</span>]: <span class="string">'2.360'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">20</span>]: <span class="string">'%.4f'</span>%<span class="number">2.36</span>                                                   </span><br><span class="line">Out[<span class="number">20</span>]: <span class="string">'2.3600'</span></span><br></pre></td></tr></table></figure><h5 id="c"><a href="#c" class="headerlink" title="%c"></a><code>%c</code></h5><blockquote><p>%ASCII字符</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">21</span>]: <span class="string">'%c'</span>%<span class="number">97</span>                                                       </span><br><span class="line">Out[<span class="number">21</span>]: <span class="string">'a'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: <span class="string">'%c'</span>%<span class="number">65</span>                                                       </span><br><span class="line">Out[<span class="number">22</span>]: <span class="string">'A'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: <span class="string">'%c'</span>%<span class="number">61</span>                                                       </span><br><span class="line">Out[<span class="number">23</span>]: <span class="string">'='</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: <span class="string">'%c'</span>%<span class="number">60</span>                                                       </span><br><span class="line">Out[<span class="number">24</span>]: <span class="string">'&lt;'</span></span><br></pre></td></tr></table></figure><h5 id="o"><a href="#o" class="headerlink" title="%o"></a><code>%o</code></h5><blockquote><p>%8进制</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">25</span>]: <span class="string">'%o'</span>%<span class="number">9</span>                               </span><br><span class="line">Out[<span class="number">25</span>]: <span class="string">'11'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: <span class="string">'%o'</span>%<span class="number">10</span>                              </span><br><span class="line">Out[<span class="number">26</span>]: <span class="string">'12'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: <span class="string">'%o'</span>%<span class="number">107</span>                             </span><br><span class="line">Out[<span class="number">27</span>]: <span class="string">'153'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: <span class="string">'%o'</span>%<span class="number">17</span>                              </span><br><span class="line">Out[<span class="number">28</span>]: <span class="string">'21'</span></span><br></pre></td></tr></table></figure><h5 id="x"><a href="#x" class="headerlink" title="%x"></a><code>%x</code></h5><blockquote><p>%16进制</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">29</span>]: <span class="string">'%x'</span>%<span class="number">16</span>                              </span><br><span class="line">Out[<span class="number">29</span>]: <span class="string">'10'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">30</span>]: <span class="string">'%x'</span>%<span class="number">15</span>                              </span><br><span class="line">Out[<span class="number">30</span>]: <span class="string">'f'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">31</span>]: <span class="string">'%x'</span>%<span class="number">14</span>                              </span><br><span class="line">Out[<span class="number">31</span>]: <span class="string">'e'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: <span class="string">'%x'</span>%<span class="number">10</span>                              </span><br><span class="line">Out[<span class="number">32</span>]: <span class="string">'a'</span></span><br></pre></td></tr></table></figure><h5 id="e"><a href="#e" class="headerlink" title="%e"></a><code>%e</code></h5><blockquote><p>%科学计数法</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">33</span>]: <span class="string">'%e'</span>%<span class="number">0.01</span>                            </span><br><span class="line">Out[<span class="number">33</span>]: <span class="string">'1.000000e-02'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">34</span>]: <span class="string">'%e'</span>%<span class="number">0.001</span>                           </span><br><span class="line">Out[<span class="number">34</span>]: <span class="string">'1.000000e-03'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">35</span>]: <span class="string">'%e'</span>%<span class="number">10</span>                              </span><br><span class="line">Out[<span class="number">35</span>]: <span class="string">'1.000000e+01'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">36</span>]: <span class="string">'%e'</span>%<span class="number">100</span>                             </span><br><span class="line">Out[<span class="number">36</span>]: <span class="string">'1.000000e+02'</span></span><br></pre></td></tr></table></figure><h5 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a><code>扩展</code></h5><p><strong>%r</strong></p><blockquote><p>原始化</p></blockquote><p>演示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">43</span>]: print(<span class="string">'%s'</span>%<span class="string">'123'</span>)                    </span><br><span class="line"><span class="number">123</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">44</span>]: print(<span class="string">'%r'</span>%<span class="string">'123'</span>)                    </span><br><span class="line"><span class="string">'123'</span></span><br></pre></td></tr></table></figure><p><strong>%+6.5f</strong></p><blockquote><p>规定输出的字符串的个数和输出小数的位数</p><p>注意：%6.5f 指的是一个输出6个字符，其中5个小数</p><p>如果：前面带+就是表示输出符号</p><p>​ 前面带-号表示左对齐</p></blockquote><p>演示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">46</span>]: <span class="string">'%5.3f'</span>%<span class="number">1.2</span>                          </span><br><span class="line">Out[<span class="number">46</span>]: <span class="string">'1.200'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: <span class="string">'%6.3f'</span>%<span class="number">1.2</span>                          </span><br><span class="line">Out[<span class="number">47</span>]: <span class="string">' 1.200'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: <span class="string">'%6.3f'</span>%<span class="number">12.34567</span>                     </span><br><span class="line">Out[<span class="number">48</span>]: <span class="string">'12.346'</span></span><br><span class="line">    </span><br><span class="line">In [<span class="number">49</span>]: <span class="string">'%+6.3f'</span>%<span class="number">12.34567</span>                    </span><br><span class="line">Out[<span class="number">49</span>]: <span class="string">'+12.346'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">50</span>]: <span class="string">'%+6.3f'</span>%<span class="number">-12.34567</span>                   </span><br><span class="line">Out[<span class="number">50</span>]: <span class="string">'-12.346'</span></span><br><span class="line">    </span><br><span class="line">In [<span class="number">60</span>]: <span class="string">'%8.2f'</span>%<span class="number">13.3333</span>                      </span><br><span class="line">Out[<span class="number">60</span>]: <span class="string">'   13.33'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">61</span>]: <span class="string">'%-8.2f'</span>%<span class="number">13.3333</span>                     </span><br><span class="line">Out[<span class="number">61</span>]: <span class="string">'13.33   '</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">62</span>]: <span class="string">'%08.2f'</span>%<span class="number">13.3333</span>                     </span><br><span class="line">Out[<span class="number">62</span>]: <span class="string">'00013.33'</span></span><br></pre></td></tr></table></figure><h4 id="Python方法"><a href="#Python方法" class="headerlink" title="Python方法"></a><strong>Python方法</strong></h4><p>在Python中我们一般使用<code>format</code>来进行格式化输出</p><h5 id="format"><a href="#format" class="headerlink" title="format"></a><code>format</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">67</span>]: <span class="string">'&#123;:.2f&#125;'</span>.format(<span class="number">12.333</span>)              </span><br><span class="line">Out[<span class="number">67</span>]: <span class="string">'12.33'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">68</span>]: <span class="string">'&#123;a:.2f&#125;'</span>.format(a=<span class="number">12.333</span>)           </span><br><span class="line">Out[<span class="number">68</span>]: <span class="string">'12.33'</span></span><br></pre></td></tr></table></figure><blockquote><p>保留两位小数</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">70</span>]: <span class="string">'&#123;:.2%&#125;'</span>.format(<span class="number">0.001</span>)               </span><br><span class="line">Out[<span class="number">70</span>]: <span class="string">'0.10%'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">71</span>]: <span class="string">'&#123;:.2%&#125;'</span>.format(<span class="number">0.61</span>)                </span><br><span class="line">Out[<span class="number">71</span>]: <span class="string">'61.00%'</span></span><br></pre></td></tr></table></figure><blockquote><p>百分比格式</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">72</span>]: <span class="string">'&#123;0:x&#125;'</span>.format(<span class="number">20</span>)                   </span><br><span class="line">Out[<span class="number">72</span>]: <span class="string">'14'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">73</span>]: <span class="string">'&#123;0:x&#125;'</span>.format(<span class="number">10</span>)                   </span><br><span class="line">Out[<span class="number">73</span>]: <span class="string">'a'</span></span><br></pre></td></tr></table></figure><blockquote><p>转换成十六进制</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">74</span>]: <span class="string">'&#123;0:o&#125;'</span>.format(<span class="number">20</span>)                   </span><br><span class="line">Out[<span class="number">74</span>]: <span class="string">'24'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">75</span>]: <span class="string">'&#123;0:o&#125;'</span>.format(<span class="number">10</span>)                   </span><br><span class="line">Out[<span class="number">75</span>]: <span class="string">'12'</span></span><br></pre></td></tr></table></figure><blockquote><p>转换成8进制</p></blockquote><blockquote><p>注意：进制转换时使用{0:进制}这个格式</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">76</span>]: <span class="string">'&#123;a:&lt;10&#125;'</span>.format(a=<span class="number">12.3</span>,b=<span class="number">13.44</span>)     </span><br><span class="line">Out[<span class="number">76</span>]: <span class="string">'12.3      '</span></span><br></pre></td></tr></table></figure><blockquote><p>左对齐，长度为10</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">77</span>]: <span class="string">'&#123;a:0&lt;10&#125;'</span>.format(a=<span class="number">12.3</span>,b=<span class="number">13.44</span>)    </span><br><span class="line">    ...:                                      </span><br><span class="line">Out[<span class="number">77</span>]: <span class="string">'12.3000000'</span></span><br></pre></td></tr></table></figure><blockquote><p>数字补x(填充右边，宽度为4)</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">78</span>]: <span class="string">'&#123;a:0&gt;10&#125;'</span>.format(a=<span class="number">12.3</span>,b=<span class="number">13.44</span>)    </span><br><span class="line">Out[<span class="number">78</span>]: <span class="string">'00000012.3'</span></span><br></pre></td></tr></table></figure><blockquote><p>右对齐，长度为10</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">79</span>]: <span class="string">'&#123;a:0^10&#125;'</span>.format(a=<span class="number">12.3</span>,b=<span class="number">13.44</span>)    </span><br><span class="line">Out[<span class="number">79</span>]: <span class="string">'00012.3000'</span></span><br></pre></td></tr></table></figure><blockquote><p>两边对齐， 长度为10</p></blockquote><h3 id="字符串转义"><a href="#字符串转义" class="headerlink" title="字符串转义"></a>字符串转义</h3><p>字符前面加上 \ ，字符就不再表示字符本身的意思，表示ASCII码中不能显示字符，常见有下：</p><h5 id="n"><a href="#n" class="headerlink" title="\n"></a><code>\n</code></h5><blockquote><p>换行</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">83</span>]: print(<span class="string">'abc\nabc'</span>)                    </span><br><span class="line">abc</span><br><span class="line">abc</span><br></pre></td></tr></table></figure><h5 id="t"><a href="#t" class="headerlink" title="\t"></a><code>\t</code></h5><blockquote><p>水平制表符</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">84</span>]: print(<span class="string">'abc\tabc'</span>)                    </span><br><span class="line">abcabc</span><br></pre></td></tr></table></figure><h5 id="b"><a href="#b" class="headerlink" title="\b"></a><code>\b</code></h5><blockquote><p>退格</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">85</span>]: print(<span class="string">'abc\babc'</span>)                    </span><br><span class="line">ababc</span><br></pre></td></tr></table></figure><h5 id="r"><a href="#r" class="headerlink" title="\r"></a><code>\r</code></h5><blockquote><p>回车，当前位置移到本行开头</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">87</span>]: print(<span class="string">'abc\rbc'</span>)                     </span><br><span class="line">bcc</span><br></pre></td></tr></table></figure><h5><a href="#" class="headerlink" title="\"></a><code>\</code></h5><blockquote><p>代表反斜杠 \</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">88</span>]: print(<span class="string">'abc\\bc'</span>)                     </span><br><span class="line">abc\bc</span><br></pre></td></tr></table></figure><h5 id="’"><a href="#’" class="headerlink" title="\’"></a><code>\’</code></h5><blockquote><p>代表一个单引号，同样的 “ ？ 等符号也可以这么输出</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">95</span>]: print(<span class="string">'abc\' \"b c'</span>)                 </span><br><span class="line">abc<span class="string">' "b c</span></span><br></pre></td></tr></table></figure><h5 id="0"><a href="#0" class="headerlink" title="\0"></a><code>\0</code></h5><blockquote><p>代表一个空字符</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">96</span>]: print(<span class="string">'abc\0abc'</span>)                    </span><br><span class="line">abcabc</span><br><span class="line"></span><br><span class="line">In [<span class="number">97</span>]: print(<span class="string">'abc\0\0abc'</span>)                  </span><br><span class="line">abcabc</span><br></pre></td></tr></table></figure><h5 id="a"><a href="#a" class="headerlink" title="\a"></a><code>\a</code></h5><blockquote><p>系统提示音(交互环境需使用print)</p></blockquote><h5 id="取消转义"><a href="#取消转义" class="headerlink" title="取消转义"></a><code>取消转义</code></h5><blockquote><p>在python中如果要去掉字符串的转义，只需要在字符串前面加上 r</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">104</span>]: print(<span class="string">r'abc\b\t\nabc'</span>)              </span><br><span class="line">abc\b\t\nabc</span><br></pre></td></tr></table></figure><h3 id="格式化和转义的应用"><a href="#格式化和转义的应用" class="headerlink" title="格式化和转义的应用"></a>格式化和转义的应用</h3><p>格式化得到的结果都是字符串，通过把位置预先留出来，后期再往其中填入内容可以让输出内容更加整洁美观，而又具有良好的可读性，同时让代码更简洁精练。</p><p>字符串的转义可以方便我们表示我们不太方便表示的字符，同时转义有些情况下又会带来麻烦，特别是在表示路径的时候，这种情况下可以在字符串前面加上 r 来去掉字符串的转义。</p><h3 id="字符串编码"><a href="#字符串编码" class="headerlink" title="字符串编码"></a>字符串编码</h3><p>对于编码这部分，我们先举个栗子来看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">105</span>]: <span class="string">'你好'</span>.encode(<span class="string">'utf-8'</span>)                  </span><br><span class="line">Out[<span class="number">105</span>]: <span class="string">b'\xe4\xbd\xa0\xe5\xa5\xbd'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">106</span>]: <span class="string">'你好'</span>.encode(<span class="string">'gbk'</span>)                    </span><br><span class="line">Out[<span class="number">106</span>]: <span class="string">b'\xc4\xe3\xba\xc3'</span></span><br></pre></td></tr></table></figure><p>我们可以通过不同的编码方式来进行编码以便我们在不同情况下使用，接下来我们来看看编码相关的知识点</p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-8/6179178.jpg" alt="1"></p><h4 id="Python对于字符集的处理"><a href="#Python对于字符集的处理" class="headerlink" title="Python对于字符集的处理"></a>Python对于字符集的处理</h4><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-8/45812485.jpg" alt=""></p><h4 id="字符编码的作用"><a href="#字符编码的作用" class="headerlink" title="字符编码的作用"></a>字符编码的作用</h4><p>Python统一了编码，这样Python在内部处理的时候不会因编码不同而出现程序不能正常执行的问题。</p><p>Python会自动根据系统环境选择编码，但是经常在文件传输的过程中，会遇到各种不同的编码，这个时候就需要我们去处理编码问题。</p><h3 id="深浅复制"><a href="#深浅复制" class="headerlink" title="深浅复制"></a>深浅复制</h3><p>举个栗子看看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">107</span>]: s = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]                           </span><br><span class="line"></span><br><span class="line">In [<span class="number">108</span>]: s2 = [<span class="string">'a'</span>, s]                           </span><br><span class="line"></span><br><span class="line">In [<span class="number">109</span>]: s2                                      </span><br><span class="line">Out[<span class="number">109</span>]: [<span class="string">'a'</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">111</span>]: id(s), id(s2), id(s2[<span class="number">1</span>])                                     </span><br><span class="line">Out[<span class="number">111</span>]: (<span class="number">139956880001800</span>, <span class="number">139956880021256</span>, <span class="number">139956880001800</span>)</span><br></pre></td></tr></table></figure><blockquote><p>我们可以看出来，s2只是引用了s的值，s2[1]的值会随着s的变化而变化。</p></blockquote><h4 id="浅复制"><a href="#浅复制" class="headerlink" title="浅复制"></a>浅复制</h4><p>举个栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">136</span>]: s = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">137</span>]: s2 = [<span class="string">'a'</span>, s]                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">138</span>]: s3 = s2.copy()                                               </span><br><span class="line"></span><br><span class="line">In [<span class="number">139</span>]: s[<span class="number">0</span>]=<span class="string">'w'</span>                                                     </span><br><span class="line"></span><br><span class="line">In [<span class="number">140</span>]: s2                                                           </span><br><span class="line">Out[<span class="number">140</span>]: [<span class="string">'a'</span>, [<span class="string">'w'</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">141</span>]: s3                                                           </span><br><span class="line">Out[<span class="number">141</span>]: [<span class="string">'a'</span>, [<span class="string">'w'</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">149</span>]: id(s2[<span class="number">1</span>]), id(s3[<span class="number">1</span>]),id(s)                                   </span><br><span class="line">Out[<span class="number">149</span>]: (<span class="number">139956883394376</span>, <span class="number">139956883394376</span>, <span class="number">139956883394376</span>)</span><br></pre></td></tr></table></figure><blockquote><p>由上面的栗子可以看出来，虽然s3是copy的，但是s3还是会随着s的变化来变化的。那我们有什么办法让s3的值不变呢?那就是深拷贝。</p></blockquote><h4 id="深复制"><a href="#深复制" class="headerlink" title="深复制"></a>深复制</h4><p>举个栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">150</span>]: s = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">151</span>]: s2 = [<span class="string">'a'</span>, s]                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">152</span>]: <span class="keyword">import</span> copy                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">153</span>]: s3 = copy.deepcopy(s2)                                       </span><br><span class="line"></span><br><span class="line">In [<span class="number">154</span>]: s2                                                           </span><br><span class="line">Out[<span class="number">154</span>]: [<span class="string">'a'</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">155</span>]: s3                                                           </span><br><span class="line">Out[<span class="number">155</span>]: [<span class="string">'a'</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">156</span>]: s[<span class="number">0</span>]=<span class="string">'r'</span>                                                     </span><br><span class="line"></span><br><span class="line">In [<span class="number">157</span>]: s2                                                           </span><br><span class="line">Out[<span class="number">157</span>]: [<span class="string">'a'</span>, [<span class="string">'r'</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">158</span>]: s3                                                           </span><br><span class="line">Out[<span class="number">158</span>]: [<span class="string">'a'</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">159</span>]: id(s),id(s2[<span class="number">1</span>]),id(s3[<span class="number">1</span>])                                    </span><br><span class="line">Out[<span class="number">159</span>]: (<span class="number">139956879794952</span>, <span class="number">139956879794952</span>, <span class="number">139956879278408</span>)</span><br></pre></td></tr></table></figure><blockquote><p>通过深拷贝，我们就实现了，原数据改变的时候，复制的数据不会随着改变。</p></blockquote><h4 id="深浅复制的应用"><a href="#深浅复制的应用" class="headerlink" title="深浅复制的应用"></a>深浅复制的应用</h4><p>深浅复制只有在<code>列表嵌套列表</code>的情况下讨论。</p><p>如果想保留修改之前的数据，就可以使用列表的复制，但要注意列表嵌套情况下的问题。</p><h3 id="bytes和bytearray"><a href="#bytes和bytearray" class="headerlink" title="bytes和bytearray"></a>bytes和bytearray</h3><h4 id="bytes二进制序列类型"><a href="#bytes二进制序列类型" class="headerlink" title="bytes二进制序列类型"></a>bytes二进制序列类型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">169</span>]: a= bytes(<span class="number">3</span>)                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">170</span>]: a[<span class="number">0</span>]                                                         </span><br><span class="line">Out[<span class="number">170</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">171</span>]: a[<span class="number">1</span>]                                                         </span><br><span class="line">Out[<span class="number">171</span>]: <span class="number">0</span></span><br></pre></td></tr></table></figure><blockquote><p>使用bytes(number)指定长度的零填充字节生成一个二进制的序列类型</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">172</span>]: bytes(<span class="string">b'abc'</span>)                                                </span><br><span class="line">Out[<span class="number">172</span>]: <span class="string">b'abc'</span></span><br></pre></td></tr></table></figure><blockquote><p>二进制字符串</p></blockquote><h4 id="bytearray二进制数组"><a href="#bytearray二进制数组" class="headerlink" title="bytearray二进制数组"></a>bytearray二进制数组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">173</span>]: a = bytearray(<span class="number">3</span>)                                             </span><br><span class="line"></span><br><span class="line">In [<span class="number">174</span>]: a                                                            </span><br><span class="line">Out[<span class="number">174</span>]: bytearray(<span class="string">b'\x00\x00\x00'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">175</span>]: a[<span class="number">1</span>]                                                         </span><br><span class="line">Out[<span class="number">175</span>]: <span class="number">0</span></span><br></pre></td></tr></table></figure><blockquote><p>使用bytearray(number)指定长度的零填充字节生成一个二进制的数组</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">176</span>]: bytearray(<span class="string">b'abc'</span>)                                            </span><br><span class="line">Out[<span class="number">176</span>]: bytearray(<span class="string">b'abc'</span>)</span><br></pre></td></tr></table></figure><blockquote><p>二进制字符串</p></blockquote><h4 id="二进制序列类型的应用"><a href="#二进制序列类型的应用" class="headerlink" title="二进制序列类型的应用"></a>二进制序列类型的应用</h4><p>二进制序列类型的用法比较少见，是python中少用的一种序列类型。</p><p>对于二进制序列类型，大家基本了解即可。</p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python基础 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python第二话之序列类型方法</title>
      <link href="/2018/12/08/2018-12-8-python-sequence/"/>
      <url>/2018/12/08/2018-12-8-python-sequence/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>[TOC]</p><h3 id="序列类型方法"><a href="#序列类型方法" class="headerlink" title="序列类型方法"></a>序列类型方法</h3><p>前面我们简单的介绍了什么是序列类型，并且简单的给大家提了下<code>字符串</code>、<code>元组</code>、<code>列表</code>的一些基本操作。接下来我们详细的来看下关于<code>字符串</code>、<code>元组</code>、<code>列表</code>的常用操作。</p><h4 id="列表常用方法"><a href="#列表常用方法" class="headerlink" title="列表常用方法"></a>列表常用方法</h4><p>我们先查看下<code>list</code>可以使用的一些方法：</p><blockquote><p>append(…)<br>| L.append(object) -&gt; None – append object to end<br>|<br>| clear(…)<br>| L.clear() -&gt; None – remove all items from L<br>|<br>| copy(…)<br>| L.copy() -&gt; list – a shallow copy of L<br>|<br>| count(…)<br>| L.count(value) -&gt; integer – return number of occurrences of value<br>|<br>| extend(…)<br>| L.extend(iterable) -&gt; None – extend list by appending elements from the iterable<br>|<br>| index(…)<br>| L.index(value, [start, [stop]]) -&gt; integer – return first index of value.<br>| Raises ValueError if the value is not present.<br>|<br>| insert(…)<br>| L.insert(index, object) – insert object before index<br>|<br>| pop(…)<br>| L.pop([index]) -&gt; item – remove and return item at index (default last).<br>| Raises IndexError if list is empty or index is out of range.<br>|<br>| remove(…)<br>| L.remove(value) -&gt; None – remove first occurrence of value.<br>| Raises ValueError if the value is not present.<br>|<br>| reverse(…)<br>| L.reverse() – reverse <em>IN PLACE</em><br>|<br>| sort(…)<br>| L.sort(key=None, reverse=False) -&gt; None – stable sort *IN PLAC</p></blockquote><p>接下来我们给大家分类讲解一下这样操作方法。</p><h5 id="增加元素"><a href="#增加元素" class="headerlink" title="增加元素"></a><code>增加元素</code></h5><p><strong>append</strong></p><p>查看下<code>append</code>这个方法的用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]                                    </span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: help(s.append)</span><br></pre></td></tr></table></figure><blockquote><p>append(…) method of builtins.list instance<br>​ L.append(object) -&gt; None – append object to end</p><p>使用方式：L.append(‘s’)</p><p>注意：append方法是在列表的最后添加元素</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">3</span>]: s.append(<span class="string">'w'</span>)                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: s                                                              </span><br><span class="line">Out[<span class="number">4</span>]: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'w'</span>]</span><br></pre></td></tr></table></figure><p><strong>insert</strong></p><p>查看<code>insert</code>的使用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">5</span>]: help(s.insert)</span><br></pre></td></tr></table></figure><blockquote><p>insert(…) method of builtins.list instance<br>​ L.insert(index, object) – insert object before index</p><p>使用方法： L.insert(index,’s’)</p><p>注意：index: 索引值，s为元素</p><p>insert方法是在指定的索引值处添加元素</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">6</span>]: s.insert(<span class="number">3</span>,<span class="string">'s'</span>)                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: s                                                              </span><br><span class="line">Out[<span class="number">7</span>]: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">'s'</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'w'</span>]</span><br></pre></td></tr></table></figure><p><strong>extend</strong></p><p>查看<code>extend</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">8</span>]: help(s.extend)</span><br></pre></td></tr></table></figure><blockquote><p>extend(…) method of builtins.list instance<br>​ L.extend(iterable) -&gt; None – extend list by appending elements from the iterable</p><p>使用方法：L.extend([1,2,’a’])</p><p>注意：extend方法是指在列表中追加可迭代器</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">9</span>]: a = [<span class="string">'w'</span>,<span class="string">'o'</span>,<span class="number">0</span>]                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: s.extend(a)                                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: s                                                             </span><br><span class="line">Out[<span class="number">11</span>]: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">'s'</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'w'</span>, <span class="string">'w'</span>, <span class="string">'o'</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><h5 id="删除元素"><a href="#删除元素" class="headerlink" title="删除元素"></a><code>删除元素</code></h5><p><strong>clear</strong></p><p>查看下<code>clear</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">14</span>]: help(s.clear)</span><br></pre></td></tr></table></figure><blockquote><p>clear(…) method of builtins.list instance<br>​ L.clear() -&gt; None – remove all items from L</p><p>使用方法：L.clear()</p><p>注意：clear是删除所有的元素</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">20</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">21</span>]: s.clear()                                                     </span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: s                                                             </span><br><span class="line">Out[<span class="number">22</span>]: []</span><br></pre></td></tr></table></figure><p><strong>pop</strong></p><p>查看<code>pop</code>的使用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">23</span>]: help(s.pop)</span><br></pre></td></tr></table></figure><blockquote><p>pop(…) method of builtins.list instance<br>​ L.pop([index]) -&gt; item – remove and return item at index (default last).<br>​ Raises IndexError if list is empty or index is out of range.</p><p>使用方法：L.pop()、L.pop(index)</p><p>注意：L.pop()指的是删除最后一个元素</p><p>​ L.pop(index)指的是指定索引值删除元素</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">24</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">25</span>]: s.pop()                                                       </span><br><span class="line">Out[<span class="number">25</span>]: <span class="string">'c'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: s.pop()                                                       </span><br><span class="line">Out[<span class="number">26</span>]: <span class="string">'b'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: s.pop()                                                       </span><br><span class="line">Out[<span class="number">27</span>]: <span class="string">'a'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: s.pop()                                                       </span><br><span class="line">Out[<span class="number">28</span>]: <span class="number">5</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: s.pop(<span class="number">1</span>)                                                      </span><br><span class="line">Out[<span class="number">29</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">30</span>]: s                                                             </span><br><span class="line">Out[<span class="number">30</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure><p><strong>remove</strong></p><p>查看<code>remove</code>的使用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">35</span>]: help(s.remove)</span><br></pre></td></tr></table></figure><blockquote><p>remove(…) method of builtins.list instance<br>​ L.remove(value) -&gt; None – remove first occurrence of value.<br>​ Raises ValueError if the value is not present.</p><p>使用方法：L.remove(obj)</p><p>注意：移除指定元素从左边开始的第一个。</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">56</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]                             </span><br><span class="line"></span><br><span class="line">In [<span class="number">57</span>]: s.remove(<span class="number">2</span>)                                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">58</span>]: s                                                             </span><br><span class="line">Out[<span class="number">58</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">59</span>]: s.remove(<span class="number">2</span>)                                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">60</span>]: s                                                             </span><br><span class="line">Out[<span class="number">60</span>]: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="number">1</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure><h5 id="修改元素"><a href="#修改元素" class="headerlink" title="修改元素"></a><code>修改元素</code></h5><p>修改元素在列表中就非常简单了。</p><blockquote><p>使用方法：L[index] = obj</p><p>注意：修改元素是根据元素的索引来修改。</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">61</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]                                               </span><br><span class="line"></span><br><span class="line">In [<span class="number">62</span>]: s[<span class="number">3</span>]=<span class="string">'s'</span>                                                      </span><br><span class="line"></span><br><span class="line">In [<span class="number">63</span>]: s                                                             </span><br><span class="line">Out[<span class="number">63</span>]: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="string">'s'</span>, <span class="number">5</span>]</span><br></pre></td></tr></table></figure><h5 id="查找元素"><a href="#查找元素" class="headerlink" title="查找元素"></a><code>查找元素</code></h5><p><strong>index</strong></p><p>查看index的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">64</span>]: help(s.index)</span><br></pre></td></tr></table></figure><blockquote><p>index(…) method of builtins.list instance<br>​ L.index(value, [start, [stop]]) -&gt; integer – return first index of value.<br>​ Raises ValueError if the value is not present.</p><p>使用方法：L.index(obj) , L.index(value, [start, [stop]])</p><p>注意：L.index(obj) 从列表中找某个值第一个匹配项的索引位置。</p><p>​ L.index(value, [start, [stop]])指定索引范围查找元素</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">71</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]                                               </span><br><span class="line"></span><br><span class="line">In [<span class="number">72</span>]: s.index(<span class="number">1</span>)                                                    </span><br><span class="line">Out[<span class="number">72</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">73</span>]: s.index(<span class="number">3</span>)                                                    </span><br><span class="line">Out[<span class="number">73</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">98</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]                             </span><br><span class="line"></span><br><span class="line">In [<span class="number">99</span>]: s.index(<span class="number">2</span>,<span class="number">2</span>)                                                  </span><br><span class="line">Out[<span class="number">99</span>]: <span class="number">9</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">100</span>]: s.index(<span class="number">2</span>)                                                   </span><br><span class="line">Out[<span class="number">100</span>]: <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>count</strong></p><p>查看<code>count</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">74</span>]: help(s.count)</span><br></pre></td></tr></table></figure><blockquote><p>count(…) method of builtins.list instance<br>​ L.count(value) -&gt; integer – return number of occurrences of value</p><p>使用方法：L.count(obj)</p><p>注意：统计某个元素在列表中出现的次数。</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">75</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>]                                         </span><br><span class="line"></span><br><span class="line">In [<span class="number">76</span>]: s.count(<span class="number">2</span>)                                                    </span><br><span class="line">Out[<span class="number">76</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">77</span>]: s.count(<span class="number">3</span>)                                                    </span><br><span class="line">Out[<span class="number">77</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">78</span>]: s.count(<span class="number">1</span>)                                                    </span><br><span class="line">Out[<span class="number">78</span>]: <span class="number">1</span></span><br></pre></td></tr></table></figure><h5 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a><code>扩展</code></h5><p><strong>copy</strong></p><p>查看<code>copy</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">79</span>]: help(s.copy)</span><br></pre></td></tr></table></figure><blockquote><p>copy(…) method of builtins.list instance<br>​ L.copy() -&gt; list – a shallow copy of L</p><p>使用方法： L.copy()</p><p>注意： 复制列表，和L[:]的复制方式一样属于浅复制。</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">101</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]                                              </span><br><span class="line"></span><br><span class="line">In [<span class="number">102</span>]: a = s.copy()                                                 </span><br><span class="line"></span><br><span class="line">In [<span class="number">103</span>]: a                                                            </span><br><span class="line">Out[<span class="number">103</span>]: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">104</span>]: id(s)                                                        </span><br><span class="line">Out[<span class="number">104</span>]: <span class="number">140462445434696</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">105</span>]: id(a)                                                        </span><br><span class="line">Out[<span class="number">105</span>]: <span class="number">140462410623560</span></span><br></pre></td></tr></table></figure><p><strong>reverse</strong></p><p>查看<code>reverse</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">83</span>]: help(s.reverse)</span><br></pre></td></tr></table></figure><blockquote><p>reverse(…) method of builtins.list instance<br>​ L.reverse() – reverse <em>IN PLACE</em></p><p>使用方法：L.reverse()</p><p>注意： 反向列表中元素。</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">84</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]                                               </span><br><span class="line"></span><br><span class="line">In [<span class="number">85</span>]: s.reverse()                                                   </span><br><span class="line"></span><br><span class="line">In [<span class="number">86</span>]: s                                                             </span><br><span class="line">Out[<span class="number">86</span>]: [<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><p><strong>sort</strong></p><p>查看<code>sort</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">87</span>]: help(s.sort)</span><br></pre></td></tr></table></figure><blockquote><p>sort(…) method of builtins.list instance<br>​ L.sort(key=None, reverse=False) -&gt; None – stable sort <em>IN PLACE</em></p><p>使用方法：L.sort()</p><p>注意： 对原列表进行排序。列表中的元素要类型相同 (key = len int lambda)，不同元素需要改变元素类型，然后根据ASCII码进行排序。</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">92</span>]: s                                                             </span><br><span class="line">Out[<span class="number">92</span>]: [<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">93</span>]: s.sort()                                                      </span><br><span class="line"></span><br><span class="line">In [<span class="number">94</span>]: s                                                             </span><br><span class="line">Out[<span class="number">94</span>]: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">    </span><br><span class="line">In [<span class="number">110</span>]: s = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="string">'b'</span>,<span class="string">'a'</span>]                                      </span><br><span class="line"></span><br><span class="line">In [<span class="number">111</span>]: s.sort()                                                     </span><br><span class="line">-----------------------------------------------------------------------</span><br><span class="line">TypeError                             Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-111</span><span class="number">-474</span>c8408a842&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">----&gt; 1 s.sort()</span><br><span class="line"></span><br><span class="line">TypeError: unorderable types: str() &lt; int()</span><br><span class="line"></span><br><span class="line">In [<span class="number">112</span>]: s.sort(key=str)                                              </span><br><span class="line"></span><br><span class="line">In [<span class="number">113</span>]: s                                                            </span><br><span class="line">Out[<span class="number">113</span>]: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="string">'a'</span>, <span class="string">'b'</span>]</span><br></pre></td></tr></table></figure><h4 id="字符串常用方法"><a href="#字符串常用方法" class="headerlink" title="字符串常用方法"></a>字符串常用方法</h4><h5 id="增加元素-1"><a href="#增加元素-1" class="headerlink" title="增加元素"></a><code>增加元素</code></h5><p><strong>使用 +</strong></p><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">215</span>]: a = <span class="string">'hello'</span>                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">216</span>]: b = <span class="string">'python'</span>                                                 </span><br><span class="line"></span><br><span class="line">In [<span class="number">217</span>]: c = <span class="string">'!'</span>                                                      </span><br><span class="line"></span><br><span class="line">In [<span class="number">218</span>]: a+b+c                                                        </span><br><span class="line">Out[<span class="number">218</span>]: <span class="string">'hellopython!'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">219</span>]: a+<span class="string">' '</span>+b+<span class="string">' '</span>+c                                                </span><br><span class="line">Out[<span class="number">219</span>]: <span class="string">'hello python !'</span></span><br></pre></td></tr></table></figure><p><strong>格式化字符串</strong></p><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">220</span>]: a = <span class="string">'hello'</span>                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">221</span>]: b = <span class="string">'python'</span>                                                 </span><br><span class="line"></span><br><span class="line">In [<span class="number">222</span>]: c = <span class="string">'!'</span>                                                      </span><br><span class="line"></span><br><span class="line">In [<span class="number">223</span>]: <span class="string">'%s %s %s'</span>%(a,b,c)                                           </span><br><span class="line">Out[<span class="number">223</span>]: <span class="string">'hello python !'</span></span><br></pre></td></tr></table></figure><p><strong>使用join</strong></p><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">229</span>]: a = <span class="string">'hello'</span>                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">230</span>]: b = <span class="string">'python'</span>                                                 </span><br><span class="line"></span><br><span class="line">In [<span class="number">231</span>]: c = <span class="string">'!'</span>                                                      </span><br><span class="line"></span><br><span class="line">In [<span class="number">232</span>]: <span class="string">' '</span>.join([a,b,c])                                            </span><br><span class="line">Out[<span class="number">232</span>]: <span class="string">'hello python !'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">233</span>]: <span class="string">'****'</span>.join(<span class="string">'abc'</span>)                                           </span><br><span class="line">Out[<span class="number">233</span>]: <span class="string">'a****b****c'</span></span><br></pre></td></tr></table></figure><p><strong>使用format</strong></p><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">234</span>]: a = <span class="string">'hello'</span>                                                  </span><br><span class="line"></span><br><span class="line">In [<span class="number">235</span>]: b = <span class="string">'python'</span>                                                 </span><br><span class="line"></span><br><span class="line">In [<span class="number">236</span>]: c = <span class="string">'!'</span>                                                      </span><br><span class="line"></span><br><span class="line">In [<span class="number">237</span>]: <span class="string">'&#123;&#125; &#123;&#125; &#123;&#125;'</span>.format(a,b,c)                                     </span><br><span class="line">Out[<span class="number">237</span>]: <span class="string">'hello python !'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">238</span>]: <span class="string">'&#123;0&#125; &#123;1&#125; &#123;2&#125;'</span>.format(a,b,c)                                  </span><br><span class="line">Out[<span class="number">238</span>]: <span class="string">'hello python !'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">239</span>]: <span class="string">'&#123;2&#125; &#123;1&#125; &#123;0&#125;'</span>.format(a,b,c)                                  </span><br><span class="line">Out[<span class="number">239</span>]: <span class="string">'! python hello'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">240</span>]: <span class="string">'&#123;1&#125; &#123;1&#125; &#123;1&#125;'</span>.format(a,b,c)                                  </span><br><span class="line">Out[<span class="number">240</span>]: <span class="string">'python python python'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">241</span>]: <span class="string">'&#123;n1&#125; &#123;n2&#125; &#123;n3&#125;'</span>.format(n1=a, n2=b, n3=c)                    </span><br><span class="line">Out[<span class="number">241</span>]: <span class="string">'hello python !'</span></span><br></pre></td></tr></table></figure><h5 id="删除元素-1"><a href="#删除元素-1" class="headerlink" title="删除元素"></a><code>删除元素</code></h5><p><strong>replace</strong></p><p>查看<code>replace</code> 的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.replace)</span><br></pre></td></tr></table></figure><blockquote><p>replace(…) method of builtins.str instance<br>​ S.replace(old, new[, count]) -&gt; str<br>​ Return a copy of S with all occurrences of substring old replaced by new. If the optional argument count is given, only the first count occurrences are replaced.</p><p>使用方法：s.replace (x,y) ：</p><p>注意： 子串替换,在字符串s中出现字符串x的任意位置都用y进行替换</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">205</span>]: s = <span class="string">'abc cnn dnn'</span>                                            </span><br><span class="line"></span><br><span class="line">In [<span class="number">206</span>]: s.replace(<span class="string">'a'</span>,<span class="string">'2'</span>)                                           </span><br><span class="line">Out[<span class="number">206</span>]: <span class="string">'2bc cnn dnn'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">207</span>]: s.replace(<span class="string">'n'</span>,<span class="string">'w'</span>,<span class="number">1</span>)                                         </span><br><span class="line">Out[<span class="number">207</span>]: <span class="string">'abc cwn dnn'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">208</span>]: s.replace(<span class="string">'n'</span>,<span class="string">'w'</span>,<span class="number">2</span>)                                         </span><br><span class="line">Out[<span class="number">208</span>]: <span class="string">'abc cww dnn'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">209</span>]: s.replace(<span class="string">'n'</span>,<span class="string">'w'</span>)                                           </span><br><span class="line">Out[<span class="number">209</span>]: <span class="string">'abc cww dww'</span></span><br></pre></td></tr></table></figure><h5 id="修改元素-1"><a href="#修改元素-1" class="headerlink" title="修改元素"></a><code>修改元素</code></h5><p><strong>upper</strong></p><p>查看<code>upper</code> 的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">116</span>]: help(s.upper)</span><br></pre></td></tr></table></figure><blockquote><p>upper(…) method of builtins.str instance<br>​ S.upper() -&gt; str</p><p>Return a copy of S converted to uppercase.</p><p>使用方法： s.upper ()</p><p>注意： 将字符串转为大写</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">189</span>]: s = <span class="string">'123456abc'</span>                                              </span><br><span class="line"></span><br><span class="line">In [<span class="number">190</span>]: s.upper()                                                    </span><br><span class="line">Out[<span class="number">190</span>]: <span class="string">'123456ABC'</span></span><br></pre></td></tr></table></figure><p><strong>lower</strong></p><p>查看<code>lower</code> 的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">116</span>]: help(s.lower)</span><br></pre></td></tr></table></figure><blockquote><p>lower(…) method of builtins.str instance<br>​ S.lower() -&gt; str<br>​ Return a copy of the string S converted to lowercase.</p><p>使用方法：s.lower ()</p><p>注意：将字符串转为小写</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">193</span>]: s = <span class="string">'123456abcDF'</span>                                            </span><br><span class="line"></span><br><span class="line">In [<span class="number">194</span>]: s.lower()                                                    </span><br><span class="line">Out[<span class="number">194</span>]: <span class="string">'123456abcdf'</span></span><br></pre></td></tr></table></figure><p><strong>strip(lstrip、rstrip)</strong></p><p>查看<code>strip(lstrip、rstrip)</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">116</span>]: help(s.strip) </span><br><span class="line">In [<span class="number">117</span>]: help(s.lstrip) </span><br><span class="line">In [<span class="number">118</span>]: help(s.rstrip)</span><br></pre></td></tr></table></figure><blockquote><p>strip(…) method of builtins.str instance<br>​ S.strip([chars]) -&gt; str<br>Return a copy of the string S with leading and trailing whitespace removed. If chars is given and not None, remove characters in chars instead</p><p>使用方式：s.trip()</p><p>注意：去除两边的空格</p></blockquote><blockquote><p>lstrip(…) method of builtins.str instance<br>​ S.lstrip([chars]) -&gt; str</p><p>Return a copy of the string S with leading whitespace removed. If chars is given and not None, remove characters in chars instead</p><p>使用方式：s.lstrip()</p><p>注意:去除左边的空格</p></blockquote><blockquote><p>rstrip(…) method of builtins.str instance<br>​ S.rstrip([chars]) -&gt; str<br>Return a copy of the string S with trailing whitespace removed. If chars is given and not None, remove characters in chars instead</p><p>使用方式：s.rstrip()</p><p>注意：去除右边空格</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">195</span>]: s = <span class="string">'    abc    '</span>                                            </span><br><span class="line"></span><br><span class="line">In [<span class="number">196</span>]: s.strip()                                                    </span><br><span class="line">Out[<span class="number">196</span>]: <span class="string">'abc'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">197</span>]: s.lstrip()                                                   </span><br><span class="line">Out[<span class="number">197</span>]: <span class="string">'abc    '</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">198</span>]: s.rstrip()                                                   </span><br><span class="line">Out[<span class="number">198</span>]: <span class="string">'    abc'</span></span><br></pre></td></tr></table></figure><p><strong>capitalize</strong></p><p>查看<code>capitalize</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.capitalize)</span><br></pre></td></tr></table></figure><blockquote><p>capitalize(…) method of builtins.str instance<br>​ S.capitalize() -&gt; str<br>​ Return a capitalized version of S, i.e. make the first character have upper case and the rest lower case.</p><p>使用方式： s.capitalize()</p><p>注意：首字母大写</p></blockquote><p><strong>演示：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [201]: s = &apos;abc cnn dnn&apos;                                            </span><br><span class="line"></span><br><span class="line">In [202]: s.capitalize()                                               </span><br><span class="line">Out[202]: &apos;Abc cnn dnn&apos;</span><br></pre></td></tr></table></figure><p><strong>title</strong></p><p>查看<code>title</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.title)</span><br></pre></td></tr></table></figure><blockquote><p>title(…) method of builtins.str instance<br>​ S.title() -&gt; str<br>​ Return a titlecased version of S, i.e. words start with title case characters, all remaining cased characters have lower case.</p><p>使用方式： S.title()</p><p>注意：每个单词的首字母大写</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">203</span>]: s = <span class="string">'abc cnn dnn'</span>                                            </span><br><span class="line"></span><br><span class="line">In [<span class="number">204</span>]: s.title()                                                    </span><br><span class="line">Out[<span class="number">204</span>]: <span class="string">'Abc Cnn Dnn'</span></span><br></pre></td></tr></table></figure><p><strong>split</strong></p><p>查看<code>split</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.split)</span><br></pre></td></tr></table></figure><blockquote><p>split(…) method of builtins.str instance<br>​ S.split(sep=None, maxsplit=-1) -&gt; list of strings<br>​ Return a list of the words in S, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done. If sep is not specified or is None, any whitespace string is a separator and empty strings are removed from the result.</p><p>使用方法：s.split()，s.split(a,b)</p><p>注意：s.split()指的是返回一系列用空格分割的字符串列表</p><p>​ s.split(a,b)指的是a,b为可选参数，a是将要分割的字符串，b是说明最多要分割几个</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">210</span>]: s = <span class="string">'abc123cba'</span>                                              </span><br><span class="line"></span><br><span class="line">In [<span class="number">211</span>]: s.split(<span class="string">'b'</span>)                                                 </span><br><span class="line">Out[<span class="number">211</span>]: [<span class="string">'a'</span>, <span class="string">'c123c'</span>, <span class="string">'a'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">213</span>]: s = <span class="string">'abc cnn dnn'</span>                                            </span><br><span class="line"></span><br><span class="line">In [<span class="number">214</span>]: s.split(<span class="string">' '</span>)                                                 </span><br><span class="line">Out[<span class="number">214</span>]: [<span class="string">'abc'</span>, <span class="string">'cnn'</span>, <span class="string">'dnn'</span>]</span><br></pre></td></tr></table></figure><h5 id="查找元素-1"><a href="#查找元素-1" class="headerlink" title="查找元素"></a><code>查找元素</code></h5><p><strong>count</strong></p><p>查看<code>count</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.count)</span><br></pre></td></tr></table></figure><blockquote><p>count(…) method of builtins.str instance<br>​ S.count(sub[, start[, end]]) -&gt; int<br>​ Return the number of non-overlapping occurrences of substring sub in string S[start:end]. Optional arguments start and end are interpreted as in slice notation.</p><p>使用方法：s.count(x)</p><p>注意： 返回字符串x在s中出现的次数，带可选参数</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">138</span>]: s = <span class="string">'abc123abc321678'</span>                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">139</span>]: s.count(<span class="string">'a'</span>)                                                 </span><br><span class="line">Out[<span class="number">139</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">140</span>]: s.count(<span class="string">'8'</span>)                                                 </span><br><span class="line">Out[<span class="number">140</span>]: <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>index</strong></p><p>查看<code>index</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.index)</span><br></pre></td></tr></table></figure><blockquote><p>index(…) method of builtins.str instance<br>​ S.index(sub[, start[, end]]) -&gt; int<br>​ Like S.find() but raise ValueError when the substring is not found</p><p>使用方法：s.index(x)</p><p>注意：返回字符串中出现x的最左端的索引值，如果不在则抛出valueError异常</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">141</span>]: s = <span class="string">'abc123abc321678'</span>                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">142</span>]: s.index(<span class="string">'a'</span>)                                                 </span><br><span class="line">Out[<span class="number">142</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">143</span>]: s.index(<span class="string">'a'</span>,<span class="number">2</span>)                                               </span><br><span class="line">Out[<span class="number">143</span>]: <span class="number">6</span></span><br></pre></td></tr></table></figure><p><strong>find</strong></p><p>查看<code>find</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.find)</span><br></pre></td></tr></table></figure><blockquote><p>find(…) method of builtins.str instance<br>​ S.find(sub[, start[, end]]) -&gt; int<br>​ Return the lowest index in S where substring sub is found, such that sub is contained within S[start:end].Optional<br>arguments start and end are interpreted as in slice notation.</p><p>Return -1 on failure.</p><p>使用方法：s.find(x)</p><p>注意：返回字符串中出现x的最左端字符的索引值，如果不在则返回-1</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">146</span>]: s = <span class="string">'abc123abc321678'</span>                                        </span><br><span class="line"></span><br><span class="line">In [<span class="number">147</span>]: s.index(<span class="string">'a'</span>,<span class="number">1</span>,<span class="number">4</span>)  <span class="comment"># index没有找到就报错                                            </span></span><br><span class="line">-----------------------------------------------------------------------</span><br><span class="line">ValueError                            Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-147</span>-fbbfe36ae6a6&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">----&gt; 1 s.index('a',1,4)</span><br><span class="line"></span><br><span class="line">ValueError: substring <span class="keyword">not</span> found</span><br><span class="line"></span><br><span class="line">In [<span class="number">148</span>]: s.find(<span class="string">'a'</span>,<span class="number">1</span>,<span class="number">4</span>)  <span class="comment"># find 没有找到就提示-1                                             </span></span><br><span class="line">Out[<span class="number">148</span>]: <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">149</span>]: s.find(<span class="string">'a'</span>,<span class="number">1</span>)                                                </span><br><span class="line">Out[<span class="number">149</span>]: <span class="number">6</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">150</span>]: s.find(<span class="string">'a'</span>)                                                  </span><br><span class="line">Out[<span class="number">150</span>]: <span class="number">0</span></span><br></pre></td></tr></table></figure><p><strong>isdigit</strong></p><p>查看<code>isdigit</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.isdigit)</span><br></pre></td></tr></table></figure><blockquote><p>isdigit(…) method of builtins.str instance<br>​ S.isdigit() -&gt; bool<br>​ Return True if all characters in S are digits and there is at least one character in S, False otherwise.</p><p>使用方法：s.isdigit ()</p><p>注意 ：测试是否全是数字，都是数字则返回 True 否则返回 False.</p></blockquote><p><strong>演示</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">151</span>]: s = <span class="string">'123456'</span>                                                 </span><br><span class="line"></span><br><span class="line">In [<span class="number">152</span>]: s1 = <span class="string">'123abc'</span>                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">153</span>]: s2 = <span class="string">'abcdef'</span>                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">154</span>]: s.isdigit()                                                  </span><br><span class="line">Out[<span class="number">154</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">155</span>]: s1.isdigit()                                                 </span><br><span class="line">Out[<span class="number">155</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">156</span>]: s2.isdigit()                                                 </span><br><span class="line">Out[<span class="number">156</span>]: <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p><strong>isalpha</strong></p><p>查看<code>isalpha</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.isalpha )</span><br></pre></td></tr></table></figure><blockquote><p>isalpha(…) method of builtins.str instance<br>​ S.isalpha() -&gt; bool<br>​ Return True if all characters in S are alphabetic and there is at least one character in S, False otherwise.</p><p>使用方法：s.isalpha ()</p><p>注意 ：测试是否全是字母，都是字母则返回 True,否则返回 False.</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">157</span>]: s = <span class="string">'123456'</span>                                                 </span><br><span class="line"></span><br><span class="line">In [<span class="number">158</span>]: s1 = <span class="string">'123abc'</span>                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">159</span>]: s2 = <span class="string">'abcdef'</span>                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">160</span>]: s.isalpha()                                                  </span><br><span class="line">Out[<span class="number">160</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">161</span>]: s1.isalpha()                                                 </span><br><span class="line">Out[<span class="number">161</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">162</span>]: s2.isalpha()                                                 </span><br><span class="line">Out[<span class="number">162</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p><strong>endswith</strong></p><p>查看<code>endswith</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.endswith )</span><br></pre></td></tr></table></figure><blockquote><p>endswith(…) method of builtins.str instance<br>​ S.endswith(suffix[, start[, end]]) -&gt; bool<br>​ Return True if S ends with the specified suffix, False otherwise. With optional start, test S beginning at that position. With optional end, stop comparing S at that position. suffix can also be a tuple of strings to try.</p><p>使用方法：s.endswith(x)</p><p>注意：如果字符串s以x结尾，返回True</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">163</span>]: s = <span class="string">'123456abc'</span>                                              </span><br><span class="line"></span><br><span class="line">In [<span class="number">164</span>]: s1 = <span class="string">'123abced'</span>                                              </span><br><span class="line"></span><br><span class="line">In [<span class="number">165</span>]: s2 = <span class="string">'abcdef'</span>                                                </span><br><span class="line"></span><br><span class="line">In [<span class="number">166</span>]: s.endswith(<span class="string">'c'</span>)                                              </span><br><span class="line">Out[<span class="number">166</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">167</span>]: s.endswith(<span class="string">'2'</span>)                                              </span><br><span class="line">Out[<span class="number">167</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">168</span>]: s1.endswith(<span class="string">'d'</span>)                                             </span><br><span class="line">Out[<span class="number">168</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">169</span>]: s2.endswith(<span class="string">'f'</span>)                                             </span><br><span class="line">Out[<span class="number">169</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure><blockquote><p>备注：方法s.startwith与s.endwith相反，前者是以什么开始，后者是以什么结尾。</p></blockquote><p><strong>islower</strong></p><p>查看<code>islower</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.islower )</span><br></pre></td></tr></table></figure><blockquote><p>islower(…) method of builtins.str instance<br>​ S.islower() -&gt; bool<br>​ Return True if all cased characters in S are lowercase and there is at least one cased character in S, False otherwise.</p><p>使用方法：s.islower ()</p><p>注意：测试是否全是小写</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">173</span>]: s = <span class="string">'123456abc'</span>                                              </span><br><span class="line"></span><br><span class="line">In [<span class="number">174</span>]: s1 = <span class="string">'abcDE'</span>                                                 </span><br><span class="line"></span><br><span class="line">In [<span class="number">175</span>]: s2 = <span class="string">'DE'</span>                                                    </span><br><span class="line"></span><br><span class="line">In [<span class="number">176</span>]: s.islower()                                                  </span><br><span class="line">Out[<span class="number">176</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">177</span>]: s1.islower()                                                 </span><br><span class="line">Out[<span class="number">177</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">178</span>]: s2.islower()                                                 </span><br><span class="line">Out[<span class="number">178</span>]: <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p><strong>isupper</strong></p><p>查看<code>isupper</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.isupper )</span><br></pre></td></tr></table></figure><blockquote><p>isupper(…) method of builtins.str instance<br>​ S.isupper() -&gt; bool<br>​ Return True if all cased characters in S are uppercase and there is at least one cased character in S, False otherwise.</p><p>使用方式：s.isupper ()</p><p>注意：测试是否全是大写</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">181</span>]: s = <span class="string">'123456abc'</span>                                              </span><br><span class="line"></span><br><span class="line">In [<span class="number">182</span>]: s1 = <span class="string">'abcDE'</span>                                                 </span><br><span class="line"></span><br><span class="line">In [<span class="number">183</span>]: s2 = <span class="string">'DE'</span>                                                    </span><br><span class="line"></span><br><span class="line">In [<span class="number">184</span>]: s3 = <span class="string">'123ADFAFA'</span>                                             </span><br><span class="line"></span><br><span class="line">In [<span class="number">185</span>]: s.isupper()                                                  </span><br><span class="line">Out[<span class="number">185</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">186</span>]: s1.isupper()                                                 </span><br><span class="line">Out[<span class="number">186</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">187</span>]: s2.isupper()                                                 </span><br><span class="line">Out[<span class="number">187</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">188</span>]: s3.isupper()                                                 </span><br><span class="line">Out[<span class="number">188</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure><h4 id="元组常用方法"><a href="#元组常用方法" class="headerlink" title="元组常用方法"></a>元组常用方法</h4><blockquote><p>元组为不可变序列，所以只有两种方法。</p></blockquote><h5 id="查找元素-2"><a href="#查找元素-2" class="headerlink" title="查找元素"></a><code>查找元素</code></h5><p><strong>count</strong></p><p>查看<code>count</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.count )</span><br></pre></td></tr></table></figure><blockquote><p>count(…) method of builtins.tuple instance<br>​ T.count(value) -&gt; integer – return number of occurrences of value</p><p>使用方法：s.count(value)</p><p>注意：统计元素的个数</p></blockquote><p><strong>演示：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">243</span>]: s = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)                                          </span><br><span class="line"></span><br><span class="line">In [<span class="number">244</span>]: s.count(<span class="number">1</span>)                                                   </span><br><span class="line">Out[<span class="number">244</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">245</span>]: s.count(<span class="number">4</span>)                                                   </span><br><span class="line">Out[<span class="number">245</span>]: <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>index</strong></p><p>查看<code>index</code>的使用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">115</span>]: help(s.index )</span><br></pre></td></tr></table></figure><blockquote><p>index(…) method of builtins.tuple instance<br>​ T.index(value, [start, [stop]]) -&gt; integer – return first index of value.<br>​ Raises ValueError if the value is not present.</p><p>使用方式： T.index(value, [start, [stop]])</p><p>注意：查看元素的索引</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">246</span>]: s = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)                                          </span><br><span class="line"></span><br><span class="line">In [<span class="number">247</span>]: s.index(<span class="number">1</span>)                                                   </span><br><span class="line">Out[<span class="number">247</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">248</span>]: s.index(<span class="number">1</span>,<span class="number">1</span>)                                                 </span><br><span class="line">Out[<span class="number">248</span>]: <span class="number">6</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">249</span>]: s.index(<span class="number">4</span>)                                                   </span><br><span class="line">Out[<span class="number">249</span>]: <span class="number">3</span></span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python基础 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>python第一话之数值类型和序列类型</title>
      <link href="/2018/12/01/2018-12-01-python-numerical-sequence/"/>
      <url>/2018/12/01/2018-12-01-python-numerical-sequence/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>[TOC]</p><h3 id="Python的数值类型"><a href="#Python的数值类型" class="headerlink" title="Python的数值类型"></a>Python的数值类型</h3><p>Python中的基本数据类型有数值类型、字符串型、列表、元组、字典、集合等。本节介绍数值类型。数值类型包括整型、布尔型、浮点型和复数类型。</p><h4 id="基本整形四则运算"><a href="#基本整形四则运算" class="headerlink" title="基本整形四则运算"></a>基本整形四则运算</h4><p>用Python实现简单的加减乘除</p><p>我们先进入<code>Python的交互模式</code></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-1/36226595.jpg" alt="python"></p><p>或者执行<code>ipython</code></p><p><img src="http://eveseven.oss-cn-shanghai.aliyuncs.com/18-12-1/65669846.jpg" alt="ipython"></p><h5 id="加法"><a href="#加法" class="headerlink" title="加法"></a><code>加法</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="number">1</span>+<span class="number">1</span></span><br><span class="line">Out[<span class="number">1</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: <span class="number">12</span>+<span class="number">2</span></span><br><span class="line">Out[<span class="number">2</span>]: <span class="number">14</span></span><br></pre></td></tr></table></figure><h5 id="减法"><a href="#减法" class="headerlink" title="减法"></a><code>减法</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">3</span>]: <span class="number">21</span><span class="number">-1</span></span><br><span class="line">Out[<span class="number">3</span>]: <span class="number">20</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: <span class="number">20</span><span class="number">-22</span></span><br><span class="line">Out[<span class="number">4</span>]: <span class="number">-2</span></span><br></pre></td></tr></table></figure><h5 id="乘法"><a href="#乘法" class="headerlink" title="乘法"></a><code>乘法</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">5</span>]: <span class="number">2</span>*<span class="number">2</span></span><br><span class="line">Out[<span class="number">5</span>]: <span class="number">4</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: <span class="number">10</span>*<span class="number">10</span></span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">100</span></span><br></pre></td></tr></table></figure><h5 id="除法"><a href="#除法" class="headerlink" title="除法"></a><code>除法</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">9</span>]: <span class="number">4</span>/<span class="number">2</span></span><br><span class="line">Out[<span class="number">9</span>]: <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: <span class="number">2</span>/<span class="number">2</span></span><br><span class="line">Out[<span class="number">10</span>]: <span class="number">1.0</span></span><br></pre></td></tr></table></figure><blockquote><p>以上就是我们所遇到的一些基本的加减乘除运算，接下来我们再看看其他形式的扩展。</p></blockquote><h4 id="保存计算结果-基本的赋值运算"><a href="#保存计算结果-基本的赋值运算" class="headerlink" title="保存计算结果-基本的赋值运算"></a>保存计算结果-基本的赋值运算</h4><p>在我们后面的学习中，经常会遇到需要保存计算结果的情况，下面我们来看一下如何保存我们的计算结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">11</span>]: a = <span class="number">1</span>+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: <span class="number">1</span>+<span class="number">1</span></span><br><span class="line">Out[<span class="number">12</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: a</span><br><span class="line">Out[<span class="number">13</span>]: <span class="number">2</span></span><br></pre></td></tr></table></figure><h4 id="基本整数与小数四则运算"><a href="#基本整数与小数四则运算" class="headerlink" title="基本整数与小数四则运算"></a>基本整数与小数四则运算</h4><p>计算机在计算的时候，除了整数运算，还有小数运算，还有小数和整数的混合运算。</p><h5 id="加法-1"><a href="#加法-1" class="headerlink" title="加法"></a><code>加法</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">14</span>]: <span class="number">2</span>+<span class="number">1.2</span></span><br><span class="line">Out[<span class="number">14</span>]: <span class="number">3.2</span></span><br></pre></td></tr></table></figure><h5 id="减法-1"><a href="#减法-1" class="headerlink" title="减法"></a><code>减法</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">16</span>]: <span class="number">2.2</span><span class="number">-2</span></span><br><span class="line">Out[<span class="number">16</span>]: <span class="number">0.20000000000000018</span></span><br></pre></td></tr></table></figure><h5 id="乘法-1"><a href="#乘法-1" class="headerlink" title="乘法"></a><code>乘法</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">17</span>]: <span class="number">3.3</span>*<span class="number">2</span></span><br><span class="line">Out[<span class="number">17</span>]: <span class="number">6.6</span></span><br></pre></td></tr></table></figure><h5 id="除法-1"><a href="#除法-1" class="headerlink" title="除法"></a><code>除法</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">18</span>]: <span class="number">2.4</span>/<span class="number">4</span></span><br><span class="line">Out[<span class="number">18</span>]: <span class="number">0.6</span></span><br></pre></td></tr></table></figure><blockquote><p>小数一般是<code>float</code>类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; In [<span class="number">19</span>]: b = <span class="number">3</span>+<span class="number">2.2</span></span><br><span class="line">&gt; </span><br><span class="line">&gt; In [<span class="number">20</span>]: type(b)</span><br><span class="line">&gt; Out[<span class="number">20</span>]: float</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>但是<code>浮点数</code>不是我们真正看到的数，比如<code>1.2</code>实际是<code>1.1999999999</code></p><p>所以小数计算都是不精确的，那么我们如何进行精确计算呢？</p></blockquote><h4 id="精确计算-decimal的运算"><a href="#精确计算-decimal的运算" class="headerlink" title="精确计算-decimal的运算"></a>精确计算-decimal的运算</h4><p>在<code>Python</code>中如果我们要实现精确计算，我们是使用<code>decimal</code>这个库函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">27</span>]: <span class="keyword">import</span> decimal </span><br><span class="line"></span><br><span class="line">In [<span class="number">28</span>]: a = decimal.Decimal(<span class="string">'2.2'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">29</span>]: b = decimal.Decimal(<span class="string">'2'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">30</span>]: a-b</span><br><span class="line">Out[<span class="number">30</span>]: Decimal(<span class="string">'0.2'</span>)</span><br></pre></td></tr></table></figure><h4 id="布尔型的计算"><a href="#布尔型的计算" class="headerlink" title="布尔型的计算"></a>布尔型的计算</h4><p><code>布尔型变量</code>只有<code>True</code>和<code>False</code>两种情况，<code>True</code>就是<code>1</code>，<code>False</code>就是<code>0</code>。</p><h5 id="基本情况"><a href="#基本情况" class="headerlink" title="基本情况"></a><code>基本情况</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">34</span>]: a = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">35</span>]: b = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">36</span>]: a</span><br><span class="line">Out[<span class="number">36</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: b</span><br><span class="line">Out[<span class="number">37</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">38</span>]: type(a)</span><br><span class="line">Out[<span class="number">38</span>]: bool</span><br></pre></td></tr></table></figure><h5 id="运算"><a href="#运算" class="headerlink" title="运算"></a><code>运算</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">39</span>]: a + <span class="number">1</span></span><br><span class="line">Out[<span class="number">39</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">40</span>]: a + a </span><br><span class="line">Out[<span class="number">40</span>]: <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">41</span>]: a + b</span><br><span class="line">Out[<span class="number">41</span>]: <span class="number">1</span></span><br></pre></td></tr></table></figure><h4 id="复数类型"><a href="#复数类型" class="headerlink" title="复数类型"></a>复数类型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">42</span>]: <span class="number">1</span> + <span class="number">2j</span></span><br><span class="line">Out[<span class="number">42</span>]: (<span class="number">1</span>+<span class="number">2j</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">43</span>]: a = <span class="number">1</span> + <span class="number">2j</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">44</span>]: type(a)</span><br><span class="line">Out[<span class="number">44</span>]: complex</span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>整型、布尔型、浮点型和复数类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">45</span>]: type(<span class="number">2</span>)</span><br><span class="line">Out[<span class="number">45</span>]: int</span><br><span class="line"></span><br><span class="line">In [<span class="number">46</span>]: type(<span class="keyword">True</span>)</span><br><span class="line">Out[<span class="number">46</span>]: bool</span><br><span class="line"></span><br><span class="line">In [<span class="number">47</span>]: type(<span class="number">1.2</span>)</span><br><span class="line">Out[<span class="number">47</span>]: float</span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: type(<span class="number">1</span>+<span class="number">2j</span>)</span><br><span class="line">Out[<span class="number">48</span>]: complex</span><br></pre></td></tr></table></figure><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><h5 id="整除-向下取整"><a href="#整除-向下取整" class="headerlink" title="整除-向下取整"></a><code>整除-向下取整</code></h5><p><code>//</code>在<code>Python</code>中是指的<code>向下取整</code>：1 &lt; 1.n &lt; 2 ==&gt; 1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">49</span>]: <span class="number">2.3</span> / <span class="number">2</span></span><br><span class="line">Out[<span class="number">49</span>]: <span class="number">1.15</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">50</span>]: <span class="number">2.3</span> // <span class="number">2</span></span><br><span class="line">Out[<span class="number">50</span>]: <span class="number">1.0</span></span><br></pre></td></tr></table></figure><h5 id="整除-向上取整"><a href="#整除-向上取整" class="headerlink" title="整除-向上取整"></a><code>整除-向上取整</code></h5><p><code>向上取整</code>：使用<code>math</code>库函数实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">52</span>]: <span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">In [<span class="number">53</span>]: math.ceil(<span class="number">2.3</span>/<span class="number">2</span>)</span><br><span class="line">Out[<span class="number">53</span>]: <span class="number">2</span></span><br></pre></td></tr></table></figure><h5 id="幂运算"><a href="#幂运算" class="headerlink" title="幂运算"></a><code>幂运算</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">54</span>]: <span class="number">2</span> * <span class="number">2</span> *<span class="number">2</span></span><br><span class="line">Out[<span class="number">54</span>]: <span class="number">8</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">55</span>]: <span class="number">2</span> **<span class="number">3</span></span><br><span class="line">Out[<span class="number">55</span>]: <span class="number">8</span></span><br></pre></td></tr></table></figure><h5 id="取余"><a href="#取余" class="headerlink" title="取余"></a><code>取余</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">56</span>]: <span class="number">6</span> % <span class="number">4</span></span><br><span class="line">Out[<span class="number">56</span>]: <span class="number">2</span></span><br></pre></td></tr></table></figure><p>这就是我们常见的数值类型的相关使用和运算。</p><h3 id="Python的序列类型"><a href="#Python的序列类型" class="headerlink" title="Python的序列类型"></a>Python的序列类型</h3><p>前面我们遇到的都是一些数值类型的使用与运算，但是如果我想在Python中表示字母怎么办呢？这就是我们接下来看一下序列类型。</p><h4 id="字符串-str"><a href="#字符串-str" class="headerlink" title="字符串-str"></a>字符串-<code>str</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">57</span>]: <span class="string">'abc'</span></span><br><span class="line">Out[<span class="number">57</span>]: <span class="string">'abc'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">58</span>]: type(<span class="string">'abc'</span>)</span><br><span class="line">Out[<span class="number">58</span>]: str</span><br><span class="line"></span><br><span class="line">In [<span class="number">59</span>]: <span class="string">"I'm seven"</span></span><br><span class="line">Out[<span class="number">59</span>]: <span class="string">"I'm seven"</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">60</span>]: <span class="string">"""abcde</span></span><br><span class="line"><span class="string">    ...: fghijk"""</span></span><br><span class="line">Out[<span class="number">60</span>]: <span class="string">'abcde\nfghijk'</span></span><br></pre></td></tr></table></figure><h5 id="字符串的使用"><a href="#字符串的使用" class="headerlink" title="字符串的使用"></a><code>字符串的使用</code></h5><blockquote><p>字符串是通过下标索引值来获取对应值的。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">74</span>]: a = <span class="string">'abc'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">75</span>]: a[<span class="number">0</span>]</span><br><span class="line">Out[<span class="number">75</span>]: <span class="string">'a'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">76</span>]: a[<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">76</span>]: <span class="string">'c'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">77</span>]: a[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">77</span>]: <span class="string">'b'</span></span><br></pre></td></tr></table></figure><h4 id="列表-list"><a href="#列表-list" class="headerlink" title="列表-list"></a>列表-<code>list</code></h4><blockquote><p>数值和字符串的混合使用</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">61</span>]: [<span class="string">'abc'</span>, <span class="number">123</span>, <span class="string">'dde'</span>]</span><br><span class="line">Out[<span class="number">61</span>]: [<span class="string">'abc'</span>, <span class="number">123</span>, <span class="string">'dde'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">62</span>]: type([<span class="string">'abc'</span>, <span class="number">123</span>, <span class="string">'dde'</span>])</span><br><span class="line">Out[<span class="number">62</span>]: list</span><br></pre></td></tr></table></figure><h5 id="列表的使用-简单取值"><a href="#列表的使用-简单取值" class="headerlink" title="列表的使用-简单取值"></a><code>列表的使用-简单取值</code></h5><blockquote><p>根据下标取出对应数据，下标从0开始计数：0,1,2,3…</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">65</span>]: a = [<span class="string">'abc'</span>, <span class="number">123</span>, <span class="string">'dde'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">66</span>]: a[<span class="number">0</span>]</span><br><span class="line">Out[<span class="number">66</span>]: <span class="string">'abc'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">67</span>]: a[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">67</span>]: <span class="number">123</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">68</span>]: a[<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">68</span>]: <span class="string">'dde'</span></span><br></pre></td></tr></table></figure><h5 id="列表的使用-切片"><a href="#列表的使用-切片" class="headerlink" title="列表的使用-切片"></a><code>列表的使用-切片</code></h5><blockquote><p>通过<code>list[start_index : end_index : stride ]</code>来进行<code>切片</code>，<code>切片</code>方式类似数学中的<code>左闭右开区间</code></p><p><code>start_index:</code> 开始的索引值</p><p><code>end_index:</code> 结束的索引值</p><p><code>stride:</code> 步长</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">78</span>]: a = [<span class="string">'abc'</span>, <span class="number">123</span>, <span class="string">'dde'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">79</span>]: a[:<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">79</span>]: [<span class="string">'abc'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">80</span>]: a[:<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">80</span>]: [<span class="string">'abc'</span>, <span class="number">123</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">81</span>]: a[<span class="number">1</span>:]</span><br><span class="line">Out[<span class="number">81</span>]: [<span class="number">123</span>, <span class="string">'dde'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">82</span>]: a[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">Out[<span class="number">82</span>]: [<span class="number">123</span>, <span class="string">'dde'</span>]</span><br><span class="line">    </span><br><span class="line">In [<span class="number">84</span>]: a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">85</span>]: a[<span class="number">1</span>:<span class="number">6</span>:<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">85</span>]: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]</span><br></pre></td></tr></table></figure><p>其中：<code>[-1]:</code> 表示倒着计数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">92</span>]: a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">93</span>]: a[<span class="number">1</span>:]</span><br><span class="line">Out[<span class="number">93</span>]: [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">94</span>]: a[<span class="number">-1</span>:]</span><br><span class="line">Out[<span class="number">94</span>]: [<span class="string">'c'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">95</span>]: a[<span class="number">2</span>:<span class="number">-1</span>]</span><br><span class="line">Out[<span class="number">95</span>]: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="string">'a'</span>, <span class="string">'b'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">96</span>]: a[<span class="number">2</span>:<span class="number">8</span>]</span><br><span class="line">Out[<span class="number">96</span>]: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="string">'a'</span>, <span class="string">'b'</span>]</span><br></pre></td></tr></table></figure><h4 id="元组-tuple"><a href="#元组-tuple" class="headerlink" title="元组-tuple"></a>元组-<code>tuple</code></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">63</span>]: (<span class="number">123</span>, <span class="string">'abc'</span>, <span class="string">'seven'</span>)</span><br><span class="line">Out[<span class="number">63</span>]: (<span class="number">123</span>, <span class="string">'abc'</span>, <span class="string">'seven'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">64</span>]: type((<span class="number">123</span>, <span class="string">'abc'</span>, <span class="string">'seven'</span>))</span><br><span class="line">Out[<span class="number">64</span>]: tuple</span><br></pre></td></tr></table></figure><h5 id="元组的使用"><a href="#元组的使用" class="headerlink" title="元组的使用"></a><code>元组的使用</code></h5><blockquote><p>元组的使用是和列表的类似的。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">70</span>]: b = (<span class="number">123</span>, <span class="string">'abc'</span>, <span class="string">'seven'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">71</span>]: b[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">71</span>]: <span class="string">'abc'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">72</span>]: b[<span class="number">0</span>]</span><br><span class="line">Out[<span class="number">72</span>]: <span class="number">123</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">73</span>]: b[<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">73</span>]: <span class="string">'seven'</span></span><br></pre></td></tr></table></figure><h4 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h4><blockquote><p>我们经常会使用到这几种类型， 所以这几种类型间的转换又尤为关键</p></blockquote><h5 id="字符串转列表"><a href="#字符串转列表" class="headerlink" title="字符串转列表"></a><code>字符串转列表</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">111</span>]: a = <span class="string">'abcd'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">112</span>]: b = list(a)</span><br><span class="line"></span><br><span class="line">In [<span class="number">113</span>]: type(a)</span><br><span class="line">Out[<span class="number">113</span>]: str</span><br><span class="line"></span><br><span class="line">In [<span class="number">114</span>]: type(b)</span><br><span class="line">Out[<span class="number">114</span>]: list</span><br><span class="line"></span><br><span class="line">In [<span class="number">115</span>]: b</span><br><span class="line">Out[<span class="number">115</span>]: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]</span><br></pre></td></tr></table></figure><h5 id="列表转字符串"><a href="#列表转字符串" class="headerlink" title="列表转字符串"></a><code>列表转字符串</code></h5><blockquote><p>列表变成字符串会把列表里的中括号和空格也变成字符串</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">123</span>]: a = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">124</span>]: b = str(a)</span><br><span class="line"></span><br><span class="line">In [<span class="number">125</span>]: type(a)</span><br><span class="line">Out[<span class="number">125</span>]: list</span><br><span class="line"></span><br><span class="line">In [<span class="number">126</span>]: type(b)</span><br><span class="line">Out[<span class="number">126</span>]: str</span><br><span class="line"></span><br><span class="line">In [<span class="number">127</span>]: b</span><br><span class="line">Out[<span class="number">127</span>]: <span class="string">"['a', 'b', 'c', 'd']"</span></span><br></pre></td></tr></table></figure><h5 id="字符串转元组"><a href="#字符串转元组" class="headerlink" title="字符串转元组"></a><code>字符串转元组</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">116</span>]: a = <span class="string">'abcd'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">117</span>]: b = tuple(a)</span><br><span class="line"></span><br><span class="line">In [<span class="number">118</span>]: type(a)</span><br><span class="line">Out[<span class="number">118</span>]: str</span><br><span class="line"></span><br><span class="line">In [<span class="number">119</span>]: type(b)</span><br><span class="line">Out[<span class="number">119</span>]: tuple</span><br><span class="line"></span><br><span class="line">In [<span class="number">120</span>]: b</span><br><span class="line">Out[<span class="number">120</span>]: (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>)</span><br></pre></td></tr></table></figure><h5 id="元组转字符串"><a href="#元组转字符串" class="headerlink" title="元组转字符串"></a><code>元组转字符串</code></h5><blockquote><p>列表变成字符串会把列表里的小括号和空格也变成字符串</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">128</span>]: a = (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">129</span>]: b = str(a)</span><br><span class="line"></span><br><span class="line">In [<span class="number">130</span>]: type(a)</span><br><span class="line">Out[<span class="number">130</span>]: tuple</span><br><span class="line"></span><br><span class="line">In [<span class="number">131</span>]: type(b)</span><br><span class="line">Out[<span class="number">131</span>]: str</span><br><span class="line"></span><br><span class="line">In [<span class="number">132</span>]: b</span><br><span class="line">Out[<span class="number">132</span>]: <span class="string">"('a', 'b', 'c', 'd')"</span></span><br></pre></td></tr></table></figure><h4 id="元组和列表的区别"><a href="#元组和列表的区别" class="headerlink" title="元组和列表的区别"></a>元组和列表的区别</h4><blockquote><p>在我们前面的接触过程中，列表和元组基本的功能是一样的，那列表和元组都存在，是为什么呢？</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">133</span>]: a = (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">134</span>]: b = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">135</span>]: a[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">135</span>]: <span class="string">'b'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">136</span>]: b[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">136</span>]: <span class="string">'b'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">137</span>]: b[<span class="number">1</span>] = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">138</span>]: b</span><br><span class="line">Out[<span class="number">138</span>]: [<span class="string">'a'</span>, <span class="number">3</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">139</span>]: a[<span class="number">1</span>] = <span class="number">3</span></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">TypeError                                 Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-139</span><span class="number">-23</span>f2cf2bdf70&gt; <span class="keyword">in</span> &lt;module&gt;()</span><br><span class="line">----&gt; 1 a[1] = 3</span><br><span class="line"></span><br><span class="line">TypeError: <span class="string">'tuple'</span> object does <span class="keyword">not</span> support item assignment</span><br></pre></td></tr></table></figure><p><code>扩展：</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">140</span>]: a = <span class="string">'abcdefg'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">141</span>]: type(a)</span><br><span class="line">Out[<span class="number">141</span>]: str</span><br><span class="line"></span><br><span class="line">In [<span class="number">142</span>]: a[<span class="number">1</span>]</span><br><span class="line">Out[<span class="number">142</span>]: <span class="string">'b'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">143</span>]: a[<span class="number">1</span>] = <span class="string">'s'</span></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">TypeError                                 Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-143</span><span class="number">-55e6</span>e4038777&gt; <span class="keyword">in</span> &lt;module&gt;()</span><br><span class="line">----&gt; 1 a[1] = 's'</span><br><span class="line"></span><br><span class="line">TypeError: <span class="string">'str'</span> object does <span class="keyword">not</span> support item assignment</span><br></pre></td></tr></table></figure><blockquote><p>总结：列表：可变</p><p>​ 元组： 不可变</p><p>​ 字符串： 不可变</p><p>所以，在序列类型中，只有列表才是可变的类型。</p></blockquote><h4 id="更改字符串和元组的元素"><a href="#更改字符串和元组的元素" class="headerlink" title="更改字符串和元组的元素"></a>更改字符串和元组的元素</h4><blockquote><p>前面我们讲了在序列类型中，只有列表才是可变的类型。那我们如何来更改不可变数据类型的元素呢？</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">151</span>]: a = <span class="string">'abcdefg'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">152</span>]: a = a[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">In [<span class="number">153</span>]: a</span><br><span class="line">Out[<span class="number">153</span>]: <span class="string">'bcdefg'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">154</span>]: a = (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">155</span>]: a = a[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">156</span>]: a</span><br><span class="line">Out[<span class="number">156</span>]: (<span class="string">'b'</span>, <span class="string">'c'</span>)</span><br></pre></td></tr></table></figure><h4 id="拆包"><a href="#拆包" class="headerlink" title="拆包"></a>拆包</h4><blockquote><p>元组拆包可以应用到任何迭代对象上， 唯一的要求是， 被可迭代对象中的元素数量必须要和这些元素的元组的空档数一致， 除非我们用* 来表示忽略多余的元素。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">157</span>]: a = (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">158</span>]: x,*y,z = a</span><br><span class="line"></span><br><span class="line">In [<span class="number">159</span>]: x</span><br><span class="line">Out[<span class="number">159</span>]: <span class="string">'a'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">160</span>]: z</span><br><span class="line">Out[<span class="number">160</span>]: <span class="string">'d'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">161</span>]: y</span><br><span class="line">Out[<span class="number">161</span>]: [<span class="string">'b'</span>, <span class="string">'c'</span>]</span><br><span class="line">In [<span class="number">162</span>]: x,y,z = a</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">ValueError                                Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input<span class="number">-162</span><span class="number">-57</span>ae45ef0060&gt; <span class="keyword">in</span> &lt;module&gt;()</span><br><span class="line">----&gt; 1 x,y,z = a</span><br><span class="line"></span><br><span class="line">ValueError: too many values to unpack (expected <span class="number">3</span>)</span><br></pre></td></tr></table></figure><blockquote><p><code>x</code> 接收第一个元素，<code>z</code> 接收最后一个元素，由于<code>y</code>前面有<code>*</code>号，所以剩余的元素由<code>y</code>接收</p><p>总结：有多少个元素就需要多少个变量来接收，除非有<code>*</code>号，不然就会报错。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">163</span>]: x,y,z = a,a,a</span><br><span class="line"></span><br><span class="line">In [<span class="number">164</span>]: x</span><br><span class="line">Out[<span class="number">164</span>]: (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">165</span>]: y</span><br><span class="line">Out[<span class="number">165</span>]: (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">166</span>]: z</span><br><span class="line">Out[<span class="number">166</span>]: (<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>)</span><br></pre></td></tr></table></figure><h4 id="变量的赋值"><a href="#变量的赋值" class="headerlink" title="变量的赋值"></a>变量的赋值</h4><blockquote><p>变量的保存都是保存在内存中</p><p>注意：<code>变量</code>是没有类型的，有类型的是他所<code>指向的数据</code></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">167</span>]: a = <span class="number">123</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">168</span>]: id(a)</span><br><span class="line">Out[<span class="number">168</span>]: <span class="number">10923232</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">169</span>]: b = <span class="string">'abc'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">170</span>]: id(b)</span><br><span class="line">Out[<span class="number">170</span>]: <span class="number">140343125492152</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">171</span>]: a = <span class="string">'111'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">172</span>]: id(a)</span><br><span class="line">Out[<span class="number">172</span>]: <span class="number">140342939172344</span></span><br></pre></td></tr></table></figure><blockquote><p><code>id()</code>：查看数据的地址</p><p>总结：赋值给变量是保存在内存中，重新赋值后，变量指向新的地址</p></blockquote><h4 id="变量的引用-成员运算"><a href="#变量的引用-成员运算" class="headerlink" title="变量的引用-成员运算"></a>变量的引用-成员运算</h4><blockquote><p>通过<code>in</code>或者<code>not in</code> 来进行成员运算</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">173</span>]: a = <span class="string">'abcd123'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">174</span>]: <span class="string">'c'</span> <span class="keyword">in</span> a</span><br><span class="line">Out[<span class="number">174</span>]: <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">175</span>]: <span class="string">'8'</span> <span class="keyword">in</span> a</span><br><span class="line">Out[<span class="number">175</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">176</span>]: <span class="string">'a'</span> <span class="keyword">not</span> <span class="keyword">in</span> a</span><br><span class="line">Out[<span class="number">176</span>]: <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">177</span>]: <span class="string">'q'</span> <span class="keyword">not</span> <span class="keyword">in</span> a</span><br><span class="line">Out[<span class="number">177</span>]: <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python基础 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>开发常用工具</title>
      <link href="/2018/11/22/2016-06-02-Develop_Tool/"/>
      <url>/2018/11/22/2016-06-02-Develop_Tool/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>工欲善其事必先利其器，选择一些好的工具可以成吨的提高自己的工作效率。</p><h3 id="个人开发常用工具的收集"><a href="#个人开发常用工具的收集" class="headerlink" title="个人开发常用工具的收集"></a>个人开发常用工具的收集</h3><ul><li><a href="https://www.zybuluo.com/mdeditor" target="_blank" rel="noopener">cmd Markdown</a> 作业部落出版的Markdown编辑器</li><li><a href="https://github.com/rest-client/rest-client" target="_blank" rel="noopener">RESTClient</a> 一个开源的客户端HTTP调试工具。</li><li><a href="https://github.com/getlantern/lantern" target="_blank" rel="noopener">lantern</a> 蓝灯,一款开源的翻墙工具。</li><li><a href="https://www.charlesproxy.com/" target="_blank" rel="noopener">Charles</a> 青花瓷, 一款HTTP/HTTPS的抓包工具。<ul><li><a href="http://blog.devtang.com/2015/11/14/charles-introduction/" target="_blank" rel="noopener">Charles 从入门到精通</a></li></ul></li><li><a href="http://www.sublimetext.com" target="_blank" rel="noopener">Sublime</a> 一款强大的IDE,支持Python、JS、JSON格式化等等…更重要的是<code>Sublime</code>支持的插件很多。<ul><li><a href="http://www.xuanfengge.com/practical-collection-of-sublime-plug-in.html" target="_blank" rel="noopener">实用的sublime插件集合</a></li></ul></li></ul><h3 id="文章配图网站"><a href="#文章配图网站" class="headerlink" title="文章配图网站"></a>文章配图网站</h3><p>还在为文章配图而苦恼吗？点击店面的网站吧，各种各样的图片帮你丰富你的文章。</p><p><a href="http://www.gratisography.com/" target="_blank" rel="noopener">Gratisography</a> gratisography 里面的图片每周都会更新，很多时尚流行的照片在里面，并且适合用在设计项目上。</p><p><a href="http://www.ssyer.com/home-index.html" target="_blank" rel="noopener">ssyer</a>国内的网站，不需要翻墙，速度很快，图片最全。完全免费的图片库。</p><p><a href="https://pixabay.com/" target="_blank" rel="noopener">Pixabay</a> 不同类型的高清摄影照片。</p><h3 id="UI设计网站"><a href="#UI设计网站" class="headerlink" title="UI设计网站"></a>UI设计网站</h3><p>作为一个开发者，自己写些小程序的时候经常会为没有UI而烦恼，下面就是一些UI设计网站，有新颖的UI界面设计，也有单独的UI元素，icon等。</p><p><a href="http://www.ui.cn/" target="_blank" rel="noopener">UI中国</a> 国内潮流的UI设计作品。</p><p><a href="http://www.webdesigndev.com/" target="_blank" rel="noopener">webdesigndev</a> 国外网站设计文章，各种各样的资料看到你眼花缭乱。</p><p><a href="https://dribbble.com/" target="_blank" rel="noopener">dribbble</a> 接触过设计的应该都知道，一个很好的UI设计平台。</p><p><a href="http://www.flaticon.com/" target="_blank" rel="noopener">flaticon</a> 各种icon的设计，一定有你想想要的。</p><p><br></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2016/06/Develop_Tool/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 开发工具 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-线性回归算法</title>
      <link href="/2018/11/22/2018-07-21-ml-linearRegression-python/"/>
      <url>/2018/11/22/2018-07-21-ml-linearRegression-python/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h2 id="首先导入所需要的库文件"><a href="#首先导入所需要的库文件" class="headerlink" title="首先导入所需要的库文件"></a>首先导入所需要的库文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt  # 导入可视化库</span><br><span class="line">import numpy as np               # 导入数据处理库</span><br><span class="line">from sklearn import datasets     # 导入sklearn自带的数据集</span><br></pre></td></tr></table></figure><h2 id="把我们的参数值求解公式-theta-X-TX-1-X-TY-转换为代码"><a href="#把我们的参数值求解公式-theta-X-TX-1-X-TY-转换为代码" class="headerlink" title="把我们的参数值求解公式$\theta=(X^TX)^{-1}X^TY$转换为代码"></a>把我们的参数值求解公式$\theta=(X^TX)^{-1}X^TY$转换为代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def fit(self, X, y):                    # 训练集的拟合</span><br><span class="line">        X = np.insert(X, 0, 1, axis=1)  # 增加一个维度</span><br><span class="line">        print (X.shape)        </span><br><span class="line">        X_ = np.linalg.inv(X.T.dot(X))  # 公式求解 -- X.T表示转置，X.dot(Y)表示矩阵相乘</span><br><span class="line">        self.w = X_.dot(X.T).dot(y)     # 返回theta的值</span><br></pre></td></tr></table></figure><h1 id="其中：-X-TX-1-表示为："><a href="#其中：-X-TX-1-表示为：" class="headerlink" title="其中：$(X^TX)^{-1} $表示为："></a>其中：$(X^TX)^{-1} $表示为：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_ = np.linalg.inv(X.T.dot(X))  # 公式求解 -- X.T表示转置，X.dot(Y)表示矩阵相乘</span><br><span class="line"> np.linalg.inv() 表示求逆矩阵</span><br></pre></td></tr></table></figure><h1 id="其中：-X-TY-表示为："><a href="#其中：-X-TY-表示为：" class="headerlink" title="其中：$X^TY $表示为："></a>其中：$X^TY $表示为：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.T.dot(X)</span><br></pre></td></tr></table></figure><h1 id="所以完整公式-theta-X-TX-1-X-TY-表示为："><a href="#所以完整公式-theta-X-TX-1-X-TY-表示为：" class="headerlink" title="所以完整公式$\theta=(X^TX)^{-1}X^TY$表示为："></a>所以完整公式$\theta=(X^TX)^{-1}X^TY$表示为：</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_.dot(X.T).dot(y)</span><br></pre></td></tr></table></figure><h2 id="由于我们最终得到的线性回归函数是-y-theta-x-b-即预测函数："><a href="#由于我们最终得到的线性回归函数是-y-theta-x-b-即预测函数：" class="headerlink" title="由于我们最终得到的线性回归函数是$y=\theta x+b$即预测函数："></a>由于我们最终得到的线性回归函数是$y=\theta x+b$即预测函数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def predict(self, X):               # 测试集的测试反馈</span><br><span class="line">                                    # 为偏置权值插入常数项</span><br><span class="line">    X = np.insert(X, 0, 1, axis=1)  # 增加一个维度</span><br><span class="line">    y_pred = X.dot(self.w)          # 测试集与拟合的训练集相乘</span><br><span class="line">    return y_pred                   # 返回最终的预测值</span><br></pre></td></tr></table></figure><h2 id="其中得到的预测结果y-pred-X-text-cdot-theta"><a href="#其中得到的预测结果y-pred-X-text-cdot-theta" class="headerlink" title="其中得到的预测结果y_pred=X_text$\cdot\theta$"></a>其中得到的预测结果y_pred=X_text$\cdot\theta$</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred = X.dot(self.w)          # 测试集与参数值相乘</span><br></pre></td></tr></table></figure><h2 id="最终得出线性回归代码："><a href="#最终得出线性回归代码：" class="headerlink" title="最终得出线性回归代码："></a>最终得出线性回归代码：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class LinearRegression():</span><br><span class="line">    def __init__(self):          # 新建变量</span><br><span class="line">        self.w = None</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):         # 训练集的拟合</span><br><span class="line">        X = np.insert(X, 0, 1, axis=1)  # 增加一个维度</span><br><span class="line">        print (X.shape)        </span><br><span class="line">        X_ = np.linalg.inv(X.T.dot(X))  # 公式求解 -- X.T表示转置，X.dot(Y)表示矩阵相乘</span><br><span class="line">        self.w = X_.dot(X.T).dot(y)     # 返回theta的值</span><br><span class="line"></span><br><span class="line">    def predict(self, X):               # 测试集的测试反馈</span><br><span class="line">                                        # 为偏置权值插入常数项</span><br><span class="line">        X = np.insert(X, 0, 1, axis=1)  # 增加一个维度</span><br><span class="line">        y_pred = X.dot(self.w)          # 测试集与拟合的训练集相乘</span><br><span class="line">        return y_pred                   # 返回最终的预测值</span><br></pre></td></tr></table></figure><h2 id="同时我们需要得出预测值与真实值的一个平方平均值"><a href="#同时我们需要得出预测值与真实值的一个平方平均值" class="headerlink" title="同时我们需要得出预测值与真实值的一个平方平均值"></a>同时我们需要得出预测值与真实值的一个平方平均值</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def mean_squared_error(y_true, y_pred):</span><br><span class="line">                                        #真实数据与预测数据之间的差值（平方平均）</span><br><span class="line">    mse = np.mean(np.power(y_true - y_pred, 2))</span><br><span class="line">    return mse</span><br></pre></td></tr></table></figure><h2 id="最后就是进行我数据的加载，训练，测试过程以及可视化"><a href="#最后就是进行我数据的加载，训练，测试过程以及可视化" class="headerlink" title="最后就是进行我数据的加载，训练，测试过程以及可视化"></a>最后就是进行我数据的加载，训练，测试过程以及可视化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def main():</span><br><span class="line">    # 第一步：导入数据</span><br><span class="line">    # 加载糖尿病数据集</span><br><span class="line">    </span><br><span class="line">    diabetes = datasets.load_diabetes()</span><br><span class="line">    # 只使用其中一个特征值</span><br><span class="line">    X = diabetes.data[:, np.newaxis, 2]</span><br><span class="line">    print (X.shape)</span><br><span class="line"></span><br><span class="line">    #第二步：将数据分为训练集以及测试集</span><br><span class="line">    x_train, x_test = X[:-20], X[-20:]</span><br><span class="line">    y_train, y_test = diabetes.target[:-20], diabetes.target[-20:]</span><br><span class="line"></span><br><span class="line">    #第三步：导入线性回归类（之前定义的）</span><br><span class="line">    clf = LinearRegression()</span><br><span class="line">    clf.fit(x_train, y_train)    # 训练</span><br><span class="line">    y_pred = clf.predict(x_test) # 测试</span><br><span class="line"></span><br><span class="line">    #第四步：测试误差计算（需要引入一个函数）</span><br><span class="line">    # 打印平均值平方误差</span><br><span class="line">    print (&quot;Mean Squared Error:&quot;, mean_squared_error(y_test, y_pred))</span><br><span class="line"></span><br><span class="line">    #matplotlib可视化输出</span><br><span class="line">    # Plot the results</span><br><span class="line">    plt.scatter(x_test[:,0], y_test,  color=&apos;black&apos;)         # 散点输出</span><br><span class="line">    plt.plot(x_test[:,0], y_pred, color=&apos;blue&apos;, linewidth=3) # 预测输出</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h2 id="可视化结果"><a href="#可视化结果" class="headerlink" title="可视化结果"></a>可视化结果</h2><p><img src="/images/ml/5.png" alt="image"></p><h2 id="完整线性回归的代码"><a href="#完整线性回归的代码" class="headerlink" title="完整线性回归的代码"></a>完整线性回归的代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt  # 导入可视化库</span><br><span class="line">import numpy as np               # 导入数据处理库</span><br><span class="line">from sklearn import datasets     # 导入sklearn自带的数据集</span><br><span class="line">import csv</span><br><span class="line"></span><br><span class="line">class LinearRegression():</span><br><span class="line">    def __init__(self):          # 新建变量</span><br><span class="line">        self.w = None</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):         # 训练集的拟合</span><br><span class="line">        X = np.insert(X, 0, 1, axis=1)  # 增加一个维度</span><br><span class="line">        print (X.shape)        </span><br><span class="line">        X_ = np.linalg.inv(X.T.dot(X))  # 公式求解 -- X.T表示转置，X.dot(Y)表示矩阵相乘</span><br><span class="line">        self.w = X_.dot(X.T).dot(y)     # 返回theta的值</span><br><span class="line"></span><br><span class="line">    def predict(self, X):               # 测试集的测试反馈</span><br><span class="line">                                        # 为偏置权值插入常数项</span><br><span class="line">        X = np.insert(X, 0, 1, axis=1)  # 增加一个维度</span><br><span class="line">        y_pred = X.dot(self.w)          # 测试集与拟合的训练集相乘</span><br><span class="line">        return y_pred                   # 返回最终的预测值</span><br><span class="line"></span><br><span class="line">def mean_squared_error(y_true, y_pred):</span><br><span class="line">                                        #真实数据与预测数据之间的差值（平方平均）</span><br><span class="line">    mse = np.mean(np.power(y_true - y_pred, 2))</span><br><span class="line">    return mse</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    # 第一步：导入数据</span><br><span class="line">    # 加载糖尿病数据集</span><br><span class="line">    diabetes = datasets.load_diabetes()</span><br><span class="line">    # 只使用其中一个特征值(把一个422x10的矩阵提取其中一列变成422x1)</span><br><span class="line">    X = diabetes.data[:, np.newaxis, 2]  # np.newaxis的作用就是在原来的数组上增加一个维度。2表示提取第三列数据</span><br><span class="line">    print (X.shape)</span><br><span class="line"></span><br><span class="line">    # 第二步：将数据分为训练集以及测试集</span><br><span class="line">    x_train, x_test = X[:-20], X[-20:]</span><br><span class="line">    print(x_train.shape,x_test.shape)  # (422, 1) (20, 1)</span><br><span class="line">    # 将目标分为训练/测试集合</span><br><span class="line">    y_train, y_test = diabetes.target[:-20], diabetes.target[-20:]</span><br><span class="line">    print(y_train.shape,y_test.shape)  # (422,) (20,)</span><br><span class="line"></span><br><span class="line">    #第三步：导入线性回归类（之前定义的）</span><br><span class="line">    clf = LinearRegression()</span><br><span class="line">    clf.fit(x_train, y_train)    # 训练</span><br><span class="line">    y_pred = clf.predict(x_test) # 测试</span><br><span class="line"></span><br><span class="line">    #第四步：测试误差计算（需要引入一个函数）</span><br><span class="line">    # 打印平均值平方误差</span><br><span class="line">    print (&quot;Mean Squared Error:&quot;, mean_squared_error(y_test, y_pred))  # Mean Squared Error: 2548.072398725972</span><br><span class="line"></span><br><span class="line">    #matplotlib可视化输出</span><br><span class="line">    # Plot the results</span><br><span class="line">    plt.scatter(x_test[:,0], y_test,  color=&apos;black&apos;)         # 散点输出</span><br><span class="line">    plt.plot(x_test[:,0], y_pred, color=&apos;blue&apos;, linewidth=3) # 预测输出</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/ml-linearRegression-python/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-线性回归算法</title>
      <link href="/2018/11/22/2018-07-20-ml-linearRegression/"/>
      <url>/2018/11/22/2018-07-20-ml-linearRegression/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h1 id="线性回归–-y-wx-b"><a href="#线性回归–-y-wx-b" class="headerlink" title="线性回归–$y=wx+b$"></a><strong>线性回归</strong>–$y=wx+b$</h1><p><strong>回归</strong>，统计学术语，表示变量之间的某种数量依存关系，并由此引出回归方程，回归系数。</p><h3 id="线性回归（Linear-Regression）"><a href="#线性回归（Linear-Regression）" class="headerlink" title="线性回归（Linear Regression）"></a><strong>线性回归（Linear Regression）</strong></h3><p>数理统计中回归分析，用来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，其表达形式为$y = wx+e$，e为误差服从均值为0的正态分布，其中只有一个自变量的情况称为简单回归，多个自变量的情况叫多元回归。</p><p>注意，统计学中的回归并如线性回归非与严格直线函数完全能拟合，所以我们统计中称之为回归用以与其直线函数区别。</p><p>我们先来看下这个图</p><p><img src="/images/math/1.png" alt="image"></p><p>这个是近期比较火的现金贷产品的贷款额度。这个表格表示的是<strong>可贷款的金额 </strong>与 <strong>工资 </strong>和 <strong>房屋面积</strong>之间的关系，其中 <strong>工资</strong> 和 <strong>房屋面积</strong> 为 特征，<strong>可贷款金额</strong>为目标函数值。 那么根据线性函数可得到以下公式。</p><h2 id="h-theta-x-theta-1-x-1-theta-2-x-2"><a href="#h-theta-x-theta-1-x-1-theta-2-x-2" class="headerlink" title="$h_\theta(x)=\theta_{1}x_{1}+\theta_{2}x_{2} $"></a>$h_\theta(x)=\theta_{1}x_{1}+\theta_{2}x_{2} $</h2><p>上面的这个式子是当一个模型只有两个特征$$(x_1,x_2)$$的时候的线性回归式子。 正常情况下，现金贷中可贷款的额度和用户的很多特征相关联，并不只是简单的这两个特征。所以我们需要把这个式子进行通用化。 假如有n个特征的话，那么式子就会变成下面的样子</p><h2 id="h-theta-x-theta-1-x-1-theta-2-x-2-cdot-cdot-cdot-cdot-cdot-theta-n-x-n-sum-i-1-n-theta-i-x-i"><a href="#h-theta-x-theta-1-x-1-theta-2-x-2-cdot-cdot-cdot-cdot-cdot-theta-n-x-n-sum-i-1-n-theta-i-x-i" class="headerlink" title="$h_\theta(x)=\theta_{1}x_{1}+\theta_{2}x_{2} + \cdot \cdot \cdot \cdot \cdot+\theta_{n}x_{n} = \sum_{i=1}^{n}\theta_{i}x_{i}$"></a>$h_\theta(x)=\theta_{1}x_{1}+\theta_{2}x_{2} + \cdot \cdot \cdot \cdot \cdot+\theta_{n}x_{n} = \sum_{i=1}^{n}\theta_{i}x_{i}$</h2><h3 id="利用矩阵的知识对线性公式进行整合。"><a href="#利用矩阵的知识对线性公式进行整合。" class="headerlink" title="利用矩阵的知识对线性公式进行整合。"></a>利用矩阵的知识对线性公式进行整合。</h3><p>因为机器学习中基本上都是用矩阵的方式来表示参数的，也就是说我们需要把这个多项求和的式子用矩阵的方式表达出来，这样才方便后续的计算。</p><h2 id="theta-i-times-1-theta-1-theta-2-cdot-cdot-cdot-theta-i"><a href="#theta-i-times-1-theta-1-theta-2-cdot-cdot-cdot-theta-i" class="headerlink" title="$\theta_{i \times 1} = [\theta_1,\theta_2,\cdot\cdot\cdot\theta_i,]$"></a>$\theta_{i \times 1} = [\theta_1,\theta_2,\cdot\cdot\cdot\theta_i,]$</h2><h2 id="X-i-times1-x-1-x-2-cdot-cdot-cdot-x-i"><a href="#X-i-times1-x-1-x-2-cdot-cdot-cdot-x-i" class="headerlink" title="$X_{i\times1}=[x_1,x_2,\cdot \cdot \cdot x_i]$"></a>$X_{i\times1}=[x_1,x_2,\cdot \cdot \cdot x_i]$</h2><h3 id="把上述线性函数写成矩阵相乘的形式"><a href="#把上述线性函数写成矩阵相乘的形式" class="headerlink" title="把上述线性函数写成矩阵相乘的形式"></a>把上述线性函数写成矩阵相乘的形式</h3><h2 id="theta-TX-begin-bmatrix-theta-1-theta-2-cdot-cdot-cdot-theta-i-end-bmatrix-cdot-x-1-x-2-cdot-cdot-cdot-x-i-sum-i-1-n-theta-i-x-i"><a href="#theta-TX-begin-bmatrix-theta-1-theta-2-cdot-cdot-cdot-theta-i-end-bmatrix-cdot-x-1-x-2-cdot-cdot-cdot-x-i-sum-i-1-n-theta-i-x-i" class="headerlink" title="$\theta^TX=\begin{bmatrix}  \theta_1 \\  \theta_2 \\ \cdot \\ \cdot \\ \cdot \\ \theta_i \end{bmatrix} \cdot [x_1,x_2,\cdot \cdot \cdot x_i] = \sum_{i=1}^{n}\theta_{i}x_{i} $"></a>$\theta^TX=\begin{bmatrix} \theta_1 \\ \theta_2 \\ \cdot \\ \cdot \\ \cdot \\ \theta_i \end{bmatrix} \cdot [x_1,x_2,\cdot \cdot \cdot x_i] = \sum_{i=1}^{n}\theta_{i}x_{i} $</h2><p>我们把权重参数和特征参数，都看成是1行n列的矩阵(或者是行向量)。那么就可以根据矩阵乘法的相关知识，把上述多项求和的式子，转换成矩阵的乘法的表达式。 由此我们就把多项求和化简称了 。</p><h2 id="h-theta-x-theta-TX"><a href="#h-theta-x-theta-TX" class="headerlink" title="$h_\theta(x)=\theta^TX$"></a>$h_\theta(x)=\theta^TX$</h2><h3 id="误差项的分析"><a href="#误差项的分析" class="headerlink" title="误差项的分析"></a>误差项的分析</h3><p>原式:$$y =wx+b$$ 其中$b$就是我们所说的偏移量，或者叫误差项。</p><p>我们再看看下面这个图：</p><p><img src="/images/math/2.png" alt="image"></p><p>图中的横坐标$x_1$ 和 X$x)_2$分别代表着 两个特征(工资、房屋平米) 。纵坐标Y代表目标(可贷款的额度)。其中红点代表的就是实际的目标值(每个人可贷款的额度).而平面上和红点竖向相交的点代表着我们根据线性回归模型得到的点。也就是说实际得到的钱和预估的钱之间是有一定误差的，这个就是误差项。 因为误差项是真实值和误差值之间的一个差距。那么肯定我们希望误差项越小越好。</p><p>然后我们对应整理成线性回归函数：</p><h2 id="h-theta-x-theta-Tx-varepsilon"><a href="#h-theta-x-theta-Tx-varepsilon" class="headerlink" title="$h_\theta(x)=\theta^Tx+\varepsilon$"></a>$h_\theta(x)=\theta^Tx+\varepsilon$</h2><p>我们根据实际情况，假设认为这个误差项是满足以下几个条件的。</p><ol><li>误差$\varepsilon_{(i)}$是独立。</li><li>具有相同的分布。</li><li>服从均值为0方差为$\theta^2$的高斯分布。</li></ol><p>然我们回到刚开始的现金贷产品的贷款额度问题上面</p><p>1.独立：张三和李四一起使用这款产品，可贷款额互不影响</p><p>2.同分布：张三和李四是使用的是同一款产品</p><p>3.高斯分布：绝大多数的情况下，在一个的空间内浮动不大</p><h3 id="似然函数的理解"><a href="#似然函数的理解" class="headerlink" title="似然函数的理解"></a>似然函数的理解</h3><p>由前面两步，我们已经把线性回归模型，推导成下面的这个式子了</p><h1 id="y-i-theta-Tx-i-varepsilon-i"><a href="#y-i-theta-Tx-i-varepsilon-i" class="headerlink" title="$y_{(i)}=\theta^Tx_i+\varepsilon_i$"></a>$y_{(i)}=\theta^Tx_i+\varepsilon_i$</h1><p>我们已经知道误差项是符合高斯分布的，所以误差项的概率值：</p><h1 id="P-varepsilon-i-frac-1-sqrt-2-pi-sigma-e-frac-varepsilon-i-2-2-sigma-2"><a href="#P-varepsilon-i-frac-1-sqrt-2-pi-sigma-e-frac-varepsilon-i-2-2-sigma-2" class="headerlink" title="$P(\varepsilon_i)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(\varepsilon_i)^2}{2\sigma^2})}$"></a>$P(\varepsilon_i)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(\varepsilon_i)^2}{2\sigma^2})}$</h1><p>然后把误差值带入式子中:</p><h1 id="P-y-i-x-i-theta-frac-1-sqrt-2-pi-sigma-e-frac-y-i-theta-Tx-i-2-2-sigma-2"><a href="#P-y-i-x-i-theta-frac-1-sqrt-2-pi-sigma-e-frac-y-i-theta-Tx-i-2-2-sigma-2" class="headerlink" title="$P(y_i|x_i,\theta)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$"></a>$P(y_i|x_i,\theta)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$</h1><p>由于是误差值，所以是越小越好，所以我们接下来就是讨论什么样的特征值和特征组合能够让误差值最小，现在就要看似然函数的作用了，似然函数的作用就是要根据样本求什么样的参数和特征的组成能够接近真实值，所以越接近真实值则误差就越小。</p><p>引入似然函数(似然函数就是求能让真实值和预测值相等的那个参数的 )：</p><h1 id="L-theta-prod-i-1-N-P-y-i-x-i-theta-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-theta-Tx-i-2-2-sigma-2"><a href="#L-theta-prod-i-1-N-P-y-i-x-i-theta-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-theta-Tx-i-2-2-sigma-2" class="headerlink" title="$L(\theta) = \prod_{i=1}^{N} P(y_i|x_i,\theta)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$"></a>$L(\theta) = \prod_{i=1}^{N} P(y_i|x_i,\theta)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$</h1><p>$\prod$表示各元素相乘的结果</p><p>上面的式子是多个参数的乘积的形式，很难进行计算，所以我们又采用了对数的一个小技巧，把多个数相乘，转化成多个数相加的形式。</p><p>因为对数的性质</p><h2 id="logA-cdot-B-logA-logB"><a href="#logA-cdot-B-logA-logB" class="headerlink" title="$logA\cdot B = logA+logB$"></a>$logA\cdot B = logA+logB$</h2><p>根据上面的这种换算关系，我们就把似然函数的式子换算成下面的这个。 (因为似然函数是越大越好，似然函数的值和对数似然函数的值是成正比的，对值求对数，并不会影响到最后求极限的值。所以才敢进行对数处理。)</p><h1 id="l-theta-logL-theta-log-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-theta-Tx-i-2-2-sigma-2"><a href="#l-theta-logL-theta-log-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-theta-Tx-i-2-2-sigma-2" class="headerlink" title="$l(\theta) = logL(\theta) = log\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$"></a>$l(\theta) = logL(\theta) = log\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$</h1><p>对上式进行整理：</p><h1 id="l-theta-logL-theta-sum-i-1-N-log-frac-1-sqrt-2-pi-sigma-e-frac-y-i-theta-Tx-i-2-2-sigma-2"><a href="#l-theta-logL-theta-sum-i-1-N-log-frac-1-sqrt-2-pi-sigma-e-frac-y-i-theta-Tx-i-2-2-sigma-2" class="headerlink" title="$l(\theta) = logL(\theta) = \sum_{i=1}^{N}log\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$"></a>$l(\theta) = logL(\theta) = \sum_{i=1}^{N}log\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})}$</h1><h1 id="sum-i-1-N-log-frac-1-sqrt-2-pi-sigma-loge-frac-y-i-theta-Tx-i-2-2-sigma-2"><a href="#sum-i-1-N-log-frac-1-sqrt-2-pi-sigma-loge-frac-y-i-theta-Tx-i-2-2-sigma-2" class="headerlink" title="$= \sum_{i=1}^{N}(log\frac{1}{\sqrt{2\pi}\sigma}+loge^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})})$"></a>$= \sum_{i=1}^{N}(log\frac{1}{\sqrt{2\pi}\sigma}+loge^{-(\frac{(y_i-\theta^Tx_i)^2}{2\sigma^2})})$</h1><h1 id="Nlog-frac-1-sqrt-2-pi-sigma-frac-1-2-sigma-2-sum-i-1-N-y-i-theta-Tx-i-2"><a href="#Nlog-frac-1-sqrt-2-pi-sigma-frac-1-2-sigma-2-sum-i-1-N-y-i-theta-Tx-i-2" class="headerlink" title="$= Nlog\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2 $"></a>$= Nlog\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2 $</h1><p>因为：</p><p>$Nlog\frac{1}{\sqrt{2\pi}\sigma}$ 是一个定值</p><p>似然函数是要越大越好</p><p>所以：</p><h2 id="frac-1-2-sigma-2-y-i-theta-Tx-i-2-越小越好"><a href="#frac-1-2-sigma-2-y-i-theta-Tx-i-2-越小越好" class="headerlink" title="$-\frac{1}{2\sigma^2}(y_i-\theta^Tx_i)^2$越小越好"></a>$-\frac{1}{2\sigma^2}(y_i-\theta^Tx_i)^2$越小越好</h2><p>再因为：</p><h2 id="frac-1-2-sigma-2-也为定值"><a href="#frac-1-2-sigma-2-也为定值" class="headerlink" title="$-\frac{1}{2\sigma^2}$也为定值"></a>$-\frac{1}{2\sigma^2}$也为定值</h2><p>最终：</p><p>$l(\theta) = \sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$</p><h2 id="sum-i-1-N-y-i-theta-Tx-i-2-越小越好——最小二乘法（损失函数）"><a href="#sum-i-1-N-y-i-theta-Tx-i-2-越小越好——最小二乘法（损失函数）" class="headerlink" title="$\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$越小越好——最小二乘法（损失函数）"></a>$\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$越小越好——最小二乘法（损失函数）</h2><h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a><strong>最小二乘法</strong></h3><p>上述代价函数中使用的均方误差，其实对应了我们常用的欧几里得的距离（欧式距离，<strong>Euclidean Distance</strong>）, 基于均方误差最小化进行模型求解的方法称为“最小二乘法”（<strong>least square method</strong>），即通过最小化误差的平方和寻找数据的最佳函数匹配；</p><p>当函数子变量为一维时，最小二乘法就蜕变成寻找一条直线；</p><p>然后我们把得到的损失函数推广到n维，转换成矩阵形式（参考前面利用矩阵的知识对线性公式进行整合）：</p><h1 id="J-theta-sum-i-1-N-y-i-theta-Tx-i-2-损失函数"><a href="#J-theta-sum-i-1-N-y-i-theta-Tx-i-2-损失函数" class="headerlink" title="$J(\theta)=\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$       损失函数"></a>$J(\theta)=\sum_{i=1}^{N}(y_i-\theta^Tx_i)^2$ 损失函数</h1><h3 id="其对应的均方误差表示为如下矩阵"><a href="#其对应的均方误差表示为如下矩阵" class="headerlink" title="其对应的均方误差表示为如下矩阵"></a>其对应的均方误差表示为如下矩阵</h3><h1 id="J-theta-y-X-theta-T-y-X-theta"><a href="#J-theta-y-X-theta-T-y-X-theta" class="headerlink" title="$J(\theta) = {(y-X\theta)^T(y-X\theta)}$"></a>$J(\theta) = {(y-X\theta)^T(y-X\theta)}$</h1><h5 id="其中X："><a href="#其中X：" class="headerlink" title="其中X："></a>其中X：</h5><p>$X=\begin{bmatrix} 1 &amp;&amp; x_1^T \\ 1 &amp;&amp; x_2^T \\ \cdot \\ \cdot \\ \cdot \\ 1 &amp;&amp; x_N^T \end{bmatrix} =\begin{bmatrix} 1 &amp;&amp; x_{11} &amp;&amp; x_{12} &amp;&amp; \cdot \cdot \cdot x_{1n} \\ 1 &amp;&amp; x_{21} &amp;&amp; x_{22} &amp;&amp; \cdot \cdot \cdot x_{2n} \\ \cdot \\ \cdot \\ \cdot \\ 1&amp;&amp; x_{m1} &amp;&amp; x_{m2} &amp;&amp; \cdot \cdot \cdot x_{mn} \end{bmatrix} $</p><p>对$\theta​$求导</p><h2 id="J-theta-y-X-theta-T-y-X-theta-y-Ty-y-Tx-theta-theta-Tx-Ty-theta-Tx-Tx-theta"><a href="#J-theta-y-X-theta-T-y-X-theta-y-Ty-y-Tx-theta-theta-Tx-Ty-theta-Tx-Tx-theta" class="headerlink" title="$J(\theta) = {(y-X\theta)^T(y-X\theta)}=y^Ty-y^Tx\theta-\theta^Tx^Ty+\theta^Tx^Tx\theta$"></a>$J(\theta) = {(y-X\theta)^T(y-X\theta)}=y^Ty-y^Tx\theta-\theta^Tx^Ty+\theta^Tx^Tx\theta$</h2><h1 id="frac-partial-J-theta-partial-theta-frac-partial-y-Ty-partial-theta-frac-partial-y-Tx-theta-partial-theta-frac-partial-theta-Tx-Ty-partial-theta-frac-partial-theta-Tx-Tx-theta-partial-theta"><a href="#frac-partial-J-theta-partial-theta-frac-partial-y-Ty-partial-theta-frac-partial-y-Tx-theta-partial-theta-frac-partial-theta-Tx-Ty-partial-theta-frac-partial-theta-Tx-Tx-theta-partial-theta" class="headerlink" title="$\frac{\partial J(\theta)}{\partial(\theta)} = \frac{\partial y^Ty}{\partial(\theta)} - \frac{\partial y^Tx\theta}{\partial(\theta)} - \frac{\partial \theta^Tx^Ty}{\partial(\theta)} + \frac{\partial \theta^Tx^Tx\theta}{\partial(\theta)} $"></a>$\frac{\partial J(\theta)}{\partial(\theta)} = \frac{\partial y^Ty}{\partial(\theta)} - \frac{\partial y^Tx\theta}{\partial(\theta)} - \frac{\partial \theta^Tx^Ty}{\partial(\theta)} + \frac{\partial \theta^Tx^Tx\theta}{\partial(\theta)} $</h1><h1 id="frac-partial-J-theta-partial-theta-0-x-Ty-x-Ty-2x-Tx-theta"><a href="#frac-partial-J-theta-partial-theta-0-x-Ty-x-Ty-2x-Tx-theta" class="headerlink" title="$\frac{\partial J(\theta)}{\partial(\theta)} = 0-x^Ty-x^Ty+2x^Tx\theta$"></a>$\frac{\partial J(\theta)}{\partial(\theta)} = 0-x^Ty-x^Ty+2x^Tx\theta$</h1><h1 id="frac-partial-J-theta-partial-theta-2x-T-x-theta-y"><a href="#frac-partial-J-theta-partial-theta-2x-T-x-theta-y" class="headerlink" title="$\frac{\partial J(\theta)}{\partial(\theta)} =2x^T(x\theta-y)$"></a>$\frac{\partial J(\theta)}{\partial(\theta)} =2x^T(x\theta-y)$</h1><p>根据导数的性质，该值在导数为0时为最小</p><p>所以：根据微积分定理，令上式等于零，可以得到 θ 最优的闭式解。当</p><h2 id="2-x-Ty-x-Tx-theta-0-时取得最小"><a href="#2-x-Ty-x-Tx-theta-0-时取得最小" class="headerlink" title="$2(x^Ty-x^Tx\theta)=0$时取得最小"></a>$2(x^Ty-x^Tx\theta)=0$时取得最小</h2><p>矩阵求导的知识：</p><p><img src="/images/math/3.png" alt="image"></p><h1 id="最终：-theta-x-Tx-1-x-Ty"><a href="#最终：-theta-x-Tx-1-x-Ty" class="headerlink" title="最终：$\theta = (x^Tx)^{-1}x^Ty$"></a>最终：$\theta = (x^Tx)^{-1}x^Ty$</h1><p>X和Y都是已知的，那么得到了最终的参数值。</p><h3 id="那我们来看看数学原理"><a href="#那我们来看看数学原理" class="headerlink" title="那我们来看看数学原理"></a>那我们来看看<strong>数学原理</strong></h3><p><strong>微积分角度来讲</strong>，最小二乘法是采用非迭代法，针对代价函数求导数而得出全局极值，进而对所给定参数进行估算。</p><p><strong>计算数学角度来讲</strong>，最小二乘法的本质上是一个线性优化问题，试图找到一个最优解。</p><p><strong>线性代数角度来讲</strong>，最小二乘法是求解线性方程组，当方程个数大于未知量个数，其方程本身 无解，而最小二乘法则试图找到最优残差。</p><p><strong>几何角度来讲</strong>，最小二乘法中的几何意义是高维空间中的一个向量在低维子空间的投影。</p><p><strong>概率论角度来讲</strong>，如果数据的观测误差是/或者满足高斯分布，则最小二乘解就是使得观测数据出现概率最大的解，即<strong>最大似然估计-Maximum Likelihood Estimate，MLE</strong>（利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值）。</p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/ml-linearRegression/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Xpath提取网页数据</title>
      <link href="/2018/11/21/2018-11-21-reptile-xml/"/>
      <url>/2018/11/21/2018-11-21-reptile-xml/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><h3 id="Xpath提取网页数据"><a href="#Xpath提取网页数据" class="headerlink" title="Xpath提取网页数据"></a>Xpath提取网页数据</h3><p>XPath 是一门在 XML 文档中查找信息的语言。XPath 用于在 XML 文档中通过元素和属性进行导航。</p><p>相比于<code>BeautifulSoup</code>，<code>Xpath</code>在提取数据时会更加的方便。</p><hr><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在Python中很多库都有提供<code>Xpath</code>的功能，但是最基本的还是<code>lxml</code>这个库，效率最高。在之前<code>BeautifulSoup</code>章节中我们也介绍到了<code>lxml</code>是如何安装的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install lxml</span><br></pre></td></tr></table></figure><hr><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><p>XPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。</p><p>我们将用以下的HTML文档来进行演示：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">html_doc = '''<span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bookstore</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">book</span> <span class="attr">category</span>=<span class="string">"COOKING"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span>Everyday Italian<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">author</span>&gt;</span>Giada De Laurentiis<span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">year</span>&gt;</span>2005<span class="tag">&lt;/<span class="name">year</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">price</span>&gt;</span>30.00<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">book</span> <span class="attr">category</span>=<span class="string">"CHILDREN"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span>Harry Potter<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">author</span>&gt;</span>J K. Rowling<span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">year</span>&gt;</span>2005<span class="tag">&lt;/<span class="name">year</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">price</span>&gt;</span>29.99<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">book</span> <span class="attr">category</span>=<span class="string">"WEB"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span>XQuery Kick Start<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">author</span>&gt;</span>James McGovern<span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">author</span>&gt;</span>Per Bothner<span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">author</span>&gt;</span>Kurt Cagle<span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">author</span>&gt;</span>James Linn<span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">author</span>&gt;</span>Vaidyanathan Nagarajan<span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">year</span>&gt;</span>2003<span class="tag">&lt;/<span class="name">year</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">price</span>&gt;</span>49.99<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">book</span> <span class="attr">category</span>=<span class="string">"WEB"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">title</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span>Learning XML<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">author</span>&gt;</span>Erik T. Ray<span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">year</span>&gt;</span>2003<span class="tag">&lt;/<span class="name">year</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">price</span>&gt;</span>39.95<span class="tag">&lt;/<span class="name">price</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">book</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bookstore</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span>'''</span><br></pre></td></tr></table></figure><p>导入语句，并生成HTML的DOM树：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">page = etree.HTML(html_doc)</span><br></pre></td></tr></table></figure><hr><h3 id="路径查找"><a href="#路径查找" class="headerlink" title="路径查找"></a>路径查找</h3><table><thead><tr><th style="text-align:left">表达式</th><th>描述</th></tr></thead><tbody><tr><td style="text-align:left">nodename</td><td>选取此节点的子节点。</td></tr><tr><td style="text-align:left">/</td><td>从根节点选取。</td></tr><tr><td style="text-align:left">//</td><td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。</td></tr><tr><td style="text-align:left">.</td><td>选取当前节点。</td></tr><tr><td style="text-align:left">..</td><td>选取当前节点的父节点。</td></tr><tr><td style="text-align:left">@</td><td>选取属性。</td></tr></tbody></table><ul><li><p>查找当前节点的子节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: page.xpath(<span class="string">'head'</span>)</span><br><span class="line">Out[<span class="number">1</span>]: [&lt;Element head at <span class="number">0x111c74c48</span>&gt;]</span><br></pre></td></tr></table></figure></li><li><p>从根节点进行查找</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: page.xpath(<span class="string">'/html'</span>)</span><br><span class="line">Out[<span class="number">2</span>]: [&lt;Element html at <span class="number">0x11208be88</span>&gt;]</span><br></pre></td></tr></table></figure></li><li><p>从整个文档中所有节点查找</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">3</span>]: page.xpath(<span class="string">'//book'</span>)</span><br><span class="line">Out[<span class="number">3</span>]:</span><br><span class="line">[&lt;Element book at <span class="number">0x1128c02c8</span>&gt;,</span><br><span class="line"> &lt;Element book at <span class="number">0x111c74108</span>&gt;,</span><br><span class="line"> &lt;Element book at <span class="number">0x111fd2288</span>&gt;,</span><br><span class="line"> &lt;Element book at <span class="number">0x1128da348</span>&gt;]</span><br></pre></td></tr></table></figure></li><li><p>选取当前节点的父节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: page.xpath(<span class="string">'//book'</span>)[<span class="number">0</span>].xpath(<span class="string">'..'</span>)</span><br><span class="line">Out[<span class="number">4</span>]: [&lt;Element bookstore at <span class="number">0x1128c0ac8</span>&gt;]</span><br></pre></td></tr></table></figure></li><li><p>选取属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">5</span>]: page.xpath(<span class="string">'//book'</span>)[<span class="number">0</span>].xpath(<span class="string">'@category'</span>)</span><br><span class="line">Out[<span class="number">5</span>]: [<span class="string">'COOKING'</span>]</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="节点查找"><a href="#节点查找" class="headerlink" title="节点查找"></a>节点查找</h3><table><thead><tr><th>表达式</th><th>结果</th></tr></thead><tbody><tr><td>nodename[1]</td><td>选取第一个元素。</td></tr><tr><td>nodename[last()]</td><td>选取最后一个元素。</td></tr><tr><td>nodename[last()-1]</td><td>选取倒数第二个元素。</td></tr><tr><td>nodename[position()&lt;3]</td><td>选取前两个子元素。</td></tr><tr><td>nodename[@lang]</td><td>选取拥有名为 lang 的属性的元素。</td></tr><tr><td>nodename[@lang=’eng’]</td><td>选取拥有lang属性，且值为 eng 的元素。</td></tr></tbody></table><ul><li><p>选取第二个book元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: page.xpath(<span class="string">'//book[2]/@category'</span>)</span><br><span class="line">Out[<span class="number">1</span>]: [<span class="string">'CHILDREN'</span>]</span><br></pre></td></tr></table></figure></li><li><p>选取倒数第三个book元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: page.xpath(<span class="string">'//book[last()-2]/@category'</span>)</span><br><span class="line">Out[<span class="number">2</span>]: [<span class="string">'CHILDREN'</span>]</span><br></pre></td></tr></table></figure></li><li><p>选取第二个元素开始的所有元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">3</span>]: page.xpath(<span class="string">'//book[position() &gt; 1]/@category'</span>)</span><br><span class="line">Out[<span class="number">3</span>]: [<span class="string">'CHILDREN'</span>, <span class="string">'WEB'</span>, <span class="string">'WEB'</span>]</span><br></pre></td></tr></table></figure></li><li><p>选取category属性为WEB的的元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: page.xpath(<span class="string">'//book[@category="WEB"]/@category'</span>)</span><br><span class="line">Out[<span class="number">4</span>]: [<span class="string">'WEB'</span>, <span class="string">'WEB'</span>]</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="未知节点"><a href="#未知节点" class="headerlink" title="未知节点"></a>未知节点</h3><table><thead><tr><th>通配符</th><th>描述</th></tr></thead><tbody><tr><td>*</td><td>匹配任何元素节点。</td></tr><tr><td>@*</td><td>匹配任何属性节点。</td></tr></tbody></table><ul><li><p>匹配第一个book元素下的所有元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: page.xpath(<span class="string">'//book[1]/*'</span>)</span><br><span class="line">Out[<span class="number">1</span>]:</span><br><span class="line">[&lt;Element title at <span class="number">0x111f76788</span>&gt;,</span><br><span class="line"> &lt;Element author at <span class="number">0x111f76188</span>&gt;,</span><br><span class="line"> &lt;Element year at <span class="number">0x1128c1a88</span>&gt;,</span><br><span class="line"> &lt;Element price at <span class="number">0x1128c1cc8</span>&gt;]</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="获取节点中的文本"><a href="#获取节点中的文本" class="headerlink" title="获取节点中的文本"></a>获取节点中的文本</h3><ul><li><p>用<code>text()</code>获取某个节点下的文本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: page.xpath(<span class="string">'//book[1]/author/text()'</span>)</span><br><span class="line">Out[<span class="number">1</span>]: [<span class="string">'Giada De Laurentiis'</span>]</span><br></pre></td></tr></table></figure><p>如果这个节点下有多个文本，则只能取到一段。</p></li><li><p>用<code>string()</code>获取某个节点下所有的文本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: page.xpath(<span class="string">'string(//book[1])'</span>)</span><br><span class="line">Out[<span class="number">2</span>]: <span class="string">'\n            Everyday Italian\n            Giada De Laurentiis\n            2005\n            30.00\n        '</span></span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="选取多个路径"><a href="#选取多个路径" class="headerlink" title="选取多个路径"></a>选取多个路径</h3><p>通过在路径表达式中使用“|”运算符，您可以选取若干个路径。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: page.xpath(<span class="string">'//book[1]/title/text() | //book[1]/author/text()'</span>)</span><br><span class="line">Out[<span class="number">1</span>]: [<span class="string">'Everyday Italian'</span>, <span class="string">'Giada De Laurentiis'</span>]</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>logging-Python多层级日志输出</title>
      <link href="/2018/11/21/2018-11-21-python-log/"/>
      <url>/2018/11/21/2018-11-21-python-log/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><p>本文地址：<a href="https://www.jianshu.com/p/3be28b5d2ff8" target="_blank" rel="noopener">https://www.jianshu.com/p/3be28b5d2ff8</a></p><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在应用的开发过程中，我们常常需要去记录应用的状态，事件，结果。而Python最基础的Print很难满足我们的需求，这种情况下我们就需要使用python的另一个标准库：<code>logging</code>。</p><p>这是一个专门用于记录日志的模块。相对于Print来说，<code>logging</code>提供了日志信息的分级，格式化，过滤等功能。如果在程序中定义了丰富而有条理的log信息，那么可以非常方便的去分析程序的运行状况，在有问题时也能够方便的去定位问题，分析问题。</p><p>以下是具体的一些应用场景。</p><table><thead><tr><th style="text-align:left">执行的任务</th><th style="text-align:left">这项任务的最佳工具</th></tr></thead><tbody><tr><td style="text-align:left">显示控制台输出</td><td style="text-align:left">print()</td></tr><tr><td style="text-align:left">报告在程序正常运行期间发生的事件</td><td style="text-align:left">logging.info()或 logging.debug()</td></tr><tr><td style="text-align:left">发出有关特定运行时事件的警告</td><td style="text-align:left">logging.warning()</td></tr><tr><td style="text-align:left">报告有关特定运行时事件的错误</td><td style="text-align:left">抛出异常</td></tr><tr><td style="text-align:left">报告错误但不抛出异常</td><td style="text-align:left">logging.error()， logging.exception()或 logging.critical()</td></tr></tbody></table><hr><h3 id="基础用法"><a href="#基础用法" class="headerlink" title="基础用法"></a>基础用法</h3><p>以下是一些<code>logging</code>最基础的使用方法，如果不需要深入的去定制log的话，那么只需要使用最基础的部分即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: logging.info(<span class="string">'hello world'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: logging.warning(<span class="string">'good luck'</span>)</span><br><span class="line">WARNING:root:good luck</span><br></pre></td></tr></table></figure><p>可以看到，<code>logging.info()</code>的日志信息没有被输出，而<code>logging.warning()</code>的日志信息被输出了，这就是因为<code>logging</code>的日志信息分为几个不同的重要性级别，而默认输出的级别则是<code>warning</code>，也就是说，重要性大于等于<code>warning</code>的信息才会被输出。</p><p>以下是<code>logging</code>模块中信息的五个级别，重要性从上往下递增。</p><table><thead><tr><th>等级</th><th>什么时候使用</th></tr></thead><tbody><tr><td><code>DEBUG</code></td><td>详细信息，通常仅在Debug时使用。</td></tr><tr><td><code>INFO</code></td><td>程序正常运行时输出的信息。</td></tr><tr><td><code>WARNING</code></td><td>表示有些预期之外的情况发生，或者在将来可能发生什么情况。程序依然能按照预期运行。</td></tr><tr><td><code>ERROR</code></td><td>因为一些严重的问题，程序的某些功能无法使用了。</td></tr><tr><td><code>CRITICAL</code></td><td>发生了严重的错误，程序已经无法运行。</td></tr></tbody></table><p>我们也可以通过设置来设定输出日志的级别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: logging.basicConfig(level=logging.DEBUG)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: logging.info(<span class="string">'hello world'</span>)</span><br><span class="line">INFO:root:hello world</span><br></pre></td></tr></table></figure><p>可以看到，在设定了<code>level</code>参数为<code>logging.DEBUG</code>后，<code>logging.info()</code>的日志信息就正常输出了。</p><hr><h3 id="logging-basicConfig-kwargs"><a href="#logging-basicConfig-kwargs" class="headerlink" title="logging.basicConfig(*\kwargs*)"></a>logging.basicConfig(*<em>\</em>kwargs*)</h3><p>通过<code>basicConfig()</code>方法可以为<code>logging</code>做一些简单的配置。此方法可以传递一些关键字参数。</p><ul><li><p><strong>filename</strong></p><p>文件名参数，如果指定了这个参数，那么<code>logging</code>会把日志信息输入到指定的文件之中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.basicConfig(filename=<span class="string">'example.log'</span>)</span><br><span class="line">logging.warning(<span class="string">'Hello world'</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>filemode</strong></p><p>如果指定了<code>filename</code>来输出日志到文件，那么<code>filemode</code>就是打开文件的模式，默认为’a’，追加模式。当然也可以设置为’w’，则每一次输入都会丢弃掉之前日志文件中的内容。</p></li><li><p><strong>format</strong></p><p>指定输出的log信息的格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: logging.basicConfig(format=<span class="string">'%(asctime)s %(message)s'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: logging.warning(<span class="string">'hello world'</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-07</span><span class="number">-06</span> <span class="number">16</span>:<span class="number">28</span>:<span class="number">12</span>,<span class="number">074</span> hello world</span><br></pre></td></tr></table></figure></li><li><p><strong>datefmt</strong></p><p>如果在<code>format</code>中使用了<code>asctime</code>输出时间，那么可以使用此参数控制输出日期的格式，使用方式与<code>time.strftime()</code>相同。</p></li><li><p><strong>level</strong></p><p>设置输出的日志的级别，只有高出此级别的日志信息才会被输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: logging.basicConfig(level=logging.INFO)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: logging.info(<span class="string">'hi'</span>)</span><br><span class="line">INFO:root:hi</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: logging.debug(<span class="string">'byebye'</span>)</span><br></pre></td></tr></table></figure></li></ul><p><strong>注：需要注意的是，<code>basicConfig()</code>方法是一个一次性的方法，只能用来做简单的配置，多次的调用<code>basicConfig()</code>是无效的。</strong></p><hr><h3 id="深度定制logging日志信息"><a href="#深度定制logging日志信息" class="headerlink" title="深度定制logging日志信息"></a>深度定制logging日志信息</h3><p>在深度使用<code>logging</code>来定制日志信息之前，我们需要先来了解一下<code>logging</code>的结构。<code>logging</code>的主要逻辑结构主要由以下几个组件构成：</p><ul><li><strong>Logger</strong>：提供应用程序直接使用的接口。</li><li><strong>Handler</strong>：将log信息发送到目标位置。</li><li><strong>Filter</strong>：提供更加细粒度的log信息过滤。</li><li><strong>Formatter</strong>：格式化log信息。</li></ul><p>这四个组件是<code>logging</code>模块的基础，在基础用法中的使用方式，其实也是这四大组件的封装结果。</p><p>这四个组件的关系如下所示：</p><p><img src="/images/py/68.png" alt="logging-结构"></p><p><code>logger</code>主要为外部提供使用的api接口，而每个<code>logger</code>下可以设置多个<code>Handler</code>，来将log信息输出到多个位置，而每一个<code>Handler</code>下又可以设置一个<code>Formatter</code>和多个<code>Filter</code>来定制输出的信息。</p><hr><h3 id="Logger"><a href="#Logger" class="headerlink" title="Logger"></a>Logger</h3><p><code>Logger</code>这个对象主要有三个任务要做：</p><ul><li>向外部提供使用接口。</li><li>基于日志严重等级（默认的过滤组件）或filter对象来决定要对哪些日志进行后续处理。</li><li>将日志消息传送给所有符合输出级别的<code>Handlers</code>。</li></ul><h4 id="logging-getLogger-name-None"><a href="#logging-getLogger-name-None" class="headerlink" title="logging.getLogger(name=None)"></a>logging.getLogger(name=None)</h4><p>首先，我们需要通过<code>getLogger()</code>方法来生成一个<code>Logger</code>，这个方法中有一个参数name，则是生成的<code>Logger</code>的名称，如果不传或者传入一个空值的话，<code>Logger</code>的名称默认为root。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: logger = logging.getLogger(<span class="string">'nanbei'</span>)</span><br></pre></td></tr></table></figure><p>需要注意的是，只要在同一个解释器的进程中，那么相同的<code>Logger</code>名称，使用<code>getLogger()</code>方法将会指向同一个<code>Logger</code>对象。</p><p>而使用<code>logger</code>的一个好习惯，是生成一个模块级别的<code>Logger</code>对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: logger = logging.getLogger(__name__)</span><br></pre></td></tr></table></figure><p>通过这种方式，我们可以让<code>logger</code>清楚的记录下事件发生的模块位置。</p><p>除此之外，<code>logger</code>对象是有层级结构的：</p><ul><li><p><code>Logger</code>的名称可以是一个以<code>.</code>分割的层级结构，每个<code>.</code>后面的<code>Logger</code>都是<code>.</code>前面的<code>logger</code>的子辈。</p><p>例如，有一个名称为<strong>nanbei</strong>的<code>logger</code>，其它名称分别为<strong>nanbei.a</strong>，<strong>nanbei.b</strong>和<strong>nanbei.a.c</strong>都是<strong>nanbei</strong>的后代。</p></li><li><p>子<code>Logger</code>在完成对日志消息的处理后，默认会将log日志消息传递给它们的父辈<code>Logger</code>相关的<code>Handler</code>。</p><p>因此，我们不不需要去配置每一个的<code>Logger</code>，只需要将程序中一个顶层的<code>Logger</code>配置好，然后按照需要创建子<code>Logger</code>就好了。也可以通过将一个<code>logger</code>的<code>propagate</code>属性设置为False来关闭这种传递机制。</p></li></ul><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> logging</span><br><span class="line"><span class="comment"># 生成一个名称为nanbei的Logger</span></span><br><span class="line">In [<span class="number">2</span>]: logger = logging.getLogger(<span class="string">'nanbei'</span>)</span><br><span class="line"><span class="comment"># 生成一个StreamHandler，这个Handler可以将日志输出到console中</span></span><br><span class="line">In [<span class="number">3</span>]: sh = logging.StreamHandler()</span><br><span class="line"><span class="comment"># 生成一个Formatter对象，使输出日志时只显示Logger名称和日志信息</span></span><br><span class="line">In [<span class="number">4</span>]: fmt = logging.Formatter(fmt=<span class="string">'%(name)s - %(message)s'</span>)</span><br><span class="line"><span class="comment"># 设置Formatter到StreamHandler中</span></span><br><span class="line">In [<span class="number">5</span>]: sh.setFormatter(fmt)</span><br><span class="line"><span class="comment"># 将Handler添加到Logger中</span></span><br><span class="line">In [<span class="number">6</span>]: logger.addHandler(sh)</span><br><span class="line"><span class="comment"># 生成一个nanbei的子Logger：nanbei.child</span></span><br><span class="line">In [<span class="number">7</span>]: child_logger = logging.getLogger(<span class="string">'nanbei.child'</span>)</span><br><span class="line"><span class="comment"># 可以看到两个Logger输出的日志信息都使用了相同的日志格式</span></span><br><span class="line">In [<span class="number">8</span>]: logger.warning(<span class="string">'hello'</span>)</span><br><span class="line">nanbei - hello</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: child_logger.warning(<span class="string">'hello'</span>)</span><br><span class="line">nanbei.child - hello</span><br></pre></td></tr></table></figure><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>在<code>Logger</code>对象中，主要提供了以下方法：</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>Logger.setLevel()</td><td>设置日志器将会处理的日志消息的最低输出级别</td></tr><tr><td>Logger.addHandler() 和 Logger.removeHandler()</td><td>为该logger对象添加、移除一个handler对象</td></tr><tr><td>Logger.addFilter() 和 Logger.removeFilter()</td><td>为该logger对象添加、移除一个filter对象</td></tr><tr><td>Logger.debug()，Logger.info()，Logger.warning()，Logger.error()，Logger.critical()</td><td>输出一条与方法名对应等级的日志</td></tr><tr><td>Logger.exception()</td><td>输出一条与Logger.error()类似的日志</td></tr><tr><td>Logger.log()</td><td>可以传入一个明确的日志level参数来输出一条日志</td></tr></tbody></table><hr><h3 id="Handler"><a href="#Handler" class="headerlink" title="Handler"></a>Handler</h3><p><code>Handler</code>的作用主要是把log信息输出到我们希望的目标位置，其提供了如下的方法以供使用：</p><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>Handler.setLevel()</td><td>设置handler处理日志消息的最低级别</td></tr><tr><td>Handler.setFormatter()</td><td>为handler设置一个格式器对象</td></tr><tr><td>Handler.addFilter() 和 Handler.removeFilter()</td><td>为handler添加、删除一个过滤器对象</td></tr></tbody></table><p>我们可以通过这几个方法，给每一个<code>Handler</code>设置一个<code>Formatter</code>和多个<code>Filter</code>，来定制不同的输出log信息的策略。</p><p>而<code>Handler</code>本身是一个基类，不应该直接实例化使用，我们应该使用的是其多种多样的子类，每一个不同的子类可以将日志信息输出到不同的目标位置，以下是一些常用的<code>Handler</code>。</p><table><thead><tr><th>Handler</th><th>描述</th></tr></thead><tbody><tr><td>logging.StreamHandler</td><td>将日志消息发送到输出到Stream，如std.out, std.err或任何file-like对象。</td></tr><tr><td>logging.FileHandler</td><td>将日志消息发送到磁盘文件，默认情况下文件大小会无限增长</td></tr><tr><td>logging.handlers.RotatingFileHandler</td><td>将日志消息发送到磁盘文件，并支持日志文件按大小切割</td></tr><tr><td>logging.hanlders.TimedRotatingFileHandler</td><td>将日志消息发送到磁盘文件，并支持日志文件按时间切割</td></tr><tr><td>logging.handlers.HTTPHandler</td><td>将日志消息以GET或POST的方式发送给一个HTTP服务器</td></tr><tr><td>logging.handlers.SMTPHandler</td><td>将日志消息发送给一个指定的email地址</td></tr><tr><td>logging.NullHandler</td><td>该Handler实例会忽略error messages，通常被想使用logging的library开发者使用来避免’No handlers could be found for logger XXX’信息的出现。</td></tr></tbody></table><hr><h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><p><code>Filter</code>可以被<code>Handler</code>和<code>Logger</code>用来做比level分级更细粒度的、更复杂的过滤功能。</p><p><code>Filter</code>是一个过滤器基类，它可以通过name参数，来使这个<code>logger</code>下的日志通过过滤。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">logging</span>.<span class="title">Filter</span><span class="params">(name=<span class="string">''</span>)</span></span></span><br></pre></td></tr></table></figure><p>比如，一个<code>Filter</code>实例化时传递的name参数值为<code>A.B</code>，那么该<code>Filter</code>实例将只允许名称为类似如下规则的<code>Loggers</code>产生的日志通过过滤：<code>A.B</code>，<code>A.B.C</code>，<code>A.B.C.D</code>，<code>A.B.D</code>。</p><p>而名称为<code>A.BB</code>，<code>B.A.B</code>的<code>Loggers</code>产生的日志则会被过滤掉。如果name的值为空字符串，则允许所有的日志事件通过过滤。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: logger = logging.getLogger(<span class="string">'nanbei'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: filt = logging.Filter(name=<span class="string">'nanbei.a'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: sh = logging.StreamHandler()</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: sh.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: sh.addFilter(filt)</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: logger.addHandler(sh)</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: logging.getLogger(<span class="string">'nanbei.a.b'</span>).warning(<span class="string">'i am nanbei.a.b'</span>)</span><br><span class="line">i am nanbei.a.b</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: logging.getLogger(<span class="string">'nanbei.b.b'</span>).warning(<span class="string">'i am nanbei.a.b'</span>)</span><br></pre></td></tr></table></figure><p>可以看到，名称为<strong>nanbei.b.b</strong>的<code>Logger</code>的日志没有被输出。</p><hr><h3 id="Formatter"><a href="#Formatter" class="headerlink" title="Formatter"></a>Formatter</h3><p><code>Formater</code>对象用于配置日志信息的最终顺序、结构和内容。</p><p><code>Formatter</code>类的构造方法定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logging.Formatter.__init__(fmt=<span class="keyword">None</span>, datefmt=<span class="keyword">None</span>, style=<span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><ul><li><p><strong>fmt</strong></p><p>这个参数主要用于格式化log信息整体的输出。</p><p>以下是可以用来格式化的字段：</p><p>| 字段/属性名称 | 使用格式 | 描述 |<br>| :————– | :—————— | :———————————————————-: |<br>| asctime | %(asctime)s | 日志事件发生的时间–人类可读时间，如：2003-07-08 16:49:45,896 |<br>| created | %(created)f | 日志事件发生的时间–时间戳，就是当时调用time.time()函数返回的值 |<br>| relativeCreated | %(relativeCreated)d | 日志事件发生的时间相对于logging模块加载时间的相对毫秒数（目前还不知道干嘛用的） |<br>| msecs | %(msecs)d | 日志事件发生事件的毫秒部分 |<br>| levelname | %(levelname)s | 该日志记录的文字形式的日志级别（’DEBUG’, ‘INFO’, ‘WARNING’, ‘ERROR’, ‘CRITICAL’） |<br>| levelno | %(levelno)s | 该日志记录的数字形式的日志级别（10, 20, 30, 40, 50） |<br>| name | %(name)s | 所使用的日志器名称，默认是’root’，因为默认使用的是 rootLogger |<br>| message | %(message)s | 日志记录的文本内容，通过 <code>msg % args</code>计算得到的 |<br>| pathname | %(pathname)s | 调用日志记录函数的源码文件的全路径 |<br>| filename | %(filename)s | pathname的文件名部分，包含文件后缀 |<br>| module | %(module)s | filename的名称部分，不包含后缀 |<br>| lineno | %(lineno)d | 调用日志记录函数的源代码所在的行号 |<br>| funcName | %(funcName)s | 调用日志记录函数的函数名 |<br>| process | %(process)d | 进程ID |<br>| processName | %(processName)s | 进程名称，Python 3.1新增 |<br>| thread | %(thread)d | 线程ID |<br>| threadName | %(thread)s | 线程名称 |</p></li><li><p><strong>datefmt</strong></p><p>如果在dmt中指定了asctime，那么这个参数可以用来格式化asctime的输出，使用方式与time.strftime()相同。</p></li><li><p><strong>style</strong></p><p>Python 3.2新增的参数，可取值为 ‘%’, ‘{‘和 ‘$’，如果不指定该参数则默认使用’%’。</p></li></ul><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>HTML解析库BeautifulSoup4</title>
      <link href="/2018/11/21/2018-11-21-reptile-beautifulsoup4/"/>
      <url>/2018/11/21/2018-11-21-reptile-beautifulsoup4/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><h3 id="HTML解析库BeautifulSoup4"><a href="#HTML解析库BeautifulSoup4" class="headerlink" title="HTML解析库BeautifulSoup4"></a>HTML解析库BeautifulSoup4</h3><p><code>BeautifulSoup</code> 是一个可以从HTML或XML文件中提取数据的Python库，它的使用方式相对于正则来说更加的简单方便，常常能够节省我们大量的时间。</p><p><code>BeautifulSoup</code>也是有官方中文文档的：<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html" target="_blank" rel="noopener">https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html</a></p><hr><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code>BeautifulSoup</code>的安装也是非常方便的，pip安装即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install beautifulsoup4</span><br></pre></td></tr></table></figure><hr><h3 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h3><p>以下是一段HTML代码，作为例子被多次用到，这是 <em>爱丽丝梦游仙境</em> 中的一段内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">html_doc = <span class="string">"""</span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line"><span class="string">&lt;body&gt;</span></span><br><span class="line"><span class="string">&lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were</span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;,</span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and</span></span><br><span class="line"><span class="string">&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;</span></span><br><span class="line"><span class="string">and they lived at the bottom of a well.&lt;/p&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;p class="story"&gt;...&lt;/p&gt;</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>我们获取的网页数据通常会像上面这样是完全的字符串格式，所以我们首先需要使用<code>BeautifulSoup</code>来解析这段字符串。然后会获得一个<code>BeautifulSoup</code>对象，通过这个对象我们就可以进行一系列操作了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: soup = BeautifulSoup(html_doc)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: soup.title</span><br><span class="line">Out[<span class="number">3</span>]: &lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [4]: soup.title.name</span></span><br><span class="line"><span class="string">Out[4]: '</span>title<span class="string">'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [5]: soup.title.string</span></span><br><span class="line"><span class="string">Out[5]: "The Dormouse'</span>s story<span class="string">"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [6]: soup.title.parent.name</span></span><br><span class="line"><span class="string">Out[6]: 'head'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [7]: soup.p</span></span><br><span class="line"><span class="string">Out[7]: &lt;p class="</span>title<span class="string">"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [8]: soup.p['class']</span></span><br><span class="line"><span class="string">Out[8]: ['title']</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [9]: soup.a</span></span><br><span class="line"><span class="string">Out[9]: &lt;a class="</span>siste<span class="string">r" href="</span>http://example.com/elsie<span class="string">" id="</span>link1<span class="string">"&gt;Elsie&lt;/a&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [10]: soup.find_all('a')</span></span><br><span class="line"><span class="string">Out[10]:</span></span><br><span class="line"><span class="string">[&lt;a class="</span>siste<span class="string">r" href="</span>http://example.com/elsie<span class="string">" id="</span>link1<span class="string">"&gt;Elsie&lt;/a&gt;,</span></span><br><span class="line"><span class="string"> &lt;a class="</span>siste<span class="string">r" href="</span>http://example.com/lacie<span class="string">" id="</span>link2<span class="string">"&gt;Lacie&lt;/a&gt;,</span></span><br><span class="line"><span class="string"> &lt;a class="</span>siste<span class="string">r" href="</span>http://example.com/tillie<span class="string">" id="</span>link3<span class="string">"&gt;Tillie&lt;/a&gt;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [11]: soup.find(id="</span>link3<span class="string">")</span></span><br><span class="line"><span class="string">Out[11]: &lt;a class="</span>siste<span class="string">r" href="</span>http://example.com/tillie<span class="string">" id="</span>link3<span class="string">"&gt;Tillie&lt;/a&gt;</span></span><br></pre></td></tr></table></figure><p>可以看到，相对于正则来说，操作简单了不止一个量级。</p><hr><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h3 id="指定解析器"><a href="#指定解析器" class="headerlink" title="指定解析器"></a>指定解析器</h3><p>在上面的例子中，我们可以看到在查找数据之前，是有一个解析网页的过程的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html_doc)</span><br></pre></td></tr></table></figure><p><code>BeautifulSoup</code>会自动的在系统中选定一个可用的解析器，以下是主要的几种解析器：</p><table><thead><tr><th>解析器</th><th>使用方法</th><th>优势</th><th>劣势</th></tr></thead><tbody><tr><td>Python标准库</td><td><code>BeautifulSoup(markup, &quot;html.parser&quot;)</code></td><td>Python的内置标准库执行速度适中文档容错能力强</td><td>Python 2.7.3 or 3.2.2)前 的版本中文档容错能力差</td></tr><tr><td>lxml HTML 解析器</td><td><code>BeautifulSoup(markup, &quot;lxml&quot;)</code></td><td>速度快文档容错能力强</td><td>需要安装C语言库</td></tr><tr><td>lxml XML 解析器</td><td><code>BeautifulSoup(markup, [&quot;lxml&quot;, &quot;xml&quot;])`</code>BeautifulSoup(markup, “xml”)`</td><td>速度快唯一支持XML的解析器</td><td>需要安装C语言库</td></tr><tr><td>html5lib</td><td><code>BeautifulSoup(markup, &quot;html5lib&quot;)</code></td><td>最好的容错性以浏览器的方式解析文档生成HTML5格式的文档</td><td>速度慢不依赖外部扩展</td></tr></tbody></table><p>由于这个解析的过程在大规模的爬取中是会影响到整个爬虫系统的速度的，所以推荐使用的是<code>lxml</code>，速度会快很多，而<code>lxml</code>需要单独安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install lxml</span><br></pre></td></tr></table></figure><p>安装成功后，在解析网页的时候，指定为lxml即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html_doc, <span class="string">'lxml'</span>)</span><br></pre></td></tr></table></figure><p>注意：<strong>如果一段HTML或XML文档格式不正确的话,那么在不同的解析器中返回的结果可能是不一样的，所以要指定某一个解析器。</strong></p><hr><h3 id="节点对象"><a href="#节点对象" class="headerlink" title="节点对象"></a>节点对象</h3><p>BeautifulSoup将复杂HTML文档转换成一个复杂的树形结构，每个节点都是Python对象，所有对象可以归纳为4种：<code>Tag</code>，<code>NavigableString</code>，<code>BeautifulSoup</code>，<code>Comment</code>。</p><h3 id="tag"><a href="#tag" class="headerlink" title="tag"></a>tag</h3><p><code>tag</code>就是标签的意思，<code>tag</code>还有许多的方法和属性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(<span class="string">'&lt;b class="boldest"&gt;Extremely bold&lt;/b&gt;'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tag = soup.b</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>type(tag)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">bs4</span>.<span class="title">element</span>.<span class="title">Tag</span>'&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p>name</p><p>每一个<code>tag</code>对象都有<code>name</code>属性，为标签的名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tag.name</span><br><span class="line"><span class="string">'b'</span></span><br></pre></td></tr></table></figure></li><li><p>Attributes</p><p>在HTML中，<code>tag</code>可能有多个属性，所以<code>tag</code>属性的取值跟字典相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tag[<span class="string">'class'</span>]</span><br><span class="line"><span class="string">'boldest'</span></span><br></pre></td></tr></table></figure><p>如果某个<code>tag</code>属性有多个值，那么返回的则是列表格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup = BeautifulSoup(<span class="string">'&lt;p class="body strikeout"&gt;&lt;/p&gt;'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.p[<span class="string">'class'</span>]</span><br><span class="line">[<span class="string">"body"</span>, <span class="string">"strikeout"</span>]</span><br></pre></td></tr></table></figure></li><li><p>get_text()</p><p>通过<code>get_text()</code>方法我们可以获取某个<code>tag</code>下所有的文本内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: soup.body.get_text()</span><br><span class="line">Out[<span class="number">1</span>]: <span class="string">"The Dormouse's story\nOnce upon a time there were three little sisters; and their names were\nElsie,\nLacie and\nTillie;\nand they lived at the bottom of a well.\n...\n"</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="NavigableString"><a href="#NavigableString" class="headerlink" title="NavigableString"></a>NavigableString</h3><p><code>NavigableString</code>的意思是可以遍历的字符串，一般被标签包裹在其中的的文本就是<code>NavigableString</code>格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: soup = BeautifulSoup(<span class="string">'&lt;p&gt;No longer bold&lt;/p&gt;'</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: soup.p.string</span><br><span class="line">Out[<span class="number">2</span>]: <span class="string">'No longer bold'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: type(soup.p.string)</span><br><span class="line">Out[<span class="number">3</span>]: bs4.element.NavigableString</span><br></pre></td></tr></table></figure><h3 id="BeautifulSoup"><a href="#BeautifulSoup" class="headerlink" title="BeautifulSoup"></a>BeautifulSoup</h3><p><code>BeautifulSoup</code>对象就是解析网页获得的对象。</p><h3 id="Comment"><a href="#Comment" class="headerlink" title="Comment"></a>Comment</h3><p><code>Comment</code>指的是在网页中的注释以及特殊字符串。</p><hr><h3 id="Tag与遍历文档树"><a href="#Tag与遍历文档树" class="headerlink" title="Tag与遍历文档树"></a>Tag与遍历文档树</h3><p><code>tag</code>对象可以说是<code>BeautifulSoup</code>中最为重要的对象，通过<code>BeautifulSoup</code>来提取数据基本都围绕着这个对象来进行操作。</p><p>首先，一个节点中是可以包含多个子节点和多个字符串的。例如<code>html</code>节点中包含着<code>head</code>和<code>body</code>节点。所以<code>BeautifulSoup</code>就可以将一个HTML的网页用这样一层层嵌套的节点来进行表示。</p><p>以上方的爱丽丝梦游仙境为例：</p><h3 id="contents和children"><a href="#contents和children" class="headerlink" title="contents和children"></a>contents和children</h3><p>通过<code>contents</code>可以获取某个节点所有的子节点，包括里面的<code>NavigableString</code>对象。获取的子节点是列表格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: soup.head.contents</span><br><span class="line">Out[<span class="number">1</span>]: [&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;]</span></span><br></pre></td></tr></table></figure><p>而通过<code>children</code>同样的是获取某个节点的所有子节点，但是返回的是一个迭代器，这种方式会比列表格式更加的节省内存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: tags = soup.head.children</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: tags</span><br><span class="line">Out[<span class="number">2</span>]: &lt;list_iterator at <span class="number">0x110f76940</span>&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="keyword">for</span> tag <span class="keyword">in</span> tags:</span><br><span class="line">            print(tag)</span><br><span class="line"></span><br><span class="line">&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;</span></span><br></pre></td></tr></table></figure><h3 id="descendants"><a href="#descendants" class="headerlink" title="descendants"></a>descendants</h3><p>上面的<code>contents</code>和<code>children</code>获取的是某个节点的直接子节点，而无法获得子孙节点。通过<code>descendants</code>可以获得所有子孙节点，返回的结果跟<code>children</code>一样，需要迭代或者转类型使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: len(list(soup.body.descendants))</span><br><span class="line">Out[<span class="number">1</span>]: <span class="number">19</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: len(list(soup.body.children))</span><br><span class="line">Out[<span class="number">2</span>]: <span class="number">6</span></span><br></pre></td></tr></table></figure><h3 id="string和strings"><a href="#string和strings" class="headerlink" title="string和strings"></a>string和strings</h3><p>我们常常会遇到需要获取某个节点中的文本值的情况，如果这个节点中只有一个字符串，那么使用<code>string</code>可以正常将其取出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: soup.title.string</span><br><span class="line">Out[<span class="number">1</span>]: <span class="string">"The Dormouse's story"</span></span><br></pre></td></tr></table></figure><p>而如果这个节点中有多个字符串的时候，<code>BeautifulSoup</code>就无法确定要取出哪个字符串了，这时候需要使用<code>strings</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: list(soup.body.strings)</span><br><span class="line">Out[<span class="number">1</span>]:</span><br><span class="line">[<span class="string">"The Dormouse's story"</span>,</span><br><span class="line"> <span class="string">'\n'</span>,</span><br><span class="line"> <span class="string">'Once upon a time there were three little sisters; and their names were\n'</span>,</span><br><span class="line"> <span class="string">'Elsie'</span>,</span><br><span class="line"> <span class="string">',\n'</span>,</span><br><span class="line"> <span class="string">'Lacie'</span>,</span><br><span class="line"> <span class="string">' and\n'</span>,</span><br><span class="line"> <span class="string">'Tillie'</span>,</span><br><span class="line"> <span class="string">';\nand they lived at the bottom of a well.'</span>,</span><br><span class="line"> <span class="string">'\n'</span>,</span><br><span class="line"> <span class="string">'...'</span>,</span><br><span class="line"> <span class="string">'\n'</span>]</span><br></pre></td></tr></table></figure><p>而使用<code>stripped_strings</code>可以将全是空白的行去掉。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: list(soup.body.stripped_strings)</span><br><span class="line">Out[<span class="number">1</span>]:</span><br><span class="line">[<span class="string">"The Dormouse's story"</span>,</span><br><span class="line"> <span class="string">'Once upon a time there were three little sisters; and their names were'</span>,</span><br><span class="line"> <span class="string">'Elsie'</span>,</span><br><span class="line"> <span class="string">','</span>,</span><br><span class="line"> <span class="string">'Lacie'</span>,</span><br><span class="line"> <span class="string">'and'</span>,</span><br><span class="line"> <span class="string">'Tillie'</span>,</span><br><span class="line"> <span class="string">';\nand they lived at the bottom of a well.'</span>,</span><br><span class="line"> <span class="string">'...'</span>]</span><br></pre></td></tr></table></figure><h3 id="父节点parent和parents"><a href="#父节点parent和parents" class="headerlink" title="父节点parent和parents"></a>父节点parent和parents</h3><p>有时我们也需要去获取某个节点的父节点，也就是包裹着当前节点的节点。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: soup.b.parent</span><br><span class="line">Out[1]: &lt;p class="title"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;</span><br></pre></td></tr></table></figure><p>而使用<code>parents</code>则可以获得当前节点递归到顶层的所有父辈元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: [i.name <span class="keyword">for</span> i <span class="keyword">in</span> soup.b.parents]</span><br><span class="line">Out[<span class="number">1</span>]: [<span class="string">'p'</span>, <span class="string">'body'</span>, <span class="string">'html'</span>, <span class="string">'[document]'</span>]</span><br></pre></td></tr></table></figure><h3 id="兄弟节点"><a href="#兄弟节点" class="headerlink" title="兄弟节点"></a>兄弟节点</h3><p>兄弟节点指的就是父节点相同的节点。</p><ul><li><p><strong>next_sibling 和 previous_sibling</strong></p><p>兄弟节点选取的方法与当前节点的位置有关，<code>next_sibling</code>获取的是当前节点的下一个兄弟节点，<code>previous_sibling</code>获取的是当前节点的上一个兄弟节点。</p><p>所以，兄弟节点中排第一个的节点是没有<code>previous_sibling</code>的，最后一个节点是没有<code>next_sibling</code>的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">51</span>]: soup.head.next_sibling</span><br><span class="line">Out[<span class="number">51</span>]: <span class="string">'\n'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">52</span>]: soup.head.previos_sibling</span><br><span class="line"></span><br><span class="line">In [<span class="number">59</span>]: soup.body.previous_sibling</span><br><span class="line">Out[<span class="number">59</span>]: <span class="string">'\n'</span></span><br></pre></td></tr></table></figure></li><li><p><strong>next_siblings 和 previous_siblings</strong></p><p>相对应的，<code>next_siblings</code>获取的是下方所有的兄弟节点，<code>previous_siblings</code>获取的上方所有的兄弟节点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">47</span>]: [i.name <span class="keyword">for</span> i <span class="keyword">in</span> soup.head.next_siblings]</span><br><span class="line">Out[<span class="number">47</span>]: [<span class="keyword">None</span>, <span class="string">'body'</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">48</span>]: [i.name <span class="keyword">for</span> i <span class="keyword">in</span> soup.body.next_siblings]</span><br><span class="line">Out[<span class="number">48</span>]: []</span><br><span class="line"></span><br><span class="line">In [<span class="number">49</span>]: [i.name <span class="keyword">for</span> i <span class="keyword">in</span> soup.body.previous_siblings]</span><br><span class="line">Out[<span class="number">49</span>]: [<span class="keyword">None</span>, <span class="string">'head'</span>]</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="find-all"><a href="#find-all" class="headerlink" title="find_all()"></a>find_all()</h3><p>上方这种直接通过属性来进行访问属性的方法，很多时候只能适用于比较简单的一些场景，所以<code>BeautifulSoup</code>还提供了搜索整个文档树的方法<code>find_all()</code>。</p><p>需要注意的是，<code>find_all()</code>方法基本所有节点对象都能调用。</p><h3 id="通过name搜索"><a href="#通过name搜索" class="headerlink" title="通过name搜索"></a>通过name搜索</h3><p>就像以下演示的，<code>find_all()</code>可以直接查找出整个文档树中所有的b标签，并返回列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(<span class="string">'b'</span>)</span><br><span class="line">[&lt;b&gt;The Dormouse<span class="string">'s story&lt;/b&gt;]</span></span><br></pre></td></tr></table></figure><p>而如果传入的是一个列表，则会与列表中任意一个元素进行匹配。可以看到，搜索的结果包含了所有的a标签和b标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all([<span class="string">"a"</span>, <span class="string">"b"</span>])</span><br><span class="line">[&lt;b&gt;The Dormouse<span class="string">'s story&lt;/b&gt;,</span></span><br><span class="line"><span class="string"> &lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></span><br><span class="line"><span class="string"> &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></span><br><span class="line"><span class="string"> &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></span><br></pre></td></tr></table></figure><h3 id="通过属性搜索"><a href="#通过属性搜索" class="headerlink" title="通过属性搜索"></a>通过属性搜索</h3><p>我们在搜索的时候一般只有标签名是不够的，因为可能同名的标签很多，那么这时候我们就要通过标签的属性来进行搜索。</p><p>这时候我们可以通过传递给attrs一个字典参数来搜索属性。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: soup.find_all(attrs=&#123;<span class="string">'class'</span>: <span class="string">'sister'</span>&#125;)</span><br><span class="line">Out[<span class="number">1</span>]:</span><br><span class="line">[&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span><br><span class="line"> &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span><br><span class="line"> &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span><br></pre></td></tr></table></figure><p>可以看到找出了所有class属性为sister的标签。</p><h3 id="通过文本搜索"><a href="#通过文本搜索" class="headerlink" title="通过文本搜索"></a>通过文本搜索</h3><p>在<code>find_all()</code>方法中，还可以根据文本内容来进行搜索。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(text=<span class="string">"Elsie"</span>)</span><br><span class="line">[<span class="string">u'Elsie'</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(text=[<span class="string">"Tillie"</span>, <span class="string">"Elsie"</span>, <span class="string">"Lacie"</span>])</span><br><span class="line">[<span class="string">u'Elsie'</span>, <span class="string">u'Lacie'</span>, <span class="string">u'Tillie'</span>]</span><br></pre></td></tr></table></figure><p>可见找到的都是字符串对象，如果想要找到包含某个文本的<code>tag</code>，加上<code>tag</code>名即可。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.find_all(<span class="string">"a"</span>, text=<span class="string">"Elsie"</span>)</span><br><span class="line">[&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;Elsie&lt;/a&gt;]</span><br></pre></td></tr></table></figure><h3 id="限制查找范围为子节点"><a href="#限制查找范围为子节点" class="headerlink" title="限制查找范围为子节点"></a>限制查找范围为子节点</h3><p><code>find_all()</code>方法会默认的去所有的子孙节点中搜索，而如果将<code>recursive</code>参数设置为False，则可以将搜索范围限制在直接子节点中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.html.find_all(<span class="string">"title"</span>)</span><br><span class="line">[&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&gt;&gt;&gt; soup.html.find_all("title", recursive=False)</span></span><br><span class="line"><span class="string">[]</span></span><br></pre></td></tr></table></figure><h3 id="通过正则表达式来筛选查找结果"><a href="#通过正则表达式来筛选查找结果" class="headerlink" title="通过正则表达式来筛选查找结果"></a>通过正则表达式来筛选查找结果</h3><p>在<code>BeautifulSoup</code>中，也是可以与<code>re</code>模块进行相互配合的，将re.compile编译的对象传入<code>find_all()</code>方法，即可通过正则来进行搜索。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: tags = soup.find_all(re.compile(<span class="string">"^b"</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: [i.name <span class="keyword">for</span> i <span class="keyword">in</span> tags]</span><br><span class="line">Out[<span class="number">3</span>]: [<span class="string">'body'</span>, <span class="string">'b'</span>]</span><br></pre></td></tr></table></figure><p>可以看到，找到了标签名是以’b’开头的两个标签。</p><p>同样的，也能够以正则来筛选<code>tag</code>的属性。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: soup.find_all(attrs=&#123;<span class="string">'class'</span>: re.compile(<span class="string">"si"</span>)&#125;)</span><br><span class="line">Out[<span class="number">1</span>]:</span><br><span class="line">[&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span><br><span class="line"> &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span><br><span class="line"> &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span><br></pre></td></tr></table></figure><hr><h3 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h3><p>在<code>BeautifulSoup</code>中，同样也支持使用CSS选择器来进行搜索。使用<code>select()</code>，在其中传入字符串参数，就可以使用CSS选择器的语法来找到tag。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>soup.select(<span class="string">"title"</span>)</span><br><span class="line">[&lt;title&gt;The Dormouse<span class="string">'s story&lt;/title&gt;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&gt;&gt;&gt; soup.select("p &gt; a")</span></span><br><span class="line"><span class="string">[&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></span><br><span class="line"><span class="string"> &lt;a class="sister" href="http://example.com/lacie"  id="link2"&gt;Lacie&lt;/a&gt;,</span></span><br><span class="line"><span class="string"> &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深入浅出了解TCP/IP协议</title>
      <link href="/2018/11/21/2018-11-21-reptile-tcp-ip/"/>
      <url>/2018/11/21/2018-11-21-reptile-tcp-ip/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><h3 id="深入浅出了解TCP-IP协议"><a href="#深入浅出了解TCP-IP协议" class="headerlink" title="深入浅出了解TCP/IP协议"></a>深入浅出了解TCP/IP协议</h3><p><img src="/images/reptile/10.png" alt="TCPIP协议-封面"></p><p>在当今互联网世界，上网已经不再是什么稀罕事，动动手指就能够通过互联网做到很多以前做不到的事情，但是作为一名互联网从业的开发人员，网络协议依然是必须知道的基础知识。</p><p>那么网络协议究竟是什么意思呢？</p><p><strong>其实网络协议，就是为了所有的计算机能够在同一个网络中互相传递数据，而制定的数据传输规则。</strong> 就好像我们只有说同一种语言才能互相交流一个道理，那么想要在互联网中传递数据，就得遵守标准的网络协议。</p><hr><h3 id="OSI参考模型"><a href="#OSI参考模型" class="headerlink" title="OSI参考模型"></a>OSI参考模型</h3><p>既然说到网络协议就像一种语言，同语言间才能够互相交流，那么自然网络协议也像语言一样，是多种多样的。</p><p>在这种情况下<code>国际标准化组织</code>(ISO)提出的一个试图使各种计算机在世界范围内互连为网络的标准框架，简称OSI，OSI模型，即开放式通信系统互联参考模型(Open System Interconnection,OSI/RM,Open Systems Interconnection Reference Model)。</p><p>这个模型的目的是：提供给开发者一个必须的、通用的概念以便开发完善、可以用来解释连接不同系统的框架。也就是说希望规范网络协议。</p><p>OSI模型定义了网络互联的七层框架，也就是将网络协议从软件到硬件，从上到下的分成了七层，每层都为更高一层提供服务。</p><p><img src="/images/reptile/8.png" alt="TCPIP协议-OSI七层模型"></p><p>虽然说OSI模型算是网络协议的框架标准，但是在实际的使用中，TCP/IP的五层协议使用的更加广泛。</p><hr><h3 id="TCP-IP"><a href="#TCP-IP" class="headerlink" title="TCP/IP"></a>TCP/IP</h3><p><strong>TCP/IP指的其实不只是TCP和IP这两个协议，而是一个协议簇，其中包括了IP、ICMP、TCP、http、ftp、pop3 等等的一系列协议。</strong> TCP/IP（Transmission Control Protocol/Internet Protocol）是传输控制协议和网络协议的简称，它定义了电子设备如何连入因特网，以及数据如何在它们之间传输的标准。</p><p>TCP/IP是Internet互联网上所有主机间的共同协议。</p><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>而TCP/IP协议采用五层结构，其与OSI模型的各层对应关系为</p><p><img src="/images/reptile/13.png" alt="TCPIP协议-协议结构对比"></p><h3 id="各层作用"><a href="#各层作用" class="headerlink" title="各层作用"></a>各层作用</h3><h4 id="物理层和数据链路层"><a href="#物理层和数据链路层" class="headerlink" title="物理层和数据链路层"></a>物理层和数据链路层</h4><p>物理层是定义物理介质的各种特性：</p><ol><li>机械特性</li><li>电子特性</li><li>功能特性</li><li>规程特性</li></ol><p>数据链路层是负责接收IP数据包并通过网络发送，或者从网络上接收物理帧，抽出IP数据包，交给IP层。</p><ul><li>ARP是正向地址解析协议，通过已知的IP，寻找对应主机的MAC地址。</li><li>RARP是反向地址解析协议，通过MAC地址确定IP地址。比如无盘工作站还有DHCP服务。</li></ul><p>常见的接口层协议有：</p><p>Ethernet 802.3、Token Ring 802.5、X.25、Frame relay、HDLC、PPP ATM等。</p><h4 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h4><p>负责相邻计算机之间的通信。其功能包括三方面。</p><ol><li>处理来自传输层的分组发送请求，收到请求后，将分组装入IP数据报，填充报头，选择去往信宿机的路径，然后将数据报发往适当的网络接口。</li><li>处理输入数据报：首先检查其合法性，然后进行寻径–假如该数据报已到达信宿机，则去掉报头，将剩下部分交给适当的传输协议；假如该数据报尚未到达信宿，则转发该数据报。</li><li>处理路径、流控、拥塞等问题。</li></ol><p>网络层包括：IP(Internet Protocol）协议、ICMP(Internet Control Message Protocol)</p><p>控制报文协议、ARP(Address Resolution Protocol）地址转换协议、RARP(Reverse ARP)反向地址转换协议。</p><ul><li>IP是网络层的核心，通过路由选择将下一条IP封装后交给接口层。IP数据报是无连接服务。</li><li>ICMP是网络层的补充，可以回送报文。用来检测网络是否通畅。Ping命令就是发送ICMP的echo包，通过回送的echo relay进行网络测试。</li></ul><h4 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h4><p>提供应用程序间的通信。其功能包括：一、格式化信息流；二、提供可靠传输。为实现后者，传输层协议规定接收端必须发回确认，并且假如分组丢失，必须重新发送，即耳熟能详的“三次握手”过程，从而提供可靠的数据传输。</p><p>传输层协议主要是：传输控制协议TCP(Transmission Control Protocol）和用户数据报协议UDP(User Datagram protocol）。</p><h4 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h4><p>向用户提供一组常用的应用程序，比如电子邮件、文件传输访问、远程登录等。远程登录TELNET使用TELNET协议提供在网络其它主机上注册的接口。TELNET会话提供了基于字符的虚拟终端。文件传输访问FTP使用FTP协议来提供网络内机器间的文件拷贝功能。</p><p>应用层协议主要包括如下几个：FTP、TELNET、DNS、SMTP、NFS、HTTP。</p><ul><li>FTP(File Transfer Protocol）是文件传输协议，一般上传下载用FTP服务，数据端口是20H，控制端口是21H。</li><li>Telnet服务是用户远程登录服务，使用23H端口，使用明码传送，保密性差、简单方便。</li><li>DNS(Domain Name Service）是域名解析服务，提供域名到IP地址之间的转换，使用端口53。</li><li>SMTP(Simple Mail Transfer Protocol）是简单邮件传输协议，用来控制信件的发送、中转，使用端口25。</li><li>NFS（Network File System）是网络文件系统，用于网络中不同主机间的文件共享。</li><li>HTTP(Hypertext Transfer Protocol）是超文本传输协议，用于实现互联网中的WWW服务，使用端口80。</li></ul><hr><h2 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h2><p>TCP（Transmission Control Protocol）是一种面向连接的、可靠的、基于字节流的传输层通信协议，由IETF的RFC 793定义。</p><p>在TCP/IP协议簇中，TCP处于传输层中。</p><p>应用层向TCP层发送用于网间传输的、用8位字节表示的数据流，然后TCP把数据流分区成适当长度的报文段。之后TCP把结果包传给IP层，由它来通过网络将包传送给接收端实体的TCP层。</p><p>TCP为了保证不发生丢包，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的包发回一个相应的确认（ACK），如果发送端实体在合理的往返时延（RTT）内未收到确认，那么对应的数据包就被假设为已丢失将会被进行重传。TCP用一个校验和函数来检验数据是否有错误，在发送和接收时都要计算校验和。</p><h3 id="TCP报文头"><a href="#TCP报文头" class="headerlink" title="TCP报文头"></a>TCP报文头</h3><p>TCP传输的数据分为头部和数据部分。</p><p><img src="/images/reptile/9.png" alt="TCPIP协议-TCP头信息"></p><p><strong>各个段位说明:</strong></p><ul><li><strong>源端口和目的端口</strong>: 各占 2 字节.端口是传输层与应用层的服务接口.传输层的复用和分用功能都要通过端口才能实现</li><li><strong>序号</strong>: 占 4 字节.TCP 连接中传送的数据流中的每一个字节都编上一个序号.序号字段的值则指的是本报文段所发送的数据的第一个字节的序号</li><li><strong>确认号</strong>: 占 4 字节,是期望收到对方的下一个报文段的数据的第一个字节的序号</li><li><strong>数据偏移/首部长度</strong>: 占 4 位,它指出 TCP 报文段的数据起始处距离 TCP 报文段的起始处有多远.“数据偏移”的单位是 32 位字(以 4 字节为计算单位)</li><li><strong>保留</strong>: 占 6 位,保留为今后使用,但目前应置为 0</li><li><strong>紧急URG</strong>: 当 URG=1 时,表明紧急指针字段有效.它告诉系统此报文段中有紧急数据,应尽快传送(相当于高优先级的数据)</li><li><strong>确认ACK</strong>: 只有当 ACK=1 时确认号字段才有效.当 ACK=0 时,确认号无效</li><li><strong>PSH(PuSH)</strong>: 接收 TCP 收到 PSH = 1 的报文段,就尽快地交付接收应用进程,而不再等到整个缓存都填满了后再向上交付</li><li><strong>RST (ReSeT)</strong>: 当 RST=1 时,表明 TCP 连接中出现严重差错（如由于主机崩溃或其他原因）,必须释放连接,然后再重新建立运输连接</li><li><strong>同步 SYN</strong>: 同步 SYN = 1 表示这是一个连接请求或连接接受报文</li><li><strong>终止 FIN</strong>: 用来释放一个连接.FIN=1 表明此报文段的发送端的数据已发送完毕,并要求释放运输连接</li><li><strong>检验和</strong>: 占 2 字节.检验和字段检验的范围包括首部和数据这两部分.在计算检验和时,要在 TCP 报文段的前面加上 12 字节的伪首部</li><li><strong>紧急指针</strong>: 占 16 位,指出在本报文段中紧急数据共有多少个字节（紧急数据放在本报文段数据的最前面）</li><li><strong>选项</strong>: 长度可变.TCP 最初只规定了一种选项,即最大报文段长度 MSS.MSS 告诉对方 TCP：“我的缓存所能接收的报文段的数据字段的最大长度是 MSS 个字节.” [MSS(Maximum Segment Size)是 TCP 报文段中的数据字段的最大长度.数据字段加上 TCP 首部才等于整个的 TCP 报文段]</li><li><strong>填充</strong>: 这是为了使整个首部长度是 4 字节的整数倍</li><li><strong>其他选项</strong>:<ul><li><strong>窗口扩大</strong>: 占 3 字节,其中有一个字节表示移位值 S.新的窗口值等于TCP 首部中的窗口位数增大到(16 + S),相当于把窗口值向左移动 S 位后获得实际的窗口大小</li><li><strong>时间戳</strong>: 占10 字节,其中最主要的字段时间戳值字段(4字节)和时间戳回送回答字段(4字节)</li><li><strong>选择确认</strong>: 接收方收到了和前面的字节流不连续的两2字节.如果这些字节的序号都在接收窗口之内,那么接收方就先收下这些数据,但要把这些信息准确地告诉发送方,使发送方不要再重复发送这些已收到的数据</li></ul></li></ul><hr><h3 id="三次握手四次挥手"><a href="#三次握手四次挥手" class="headerlink" title="三次握手四次挥手"></a>三次握手四次挥手</h3><p>既然说到TCP，不能不提到广为人知的三次握手和四次挥手，TCP协议为了保证信息传输的连接和可靠性，使用了这样的方式来保证连接的可靠性。</p><h4 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h4><p>所谓三次握手，其实指的是TCP建立连接的过程，整个建立连接的过程需要发送三个包，来确认建立连接，具体流程如下：</p><p><img src="/images/reptile/11.png" alt="TCPIP协议-三次握手"></p><ul><li>第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。</li><li>第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。</li><li>第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。</li></ul><h4 id="四次挥手"><a href="#四次挥手" class="headerlink" title="四次挥手"></a>四次挥手</h4><p>四次挥手指的则是断开连接的过程：</p><p><img src="/images/reptile/12.png" alt="TCPIP协议-四次挥手"></p><ul><li>第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。</li><li>第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。</li><li>第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。</li><li>第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。</li></ul><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python与常见加密方式</title>
      <link href="/2018/11/21/2018-11-21-python-encryption/"/>
      <url>/2018/11/21/2018-11-21-python-encryption/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><h3 id="Python与常见加密方式"><a href="#Python与常见加密方式" class="headerlink" title="Python与常见加密方式"></a>Python与常见加密方式</h3><p>本文地址：<a href="https://www.jianshu.com/p/4ba20afacce2" target="_blank" rel="noopener">https://www.jianshu.com/p/4ba20afacce2</a></p><hr><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>我们所说的加密方式，都是对二进制编码的格式进行加密的，对应到Python中，则是我们的<code>Bytes</code>。</p><p>所以当我们在Python中进行加密操作的时候，要确保我们操作的是<code>Bytes</code>，否则就会报错。</p><p>将字符串和<code>Bytes</code>互相转换可以使用<code>encode()</code>和<code>decode()</code>方法。如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法中不传参数则是以默认的utf-8编码进行转换</span></span><br><span class="line">In [<span class="number">1</span>]: <span class="string">'南北'</span>.encode()</span><br><span class="line">Out[<span class="number">1</span>]: <span class="string">b'\xe5\x8d\x97\xe5\x8c\x97'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: <span class="string">b'\xe5\x8d\x97\xe5\x8c\x97'</span>.decode()</span><br><span class="line">Out[<span class="number">2</span>]: <span class="string">'南北'</span></span><br></pre></td></tr></table></figure><p><strong>注：两位十六进制常常用来显示一个二进制字节。</strong></p><p>利用<code>binascii</code>模块可以将十六进制显示的字节转换成我们在加解密中更常用的显示方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> binascii</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: <span class="string">'南北'</span>.encode()</span><br><span class="line">Out[<span class="number">2</span>]: <span class="string">b'\xe5\x8d\x97\xe5\x8c\x97'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: binascii.b2a_hex(<span class="string">'南北'</span>.encode())</span><br><span class="line">Out[<span class="number">3</span>]: <span class="string">b'e58d97e58c97'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: binascii.a2b_hex(<span class="string">b'e58d97e58c97'</span>)</span><br><span class="line">Out[<span class="number">4</span>]: <span class="string">b'\xe5\x8d\x97\xe5\x8c\x97'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: binascii.a2b_hex(<span class="string">b'e58d97e58c97'</span>).decode()</span><br><span class="line">Out[<span class="number">5</span>]: <span class="string">'南北'</span></span><br></pre></td></tr></table></figure><hr><h2 id="URL编码"><a href="#URL编码" class="headerlink" title="URL编码"></a>URL编码</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>正常的URL中是只能包含ASCII字符的，也就是字符、数字和一些符号。而URL编码就是一种浏览器用来避免url中出现特殊字符（如汉字）的编码方式。</p><p>其实就是将超出ASCII范围的字符转换成带<code>%</code>的十六进制格式。</p><h3 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># quote()方法会自动将str转换成bytes，所以这里传入str和bytes都可以</span></span><br><span class="line">In [<span class="number">2</span>]: parse.quote(<span class="string">'南北'</span>)</span><br><span class="line">Out[<span class="number">2</span>]: <span class="string">'%E5%8D%97%E5%8C%97'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: parse.unquote(<span class="string">'%E5%8D%97%E5%8C%97'</span>)</span><br><span class="line">Out[<span class="number">3</span>]: <span class="string">'南北'</span></span><br></pre></td></tr></table></figure><hr><h2 id="Base64编码"><a href="#Base64编码" class="headerlink" title="Base64编码"></a>Base64编码</h2><h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h3><p>Base64是一种用64个字符来表示任意二进制数据的方法。</p><p>Base64编码可以成为密码学的基石。可以将任意的二进制数据进行Base64编码。所有的数据都能被编码为并只用65个字符就能表示的文本文件。（ 65字符：A~Z a~z 0~9 + / = ）编码后的数据~=编码前数据的4/3，会大1/3左右。</p><h3 id="Base64编码的原理"><a href="#Base64编码的原理" class="headerlink" title="Base64编码的原理"></a>Base64编码的原理</h3><p><img src="/images/py/69.png" alt="加密方式-base64原理"></p><ol><li>将所有字符转化为ASCII码。</li><li>将ASCII码转化为8位二进制 。</li><li>将二进制3个归成一组(不足3个在后边补0)共24位，再拆分成4组，每组6位。</li><li>统一在6位二进制前补两个0凑足8位。</li><li>将补0后的二进制转为十进制。</li><li>从Base64编码表获取十进制对应的Base64编码。</li></ol><h3 id="Base64编码的说明"><a href="#Base64编码的说明" class="headerlink" title="Base64编码的说明"></a>Base64编码的说明</h3><ol><li>转换的时候，将三个byte的数据，先后放入一个24bit的缓冲区中，先来的byte占高位。</li><li>数据不足3byte的话，于缓冲区中剩下的bit用0补足。然后，每次取出6个bit，按照其值选择查表选择对应的字符作为编码后的输出。</li><li>不断进行，直到全部输入数据转换完成。</li><li>如果最后剩下两个输入数据，在编码结果后加1个“=”。</li><li>如果最后剩下一个输入数据，编码结果后加2个“=”。</li><li>如果没有剩下任何数据，就什么都不要加，这样才可以保证资料还原的正确性。</li></ol><h3 id="Python的Base64使用"><a href="#Python的Base64使用" class="headerlink" title="Python的Base64使用"></a>Python的Base64使用</h3><p>Python内置的<code>base64</code>模块可以直接进行base64的编解码</p><p><strong><em>注意：用于base64编码的，要么是ASCII包含的字符，要么是二进制数据</em></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: base64.b64encode(<span class="string">b'hello world'</span>)</span><br><span class="line">Out[<span class="number">2</span>]: <span class="string">b'aGVsbG8gd29ybGQ='</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: base64.b64decode(<span class="string">b'aGVsbG8gd29ybGQ='</span>)</span><br><span class="line">Out[<span class="number">3</span>]: <span class="string">b'hello world'</span></span><br></pre></td></tr></table></figure><hr><h2 id="MD5（信息-摘要算法）"><a href="#MD5（信息-摘要算法）" class="headerlink" title="MD5（信息-摘要算法）"></a>MD5（信息-摘要算法）</h2><h3 id="简述-1"><a href="#简述-1" class="headerlink" title="简述"></a>简述</h3><p>message-digest algorithm 5（信息-摘要算法）。经常说的“MD5加密”，就是它→信息-摘要算法。</p><p>md5，其实就是一种算法。可以将一个字符串，或文件，或压缩包，执行md5后，就可以生成一个固定长度为128bit的串。这个串，基本上是唯一的。</p><h3 id="不可逆性"><a href="#不可逆性" class="headerlink" title="不可逆性"></a>不可逆性</h3><p>每个人都有不同的指纹，看到这个人，可以得出他的指纹等信息，并且唯一对应，但你只看一个指纹，是不可能看到或读到这个人的长相或身份等信息。</p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ol><li>压缩性：任意长度的数据，算出的MD5值长度都是固定的。</li><li>容易计算：从原数据计算出MD5值很容易。</li><li>抗修改性：对原数据进行任何改动，哪怕只修改1个字节，所得到的MD5值都有很大区别。</li><li>强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5值的数据（即伪造数据）是非常困难的。</li></ol><p>举个栗子：世界上只有一个我，但是但是妞却是非常非常多的，以一个有限的我对几乎是无限的妞，所以可能能搞定非常多（100+）的妞，这个理论上的确是通的，可是实际情况下….</p><h3 id="Python的MD5使用"><a href="#Python的MD5使用" class="headerlink" title="Python的MD5使用"></a>Python的MD5使用</h3><p>由于MD5模块在python3中被移除，在python3中使用<code>hashlib</code>模块进行md5操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 待加密信息</span></span><br><span class="line">str = <span class="string">'这是一个测试'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建md5对象</span></span><br><span class="line">hl = hashlib.md5()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处必须声明encode</span></span><br><span class="line"><span class="comment"># 若写法为hl.update(str)  报错为： Unicode-objects must be encoded before hashing</span></span><br><span class="line">hl.update(str.encode(encoding=<span class="string">'utf-8'</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'MD5加密前为 ：'</span> + str)</span><br><span class="line">print(<span class="string">'MD5加密后为 ：'</span> + hl.hexdigest())</span><br></pre></td></tr></table></figure><p>运行结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MD5加密前为 ：这是一个测试</span><br><span class="line">MD5加密后为 ：cfca700b9e09cf664f3ae80733274d9f</span><br></pre></td></tr></table></figure><h3 id="MD5长度"><a href="#MD5长度" class="headerlink" title="MD5长度"></a>MD5长度</h3><p>md5的长度，默认为128bit，也就是128个0和1的二进制串。这样表达是很不友好的。所以将二进制转成了16进制，每4个bit表示一个16进制，所以128/4 = 32 换成16进制表示后，为32位了。</p><p>为什么网上还有md5是16位的呢？</p><p>其实16位的长度，是从32位md5值来的。是将32位md5去掉前八位，去掉后八位得到的。</p><hr><h2 id="Python加密库PyCryptodome"><a href="#Python加密库PyCryptodome" class="headerlink" title="Python加密库PyCryptodome"></a>Python加密库PyCryptodome</h2><p>PyCrypto是 Python 中密码学方面最有名的第三方软件包。可惜的是，它的开发工作于2012年就已停止。</p><p>幸运的是，有一个该项目的分支PyCrytodome 取代了 PyCrypto 。</p><h3 id="安装与导入"><a href="#安装与导入" class="headerlink" title="安装与导入"></a>安装与导入</h3><p>安装之前需要先安装<strong><em>Microsoft Visual c++ 2015</em></strong>。</p><p>在Linux上安装，可以使用以下 pip 命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pycryptodome</span><br></pre></td></tr></table></figure><p>导入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Crypto</span><br></pre></td></tr></table></figure><p>在Windows 系统上安装则稍有不同：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pycryptodomex</span><br></pre></td></tr></table></figure><p>导入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Cryptodome</span><br></pre></td></tr></table></figure><hr><h2 id="DES"><a href="#DES" class="headerlink" title="DES"></a>DES</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>DES算法为密码体制中的对称密码体制，又被称为美国数据加密标准。</p><p>DES是一个分组加密算法，典型的DES以64位为分组对数据加密，加密和解密用的是同一个算法。</p><p>DES算法的入口参数有三个：Key、Data、Mode。其中Key为7个字节共56位，是DES算法的工作密钥；Data为8个字节64位，是要被加密或被解密的数据；Mode为DES的工作方式,有两种:加密或解密。</p><p>密钥长64位，密钥事实上是56位参与DES运算（第8、16、24、32、40、48、56、64位是校验位，使得每个密钥都有奇数个1），分组后的明文组和56位的密钥按位替代或交换的方法形成密文组。</p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入DES模块</span></span><br><span class="line"><span class="keyword">from</span> Cryptodome.Cipher <span class="keyword">import</span> DES</span><br><span class="line"><span class="keyword">import</span> binascii</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是密钥</span></span><br><span class="line">key = <span class="string">b'abcdefgh'</span></span><br><span class="line"><span class="comment"># 需要去生成一个DES对象</span></span><br><span class="line">des = DES.new(key, DES.MODE_ECB)</span><br><span class="line"><span class="comment"># 需要加密的数据</span></span><br><span class="line">text = <span class="string">'python spider!'</span></span><br><span class="line">text = text + (<span class="number">8</span> - (len(text) % <span class="number">8</span>)) * <span class="string">'='</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加密的过程</span></span><br><span class="line">encrypto_text = des.encrypt(text.encode())</span><br><span class="line">encrypto_text = binascii.b2a_hex(encrypto_text)</span><br><span class="line">print(encrypto_text)</span><br></pre></td></tr></table></figure><hr><h2 id="3DES"><a href="#3DES" class="headerlink" title="3DES"></a>3DES</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>3DES（或称为Triple DES）是三重数据加密算法（TDEA，Triple Data Encryption Algorithm）块密码的通称。它相当于是对每个数据块应用三次DES加密算法。</p><p>由于计算机运算能力的增强，原版DES密码的密钥长度变得容易被暴力破解。3DES即是设计用来提供一种相对简单的方法，即通过增加DES的密钥长度来避免类似的攻击，而不是设计一种全新的块密码算法。</p><p>3DES（即Triple DES）是DES向AES过渡的加密算法（1999年，NIST将3-DES指定为过渡的加密标准），加密算法，其具体实现如下：设Ek()和Dk()代表DES算法的加密和解密过程，K代表DES算法使用的密钥，M代表明文，C代表密文，这样：</p><p>3DES加密过程为：C=Ek3(Dk2(Ek1(M)))</p><p>3DES解密过程为：M=Dk1(EK2(Dk3(C)))</p><hr><h2 id="AES"><a href="#AES" class="headerlink" title="AES"></a>AES</h2><h3 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h3><p><strong>高级加密标准</strong>（英语：<strong>Advanced Encryption Standard</strong>，缩写：<strong>AES</strong>），在密码学中又称<strong>Rijndael加密法</strong>，是美国联邦政府采用的一种区块加密标准。这个标准用来替代原先的DES，已经被多方分析且广为全世界所使用。经过五年的甄选流程，高级加密标准由美国国家标准与技术研究院（NIST）于2001年11月26日发布于FIPS PUB 197，并在2002年5月26日成为有效的标准。2006年，高级加密标准已然成为对称密钥加密中最流行的算法之一。</p><p>AES在软件及硬件上都能快速地加解密，相对来说较易于实作，且只需要很少的存储器。作为一个新的加密标准，目前正被部署应用到更广大的范围。</p><h3 id="特点与思想"><a href="#特点与思想" class="headerlink" title="特点与思想"></a>特点与思想</h3><ol><li>抵抗所有已知的攻击。</li><li>在多个平台上速度快，编码紧凑。</li><li>设计简单。</li></ol><h3 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h3><p><img src="/images/py/70.png" alt="加密方式-对称加密"></p><p>AES为分组密码，分组密码也就是把明文分成一组一组的，每组长度相等，每次加密一组数据，直到加密完整个明文。在AES标准规范中，分组长度只能是128位，也就是说，每个分组为16个字节（每个字节8位）。密钥的长度可以使用128位、192位或256位。密钥的长度不同，推荐加密轮数也不同。</p><p><strong>一般常用的是128位</strong></p><p>###Python实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Cryptodome.Cipher <span class="keyword">import</span> AES</span><br><span class="line"><span class="keyword">from</span> Cryptodome <span class="keyword">import</span> Random</span><br><span class="line"><span class="keyword">from</span> binascii <span class="keyword">import</span> b2a_hex  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 要加密的明文</span></span><br><span class="line">data = <span class="string">'南来北往'</span></span><br><span class="line"><span class="comment"># 密钥key 长度必须为16（AES-128）、24（AES-192）、或32（AES-256）Bytes 长度.</span></span><br><span class="line"><span class="comment"># 目前AES-128足够用</span></span><br><span class="line">key = <span class="string">b'this is a 16 key'</span></span><br><span class="line"><span class="comment"># 生成长度等于AES块大小的不可重复的密钥向量</span></span><br><span class="line">iv = Random.new().read(AES.block_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用key和iv初始化AES对象, 使用MODE_CFB模式</span></span><br><span class="line">mycipher = AES.new(key, AES.MODE_CFB, iv)</span><br><span class="line"><span class="comment"># 加密的明文长度必须为16的倍数，如果长度不为16的倍数，则需要补足为16的倍数</span></span><br><span class="line"><span class="comment"># 将iv（密钥向量）加到加密的密文开头，一起传输</span></span><br><span class="line">ciphertext = iv + mycipher.encrypt(data.encode())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解密的话要用key和iv生成新的AES对象</span></span><br><span class="line">mydecrypt = AES.new(key, AES.MODE_CFB, ciphertext[:<span class="number">16</span>])</span><br><span class="line"><span class="comment"># 使用新生成的AES对象，将加密的密文解密</span></span><br><span class="line">decrypttext = mydecrypt.decrypt(ciphertext[<span class="number">16</span>:])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">'密钥k为：'</span>, key)</span><br><span class="line">print(<span class="string">'iv为：'</span>, b2a_hex(ciphertext)[:<span class="number">16</span>])</span><br><span class="line">print(<span class="string">'加密后数据为：'</span>, b2a_hex(ciphertext)[<span class="number">16</span>:])</span><br><span class="line">print(<span class="string">'解密后数据为：'</span>, decrypttext.decode())</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">密钥k为： <span class="string">b'this is a 16 key'</span></span><br><span class="line">iv为： <span class="string">b'a78a177cffd50878'</span></span><br><span class="line">加密后数据为： <span class="string">b'33f61e7678c25d795d565d40f2f68371da051202'</span></span><br><span class="line">解密后数据为： 南来北往</span><br></pre></td></tr></table></figure><hr><h3 id="RSA"><a href="#RSA" class="headerlink" title="RSA"></a>RSA</h3><h3 id="非对称加密"><a href="#非对称加密" class="headerlink" title="非对称加密"></a>非对称加密</h3><p>典型的如RSA等，常见方法，使用openssl ,keytools等工具生成一对公私钥对，使用被公钥加密的数据可以使用私钥来解密，反之亦然（被私钥加密的数据也可以被公钥解密) 。</p><p>在实际使用中私钥一般保存在发布者手中，是私有的不对外公开的，只将公钥对外公布，就能实现只有私钥的持有者才能将数据解密的方法。 这种加密方式安全系数很高，因为它不用将解密的密钥进行传递，从而没有密钥在传递过程中被截获的风险，而破解密文几乎又是不可能的。</p><p>但是算法的效率低，所以常用于很重要数据的加密，常和对称配合使用，使用非对称加密的密钥去加密对称加密的密钥。</p><h3 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h3><p><strong>RSA加密算法</strong>是一种<code>非对称加密算法</code>。在公开密钥加密和电子商业中RSA被广泛使用。</p><p>该算法基于一个十分简单的数论事实：将两个大素数相乘十分容易，但那时想要对其乘积进行因式分解却极其困难，因此可以将乘积公开作为加密密钥，即公钥，而两个大素数组合成私钥。公钥是可发布的供任何人使用，私钥则为自己所有，供解密之用。</p><h3 id="Python实现-1"><a href="#Python实现-1" class="headerlink" title="Python实现"></a>Python实现</h3><p>首先我们需要安装一个<code>rsa</code>模块：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install rsa</span><br></pre></td></tr></table></figure><p>而且，因为RSA加密算法的特性，RSA的公钥私钥都是10进制的，但公钥的值常常保存为16进制的格式，所以需要将其用<code>int()</code>方法转换为10进制格式。</p><h4 id="用网页中的公钥把数据加密"><a href="#用网页中的公钥把数据加密" class="headerlink" title="用网页中的公钥把数据加密"></a>用网页中的公钥把数据加密</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> rsa</span><br><span class="line"><span class="keyword">import</span> binascii</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用网页中获得的n和e值，将明文加密</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rsa_encrypt</span><span class="params">(rsa_n, rsa_e, message)</span>:</span></span><br><span class="line">    <span class="comment"># 用n值和e值生成公钥</span></span><br><span class="line">    key = rsa.PublicKey(rsa_n, rsa_e)</span><br><span class="line">    <span class="comment"># 用公钥把明文加密</span></span><br><span class="line">    message = rsa.encrypt(message.encode(), key)</span><br><span class="line">    <span class="comment"># 转化成常用的可读性高的十六进制</span></span><br><span class="line">    message = binascii.b2a_hex(message)</span><br><span class="line">    <span class="comment"># 将加密结果转化回字符串并返回</span></span><br><span class="line">    <span class="keyword">return</span> message.decode()</span><br><span class="line"></span><br><span class="line"><span class="comment"># RSA的公钥有两个值n和e，我们在网站中获得的公钥一般就是这样的两个值。</span></span><br><span class="line"><span class="comment"># n常常为长度为256的十六进制字符串</span></span><br><span class="line"><span class="comment"># e常常为十六进制‘10001’</span></span><br><span class="line">pubkey_n = <span class="string">'8d7e6949d411ce14d7d233d7160f5b2cc753930caba4d5ad24f923a505253b9c39b09a059732250e56c594d735077cfcb0c3508e9f544f101bdf7e97fe1b0d97f273468264b8b24caaa2a90cd9708a417c51cf8ba35444d37c514a0490441a773ccb121034f29748763c6c4f76eb0303559c57071fd89234d140c8bb965f9725'</span></span><br><span class="line">pubkey_e = <span class="string">'10001'</span></span><br><span class="line"><span class="comment"># 需要将十六进制转换成十进制</span></span><br><span class="line">rsa_n = int(pubkey_n, <span class="number">16</span>)</span><br><span class="line">rsa_e = int(pubkey_e, <span class="number">16</span>)</span><br><span class="line"><span class="comment"># 要加密的明文</span></span><br><span class="line">message = <span class="string">'南北今天很忙'</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"公钥n值长度："</span>, len(pubkey_n))</span><br><span class="line">print(rsa_encrypt(rsa_n, rsa_e, message))</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">公钥n值长度： <span class="number">256</span></span><br><span class="line"><span class="number">480</span>f302eed822c8250256511ddeb017fcb28949cc05739ae66440eecc4ab76e7a7b2f1df398aefdfef2b9bfce6d6152bf6cc1552a0ed8bebee9e094a7ce9a52622487a6412632144787aa81f6ec9b96be95890c4c28a31b3e8d9ea430080d79297c5d75cd11df04df6e71b237511164399d72ccb2f4c34022b1ea7b76189a56e</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>优雅到骨子里的Requests</title>
      <link href="/2018/11/21/2018-11-21-reptile-request/"/>
      <url>/2018/11/21/2018-11-21-reptile-request/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><h3 id="优雅到骨子里的Requests"><a href="#优雅到骨子里的Requests" class="headerlink" title="优雅到骨子里的Requests"></a>优雅到骨子里的Requests</h3><p><img src="/images/reptile/5.png" alt="Requests-标志"></p><hr><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>上一篇文章介绍了Python的网络请求库<code>urllib</code>和<code>urllib3</code>的使用方法，那么，作为同样是网络请求库的<code>Requests</code>，相对于<code>urllib</code>，有什么优点呢？</p><p>其实，只有两个词，简单优雅。</p><p><code>Requests</code>的宣言就是：<strong>HTTP for Humans</strong>。可以说，<code>Requests</code>彻底贯彻了Python所代表的简单优雅的精神。</p><p>之前的<code>urllib</code>做为Python的标准库，因为历史原因，使用的方式可以说是非常的麻烦而复杂的，而且官方文档也十分的简陋，常常需要去查看源码。与之相反的是，<code>Requests</code>的使用方式非常的简单、直观、人性化，让程序员的精力完全从库的使用中解放出来。</p><p>甚至在官方的urllib.request的文档中，有这样一句话来推荐<code>Requests</code>：</p><blockquote><p>The <strong>Requests packageis</strong> recommended for a higher-level HTTP client interface.</p></blockquote><p><code>Requests</code>的官方文档同样也非常的完善详尽，而且少见的有中文官方文档：<a href="http://cn.python-requests.org/zh_CN/latest/" target="_blank" rel="noopener">http://cn.python-requests.org/zh_CN/latest/</a>。</p><p>当然，为了保证准确性，还是尽量查看英文文档为好。</p><hr><h3 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h3><p><code>Requests</code>的作者<strong>Kenneth Reitz</strong>同样是一个富有传奇色彩的人物。</p><p><strong>Kenneth Reitz</strong>在有着“云服务鼻祖”之称的Heroku 公司，28岁时就担任了Python 语言的总架构师。他做了什么呢？随便列几个项目名称: requests、python-guide、pipenv、legit、autoenv，当然它也给Python界很多知名的开源项目贡献了代码，比如Flask。</p><p>可以说他是Python领域举足轻重的人物，他的代码追求一种强迫症般的美感。</p><p>大佬的传奇还不止于此，这是他当年在PyCON演讲时的照片：</p><p><img src="/images/reptile/6.png" alt="Requests-作者1"></p><p>非常可爱的小胖子，同时也符合着大众对于程序员的一些刻板印象：胖、不太修边幅、腼腆。</p><p>但是几年后，他变成了这样：</p><p><img src="/images/reptile/7.png" alt="Requests-作者2"></p><p>emmmmm，帅哥，你这是去哪整的容？</p><p>哈哈，开个玩笑。不过确实外貌方面的改变非常的巨大，由一个小肥宅的形象变得帅气潇洒。可见只要愿意去追求，我们都能变成我们想要的样子。</p><hr><h3 id="例子与特性"><a href="#例子与特性" class="headerlink" title="例子与特性"></a>例子与特性</h3><p>可以说<code>Requests</code>最大的特性就是其风格的简单直接优雅。无论是请求方法，还是响应结果的处理，还有cookies，url参数，post提交数据，都体现出了这种风格。</p><p>以下是一个简单例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.status_code</span><br><span class="line"><span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.headers[<span class="string">'content-type'</span>]</span><br><span class="line"><span class="string">'application/json; charset=utf8'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.encoding</span><br><span class="line"><span class="string">'utf-8'</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.text</span><br><span class="line"><span class="string">u'&#123;"type":"User"...'</span></span><br></pre></td></tr></table></figure><p>可以看到，不论是请求的发起还是相应的处理，都是非常直观明了的。</p><p><code>Requests</code>目前基本上完全满足web请求的所有需求，以下是<code>Requests</code>的特性：</p><ul><li>Keep-Alive &amp; 连接池</li><li>国际化域名和 URL</li><li>带持久 Cookie 的会话</li><li>浏览器式的 SSL 认证</li><li>自动内容解码</li><li>基本/摘要式的身份认证</li><li>优雅的 key/value Cookie</li><li>自动解压</li><li>Unicode 响应体</li><li>HTTP(S) 代理支持</li><li>文件分块上传</li><li>流下载</li><li>连接超时</li><li>分块请求</li><li>支持 <code>.netrc</code></li></ul><p>而<code>Requests 3.0</code>目前也募集到了资金正在开发中，预计会支持async/await来实现并发请求，且可能会支持HTTP 2.0。</p><hr><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code>Requests</code>的安装非常的简单，直接PIP安装即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure><hr><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p><code>Requests</code>的请求不再像<code>urllib</code>一样需要去构造各种Request、opener和handler，使用<code>Requests</code>构造的方法，并在其中传入需要的参数即可。</p><h3 id="发起请求"><a href="#发起请求" class="headerlink" title="发起请求"></a>发起请求</h3><h3 id="请求方法"><a href="#请求方法" class="headerlink" title="请求方法"></a>请求方法</h3><p>每一个请求方法都有一个对应的API，比如GET请求就可以使用<code>get()</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">'https://www.baidu.com'</span>)</span><br></pre></td></tr></table></figure><p>而POST请求就可以使用<code>post()</code>方法，并且将需要提交的数据传递给data参数即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.post(<span class="string">'http://httpbin.org/post'</span>, data = &#123;<span class="string">'key'</span>:<span class="string">'value'</span>&#125;)</span><br></pre></td></tr></table></figure><p>而其他的请求类型，都有各自对应的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.put(<span class="string">'http://httpbin.org/put'</span>, data = &#123;<span class="string">'key'</span>:<span class="string">'value'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.delete(<span class="string">'http://httpbin.org/delete'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.head(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.options(<span class="string">'http://httpbin.org/get'</span>)</span><br></pre></td></tr></table></figure><p>非常的简单直观明了。</p><h3 id="传递URL参数"><a href="#传递URL参数" class="headerlink" title="传递URL参数"></a>传递URL参数</h3><p>传递URL参数也不用再像<code>urllib</code>中那样需要去拼接URL，而是简单的，构造一个字典，并在请求时将其传递给params参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>params = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=params)</span><br></pre></td></tr></table></figure><p>此时，查看请求的URL，则可以看到URL已经构造正确了：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(resp.url)</span><br><span class="line">http://httpbin.org/get?key2=value2&amp;key1=value1</span><br></pre></td></tr></table></figure><p>并且，有时候我们会遇到相同的url参数名，但有不同的值，而python的字典又不支持键的重名，那么我们可以把键的值用列表表示：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>params = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: [<span class="string">'value2'</span>, <span class="string">'value3'</span>]&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">'http://httpbin.org/get'</span>, params=params)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(resp.url)</span><br><span class="line">http://httpbin.org/get?key1=value1&amp;key2=value2&amp;key2=value3</span><br></pre></td></tr></table></figure><h3 id="自定义Headers"><a href="#自定义Headers" class="headerlink" title="自定义Headers"></a>自定义Headers</h3><p>如果想自定义请求的Headers，同样的将字典数据传递给headers参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>url = <span class="string">'https://api.github.com/some/endpoint'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>headers = &#123;<span class="string">'user-agent'</span>: <span class="string">'my-app/0.0.1'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(url, headers=headers)</span><br></pre></td></tr></table></figure><h3 id="自定义Cookies"><a href="#自定义Cookies" class="headerlink" title="自定义Cookies"></a>自定义Cookies</h3><p><code>Requests</code>中自定义Cookies也不用再去构造CookieJar对象，直接将字典递给cookies参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>url = <span class="string">'http://httpbin.org/cookies'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cookies = &#123;<span class="string">'cookies_are'</span>: <span class="string">'working'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(url, cookies=cookies)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.text</span><br><span class="line"><span class="string">'&#123;"cookies": &#123;"cookies_are": "working"&#125;&#125;'</span></span><br></pre></td></tr></table></figure><h3 id="设置代理"><a href="#设置代理" class="headerlink" title="设置代理"></a>设置代理</h3><p>当我们需要使用代理时，同样构造代理字典，传递给<code>proxies</code>参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">'http'</span>: <span class="string">'http://10.10.1.10:3128'</span>,</span><br><span class="line">  <span class="string">'https'</span>: <span class="string">'http://10.10.1.10:1080'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">requests.get(<span class="string">'http://example.org'</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><h3 id="重定向"><a href="#重定向" class="headerlink" title="重定向"></a>重定向</h3><p>在网络请求中，我们常常会遇到状态码是3开头的重定向问题，在<code>Requests</code>中是默认开启允许重定向的，即遇到重定向时，会自动继续访问。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">'http://github.com'</span>, allow_redirects=<span class="keyword">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.status_code</span><br><span class="line"><span class="number">301</span></span><br></pre></td></tr></table></figure><h3 id="禁止证书验证"><a href="#禁止证书验证" class="headerlink" title="禁止证书验证"></a>禁止证书验证</h3><p>有时候我们使用了抓包工具，这个时候由于抓包工具提供的证书并不是由受信任的数字证书颁发机构颁发的，所以证书的验证会失败，所以我们就需要关闭证书验证。</p><p>在请求的时候把<code>verify</code>参数设置为<code>False</code>就可以关闭证书验证了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">'http://httpbin.org/post'</span>, verify=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>但是关闭验证后，会有一个比较烦人的<code>warning</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">py:<span class="number">858</span>: InsecureRequestWarning: Unverified HTTPS request <span class="keyword">is</span> being made. Adding certificate verification <span class="keyword">is</span> strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html<span class="comment">#ssl-warnings</span></span><br><span class="line">  InsecureRequestWarning)</span><br></pre></td></tr></table></figure><p>可以使用以下方法关闭警告：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests.packages.urllib3.exceptions <span class="keyword">import</span> InsecureRequestWarning</span><br><span class="line"><span class="comment"># 禁用安全请求警告</span></span><br><span class="line">requests.packages.urllib3.disable_warnings(InsecureRequestWarning)</span><br></pre></td></tr></table></figure><h3 id="设置超时"><a href="#设置超时" class="headerlink" title="设置超时"></a>设置超时</h3><p>设置访问超时，设置<code>timeout</code>参数即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>requests.get(<span class="string">'http://github.com'</span>, timeout=<span class="number">0.001</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">requests.exceptions.Timeout: HTTPConnectionPool(host=<span class="string">'github.com'</span>, port=<span class="number">80</span>): Request timed out. (timeout=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><p>可见，通过<code>Requests</code>发起请求，只需要构造好几个需要的字典，并将其传入请求的方法中，即可完成基本的网络请求。</p><hr><h3 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h3><p>通过<code>Requests</code>发起请求获取到的，是一个<code>requests.models.Response</code>对象。通过这个对象我们可以很方便的获取响应的内容。</p><h3 id="响应内容"><a href="#响应内容" class="headerlink" title="响应内容"></a>响应内容</h3><p>之前通过<code>urllib</code>获取的响应，读取的内容都是bytes的二进制格式，需要我们自己去将结果<code>decode()</code>一次转换成字符串数据。</p><p>而<code>Requests</code>通过<code>text</code>属性，就可以获得字符串格式的响应内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">'https://api.github.com/events'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.text</span><br><span class="line"><span class="string">u'[&#123;"repository":&#123;"open_issues":0,"url":"https://github.com/...</span></span><br></pre></td></tr></table></figure><p><code>Requests</code>会自动的根据响应的报头来猜测网页的编码是什么，然后根据猜测的编码来解码网页内容，基本上大部分的网页都能够正确的被解码。而如果发现<code>text</code>解码不正确的时候，就需要我们自己手动的去指定解码的编码格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">'https://api.github.com/events'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.encoding = <span class="string">'utf-8'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.text</span><br><span class="line"><span class="string">u'[&#123;"repository":&#123;"open_issues":0,"url":"https://github.com/...</span></span><br></pre></td></tr></table></figure><p>而如果你需要获得原始的二进制数据，那么使用<code>content</code>属性即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.content</span><br><span class="line"><span class="string">b'[&#123;"repository":&#123;"open_issues":0,"url":"https://github.com/...</span></span><br></pre></td></tr></table></figure><p>如果我们访问之后获得的数据是JSON格式的，那么我们可以使用<code>json()</code>方法，直接获取转换成字典格式的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">'https://api.github.com/events'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.json()</span><br><span class="line">[&#123;<span class="string">u'repository'</span>: &#123;<span class="string">u'open_issues'</span>: <span class="number">0</span>, <span class="string">u'url'</span>: <span class="string">'https://github.com/...</span></span><br></pre></td></tr></table></figure><h3 id="状态码"><a href="#状态码" class="headerlink" title="状态码"></a>状态码</h3><p>通过<code>status_code</code>属性获取响应的状态码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.status_code</span><br><span class="line"><span class="number">200</span></span><br></pre></td></tr></table></figure><h3 id="响应报头"><a href="#响应报头" class="headerlink" title="响应报头"></a>响应报头</h3><p>通过<code>headers</code>属性获取响应的报头</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.headers</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'content-encoding'</span>: <span class="string">'gzip'</span>,</span><br><span class="line">    <span class="string">'transfer-encoding'</span>: <span class="string">'chunked'</span>,</span><br><span class="line">    <span class="string">'connection'</span>: <span class="string">'close'</span>,</span><br><span class="line">    <span class="string">'server'</span>: <span class="string">'nginx/1.0.4'</span>,</span><br><span class="line">    <span class="string">'x-runtime'</span>: <span class="string">'148ms'</span>,</span><br><span class="line">    <span class="string">'etag'</span>: <span class="string">'"e1ca502697e5c9317743dc078f67693f"'</span>,</span><br><span class="line">    <span class="string">'content-type'</span>: <span class="string">'application/json'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="服务器返回的cookies"><a href="#服务器返回的cookies" class="headerlink" title="服务器返回的cookies"></a>服务器返回的cookies</h3><p>通过<code>cookies</code>属性获取服务器返回的<code>cookies</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>url = <span class="string">'http://example.com/some/cookie/setting/url'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(url)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.cookies[<span class="string">'example_cookie_name'</span>]</span><br><span class="line"><span class="string">'example_cookie_value'</span></span><br></pre></td></tr></table></figure><h3 id="url"><a href="#url" class="headerlink" title="url"></a>url</h3><p>还可以使用<code>url</code>属性查看访问的url。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>params = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=params)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(resp.url)</span><br><span class="line">http://httpbin.org/get?key2=value2&amp;key1=value1</span><br></pre></td></tr></table></figure><hr><h3 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h3><p>在<code>Requests</code>中，实现了<code>Session(会话)</code>功能，当我们使用<code>Session</code>时，能够像浏览器一样，在没有关闭关闭浏览器时，能够保持住访问的状态。</p><p>这个功能常常被我们用于登陆之后的数据获取，使我们不用再一次又一次的传递cookies。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">session = requests.Session()</span><br><span class="line"></span><br><span class="line">session.get(<span class="string">'http://httpbin.org/cookies/set/sessioncookie/123456789'</span>)</span><br><span class="line">resp = session.get(<span class="string">'http://httpbin.org/cookies'</span>)</span><br><span class="line"></span><br><span class="line">print(resp.text)</span><br><span class="line"><span class="comment"># '&#123;"cookies": &#123;"sessioncookie": "123456789"&#125;&#125;'</span></span><br></pre></td></tr></table></figure><p>首先我们需要去生成一个<code>Session</code>对象，然后用这个<code>Session</code>对象来发起访问，发起访问的方法与正常的请求是一摸一样的。</p><p>同时，需要注意的是，如果是我们在<code>get()</code>方法中传入<code>headers</code>和<code>cookies</code>等数据，那么这些数据只在当前这一次请求中有效。如果你想要让一个<code>headers</code>在<code>Session</code>的整个生命周期内都有效的话，需要用以下的方式来进行设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置整个headers</span></span><br><span class="line">session.headers = &#123;</span><br><span class="line">    <span class="string">'user-agent'</span>: <span class="string">'my-app/0.0.1'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 增加一条headers</span></span><br><span class="line">session.headers.update(&#123;<span class="string">'x-test'</span>: <span class="string">'true'</span>&#125;)</span><br></pre></td></tr></table></figure><hr><p><strong>后记：或许有人不认可代码的美学，认为代码写的丑没事，能跑起来就好。但是我始终认为，世间万物都应该是美好的，追求美好的脚步也不应该停止。</strong></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深入浅出了解HTTP协议</title>
      <link href="/2018/11/21/2018-11-21-reptile-http/"/>
      <url>/2018/11/21/2018-11-21-reptile-http/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><h3 id="深入浅出了解HTTP协议"><a href="#深入浅出了解HTTP协议" class="headerlink" title="深入浅出了解HTTP协议"></a>深入浅出了解HTTP协议</h3><p><img src="/images/reptile/2.jpg" alt="HTTP协议-封面"></p><p>HTTP（HyperText Transfer Protocol，超文本传输协议)是互联网上应用最为广泛的一种网络协议。目前使用最普遍的一个版本是HTTP 1.1。</p><p>HTTP协议是用于从WWW服务器传输超文本到本地浏览器的传送协议。它可以使浏览器更加高效，使网络传输减少。它不仅保证计算机正确快速地传输超文本文档，还确定传输文档中的哪一部分，以及哪部分内容首先显示(如文本先于图形)等。</p><hr><h3 id="HTTP协议简介"><a href="#HTTP协议简介" class="headerlink" title="HTTP协议简介"></a>HTTP协议简介</h3><p>HTTP是一个应用层协议，由请求和响应构成，是一个标准的客户端服务器模型。</p><p>一次HTTP请求的基本流程一般是，在建立TCP连接后，由客户端向服务端发起一次请求（request），而服务器在接收到以后返回给客户端一个响应（response）。</p><p>所以我们看到的HTTP请求内容一般就分为请求和响应两部分。</p><p>HTTP协议通常承载于TCP协议之上，有时也承载于TLS或SSL协议层之上，这个时候，就成了我们常说的HTTPS。</p><p>默认HTTP的端口号为80。</p><h3 id="无状态协议"><a href="#无状态协议" class="headerlink" title="无状态协议"></a>无状态协议</h3><p>HTTP协议是无状态的，也就是说每一次HTTP请求之间都是相互独立的，没有联系的，服务端不知道客户端具体的状态。</p><p>比如客户端访问一次网页之后关闭浏览器，然后再一次启动浏览器，再访问该网站，服务器是不知道客户关闭了一次浏览器的。</p><p>这样设计的原因是因为Web服务器一般需要面对很多浏览器的并发访问，为了提高Web服务器对并发访问的处理能力，在设计HTTP协议时规定Web服务器发送HTTP应答报文和文档时，不保存发出请求的Web浏览器进程的任何状态信息。</p><hr><h3 id="HTTP请求"><a href="#HTTP请求" class="headerlink" title="HTTP请求"></a>HTTP请求</h3><p>每一个HTTP请求都由三部分组成，分别是：请求行、请求报头、请求正文。</p><h3 id="请求行"><a href="#请求行" class="headerlink" title="请求行"></a>请求行</h3><p>请求行一般由<strong>请求方法</strong>、<strong>url路径</strong>、<strong>协议版本</strong>组成，如下所示：</p><p><img src="/images/reptile/3.png" alt="HTTP协议-请求行"></p><h3 id="请求报头"><a href="#请求报头" class="headerlink" title="请求报头"></a>请求报头</h3><p>请求行下方的是则是请求报头，HTTP消息报头包括普通报头、请求报头、响应报头、实体报头。每个报头的形式如下：</p><blockquote><p>名字 + : + 空格 + 值</p></blockquote><ul><li><p><strong>Host</strong></p><p>指定的请求资源的域名（主机和端口号）。HTTP请求必须包含HOST，否则系统会以400状态码返回。</p></li><li><p><strong>User-Agant</strong></p><p>简称UA，内容包含发出请求的用户信息，通常UA包含浏览者的信息，主要是浏览器的名称版本和所用的操作系统。这个UA头不仅仅是使用浏览器才存在，只要使用了基于HTTP协议的客户端软件都会发送，无论是手机端还是PDA等，这个UA头是辨别客户端所用设备的重要依据。</p></li><li><p><strong>Accept</strong></p><p>告诉服务器可以接受的文件格式。通常这个值在各种浏览器中都差不多，不过WAP浏览器所能接受的格式要少一些，这也是用来区分WAP和计算机浏览器的主要依据之一，随着WAP浏览器的升级，其已经和计算机浏览器越来越接近，因此这个判断所起的作用也越来越弱。</p></li><li><p><strong>Cookie</strong></p><p>Cookie信息。</p></li><li><p><strong>Cache-Control</strong></p><p>指定请求和响应遵循的缓存机制。在请求消息或响应消息中设置Cache-Control并不会修改另一个消息消息处理过程中的缓存处理过程。请求时的缓存指令包括no-cache、no-store、man-age、max-stake、min-fresh、only-if-cached；响应消息中的指令包括 public、privete、no-cache、no-store、no-transform、must-revalidate、proxy-revalidate、max-age。</p></li><li><p><strong>Referer</strong></p><p>页面跳转处，表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面，是从什么网站来的。</p></li><li><p><strong>Content-Length</strong></p><p>内容长度。</p></li><li><p><strong>Content-Range</strong></p><p>响应的资源范围。可以在每次请求中标记请求的资源范围，在连接断开重连时，客户端只请求该资源未下载的部分，而不是重新请求整个资源，实现断点续传。迅雷就是基于这个原，使用多线程分段读取网络上的资源，最后再合并。</p></li><li><p><strong>Accept-Encoding</strong></p><p>指定所能接收的编码方式，通常服务器会对页面进行GZIP压缩后再输出以减少流量，一般浏览器均支持对这种压缩后的数据进行处理，但对于我们来说，如果不想接收到这些看似乱码的数据，可以指定不接收任何服务器端压缩处理，要求其原样返回。</p></li><li><p><strong>Accept-Language</strong></p><p>指浏览器可以接受的语言种类 en、en-us指英语 zh、zh-cn指中文。</p></li><li><p><strong>Connection</strong></p><p>客户端与服务器链接类型，keep-alive:保持链接，close:关闭链接。</p></li></ul><h3 id="请求正文"><a href="#请求正文" class="headerlink" title="请求正文"></a>请求正文</h3><p>请求正文通常是使用POST方法进行发送的数据，GET方法是没有请求正文的。</p><p>请求正文跟上面的消息报头一般由一个空行隔开。</p><hr><h3 id="HTTP响应"><a href="#HTTP响应" class="headerlink" title="HTTP响应"></a>HTTP响应</h3><p>HTTP响应同样也是由状态行、响应报头、报文主体三部分组成。</p><h3 id="状态行"><a href="#状态行" class="headerlink" title="状态行"></a>状态行</h3><p>状态行由HTTP协议版本号， 状态码， 状态消息三部分组成。如下所示：</p><p><img src="/images/reptile/4.png" alt="HTTP协议-状态行"></p><h3 id="响应报头"><a href="#响应报头" class="headerlink" title="响应报头"></a>响应报头</h3><ul><li><p><strong>Allow</strong></p><p>服务器支持哪些请求方法（如GET、POST等）。</p></li><li><p><strong>Date</strong></p><p>表示消息发送的时间，时间的描述格式由rfc822定义。例如，Date:Mon,31Dec200104:25:57GMT。Date描述的时间表示世界标准时，换算成本地时间，需要知道用户所在的时区。</p></li><li><p><strong>Set-Cookie</strong></p><p>非常重要的header, 用于把cookie发送到客户端浏览器，每一个写入cookie都会生成一个Set-Cookie。</p></li><li><p><strong>Expires</strong></p><p>指明应该在什么时候认为文档已经过期，从而不再缓存它，重新从服务器获取，会更新缓存。过期之前使用本地缓存。</p></li><li><p><strong>Content-Type</strong></p><p>WEB服务器告诉客户端自己响应的对象的类型和字符集。</p></li><li><p><strong>Content-Encoding</strong></p><p>文档的编码（Encode）方法。只有在解码之后才可以得到Content-Type头指定的内容类型。利用gzip压缩文档能够显著地减少HTML文档的下载时间。</p></li><li><p><strong>Content-Length</strong></p><p>指明实体正文的长度，以字节方式存储的十进制数字来表示。</p></li><li><p><strong>Location</strong></p><p>用于重定向一个新的位置，包含新的URL地址。表示客户应当到哪里去提取文档。</p></li><li><p><strong>Refresh</strong></p><p>表示浏览器应该在多少时间之后刷新文档，以秒计。</p></li></ul><h3 id="响应正文"><a href="#响应正文" class="headerlink" title="响应正文"></a>响应正文</h3><p>服务器返回的数据。</p><hr><h3 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h3><p>URL（Uniform Resource Locator），中文叫统一资源定位符。是用来标识某一处资源的地址。以下面这个URL为例，介绍下普通URL的各部分组成：</p><p><img src="/images/reptile/1.png" alt="HTTP协议-url结构"></p><ol><li><p>协议部分：该URL的协议部分为“http：”，这代表网页使用的是HTTP协议。在”HTTP”后面的“//”为分隔符。</p></li><li><p>域名部分：该URL的域名部分为“<a href="http://www.aspxfans.com”。一个URL中，也可以使用IP地址作为域名使用。" target="_blank" rel="noopener">www.aspxfans.com”。一个URL中，也可以使用IP地址作为域名使用。</a></p></li><li><p>端口部分：跟在域名后面的是端口，域名和端口之间使用“:”作为分隔符。端口不是一个URL必须的部分，如果省略端口部分，将采用默认端口。</p></li><li><p>路径部分：从域名后的第一个“/”开始到最后一个“？”为止，是路径部分，如果没有“?”,则是从域名后的最后一个“/”开始到“#”为止，是路径部分，如果没有“？”和“#”，那么从域名后的最后一个“/”开始到结束，都是路径部分。</p><p>本例中的文件名是“index.asp”。文件名部分也不是一个URL必须的部分，如果省略该部分，则使用默认的文件名。</p></li><li><p>参数部分：从“？”开始到“#”为止之间的部分为参数部分。本例中的参数部分为“boardID=5&amp;ID=24618&amp;page=1”。参数可以允许有多个参数，参数与参数之间用“&amp;”作为分隔符。</p></li><li><p>锚部分：从“#”开始到最后，都是锚部分。本例中的锚部分是“name”。锚部分也不是一个URL必须的部分。</p><p>锚部分是用来定位到页面中某个元素的。</p></li></ol><hr><h3 id="HTTP请求方法"><a href="#HTTP请求方法" class="headerlink" title="HTTP请求方法"></a>HTTP请求方法</h3><p>HTTP协议中定义的请求方法有以下几种：</p><table><thead><tr><th>序号</th><th>方法</th><th>描述</th></tr></thead><tbody><tr><td>1</td><td>GET</td><td>请求指定的页面信息，并返回实体主体。</td></tr><tr><td>2</td><td>HEAD</td><td>类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头</td></tr><tr><td>3</td><td>POST</td><td>向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。</td></tr><tr><td>4</td><td>PUT</td><td>从客户端向服务器传送的数据取代指定的文档的内容。</td></tr><tr><td>5</td><td>DELETE</td><td>请求服务器删除指定的页面。</td></tr><tr><td>6</td><td>CONNECT</td><td>HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。</td></tr><tr><td>7</td><td>OPTIONS</td><td>允许客户端查看服务器的性能。</td></tr><tr><td>8</td><td>TRACE</td><td>回显服务器收到的请求，主要用于测试或诊断。</td></tr></tbody></table><p>虽然HTTP请求中定义的方法有这么多种，但是我们平常使用的基本只有<code>GET</code>和<code>POST</code>两种方法，而且大部分网站都是禁用掉了除<code>GET</code>和<code>POST</code>外其他的方法。</p><p>因为其他几种方法通过<code>GET</code>或者<code>POST</code>都能实现，而且对于网站来说更加的安全和可控。</p><ul><li><p><code>GET</code></p><p>其实简单来说，<code>GET</code>方法一般用来负责获取数据，或者将一些简短的数据放到URL参数中传递到服务器。比<code>POST</code>更加高效和方便。</p></li><li><p><code>POST</code></p><p>由于<code>GET</code>方法最多在url中携带1024字节数据，且将数据放到URL中传递太不安全，数据量大时URL也会变得冗长。所以传递数据量大或者安全性要求高的数据的时候，最好使用<code>POST</code>方法来传递数据。</p></li></ul><hr><h3 id="状态码（status-code）"><a href="#状态码（status-code）" class="headerlink" title="状态码（status code）"></a>状态码（status code）</h3><p>当客户端向服务端发起一次请求后，服务端在返回的响应头中会包含一个HTTP状态码。下面是一些常见的状态码：</p><ul><li>200 - 请求成功</li><li>301 - 资源（网页等）被永久转移到其它URL</li><li>404 - 请求的资源（网页等）不存在</li><li>500 - 内部服务器错误</li></ul><p>HTTP的状态码是由三位数字来表示的，由第一位数字来表示状态码的类型，一般来说有五种类型：</p><table><thead><tr><th>分类</th><th>分类描述</th></tr></thead><tbody><tr><td>1**</td><td>信息，服务器收到请求，需要请求者继续执行操作</td></tr><tr><td>2**</td><td>成功，操作被成功接收并处理</td></tr><tr><td>3**</td><td>重定向，需要进一步的操作以完成请求</td></tr><tr><td>4**</td><td>客户端错误，请求包含语法错误或无法完成请求</td></tr><tr><td>5**</td><td>服务器错误，服务器在处理请求的过程中发生了错误</td></tr></tbody></table><p>以下是详细的状态码列表：</p><table><thead><tr><th>状态码</th><th>状态码英文名称</th><th>中文描述</th></tr></thead><tbody><tr><td>100</td><td>Continue</td><td>继续。<a href="http://www.dreamdu.com/webbuild/client_vs_server/" target="_blank" rel="noopener">客户端</a>应继续其请求</td></tr><tr><td>101</td><td>Switching Protocols</td><td>切换协议。服务器根据客户端的请求切换协议。只能切换到更高级的协议，例如，切换到HTTP的新版本协议</td></tr><tr><td></td><td></td><td></td></tr><tr><td>200</td><td>OK</td><td>请求成功。一般用于GET与POST请求</td></tr><tr><td>201</td><td>Created</td><td>已创建。成功请求并创建了新的资源</td></tr><tr><td>202</td><td>Accepted</td><td>已接受。已经接受请求，但未处理完成</td></tr><tr><td>203</td><td>Non-Authoritative Information</td><td>非授权信息。请求成功。但返回的meta信息不在原始的服务器，而是一个副本</td></tr><tr><td>204</td><td>No Content</td><td>无内容。服务器成功处理，但未返回内容。在未更新网页的情况下，可确保浏览器继续显示当前文档</td></tr><tr><td>205</td><td>Reset Content</td><td>重置内容。服务器处理成功，用户终端（例如：浏览器）应重置文档视图。可通过此返回码清除浏览器的表单域</td></tr><tr><td>206</td><td>Partial Content</td><td>部分内容。服务器成功处理了部分GET请求</td></tr><tr><td></td><td></td><td></td></tr><tr><td>300</td><td>Multiple Choices</td><td>多种选择。请求的资源可包括多个位置，相应可返回一个资源特征与地址的列表用于用户终端（例如：浏览器）选择</td></tr><tr><td>301</td><td>Moved Permanently</td><td>永久移动。请求的资源已被永久的移动到新URI，返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替</td></tr><tr><td>302</td><td>Found</td><td>临时移动。与301类似。但资源只是临时被移动。客户端应继续使用原有URI</td></tr><tr><td>303</td><td>See Other</td><td>查看其它地址。与301类似。使用GET和POST请求查看</td></tr><tr><td>304</td><td>Not Modified</td><td>未修改。所请求的资源未修改，服务器返回此状态码时，不会返回任何资源。客户端通常会缓存访问过的资源，通过提供一个头信息指出客户端希望只返回在指定日期之后修改的资源</td></tr><tr><td>305</td><td>Use Proxy</td><td>使用代理。所请求的资源必须通过代理访问</td></tr><tr><td>306</td><td>Unused</td><td>已经被废弃的HTTP状态码</td></tr><tr><td>307</td><td>Temporary Redirect</td><td>临时重定向。与302类似。使用GET请求重定向</td></tr><tr><td></td><td></td><td></td></tr><tr><td>400</td><td>Bad Request</td><td>客户端请求的语法错误，服务器无法理解</td></tr><tr><td>401</td><td>Unauthorized</td><td>请求要求用户的身份认证</td></tr><tr><td>402</td><td>Payment Required</td><td>保留，将来使用</td></tr><tr><td>403</td><td>Forbidden</td><td>服务器理解请求客户端的请求，但是拒绝执行此请求</td></tr><tr><td>404</td><td>Not Found</td><td>服务器无法根据客户端的请求找到资源（网页）。通过此代码，网站设计人员可设置”您所请求的资源无法找到”的个性页面</td></tr><tr><td>405</td><td>Method Not Allowed</td><td>客户端请求中的方法被禁止</td></tr><tr><td>406</td><td>Not Acceptable</td><td>服务器无法根据客户端请求的内容特性完成请求</td></tr><tr><td>407</td><td>Proxy Authentication Required</td><td>请求要求代理的身份认证，与401类似，但请求者应当使用代理进行授权</td></tr><tr><td>408</td><td>Request Time-out</td><td>服务器等待客户端发送的请求时间过长，超时</td></tr><tr><td>409</td><td>Conflict</td><td>服务器完成客户端的PUT请求是可能返回此代码，服务器处理请求时发生了冲突</td></tr><tr><td>410</td><td>Gone</td><td>客户端请求的资源已经不存在。410不同于404，如果资源以前有现在被永久删除了可使用410代码，网站设计人员可通过301代码指定资源的新位置</td></tr><tr><td>411</td><td>Length Required</td><td>服务器无法处理客户端发送的不带Content-Length的请求信息</td></tr><tr><td>412</td><td>Precondition Failed</td><td>客户端请求信息的先决条件错误</td></tr><tr><td>413</td><td>Request Entity Too Large</td><td>由于请求的实体过大，服务器无法处理，因此拒绝请求。为防止客户端的连续请求，服务器可能会关闭连接。如果只是服务器暂时无法处理，则会包含一个Retry-After的响应信息</td></tr><tr><td>414</td><td>Request-URI Too Large</td><td>请求的URI过长（URI通常为网址），服务器无法处理</td></tr><tr><td>415</td><td>Unsupported Media Type</td><td>服务器无法处理请求附带的媒体格式</td></tr><tr><td>416</td><td>Requested range not satisfiable</td><td>客户端请求的范围无效</td></tr><tr><td>417</td><td>Expectation Failed</td><td>服务器无法满足Expect的请求头信息</td></tr><tr><td></td><td></td><td></td></tr><tr><td>500</td><td>Internal Server Error</td><td>服务器内部错误，无法完成请求</td></tr><tr><td>501</td><td>Not Implemented</td><td>服务器不支持请求的功能，无法完成请求</td></tr><tr><td>502</td><td>Bad Gateway</td><td>充当网关或代理的服务器，从远端服务器接收到了一个无效的请求</td></tr><tr><td>503</td><td>Service Unavailable</td><td>由于超载或系统维护，服务器暂时的无法处理客户端的请求。延时的长度可包含在服务器的Retry-After头信息中</td></tr><tr><td>504</td><td>Gateway Time-out</td><td>充当网关或代理的服务器，未及时从远端服务器获取请求</td></tr><tr><td>505</td><td>HTTP Version not supported</td><td>服务器不支持请求的HTTP协议的版本，无法完成处理</td></tr></tbody></table><hr><h3 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h3><p><code>Cookie</code>有时也用其复数形式 <code>Cookies</code>，英文是饼干的意思。指某些网站为了辨别用户身份、进行 session 跟踪而储存在用户本地终端上的数据（通常经过加密）。最新的规范是 RFC6265 。</p><p><code>Cookie</code>其实就是由服务器发给客户端的特殊信息，而这些信息以文本文件的方式存放在客户端，然后客户端每次向服务器发送请求的时候都会带上这些特殊的信息。 服务器在接收到<code>Cookie</code>以后，会验证<code>Cookie</code>的信息，以此来辨别用户的身份。</p><p><code>Cookie</code>可以理解为一个临时通行证。</p><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p><code>Cookie</code>其实是HTTP请求头的扩展部分，由于HTTP协议是无状态的协议，所以为了在网页上实现登陆之类的需求，所以扩展了<code>Cookie</code>这样的功能。</p><p>每一次HTTP请求在数据交换完毕之后就会关闭连接，所以下一次HTTP请求就无法让服务端得知你和上一次请求的关系。而使用了<code>Cookie</code>之后，你在第一次登陆之类的请求成功之后，服务器会在<code>Response</code>的头信息中给你返回<code>Cookie</code>信息，你下一次访问的时候带上这个Cookie信息，则服务器就能识别你为上一次成功登陆的用户。</p><h3 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h3><p><code>Cookie</code>一般保存的格式为json格式，由一些属性组成。</p><ul><li>name：<code>Cookie</code>的名称</li><li>value：<code>Cookie</code>的值</li><li>domain：可以使用此<code>Cookie</code>的域名</li><li>path：可以使用此<code>Cookie</code>的页面路径</li><li>expires/Max-Age：此<code>Cookie</code>的超时时间</li><li>secure：设置是否只能通过https来传递此条<code>Cookie</code></li></ul><h3 id="domain属性"><a href="#domain属性" class="headerlink" title="domain属性"></a>domain属性</h3><p>域名一般来说分为顶级域名，二级域名，三级域名等等。</p><p>例如baidu.com是一个顶级域名，而<a href="http://www.baidu.com和map.baidu.com就是二级域名，依次类推。" target="_blank" rel="noopener">www.baidu.com和map.baidu.com就是二级域名，依次类推。</a></p><p>而在我们的<code>Cookie</code>来说，都有一个<code>domain</code>属性，这个属性限制了访问哪些域名时可以使用这一条<code>Cookie</code>。因为每个网站基本上都会分发<code>Cookie</code>，所以<code>domain</code>属性就可以让我们在访问新浪时不会带上百度分发给我们的<code>Cookie</code>。</p><p>而在同一系的域名中，顶级域名是无法使用其二级域名的<code>Cookie</code>的，也就是说访问baidu.com的时候是不会带上map.baidu.com分发的<code>Cookie</code>的，二级域名之间的<code>Cookie</code>也不可以共享。但访问二级域名时是可以使用顶级域名的<code>Cookie</code>的。</p><h3 id="path属性"><a href="#path属性" class="headerlink" title="path属性"></a>path属性</h3><p>path属性为可以访问此cookie的页面路径。 比如domain是abc.com，path是/test，那么只有/test路径下的页面可以读取此cookie。</p><h3 id="expires-Max-Age属性"><a href="#expires-Max-Age属性" class="headerlink" title="expires/Max-Age属性"></a>expires/Max-Age属性</h3><p>字段为此cookie超时时间。若设置其值为一个时间，那么当到达此时间后，此cookie失效。不设置的话默认值是Session，意思是cookie会和session一起失效。当浏览器关闭(不是浏览器标签页，而是整个浏览器) 后，此cookie失效。</p><hr><h3 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h3><p>Session，中文经常翻译为会话，其本来的含义是指有始有终的一系列动作/消息，比如打电话时从拿起电话拨号到挂断电话这中间的一系列过程可以称之为一个session。这个词在各个领域都有在使用。</p><p>而我们web领域，一般使用的是其本义，<strong>一个浏览器窗口从打开到关闭这个期间</strong>。</p><p>Session的目的则是，在一个客户从打开浏览器到关闭浏览器这个期间内，发起的所有请求都可以被识别为同一个用户。而实现的方式则是，在一个客户打开浏览器开始访问网站的时候，会生成一个SessionID，这个ID每次的访问都会带上，而服务器会识别这个SessionID并且将与这个SessionID有关的数据保存在服务器上。由此来实现客户端的状态识别。</p><p>Session与Cookie相反，Session是存储在服务器上的数据，只由客户端传上来的SessionId来进行判定，所以相对于Cookie，Session的安全性更高。</p><p>一般SessionID会在浏览器被关闭时丢弃，或者服务器会验证Session的活跃程度，例如30分钟某一个SessionID都没有活跃，那么也会被识别为失效。</p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python图像处理-Pillow</title>
      <link href="/2018/11/21/2018-11-21-python-pillow/"/>
      <url>/2018/11/21/2018-11-21-python-pillow/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><p>本文地址：<a href="https://www.jianshu.com/p/3740dec1f436" target="_blank" rel="noopener">https://www.jianshu.com/p/3740dec1f436</a></p><hr><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>Python传统的图像处理库<code>PIL</code>(Python Imaging Library )，可以说基本上是Python处理图像的标准库，功能强大，使用简单。</p><p>但是由于<code>PIL</code>不支持Python3，而且更新缓慢。所以有志愿者在<code>PIL</code>的基础上创建了一个分支版本，命名为<code>Pillow</code>，<code>Pillow</code>目前最新支持到python3.6，更新活跃，并且增添了许多新的特性。所以我们安装Pillow即可。</p><hr><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code>Pillow</code>的安装比较的简单，直接pip安装即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install Pillow</span><br></pre></td></tr></table></figure><p>但是要注意的一点是，<code>Pillow</code>和<code>PIL</code>不能共存在同一个环境中，所以如果安装的有<code>PIL</code>的话，那么安装<code>Pillow</code>之前应该删除<code>PIL</code>。</p><p>由于是继承自<code>PIL</code>的分支，所以<code>Pillow</code>的导入是这样的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL </span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br></pre></td></tr></table></figure><hr><h3 id="使用手册"><a href="#使用手册" class="headerlink" title="使用手册"></a>使用手册</h3><h3 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h3><p><code>Image</code>是Pillow中最为重要的类，实现了Pillow中大部分的功能。要创建这个类的实例主要有三个方式：</p><ol><li>从文件加载图像</li><li>处理其他图像获得</li><li>创建一个新的图像</li></ol><h4 id="读取图像"><a href="#读取图像" class="headerlink" title="读取图像"></a>读取图像</h4><p>一般来说，我们都是都过从文件加载图像来实例化这个类，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">picture = Image.open(<span class="string">'happy.png'</span>)</span><br></pre></td></tr></table></figure><p>如果没有指定图片格式的话，那么<code>Pillow</code>会自动识别文件内容为文件格式。</p><h4 id="新建图像"><a href="#新建图像" class="headerlink" title="新建图像"></a>新建图像</h4><p><code>Pillow</code>新建空白图像使用<code>new()</code>方法， 第一个参数是mode即颜色空间模式，第二个参数指定了图像的分辨率(宽x高)，第三个参数是颜色。</p><ul><li>可以直接填入常用颜色的名称。如’red’。</li><li>也可以填入十六进制表示的颜色，如<code>#FF0000</code>表示红色。</li><li>还能传入元组，比如(255, 0, 0, 255)或者(255， 0， 0)表示红色。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">picture = Image.new(<span class="string">'RGB'</span>, (<span class="number">200</span>, <span class="number">100</span>), <span class="string">'red'</span>)</span><br></pre></td></tr></table></figure><h4 id="保存图像"><a href="#保存图像" class="headerlink" title="保存图像"></a>保存图像</h4><p>保存图片的话需要使用<code>save()</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">picture.save(<span class="string">'happy.png'</span>)</span><br></pre></td></tr></table></figure><p>保存的时候，如果没有指定图片格式的话，那么<code>Pillow</code>会根据输入的后缀名决定保存的文件格式。</p><hr><h3 id="图像的坐标表示"><a href="#图像的坐标表示" class="headerlink" title="图像的坐标表示"></a>图像的坐标表示</h3><p>在Pillow中，用的是图像的<strong>左上角</strong>为坐标的原点（0，0），所以这意味着，x轴的数值是从左到右增长的，y轴的数值是从上到下增长的。</p><p>我们处理图像时，常常需要去表示一个矩形的图像区域。<code>Pillow</code>中很多方法都需要传入一个表示矩形区域的元祖参数。</p><p>这个元组参数包含四个值，分别代表矩形四条边的距离X轴或者Y轴的距离。顺序是<code>(左，顶，右，底)</code>。其实就相当于，矩形的左上顶点坐标为<code>(左，顶)</code>，矩形的右下顶点坐标为<code>(右，底)</code>，两个顶点就可以确定一个矩形的位置。</p><p>右和底坐标稍微特殊，跟python列表索引规则一样，是左闭又开的。可以理解为<code>[左, 右)</code>和<code>[顶， 底)</code>这样左闭右开的区间。比如(3, 2, 8, 9)就表示了横坐标范围[3, 7]；纵坐标范围[2, 8]的矩形区域。</p><hr><h3 id="常用属性"><a href="#常用属性" class="headerlink" title="常用属性"></a>常用属性</h3><ul><li><p><code>PIL.Image.filename</code></p><p>图像源文件的文件名或者路径，只有使用<code>open()</code>方法创建的对象有这个属性。</p><p>类型：字符串</p></li><li><p><code>PIL.Image.format</code></p><p>图像源文件的文件格式。</p></li><li><p><code>PIL.Image.mode</code></p><p>图像的模式，一般来说是“1”, “L”, “RGB”, 或者“CMYK” 。</p></li><li><p><code>PIL.Image.size</code></p><p>图像的大小</p></li><li><p><code>PIL.Image.width</code></p><p>图像的宽度</p></li><li><p><code>PIL.Image.height</code></p><p>图像的高度</p></li><li><p><code>PIL.Image.info</code></p><p>图像的一些信息，为字典格式</p></li></ul><hr><h3 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h3><h4 id="裁剪图片"><a href="#裁剪图片" class="headerlink" title="裁剪图片"></a>裁剪图片</h4><p><code>Image</code>使用<code>crop()</code>方法来裁剪图像，此方法需要传入一个矩形元祖参数，返回一个新的<code>Image</code>对象，对原图没有影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">croped_im = im.crop((<span class="number">100</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">200</span>))</span><br></pre></td></tr></table></figure><h4 id="复制与粘贴图像"><a href="#复制与粘贴图像" class="headerlink" title="复制与粘贴图像"></a>复制与粘贴图像</h4><p>复制图像使用<code>copy()</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">copyed_im = im.copy()</span><br></pre></td></tr></table></figure><p>粘贴图像使用<code>paste()</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">croped_im = im.crop((<span class="number">100</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">200</span>))</span><br><span class="line">im.paste(croped_im, (<span class="number">0</span>, <span class="number">0</span>))</span><br></pre></td></tr></table></figure><p>im对象调用了<code>paste()</code>方法，第一个参数是被裁剪下来用来粘贴的图像，第二个参数是一个位置参数元祖，这个位置参数是粘贴的图像的左顶点。</p><h4 id="调整图像的大小"><a href="#调整图像的大小" class="headerlink" title="调整图像的大小"></a>调整图像的大小</h4><p>调整图像大小使用<code>resize()</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">resized_im = im.resize((width, height))</span><br></pre></td></tr></table></figure><p><code>resize()</code>方法会返回一个重设了大小的<code>Image</code>对象。</p><h4 id="旋转图像和翻转图像"><a href="#旋转图像和翻转图像" class="headerlink" title="旋转图像和翻转图像"></a>旋转图像和翻转图像</h4><p>旋转图像使用<code>rotate()</code>方法，此方法按逆时针旋转，并返回一个新的<code>Image</code>对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 逆时针旋转90度</span></span><br><span class="line">im.rotate(<span class="number">90</span>)</span><br><span class="line">im.rotate(<span class="number">180</span>)</span><br><span class="line">im.rotate(<span class="number">20</span>, expand=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>旋转的时候，会将图片超出边界的边角裁剪掉。如果加入<code>expand=True</code>参数，就可以将图片边角保存住。</p><p>翻转图像使用<code>transpose()</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 水平翻转</span></span><br><span class="line">im.transpose(Image.FLIP_LEFT_RIGHT)</span><br><span class="line"><span class="comment"># 垂直翻转</span></span><br><span class="line">im.transpose(Image.FLIP_TOP_BOTTOM)</span><br></pre></td></tr></table></figure><h4 id="获取单个像素的值"><a href="#获取单个像素的值" class="headerlink" title="获取单个像素的值"></a>获取单个像素的值</h4><p>使用<code>getpixel</code>(<em>xy</em>)方法可以获取单个像素位置的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">im.getpixel((<span class="number">100</span>, <span class="number">100</span>))</span><br></pre></td></tr></table></figure><p>传入的xy需要是一个元祖形式的坐标。</p><p>如果图片是多通道的，那么返回的是一个元祖。</p><h4 id="通过通道分割图片"><a href="#通过通道分割图片" class="headerlink" title="通过通道分割图片"></a>通过通道分割图片</h4><h5 id="split"><a href="#split" class="headerlink" title="split()"></a>split()</h5><p><code>split()</code>可以将多通道图片按通道分割为单通道图片：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">R, G, B = im.split()</span><br></pre></td></tr></table></figure><p><code>split()</code>方法返回的是一个元祖，元祖中的元素则是分割后的单个通道的值。</p><h5 id="getchannel-channel"><a href="#getchannel-channel" class="headerlink" title="getchannel(channel)"></a>getchannel(channel)</h5><p><code>getchannel()</code>可以获取单个通道的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">R = im.getchannel(<span class="string">"R"</span>)</span><br></pre></td></tr></table></figure><h4 id="加载图片全部数据"><a href="#加载图片全部数据" class="headerlink" title="加载图片全部数据"></a>加载图片全部数据</h4><p>我们可以使用<code>load()</code>方法加载图片所有的数据，并比较方便的修改像素的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pixdata = im.load()</span><br><span class="line">pixdata[<span class="number">100</span>,<span class="number">200</span>] = <span class="number">255</span></span><br></pre></td></tr></table></figure><p>此方法返回的是一个<code>PIL.PyAccess</code>，可以通过这个类的索引来对指定坐标的像素点进行修改。</p><h4 id="关闭图片并释放内存"><a href="#关闭图片并释放内存" class="headerlink" title="关闭图片并释放内存"></a>关闭图片并释放内存</h4><p>此方法会删除图片对象并释放内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">im.close()</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>datetime-Python时间处理模块</title>
      <link href="/2018/11/21/2018-11-21-python-date/"/>
      <url>/2018/11/21/2018-11-21-python-date/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在Python的时间处理模块中，<code>time</code>这个模块主要侧重于时间戳格式的处理，而<code>datetime</code>则相当于<code>time</code>模块的高级封装，提供了更多关于日期处理的方法。</p><p>并且<code>datetime</code>的接口使用起来更加的直观，方便。</p><hr><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p><code>datetime</code>主要由五个模块组成：</p><ul><li><strong>datetime.date</strong>：表示日期的类。常用的属性有year, month, day。</li><li><strong>datetime.time</strong>：表示时间的类。常用的属性有hour, minute, second, microsecond。</li><li><strong>datetime.datetime</strong>：表示日期+时间。</li><li><strong>datetime.timedelta</strong>：表示时间间隔，即两个时间点之间的长度，常常用来做时间的加减。</li><li><strong>datetime.tzinfo</strong>：与时区有关的相关信息。</li></ul><p>在<code>datetime</code>中，使用的最多的就是<code>datetime.datetime</code>模块，而<code>datetime.timedelta</code>常常被用来修改时间。</p><p>最后，<code>datetime</code>的时间显示是与时区有关系的，所以还有一个处理时区信息的模块<code>datetime.tzinfo</code>。</p><hr><h3 id="datetime-datetime"><a href="#datetime-datetime" class="headerlink" title="datetime.datetime"></a>datetime.datetime</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">datetime</span>.<span class="title">datetime</span><span class="params">(year, month, day, hour=<span class="number">0</span>, minute=<span class="number">0</span>, second=<span class="number">0</span>, microsecond=<span class="number">0</span>, tzinfo=None, *, fold=<span class="number">0</span>)</span></span></span><br></pre></td></tr></table></figure><ul><li>MINYEAR &lt;= year &lt;= MAXYEAR</li><li>1 &lt;= month &lt;= 12</li><li>1 &lt;= day &lt;= number of days in the given month and year</li><li>0 &lt;= hour &lt; 24</li><li>0 &lt;= minute &lt; 60</li><li>0 &lt;= second &lt; 60</li><li>0 &lt;= microsecond &lt; 1000000</li><li>fold in [0, 1]</li></ul><p>这是<code>datetime.datetime</code>参数的取值范围，如果设定的值超过这个范围，那么就会抛出<code>ValueError</code>异常。</p><p>其中<code>year</code>，<code>month</code>，<code>day</code>是必须参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: datetime.datetime(year=<span class="number">2000</span>, month=<span class="number">1</span>, day=<span class="number">1</span>, hour=<span class="number">12</span>)</span><br><span class="line">Out[<span class="number">2</span>]: datetime.datetime(<span class="number">2000</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="类方法-Classmethod"><a href="#类方法-Classmethod" class="headerlink" title="类方法 Classmethod"></a>类方法 Classmethod</h3><p>这些方法大多数用来生成一个<code>datetime</code>对象。</p><ul><li><p>classmethod datetime.<strong>today()</strong></p><p>获取今天的时间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: datetime.datetime.today()</span><br><span class="line">Out[<span class="number">1</span>]: datetime.datetime(<span class="number">2018</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">15</span>, <span class="number">5</span>, <span class="number">17</span>, <span class="number">127663</span>)</span><br></pre></td></tr></table></figure></li><li><p>classmethod datetime.<strong>now(tz=None)</strong></p><p>获取当前的时间，可以传入一个<code>tzinfo</code>对象来指定某一个时区。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: datetime.datetime.now()</span><br><span class="line">Out[<span class="number">2</span>]: datetime.datetime(<span class="number">2018</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">15</span>, <span class="number">8</span>, <span class="number">30</span>, <span class="number">593801</span>)</span><br></pre></td></tr></table></figure></li><li><p>classmethod datetime.<strong>fromtimestamp(timestamp, tz=None)</strong></p><p>用一个时间戳来生成<code>datetime</code>对象，同样可以指定时区。</p><p><strong>注：时间戳需要为10位的</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">3</span>]: datetime.datetime.fromtimestamp(<span class="number">1530515475.18224</span>)</span><br><span class="line">Out[<span class="number">3</span>]: datetime.datetime(<span class="number">2018</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">15</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">182240</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="实例方法-instance-method"><a href="#实例方法-instance-method" class="headerlink" title="实例方法 instance method"></a>实例方法 instance method</h3><p>这些方法大多是一个<code>datetime</code>对象能进行的操作。</p><ul><li><p>datetime.<strong>date()</strong> 和 datetime.<strong>time()</strong></p><p>获取<code>datetime</code>对象的日期或者时间部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: datetime.datetime.now().date()</span><br><span class="line">Out[<span class="number">1</span>]: datetime.date(<span class="number">2018</span>, <span class="number">7</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">1</span>]: datetime.datetime.now().time()</span><br><span class="line">Out[<span class="number">1</span>]: datetime.time(<span class="number">15</span>, <span class="number">24</span>, <span class="number">37</span>, <span class="number">355514</span>)</span><br></pre></td></tr></table></figure></li><li><p>datetime.<strong>replace(year=self.year, month=self.month, day=self.day, hour=self.hour, minute=self.minute, second=self.second, microsecond=self.microsecond, tzinfo=self.tzinfo, * fold=0)</strong></p><p>替换<code>datetime</code>对象的指定数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: now = datetime.datetime.now()</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: now</span><br><span class="line">Out[<span class="number">2</span>]: datetime.datetime(<span class="number">2018</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">15</span>, <span class="number">26</span>, <span class="number">45</span>, <span class="number">116239</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: now.replace(year=<span class="number">2000</span>)</span><br><span class="line">Out[<span class="number">3</span>]: datetime.datetime(<span class="number">2000</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">15</span>, <span class="number">26</span>, <span class="number">45</span>, <span class="number">116239</span>)</span><br></pre></td></tr></table></figure></li><li><p>datetime.<code>timestamp()</code></p><p>转换成时间戳。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: datetime.datetime.now().timestamp()</span><br><span class="line">Out[<span class="number">1</span>]: <span class="number">1530515994.798248</span></span><br></pre></td></tr></table></figure></li><li><p>datetime.<strong>weekday()</strong></p><p>返回一个值，表示日期为星期几。0为星期一，6为星期天。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: datetime.now().weekday()</span><br><span class="line">Out[<span class="number">1</span>]: <span class="number">1</span></span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="datetime-timedelta"><a href="#datetime-timedelta" class="headerlink" title="datetime.timedelta"></a>datetime.timedelta</h3><p>在实际的使用中，我们常常会遇到这样的需求：需要给某个时间增加或减少一天，甚至是增加或减少一天三小时二十分钟。</p><p>那么在遇到这样的需求时，去计算时间戳是非常的麻烦的，所以<code>datetime.timedelta</code>这个模块使我们能够非常方便的对时间做加减。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">datetime</span>.<span class="title">timedelta</span><span class="params">(days=<span class="number">0</span>, seconds=<span class="number">0</span>, microseconds=<span class="number">0</span>, milliseconds=<span class="number">0</span>, minutes=<span class="number">0</span>, hours=<span class="number">0</span>, weeks=<span class="number">0</span>)</span></span></span><br></pre></td></tr></table></figure><p><code>datetime</code>是对某些运算符进行了重载的，所以我们可以如下操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: now</span><br><span class="line">Out[<span class="number">2</span>]: datetime.datetime(<span class="number">2018</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">15</span>, <span class="number">26</span>, <span class="number">45</span>, <span class="number">116239</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: now - timedelta(days=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">3</span>]: datetime.datetime(<span class="number">2018</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">15</span>, <span class="number">26</span>, <span class="number">45</span>, <span class="number">116239</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: now + timedelta(days=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">4</span>]: datetime.datetime(<span class="number">2018</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">15</span>, <span class="number">26</span>, <span class="number">45</span>, <span class="number">116239</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: now + timedelta(days=<span class="number">-1</span>)</span><br><span class="line">Out[<span class="number">5</span>]: datetime.datetime(<span class="number">2018</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">15</span>, <span class="number">26</span>, <span class="number">45</span>, <span class="number">116239</span>)</span><br></pre></td></tr></table></figure><hr><h3 id="strftime-和-strptime"><a href="#strftime-和-strptime" class="headerlink" title="strftime() 和 strptime()"></a>strftime() 和 strptime()</h3><p><code>datetime</code>中提供了两个方法，可以方便的把<code>datetime</code>对象转换成格式化的字符串或者把字符串转换成<code>datetime</code>对象。</p><ul><li><p>由<code>datetime</code>转换成字符串：<strong>datetime.strftime()</strong></p><p><code>strftime()</code>是datetime对象的实例方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: datetime.datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S %f'</span>)</span><br><span class="line">Out[<span class="number">1</span>]: <span class="string">'2018-07-02 15:26:45 116239'</span></span><br></pre></td></tr></table></figure></li><li><p>由字符串转换成<code>datetime</code>：<strong>datetime.datetime.strptime()</strong></p><p><code>strptime()</code>则是一个类方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: newsTime=<span class="string">'Sun, 23 Apr 2017 05:15:05 GMT'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: GMT_FORMAT = <span class="string">'%a, %d %b %Y %H:%M:%S GMT'</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: datetime.datetime.strptime(newsTime,GMT_FORMAT)</span><br><span class="line">Out[<span class="number">3</span>]: datetime.datetime(<span class="number">2017</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">5</span>, <span class="number">15</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure></li></ul><p>以下是格式化的符号：</p><ul><li>%y 两位数的年份表示（00-99）</li><li>%Y 四位数的年份表示（000-9999）</li><li>%m 月份（01-12）</li><li>%d 月内中的一天（0-31）</li><li>%H 24小时制小时数（0-23）</li><li>%I 12小时制小时数（01-12）</li><li>%M 分钟数（00=59）</li><li>%S 秒（00-59）</li><li>%a 本地简化星期名称</li><li>%A 本地完整星期名称</li><li>%b 本地简化的月份名称</li><li>%B 本地完整的月份名称</li><li>%c 本地相应的日期表示和时间表示</li><li>%j 年内的一天（001-366）</li><li>%p 本地A.M.或P.M.的等价符</li><li>%U 一年中的星期数（00-53）星期天为星期的开始</li><li>%w 星期（0-6），星期天为星期的开始</li><li>%W 一年中的星期数（00-53）星期一为星期的开始</li><li>%x 本地相应的日期表示</li><li>%X 本地相应的时间表示</li><li>%Z 当前时区的名称</li><li>%% %号本身</li></ul><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python网络请求urllib和urllib3详解</title>
      <link href="/2018/11/21/2018-11-21-reptile-urllib/"/>
      <url>/2018/11/21/2018-11-21-reptile-urllib/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><h3 id="Python网络请求urllib和urllib3详解"><a href="#Python网络请求urllib和urllib3详解" class="headerlink" title="Python网络请求urllib和urllib3详解"></a>Python网络请求urllib和urllib3详解</h3><p><code>urllib</code>是Python中请求url连接的官方标准库，在Python2中主要为urllib和urllib2，在Python3中整合成了urllib。</p><p>而urllib3则是增加了连接池等功能，两者互相都有补充的部分。</p><h3 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h3><p>urllib作为Python的标准库，基本上涵盖了基础的网络请求功能。</p><hr><h3 id="urllib-request"><a href="#urllib-request" class="headerlink" title="urllib.request"></a>urllib.request</h3><p>urllib中，<code>request</code>这个模块主要负责构造和发起网络请求，并在其中加入Headers、Proxy等。</p><h3 id="发起GET请求"><a href="#发起GET请求" class="headerlink" title="发起GET请求"></a>发起GET请求</h3><p>主要使用<code>urlopen()</code>方法来发起请求：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">resp = request.urlopen(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">print(resp.read().decode())</span><br></pre></td></tr></table></figure><p>在<code>urlopen()</code>方法中传入字符串格式的url地址，则此方法会访问目标网址，然后返回访问的结果。</p><p>访问的结果会是一个<code>http.client.HTTPResponse</code>对象，使用此对象的<code>read()</code>方法，则可以获取访问网页获得的数据。但是要注意的是，获得的数据会是<code>bytes</code>的二进制格式，所以需要<code>decode()</code>一下，转换成字符串格式。</p><h3 id="发起POST请求"><a href="#发起POST请求" class="headerlink" title="发起POST请求"></a>发起POST请求</h3><p><code>urlopen()</code>默认的访问方式是GET，当在<code>urlopen()</code>方法中传入data参数时，则会发起POST请求。<strong>注意：传递的data数据需要为bytes格式。</strong></p><p>设置timeout参数还可以设置超时时间，如果请求时间超出，那么就会抛出异常。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">resp = request.urlopen(<span class="string">'http://httpbin.org'</span>, data=<span class="string">b'word=hello'</span>, timeout=<span class="number">10</span>)</span><br><span class="line">print(resp.read().decode())</span><br></pre></td></tr></table></figure><h3 id="添加Headers"><a href="#添加Headers" class="headerlink" title="添加Headers"></a>添加Headers</h3><p>通过<code>urllib</code>发起的请求会有默认的一个Headers：”User-Agent”:”Python-urllib/3.6”，指明请求是由<code>urllib</code>发送的。</p><p>所以遇到一些验证User-Agent的网站时，我们需要自定义Headers，而这需要借助于urllib.request中的<code>Request</code>对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/get'</span></span><br><span class="line">headers = &#123;<span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要使用url和headers生成一个Request对象，然后将其传入urlopen方法中</span></span><br><span class="line">req = request.Request(url, headers=headers)</span><br><span class="line">resp = request.urlopen(req)</span><br><span class="line">print(resp.read().decode())</span><br></pre></td></tr></table></figure><h3 id="Request对象"><a href="#Request对象" class="headerlink" title="Request对象"></a>Request对象</h3><p>如上所示，<code>urlopen()</code>方法中不止可以传入字符串格式的url，也可以传入一个<code>Request</code>对象来扩展功能，<code>Request</code>对象如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(url, data=None, headers=&#123;&#125;,</span></span></span><br><span class="line"><span class="class"><span class="params">                             origin_req_host=None,</span></span></span><br><span class="line"><span class="class"><span class="params">                             unverifiable=False, method=None)</span></span></span><br></pre></td></tr></table></figure><p>构造<code>Request</code>对象必须传入url参数，data数据和headers都是可选的。</p><p>最后，<code>Request</code>方法可以使用method参数来自由选择请求的方法，如PUT，DELETE等等，默认为GET。</p><h3 id="添加Cookie"><a href="#添加Cookie" class="headerlink" title="添加Cookie"></a>添加Cookie</h3><p>为了在请求时能带上Cookie信息，我们需要重新构造一个opener。</p><p>使用request.build_opener方法来进行构造opener，将我们想要传递的cookie配置到opener中，然后使用这个opener的open方法来发起请求。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> http <span class="keyword">import</span> cookiejar</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/cookies'</span></span><br><span class="line"><span class="comment"># 创建一个cookiejar对象</span></span><br><span class="line">cookie = cookiejar.CookieJar()</span><br><span class="line"><span class="comment"># 使用HTTPCookieProcessor创建cookie处理器</span></span><br><span class="line">cookies = request.HTTPCookieProcessor(cookie)</span><br><span class="line"><span class="comment"># 并以它为参数创建Opener对象</span></span><br><span class="line">opener = request.build_opener(cookies)</span><br><span class="line"><span class="comment"># 使用这个opener来发起请求</span></span><br><span class="line">resp = opener.open(url)</span><br><span class="line">print(resp.read().decode())</span><br></pre></td></tr></table></figure><p>或者也可以把这个生成的opener使用install_opener方法来设置为全局的。</p><p>则之后使用urlopen方法发起请求时，都会带上这个cookie。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将这个opener设置为全局的opener</span></span><br><span class="line">request.install_opener(opener)</span><br><span class="line">resp = request.urlopen(url)</span><br></pre></td></tr></table></figure><h3 id="设置Proxy代理"><a href="#设置Proxy代理" class="headerlink" title="设置Proxy代理"></a>设置Proxy代理</h3><p>使用爬虫来爬取数据的时候，常常需要使用代理来隐藏我们的真实IP。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/ip'</span></span><br><span class="line">proxy = &#123;<span class="string">'http'</span>:<span class="string">'50.233.137.33:80'</span>,<span class="string">'https'</span>:<span class="string">'50.233.137.33:80'</span>&#125;</span><br><span class="line"><span class="comment"># 创建代理处理器</span></span><br><span class="line">proxies = request.ProxyHandler(proxy)</span><br><span class="line"><span class="comment"># 创建opener对象</span></span><br><span class="line">opener = request.build_opener(proxies)</span><br><span class="line"></span><br><span class="line">resp = opener.open(url)</span><br><span class="line">print(resp.read().decode())</span><br></pre></td></tr></table></figure><h3 id="下载数据到本地"><a href="#下载数据到本地" class="headerlink" title="下载数据到本地"></a>下载数据到本地</h3><p>在我们进行网络请求时常常需要保存图片或音频等数据到本地，一种方法是使用python的文件操作，将read()获取的数据保存到文件中。</p><p>而<code>urllib</code>提供了一个<code>urlretrieve()</code>方法，可以简单的直接将请求获取的数据保存成文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://python.org/'</span></span><br><span class="line">request.urlretrieve(url, <span class="string">'python.html'</span>)</span><br></pre></td></tr></table></figure><p><code>urlretrieve()</code>方法传入的第二个参数为文件保存的位置，以及文件名。</p><p>注：<code>urlretrieve()</code>方法是python2直接移植过来的方法，以后有可能在某个版本中弃用。</p><hr><h3 id="urllib-response"><a href="#urllib-response" class="headerlink" title="urllib.response"></a>urllib.response</h3><p>在使用<code>urlopen()</code>方法或者opener的<code>open()</code>方法发起请求后，获得的结果是一个<code>response</code>对象。</p><p>这个对象有一些方法和属性，可以让我们对请求返回的结果进行一些处理。</p><ul><li><p><strong>read()</strong></p><p>获取响应返回的数据，只能使用一次。</p></li><li><p><strong>getcode()</strong></p><p>获取服务器返回的状态码。</p></li><li><p><strong>getheaders()</strong></p><p>获取返回响应的响应报头。</p></li><li><p><strong>geturl()</strong></p><p>获取访问的url。</p></li></ul><hr><h3 id="urllib-parse"><a href="#urllib-parse" class="headerlink" title="urllib.parse"></a>urllib.parse</h3><p><code>urllib.parse</code>是urllib中用来解析各种数据格式的模块。</p><h3 id="urllib-parse-quote"><a href="#urllib-parse-quote" class="headerlink" title="urllib.parse.quote"></a>urllib.parse.quote</h3><p>在url中，是只能使用ASCII中包含的字符的，也就是说，ASCII不包含的特殊字符，以及中文等字符都是不可以在url中使用的。而我们有时候又有将中文字符加入到url中的需求，例如百度的搜索地址：</p><p><code>https://www.baidu.com/s?wd=南北</code></p><p>？之后的wd参数，则是我们搜索的关键词。那么我们实现的方法就是将特殊字符进行url编码，转换成可以url可以传输的格式，urllib中可以使用<code>quote()</code>方法来实现这个功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>keyword = <span class="string">'南北'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parse.quote(keyword)</span><br><span class="line"><span class="string">'%E5%8D%97%E5%8C%97'</span></span><br></pre></td></tr></table></figure><p>如果需要将编码后的数据转换回来，可以使用<code>unquote()</code>方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>parse.unquote(<span class="string">'%E5%8D%97%E5%8C%97'</span>)</span><br><span class="line"><span class="string">'南北'</span></span><br></pre></td></tr></table></figure><h3 id="urllib-parse-urlencode"><a href="#urllib-parse-urlencode" class="headerlink" title="urllib.parse.urlencode"></a>urllib.parse.urlencode</h3><p>在访问url时，我们常常需要传递很多的url参数，而如果用字符串的方法去拼接url的话，会比较麻烦，所以<code>urllib</code>中提供了<code>urlencode</code>这个方法来拼接url参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>params = &#123;<span class="string">'wd'</span>: <span class="string">'南北'</span>, <span class="string">'code'</span>: <span class="string">'1'</span>, <span class="string">'height'</span>: <span class="string">'188'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parse.urlencode(params)</span><br><span class="line"><span class="string">'wd=%E5%8D%97%E5%8C%97&amp;code=1&amp;height=188'</span></span><br></pre></td></tr></table></figure><hr><h3 id="urllib-error"><a href="#urllib-error" class="headerlink" title="urllib.error"></a>urllib.error</h3><p>在<code>urllib</code>中主要设置了两个异常，一个是<code>URLError</code>，一个是<code>HTTPError</code>，<code>HTTPError</code>是<code>URLError</code>的子类。</p><p><code>HTTPError</code>还包含了三个属性：</p><ul><li>code：请求的状态码</li><li>reason：错误的原因</li><li>headers：响应的报头</li></ul><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> urllib.error <span class="keyword">import</span> HTTPError</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: <span class="keyword">try</span>:</span><br><span class="line">   ...:     request.urlopen(<span class="string">'https://www.jianshu.com'</span>)</span><br><span class="line">   ...: <span class="keyword">except</span> HTTPError <span class="keyword">as</span> e:</span><br><span class="line">   ...:     print(e.code)</span><br><span class="line"></span><br><span class="line"><span class="number">403</span></span><br></pre></td></tr></table></figure><hr><h3 id="urllib3"><a href="#urllib3" class="headerlink" title="urllib3"></a>urllib3</h3><p>Urllib3是一个功能强大，条理清晰，用于HTTP客户端的Python库。许多Python的原生系统已经开始使用urllib3。Urllib3提供了很多python标准库urllib里所没有的重要特性：</p><ol><li>线程安全</li><li>连接池</li><li>客户端SSL/TLS验证</li><li>文件分部编码上传</li><li>协助处理重复请求和HTTP重定位</li><li>支持压缩编码</li><li>支持HTTP和SOCKS代理</li></ol><hr><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>urllib3是一个第三方库，安装非常简单，pip安装即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install urllib3</span><br></pre></td></tr></table></figure><hr><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p><code>urllib3</code>主要使用连接池进行网络请求的访问，所以访问之前我们需要创建一个连接池对象，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> urllib3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>http = urllib3.PoolManager()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r = http.request(<span class="string">'GET'</span>, <span class="string">'http://httpbin.org/robots.txt'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.status</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>r.data</span><br><span class="line"><span class="string">'User-agent: *\nDisallow: /deny\n'</span></span><br></pre></td></tr></table></figure><h3 id="设置headers"><a href="#设置headers" class="headerlink" title="设置headers"></a>设置headers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">headers=&#123;<span class="string">'X-Something'</span>: <span class="string">'value'</span>&#125;</span><br><span class="line">resp = http.request(<span class="string">'GET'</span>, <span class="string">'http://httpbin.org/headers'</span>, headers=headers)</span><br></pre></td></tr></table></figure><h3 id="设置url参数"><a href="#设置url参数" class="headerlink" title="设置url参数"></a>设置url参数</h3><p>对于GET等没有请求正文的请求方法，可以简单的通过设置<code>fields</code>参数来设置url参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fields = &#123;<span class="string">'arg'</span>: <span class="string">'value'</span>&#125;</span><br><span class="line">resp = http.request(<span class="string">'GET'</span>, <span class="string">'http://httpbin.org/get'</span>, fields=fields)</span><br></pre></td></tr></table></figure><p>如果使用的是POST等方法，则会将fields作为请求的请求正文发送。</p><p>所以，如果你的POST请求是需要url参数的话，那么需要自己对url进行拼接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fields = &#123;<span class="string">'arg'</span>: <span class="string">'value'</span>&#125;</span><br><span class="line">resp = http.request(<span class="string">'POST'</span>, <span class="string">'http://httpbin.org/get'</span>, fields=fields)</span><br></pre></td></tr></table></figure><h3 id="设置代理"><a href="#设置代理" class="headerlink" title="设置代理"></a>设置代理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> urllib3</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>proxy = urllib3.ProxyManager(<span class="string">'http://50.233.137.33:80'</span>, headers=&#123;<span class="string">'connection'</span>: <span class="string">'keep-alive'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp = proxy.request(<span class="string">'get'</span>, <span class="string">'http://httpbin.org/ip'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.status</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>resp.data</span><br><span class="line"><span class="string">b'&#123;"origin":"50.233.136.254"&#125;\n'</span></span><br></pre></td></tr></table></figure><p><strong>注：<code>urllib3</code>中没有直接设置cookies的方法和参数，只能将cookies设置到headers中</strong></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GIT-版本管理与控制</title>
      <link href="/2018/11/19/2018-11-19-git/"/>
      <url>/2018/11/19/2018-11-19-git/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="1-GIT-版本管理与控制"><a href="#1-GIT-版本管理与控制" class="headerlink" title="1.GIT-版本管理与控制"></a>1.GIT-版本管理与控制</h3><p><img src="/images/tools/1.jpg" alt="1"></p><p>转载自：简书-<a href="https://www.jianshu.com/u/948da055a416" target="_blank" rel="noopener">王南北</a></p><p>本文地址：<a href="https://www.jianshu.com/p/0e9d07ec76f9" target="_blank" rel="noopener">https://www.jianshu.com/p/0e9d07ec76f9</a></p><p><code>GIT</code>基本上是目前最为先进的分布式版本控制系统，通过<code>GIT</code>能够非常方便的管理文件多个版本，能够实现版本的回滚，比对等功能，并且支持分布式也就是多人协同工作。</p><p><code>GIT</code>也是目前使用作为广泛的版本控制软件，大名鼎鼎的<code>Github</code>网站能直接与<code>GIT</code>对接，使用<code>GIT</code>上传代码到<code>Github</code>之中。</p><h3 id="GIT部署"><a href="#GIT部署" class="headerlink" title="GIT部署"></a>GIT部署</h3><p>通常来说，<code>Linux</code>系统使用各自版本对应的包管理工具可以非常方便的安装<code>GIT</code>。例如<code>sudo apt-get install git</code>，但安装之后会有一些设置需要配置。</p><h3 id="中文乱码"><a href="#中文乱码" class="headerlink" title="中文乱码"></a>中文乱码</h3><p>安装<code>GIT</code>之后比较常见的一个问题，就是中文乱码，可以通过在命令行中设置解决：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --<span class="keyword">global</span> core.quotepath false</span><br></pre></td></tr></table></figure><h3 id="配置用户信息"><a href="#配置用户信息" class="headerlink" title="配置用户信息"></a>配置用户信息</h3><p>如果是需要跟<code>Github</code>进行对接上传代码的话，那么首先需要在<code>GIT</code>中配置你的用户名和邮箱地址：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name  <span class="string">"name"</span>  // 你的GitHub登陆名</span><br><span class="line">git config --global user.email <span class="string">"123@126.com"</span>  // 你的GitHub注册邮箱</span><br></pre></td></tr></table></figure><hr><h3 id="GIT使用"><a href="#GIT使用" class="headerlink" title="GIT使用"></a>GIT使用</h3><p>在使用<code>GIT</code>之前，得掌握一个<code>仓库</code>的概念，也就是<code>repository</code>。这个<code>repository</code>也就是一个目录，是<code>GIT</code>管理的单位，在一个<code>repository</code>中，所有文件的新建、修改、删除都会被<code>GIT</code>跟踪到，并加以管理，以便在以后进行还原等操作。</p><p>所以，使用<code>GIT</code>，首先要创建一个<code>repository</code>。</p><h3 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h3><p>创建一个仓库，主要可以有两种方式。</p><h3 id="【init】"><a href="#【init】" class="headerlink" title="【init】"></a>【init】</h3><p><code>init</code>的创建方式为从零开始创建一个仓库，首先需要有一个目录，使用<code>cd</code>进入到我们想要创建仓库的目录中，然后使用以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure><p>即可将当前目录转化为一个<code>repository</code>。目录中会出现一个<code>.git</code>目录，里面保存着所有的版本信息。</p><h3 id="【clone】"><a href="#【clone】" class="headerlink" title="【clone】"></a>【clone】</h3><p>除了自己从零开始创建仓库外，还可以使用别人的远程仓库来创建，例如<code>Github</code>上有许多项目代码，都可以使用这种方式拷贝下来。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git://git.kernel.org/pub/scm/.../linux.git</span><br></pre></td></tr></table></figure><p>这样的话会在当前目录生成一个一模一样的仓库。</p><h3 id="基础GIT命令"><a href="#基础GIT命令" class="headerlink" title="基础GIT命令"></a>基础GIT命令</h3><p>创建好仓库之后，就可以在仓库之中开始使用命令来控制此仓库文件的版本了。</p><p>在使用这些命令之前，还有几个<code>GIT</code>的基础概念需要掌握，分别是：<code>工作区(working directory)</code>，<code>暂存区(stage)</code>，<code>分支</code>，<code>版本库</code>。</p><ul><li>首先，<code>工作区</code>指的其实就是我们平常我们修改文件，查看文件的地方。</li><li><code>暂存区</code>则是类似于一个中转的区域，被叫做<code>stage</code>或者<code>index</code>。在工作区中修改了内容，那么首先需要先将修改提交到暂存区中，积累一定的修改数量，汇集成一个版本之后，再一起提交到具体的分支中。</li><li>在<code>GIT</code>管理的项目中，有<code>分支</code>这个说法，可以理解为具体的开发方向，<code>GIT</code>仓库在初始化的时候会默认创建一个<code>master</code>分支，你的文件版本就实际保存在这些分支之中。</li><li><code>暂存区</code>和<code>分支</code>合起来称为整个版本库。</li></ul><h3 id="【status】"><a href="#【status】" class="headerlink" title="【status】"></a>【status】</h3><p>在仓库中使用<code>git status</code>命令可以查看当前仓库的状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@southnorth lianjia]# git status</span><br><span class="line"># On branch master</span><br><span class="line"># Changes not staged for commit:</span><br><span class="line">#   (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">#   (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line">#</span><br><span class="line">#modified:   lianjia/settings.py</span><br><span class="line">#</span><br><span class="line"># Untracked files:</span><br><span class="line">#   (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</span><br><span class="line">#</span><br><span class="line">#setup.py</span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br></pre></td></tr></table></figure><p>其中主要列出了仓库当前所处的分支，被修改了的文件，以及没有被跟踪的文件。</p><p><strong>参数</strong>：</p><ol><li><code>-s</code> - 以短格式显示仓库状态。</li></ol><hr><h3 id="【add】"><a href="#【add】" class="headerlink" title="【add】"></a>【add】</h3><p>在<code>GIT</code>仓库之中，虽然我们说所有的文件都可以被跟踪，但是这只限于文本文件的修改，<code>GIT</code>无法跟踪二进制文件的修改。</p><p>同时，在跟踪之前也需要先将文件添加到仓库的索引中，也就是说，使用<code>add</code>命令添加到索引中的文件，才会被<code>GIT</code>跟踪。在每次你新建或者修改了文件之后，需要你使用<code>add</code>命令将这个文件先添加到暂存区之中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add filename</span><br></pre></td></tr></table></figure><p>运行了此命令之后，未跟踪文件将会从<code>Untracked files:</code>中转移到<code>Changes not staged for commit:</code>中。</p><p>有些时候，可能修改的文件比较多，一个个去用<code>add</code>命令去添加比较麻烦，所以也可以用<code>*</code>来匹配文件名，以下命令可以将所有未被跟踪的文件添加到暂存区中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add *</span><br></pre></td></tr></table></figure><hr><h3 id="【commit】"><a href="#【commit】" class="headerlink" title="【commit】"></a>【commit】</h3><p>在将新文件或者修改过后的文件添加到暂存区之后，就可以使用<code>commit</code>命令将其正式提交到仓库了。但是要注意的是，<code>commit</code>提交到仓库的文件状态，是最后一次执行<code>add</code>时文件的状态，而不是执行<code>commit</code>时文件的状态。</p><p>所以，在提交文件之前，最好都先使用<code>git status</code>检查一下，有没有需要添加的文件还没有用<code>add</code>添加到暂存区中。然后就可以运行命令了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit</span><br></pre></td></tr></table></figure><p>直接运行此命令后，会跳出一个编辑界面，一般默认是使用<code>vim</code>。如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Please enter the commit message for your changes. Lines starting</span><br><span class="line"># with &apos;#&apos; will be ignored, and an empty message aborts the commit.</span><br><span class="line">#</span><br><span class="line"># On branch master</span><br><span class="line"># Your branch is up to date with &apos;origin/master&apos;.</span><br><span class="line">#</span><br><span class="line"># Changes to be committed:</span><br><span class="line">#       modified:   &quot;hello.md&quot;</span><br></pre></td></tr></table></figure><p>这里其实是需要你输入一些关于此次<code>commit</code>的一些信息，对此次代码提交做一定的标识，方便以后如果需要还原版本的时候清楚代码的改动。对此信息保存退出后，则<code>commit</code>提交成功。</p><p><strong>参数</strong>：</p><ol><li><p><code>-a</code> - 虽然说可以使用<code>add</code>命令对<code>commit</code>提交的暂存区做很精细的改动，但是当提交的文件非常多的时候，则<code>add</code>起来会比较的麻烦。所以<code>commit</code>提供了<code>-a</code>参数，使用此参数，则会自动将已被追踪的修改过的文件添加到缓存区中，不用再手动<code>add</code>添加了。</p></li><li><p><code>-m</code> - <code>commit</code>提交的时候需要输入信息，有时候如果希望输入的信息比较少，则可以使用-m参数直接在命令行输入。如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &apos;message&apos;</span><br></pre></td></tr></table></figure></li></ol><hr><h3 id="【rm】"><a href="#【rm】" class="headerlink" title="【rm】"></a>【rm】</h3><p>如果需要移除仓库中已经被追踪的文件，那么最好使用<code>GIT</code>提供的<code>rm</code>命令来删除，会更加安全：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rm filename</span><br></pre></td></tr></table></figure><p>此删除命令会将磁盘上的文件一并删除，在<code>commit</code>后，此文件将不会再被追踪。</p><p><strong>参数</strong>：</p><ol><li><p><code>-f</code> - 如果删除的文件已经被修改过，或者已经被添加到暂存区中，那么则需要用<code>-f</code>参数强制删除。这是一个保护措施，因为还未被提交的修改不会被保存下来，是无法恢复的。</p></li><li><p><code>--cached</code> - 如果希望某个仓库中的文件不再被<code>GIT</code>跟踪，但是依然被保存在磁盘里，这种时候可以使用<code>--cached</code>来删除。在错误的添加了文件到仓库中后，这个参数非常有用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rm --cached filename</span><br></pre></td></tr></table></figure></li></ol><hr><h3 id="【reset】"><a href="#【reset】" class="headerlink" title="【reset】"></a>【reset】</h3><p>使用<code>GIT</code>最大的一个好处是，<code>GIT</code>会将你提交的每个<code>commit</code>保存下来，以供你以后在出现问题后，能够非常方便的回滚版本。回滚版本的其中一个命令就是<code>reset</code>。</p><p>在你将一些文件使用<code>add</code>命令添加到暂存区之后，使用<code>git status</code>命令查看状态时可以看到提示，如果想将添加到暂存区的文件取消暂存则可以使用以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset HEAD &lt;file&gt;...</span><br></pre></td></tr></table></figure><p>在这里，<code>HEAD</code>代表的是最近一次的<code>commit</code>，此命令的意思则是将指定文件回滚到最近一次<code>commit</code>提交的状态。</p><p>如果没有指定文件的话，那么将会回滚整个仓库的状态，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset HEAD</span><br></pre></td></tr></table></figure><h3 id="版本表示："><a href="#版本表示：" class="headerlink" title="版本表示："></a><strong>版本表示</strong>：</h3><p>回滚的时候可以指定回滚的版本，版本的表示方式有三种，默认情况下都是指向最近一次提交：</p><ol><li><ul><li><code>HEAD</code> - 最近一个提交</li><li><code>HEAD^</code> - 上一次提交（倒数第二次提交）</li><li><code>HEAD^ ^</code> - 倒数第三次提交</li><li><code>HEAD^^^</code> - 倒数第四次的提交</li></ul></li><li><ul><li><code>HEAD~0</code> - 最近一个提交</li><li><code>HEAD~1</code> - 上一次提交（倒数第二次提交）</li><li><code>HEAD~2</code> - 倒数第三次提交</li><li><code>HEAD~3</code> - 倒数第四次的提交</li></ul></li><li>每次提交的<code>SHA1</code>版本号。</li></ol><h3 id="参数："><a href="#参数：" class="headerlink" title="参数："></a><strong>参数</strong>：</h3><ol><li><code>--mixed</code> - <code>GIT</code>的默认模式，使用此模式的时候，会清空暂存区，将回滚的内容全部恢复成未暂存的状态。也就是说不会修改任何本地工作区文件，只会回滚<code>index</code>和清空暂存区。</li><li><code>--soft</code> - 使用此模式，同样不会修改任何本地工作区文件，与<code>--mixed</code>的区别主要在于，其会将回滚的内容放入暂存区中。</li><li><code>--hard</code> - 此模式是一个比较危险的命令，使用此模式，将会把仓库彻底还原到<code>commit</code>的状态。如果你的暂存区和工作区中有修改了但未提交的内容，将会彻底丢失，所以谨慎使用此模式。</li></ol><hr><h3 id="【diff】"><a href="#【diff】" class="headerlink" title="【diff】"></a>【diff】</h3><p><code>git diff</code>命令可以查看两次文件内容有什么不同。使用以下命令可以查看工作区和版本库中最新版本的区别。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff HEAD -- &lt;filename&gt;</span><br></pre></td></tr></table></figure><p>在这里<code>--</code>表示的是工作区，<code>HEAD</code>表示的是最近一次<code>commit</code>提交的版本，还可以用<code>--cached</code>代表暂存区。</p><p>在没有指定的情况下，是默认查看工作区和暂存区的区别：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff &lt;filename&gt;</span><br></pre></td></tr></table></figure><hr><h3 id="【log】"><a href="#【log】" class="headerlink" title="【log】"></a>【log】</h3><p>使用<code>git log</code>命令，将会用以下的格式输出提交的<code>commit</code>日志记录，如果记录较多的话，需要按<code>q</code>键退出查看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ git log</span><br><span class="line">commit 69b8e6b3ebff7b84d6190a374475a20482d4c3ba (HEAD -&gt; master, origin/master, origin/HEAD)</span><br><span class="line">Author: wnanbei &lt;wnanbei@gmail.com&gt;</span><br><span class="line">Date:   Thu Nov 15 17:16:53 2018 +0800</span><br><span class="line"></span><br><span class="line">    add git branch part</span><br><span class="line"></span><br><span class="line">commit 28056c5055ef9ed4156b74713c0205e8fde44713</span><br><span class="line">Author: wnanbei &lt;wnanbei@gmail.com&gt;</span><br><span class="line">Date:   Thu Nov 15 15:21:30 2018 +0800</span><br><span class="line"></span><br><span class="line">    complete basic git command</span><br><span class="line"></span><br><span class="line">commit 6f6aae904ad7551d49ab952e9e3afae70bc93c50</span><br><span class="line">Author: wnanbei &lt;wnanbei@gmail.com&gt;</span><br><span class="line">Date:   Thu Nov 8 17:19:46 2018 +0800</span><br><span class="line"></span><br><span class="line">    add git</span><br></pre></td></tr></table></figure><h3 id="参数：-1"><a href="#参数：-1" class="headerlink" title="参数："></a><strong>参数</strong>：</h3><ul><li><p><code>--oneline</code> - 每条<code>commit</code>日志只显示一行内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git log --oneline</span><br><span class="line">69b8e6b (HEAD -&gt; master, origin/master, origin/HEAD) add git branch part</span><br><span class="line">28056c5 complete basic git command</span><br><span class="line">6f6aae9 add git</span><br></pre></td></tr></table></figure></li><li><p><code>--skip</code> - 指定跳过前面几条日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git log --skip=4 --oneline</span><br><span class="line">b9922fc add git</span><br><span class="line">edd4594 change the python file name</span><br><span class="line">a9cded2 add git article</span><br></pre></td></tr></table></figure></li><li><p><code>-[length]</code> - 指定输出的日志数量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git log --oneline -2</span><br><span class="line">69b8e6b (HEAD -&gt; master, origin/master, origin/HEAD) add git branch part</span><br><span class="line">28056c5 complete basic git command</span><br></pre></td></tr></table></figure></li><li><p><code>--pretty=</code> - 使用其他格式显示提交信息，可选项有：oneline、short、medium、full、fuller、email、raw，默认为medium。</p></li><li><p><code>--graph</code> - 在左侧以图形的方式显示提交的<code>commit</code>变动，更清晰的展示分支的合并等信息。</p></li><li><p><code>--decorate</code> - 展示更多的信息，例如HEAD、分支名、tag。</p></li><li><p><code>--author</code> - 通过提交者的名字来搜索提交信息。</p></li><li><p><code>--grep</code> - 从提交的关键字搜索提交信息。</p></li><li><p><code>-p</code> - 通过路径搜索提交信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git log -p -- config/my.config</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="【tag】"><a href="#【tag】" class="headerlink" title="【tag】"></a>【tag】</h3><p>在<code>GIT</code>中还有一个非常方便的功能，就是打标签，可以给某个特定的<code>commit</code>进行标记。比较广泛的一个方式使用它来标记版本号。使用以下命令将会给当前分支最新的一个<code>commit</code>打上<code>tag</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git tag &lt;tagname&gt;</span><br></pre></td></tr></table></figure><p>如果你需要指定给某个<code>commit</code>打<code>tag</code>的话，则需要你在命令后面加上<code>commit</code>的id。</p><p>使用以下命令可以查看<code>tag</code>的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git tag  # 查看本地所有tag</span><br><span class="line">git show &lt;tagname&gt;  # 查看指定tag的详细信息</span><br><span class="line">git ls-remote --tags &lt;remotename&gt; 查看远程所有tag</span><br></pre></td></tr></table></figure><p>需要注意的是，我们创建的<code>tag</code>都是只存在于本地的，所以如果要把<code>tag</code>同步到远程仓库的话，需要额外单独的使用命令同步<code>tag</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git push &lt;remotename&gt; &lt;tagName&gt;  # 推送单个tag到远程仓库</span><br><span class="line">git push &lt;remotename&gt; --tags  # 推送所有未推送的tag到远程仓库</span><br></pre></td></tr></table></figure><h3 id="参数：-2"><a href="#参数：-2" class="headerlink" title="参数："></a><strong>参数</strong>：</h3><ul><li>-<code>a</code> - 指定<code>tag</code>的名字。</li><li><code>-m</code> - 给<code>tag</code>添加上备注的信息，与<code>commit</code>的信息类似。</li><li><code>-d</code> - 这个参数代表删除<code>tag</code>。需要注意的是如果要删除远程的<code>tag</code>，则需要本地删除后，再push到远程仓库。</li></ul><hr><h3 id="分支管理"><a href="#分支管理" class="headerlink" title="分支管理"></a>分支管理</h3><p>在<code>GIT</code>之中，有分支的概念。在这里举一个例子，你希望在你的工作项目上新增添一个功能，那么你就可以在当前项目的基础上新开一个分支，然后在这个专门的分支上开发的你新功能，而原来的工作项目不受任何影响。等到你的新功能开发完毕通过测试后，就可以将这个分支与之前的工作项目分支合并了。</p><p>这种开发方式，能够将工作从开发主线上分离开来，避免工作时影响到工作主线。</p><p>由于<code>GIT</code>的分支实现原理跟指针类似，所以创建切换合并分支都是非常迅速的。<code>GIT</code>也非常鼓励新建一个分支去完成任务，任务完成后和主分支合并，然后删除掉这个新分支，这样使用下来与直接在主分支工作是差不多的，但是安全性要高不少。</p><h3 id="【branch】"><a href="#【branch】" class="headerlink" title="【branch】"></a>【branch】</h3><p>首先，直接使用<code>git branch</code>命令是查看当前仓库的分支：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch</span><br></pre></td></tr></table></figure><p>如果在<code>git branch</code>命令后面跟上一个名字，则可以在当前仓库新建一个分支：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch working</span><br></pre></td></tr></table></figure><p>也可以使用当前分支的某历史版本创建分支，这样的话需要指定具体的<code>commit</code>的ID：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch working 169d2dc</span><br></pre></td></tr></table></figure><p>需要注意的是，仓库一般默认会有一个<code>master</code>分支，这个分支其实并没有什么特殊，跟其他新建的分支没有什么区别，只是在<code>git init</code>时默认会创建这样一个分支，大部分人也懒得去修改。</p><p><strong>参数</strong>：</p><ul><li><p><code>-d</code> - 如果在创建之后需要删除一个分支，可以加上此参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -d working</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="【checkout】"><a href="#【checkout】" class="headerlink" title="【checkout】"></a>【checkout】</h3><p>在创建了分支之后，我们所处的依然是之前的分支，要切换到新的分支的话，依然是需要我们手动切换的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout working</span><br></pre></td></tr></table></figure><p><strong>参数</strong>：</p><ul><li><code>-b</code> - 加上这个参数之后，则代表直接创建一个分支，并且切换到这个分支，也就是说可以省略掉<code>git branch</code>这个步骤。</li></ul><hr><h3 id="【merge】"><a href="#【merge】" class="headerlink" title="【merge】"></a>【merge】</h3><p>在创建了分支之后，大部分情况下最终都是要合并的，也就是将分支修改的内容和另一个分支的修改内容合并到一起。</p><p>使用<code>git merge</code>命令将可以把某一分支与当前分支合并到一起：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git merge working</span><br></pre></td></tr></table></figure><p>如果两个分支之间没有冲突的话，那么分支的合并将会非常简单，<code>GIT</code>会自行决定如何合并两个分支。但是如果两个分支之间有文件冲突的话，也就是说两个分支内都对同一个文件进行了修改这种类似的操作，<code>GIT</code>将无法决定保留哪一个分支的内容。</p><p>因为在逻辑层面上，也需要由你自己来决定，在冲突的情况下，保留哪个分支的内容。在这种情况下，合并的时候会显示类似以下的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONFLICT (content): Merge conflict in a.txt</span><br><span class="line">Automatic merge failed; fix conflicts and then commit the result.</span><br></pre></td></tr></table></figure><p>在冲突的文件内，<code>GIT</code>会将两个分支的内容都放在了一起，由你自行修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</span><br><span class="line">i am master</span><br><span class="line">=======</span><br><span class="line">hello, i am working</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt; working</span><br></pre></td></tr></table></figure><p>可以看到<code>====</code>分割上方的是当前分支的内容，下方是合并的<code>working</code>分支的内容。此时由你自行修改，处理完冲突之后，<code>add</code>添加好就可以提交了。</p><hr><h3 id="远程仓库"><a href="#远程仓库" class="headerlink" title="远程仓库"></a>远程仓库</h3><p>在前面讲的用法基本上都是本地的<code>GIT</code>用法，但是使用<code>GIT</code>很大的一个优势是可以多人协作，同时完成项目，那么这基本必然要涉及到远程仓库的使用。远程仓库可以自己在服务器上搭建，也可以使用一些其他人提供的仓库托管服务，例如<code>Github</code>这个全球最大的同性交友网站。</p><p>使用<code>init</code>命令生成的仓库中，是没有配置远程仓库的，需要自行配置。而如果是使用<code>clone</code>获取的仓库，则会将来源的远程仓库默认配置为一个名为<code>origin</code>的远程仓库，这个远程仓库没有什么特殊，只是默认起名而已。在一些比较复杂的多人合作项目中，会配置有多个远程仓库。</p><h3 id="【remote】"><a href="#【remote】" class="headerlink" title="【remote】"></a>【remote】</h3><p>使用<code>git remote</code>命令即可查看当前仓库有配置哪些远程仓库：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git remote</span><br><span class="line">origin</span><br></pre></td></tr></table></figure><p>如果你需要添加新的远程仓库，那么可以使用以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add &lt;shortname&gt; &lt;url&gt;</span><br></pre></td></tr></table></figure><p><code>&lt;shortname&gt;</code>是你给这个远程分支起的名字，这个名字只会在本地起作用。</p><p>以下还有一些显示与删除等命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote show [remote-name]  # 显示远程仓库详细的信息</span><br><span class="line">git remote rename old_name new_name  # 重命名远程仓库</span><br><span class="line">git remote rm remote_name  # 删除远程仓库</span><br></pre></td></tr></table></figure><p><strong>参数</strong>：</p><ul><li><code>-v</code> - 会显示远程仓库的url。</li></ul><hr><h3 id="【fetch】"><a href="#【fetch】" class="headerlink" title="【fetch】"></a>【fetch】</h3><p>在配置了远程仓库之后，就可以从远程仓库拉取内容了。这个命令会访问远程仓库，从中拉取所有你还没有的数据。 执行完成后，你将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git fetch [remote-name]</span><br></pre></td></tr></table></figure><p>如果需要只拉取某个分支的内容，需要在后面加上分支的名称。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git fetch origin master  # 拉取远程仓库origin中的master分支</span><br><span class="line">git fetch origin master:temp  # 拉取远程仓库origin中的master分支，并命名为temp分支</span><br></pre></td></tr></table></figure><p>需要注意的是，<code>fetch</code>这个命令只是将版本库中的内容拉取下来，并不会自动合并和修改你工作区中的内容，需要你自行手动合并。</p><p>之后需要合并拉取的内容到工作区的话，需要使用<code>git merge</code>命令。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git merge FETCH_HEAD</span><br></pre></td></tr></table></figure><p>这里的<code>FETCH_HEAD</code>是一个版本链接，记录在本地的一个文件中，指向着目前已经从远程仓库取下来的分支的末端版本。</p><p>一般来说一个比较常见且安全的使用方式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git fetch origin master:tmp  # 在本地新建一个temp分支，并将远程origin仓库的master分支代码下载到本地temp分支</span><br><span class="line">git diff tmp                 # 来比较本地代码与刚刚从远程下载下来的代码的区别</span><br><span class="line">git merge tmp                # 合并temp分支到本地的master分支</span><br><span class="line">git branch -d temp           # 如果不想保留temp分支 可以用这步删除</span><br></pre></td></tr></table></figure><hr><h3 id="【pull】"><a href="#【pull】" class="headerlink" title="【pull】"></a>【pull】</h3><p>如果觉得使用<code>fetch</code>命令比较麻烦，且确定远程仓库的内容可以安全合并的话，那么可以使用<code>pull</code>命令。<code>pull</code>命令其实是一个混合命令，相当于把<code>git fetch</code>和<code>git merge</code>这两个命令合并到了一起，一个命令直接解决问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin</span><br></pre></td></tr></table></figure><hr><h3 id="【push】"><a href="#【push】" class="headerlink" title="【push】"></a>【push】</h3><p>在多人协作完成项目时，本地工作完成后，需要推送到远程仓库中，这个时候需要使用<code>git push</code>命令来进行推送。这个命令的用法如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt;</span><br></pre></td></tr></table></figure><p>如果当前分支只有一个远程分支，那么主机名与分支名都可以省略：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push</span><br></pre></td></tr></table></figure><p>如果当前分支与远程分支存在追踪关系，则可以省略分支名，只留主机名，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin</span><br></pre></td></tr></table></figure><p>如果只省略远程分支名，则表示将分支退送到与之存在追踪关系的分支，如果远程分支不存在，则创建新的远程分支：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin master</span><br></pre></td></tr></table></figure><p>如果只省略本地分支名，则代表删除指定远程分支：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin ：master</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Anaconda python3.6及pycharm安装和简单使用--Windows</title>
      <link href="/2018/11/09/2018-11-9-anaconda-windows/"/>
      <url>/2018/11/09/2018-11-9-anaconda-windows/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="Windows-安装Anaconda基于python3-6"><a href="#Windows-安装Anaconda基于python3-6" class="headerlink" title="Windows 安装Anaconda基于python3.6"></a>Windows 安装Anaconda基于python3.6</h3><ul><li><p>anaconda是python发行的包的管理工具，其中自带python的版本，还带很多python的包.安装它比安装python好。可以省掉再安装python包的时间。推荐使用Anaconda,用Anaconda安装python的包是非常便捷高效的，比如安装scrapy框架，如果用原生python的pip安装，要安装很多依赖的包，还经常报错，但是用Anaconda直接输入conda install scrapy就可以了，其他的都不用管了，就是这么方便。另外它自带了 180多个python常用的包，简直就是好得没朋友。</p></li><li><p>要利用 Python 进行科学计算，就需要一一安装所需的模块， 而这些模块可能又依赖于其它的软件包或库，安装和使用起来相对麻烦。Anaconda 就是将科学计算所需要的模块都编译好，然后打包以发行版的形式供用户使用的一个常用的科学计算环境。</p></li></ul><h4 id="它包含了众多流行的科学、数学、工程、数据分析的-Python-包。"><a href="#它包含了众多流行的科学、数学、工程、数据分析的-Python-包。" class="headerlink" title="它包含了众多流行的科学、数学、工程、数据分析的 Python 包。"></a>它包含了众多流行的科学、数学、工程、数据分析的 Python 包。</h4><h3 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h3><h4 id="官网：https-www-anaconda-com"><a href="#官网：https-www-anaconda-com" class="headerlink" title="官网：https://www.anaconda.com/"></a><a href="https://www.anaconda.com/" target="_blank" rel="noopener">官网</a>：<a href="https://www.anaconda.com/" target="_blank" rel="noopener">https://www.anaconda.com/</a></h4><h4 id="最新版本下载地址：https-www-anaconda-com-download"><a href="#最新版本下载地址：https-www-anaconda-com-download" class="headerlink" title="最新版本下载地址：https://www.anaconda.com/download/"></a><a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">最新版本下载地址</a>：<a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">https://www.anaconda.com/download/</a></h4><h4 id="历史版本：https-repo-anaconda-com-archive"><a href="#历史版本：https-repo-anaconda-com-archive" class="headerlink" title="历史版本：https://repo.anaconda.com/archive/"></a><a href="https://repo.anaconda.com/archive/" target="_blank" rel="noopener">历史版本</a>：<a href="https://repo.anaconda.com/archive/" target="_blank" rel="noopener">https://repo.anaconda.com/archive/</a></h4><p>对应有python3.6和python2.7的版本，可自行选择。</p><p>我们这里下载的是<strong>Anaconda3-4.4.0-Windows-x86_64.exe(对应是64位python3.6版本)</strong></p><h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><ul><li><h4 id="运行可执行文件"><a href="#运行可执行文件" class="headerlink" title="运行可执行文件"></a>运行可执行文件</h4><p><img src="/images/py/1.png" alt="1"></p></li><li><h4 id="同意协议"><a href="#同意协议" class="headerlink" title="同意协议"></a>同意协议</h4><p><img src="/images/py/2.png" alt="2"></p></li><li><h4 id="选择使用者"><a href="#选择使用者" class="headerlink" title="选择使用者"></a>选择使用者</h4><p><img src="/images/py/3.png" alt="1"></p></li><li><h4 id="选择安装路径"><a href="#选择安装路径" class="headerlink" title="选择安装路径"></a>选择安装路径</h4><p><img src="/images/py/4.png" alt="1"></p></li><li><h4 id="选择是否添加环境变量和默认启动的是Anaconda-python"><a href="#选择是否添加环境变量和默认启动的是Anaconda-python" class="headerlink" title="选择是否添加环境变量和默认启动的是Anaconda python"></a>选择是否添加环境变量和默认启动的是Anaconda python</h4><blockquote><ul><li>增加Anaconda到系统环境变量</li><li>Anaconda python为默认解释器<ul><li>勾选后如本地还有其他python解释器，则不是默认使用</li></ul></li></ul></blockquote><p><img src="/images/py/5.png" alt="1"></p></li><li><h4 id="installing"><a href="#installing" class="headerlink" title="installing"></a>installing</h4><p><img src="/images/py/6.png" alt="1"></p></li><li><h4 id="安装完成"><a href="#安装完成" class="headerlink" title="安装完成"></a>安装完成</h4><p><img src="/images/py/7.png" alt="1"></p><p><img src="/images/py/8.png" alt="1"></p></li></ul><h3 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h3><ul><li><h4 id="点击打开"><a href="#点击打开" class="headerlink" title="点击打开"></a>点击打开</h4></li></ul><p><img src="/images/py/9.png" alt="1"></p><ul><li><h4 id="打开Anaconda-Navigator"><a href="#打开Anaconda-Navigator" class="headerlink" title="打开Anaconda Navigator"></a>打开Anaconda Navigator</h4><p><img src="/images/py/10.png" alt="1"></p></li><li><h4 id="主界面"><a href="#主界面" class="headerlink" title="主界面"></a>主界面</h4><p><img src="/images/py/11.png" alt="1"></p></li><li><h4 id="添加或删除python库"><a href="#添加或删除python库" class="headerlink" title="添加或删除python库"></a>添加或删除python库</h4><ul><li><h5 id="打开environment"><a href="#打开environment" class="headerlink" title="打开environment"></a>打开environment</h5><p><img src="/images/py/12.png" alt="1"></p></li><li><h5 id="搜索库文件"><a href="#搜索库文件" class="headerlink" title="搜索库文件"></a>搜索库文件</h5><p><img src="/images/py/13.png" alt="1"></p></li><li><h5 id="安装库文件"><a href="#安装库文件" class="headerlink" title="安装库文件"></a>安装库文件</h5><p><img src="/images/py/14.png" alt="1"></p></li><li><h5 id="删除或更新"><a href="#删除或更新" class="headerlink" title="删除或更新"></a>删除或更新</h5><p><img src="/images/py/15.png" alt="1"></p></li></ul></li><li><h4 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h4><p><img src="/images/py/16.png" alt="1"></p><p><img src="/images/py/17.png" alt="1"></p><p><img src="/images/py/18.png" alt="1"></p></li><li><h4 id="删除虚拟环境"><a href="#删除虚拟环境" class="headerlink" title="删除虚拟环境"></a>删除虚拟环境</h4></li></ul><p><img src="/images/py/19.png" alt="1"></p><h3 id="windows-安装pycharm"><a href="#windows-安装pycharm" class="headerlink" title="windows 安装pycharm"></a>windows 安装pycharm</h3><p>PyCharm是一种Python IDE，其带有一整套可以帮助用户在使用Python语言开发时提高其效率的工具，比如， 调试、语法高亮、Project管理、代码跳转、智能提示、自动完成、单元测试、版本控制等等。此外，该IDE提供了一些高级功能，以用于支持Django框架下的专业Web开发。同时支持Google App Engine，更酷的是，PyCharm支持IronPython！这些功能在先进代码分析程序的支持下，使 PyCharm 成为 Python 专业开发人员和刚起步人员使用的有力工具。</p><h3 id="下载地址-1"><a href="#下载地址-1" class="headerlink" title="下载地址"></a>下载地址</h3><h4 id="官网：https-www-jetbrains-com-pycharm-download-section-windows"><a href="#官网：https-www-jetbrains-com-pycharm-download-section-windows" class="headerlink" title="官网：https://www.jetbrains.com/pycharm/download/#section=windows"></a><a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">官网</a>：<a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/download/#section=windows</a></h4><p><img src="/images/py/20.png" alt="1"></p><h3 id="开始安装-1"><a href="#开始安装-1" class="headerlink" title="开始安装"></a>开始安装</h3><ul><li><h4 id="运行可执行文件-1"><a href="#运行可执行文件-1" class="headerlink" title="运行可执行文件"></a>运行可执行文件</h4><p><img src="/images/py/21.png" alt="1"></p></li><li><h4 id="选择安装地址"><a href="#选择安装地址" class="headerlink" title="选择安装地址"></a>选择安装地址</h4><p><img src="/images/py/22.png" alt="1"></p></li><li><h4 id="选择安装版本及依赖"><a href="#选择安装版本及依赖" class="headerlink" title="选择安装版本及依赖"></a>选择安装版本及依赖</h4><p><img src="/images/py/23.png" alt="1"></p></li><li><h4 id="生成快捷方式"><a href="#生成快捷方式" class="headerlink" title="生成快捷方式"></a>生成快捷方式</h4><p><img src="/images/py/24.png" alt="1"></p></li><li><h4 id="installing-1"><a href="#installing-1" class="headerlink" title="installing"></a>installing</h4><p><img src="/images/py/25.png" alt="1"></p></li><li><h4 id="安装完成-1"><a href="#安装完成-1" class="headerlink" title="安装完成"></a>安装完成</h4><p><img src="/images/py/26.png" alt="1"></p></li></ul><h3 id="简单使用-1"><a href="#简单使用-1" class="headerlink" title="简单使用"></a>简单使用</h3><ul><li><h4 id="是否导入设置"><a href="#是否导入设置" class="headerlink" title="是否导入设置"></a>是否导入设置</h4><p><img src="/images/py/27.png" alt="1"></p></li><li><h4 id="查看并接受协议"><a href="#查看并接受协议" class="headerlink" title="查看并接受协议"></a>查看并接受协议</h4><p><img src="/images/py/28.png" alt="1"></p></li><li><h4 id="选择主题"><a href="#选择主题" class="headerlink" title="选择主题"></a>选择主题</h4><p><img src="/images/py/29.png" alt="1"></p></li><li><h4 id="选择是否下载插件"><a href="#选择是否下载插件" class="headerlink" title="选择是否下载插件"></a>选择是否下载插件</h4><p><img src="/images/py/30.png" alt="1"></p></li><li><h4 id="开始使用"><a href="#开始使用" class="headerlink" title="开始使用"></a>开始使用</h4><p><img src="/images/py/31.png" alt="1"></p></li><li><h4 id="创建项目或打开项目"><a href="#创建项目或打开项目" class="headerlink" title="创建项目或打开项目"></a>创建项目或打开项目</h4><p><img src="/images/py/32.png" alt="1"></p></li><li><h4 id="项目保存路径及解释器路径"><a href="#项目保存路径及解释器路径" class="headerlink" title="项目保存路径及解释器路径"></a>项目保存路径及解释器路径</h4><p><img src="/images/py/33.png" alt="1"></p><p><img src="/images/py/34.png" alt="1"></p><p><img src="/images/py/35.png" alt="1"></p><p><img src="/images/py/36.png" alt="1"></p></li><li><h4 id="创建代码-右键项目文件"><a href="#创建代码-右键项目文件" class="headerlink" title="创建代码-右键项目文件"></a>创建代码-右键项目文件</h4><p><img src="/images/py/37.png" alt="1"></p></li><li><h4 id="编写代码并执行"><a href="#编写代码并执行" class="headerlink" title="编写代码并执行"></a>编写代码并执行</h4><p><img src="/images/py/38.png" alt="1"></p></li><li><h4 id="修改字体–依次点击File-settings"><a href="#修改字体–依次点击File-settings" class="headerlink" title="修改字体–依次点击File-settings"></a>修改字体–依次点击File-settings</h4><p><img src="/images/py/39.png" alt="1"></p></li><li><h4 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h4><p><img src="/images/py/40.png" alt="1"></p></li><li><h4 id="切换python解释器"><a href="#切换python解释器" class="headerlink" title="切换python解释器"></a>切换python解释器</h4><ul><li><h5 id="依次点击File-settings"><a href="#依次点击File-settings" class="headerlink" title="依次点击File-settings"></a>依次点击File-settings</h5><p><img src="/images/py/41.png" alt="1"></p></li><li><h5 id="增加解释器-add"><a href="#增加解释器-add" class="headerlink" title="增加解释器-add"></a>增加解释器-add</h5><p><img src="/images/py/42.png" alt="1"></p></li><li><h5 id="找到我们刚刚用Anaconda-新建的TensorFlow-seven的虚拟环境"><a href="#找到我们刚刚用Anaconda-新建的TensorFlow-seven的虚拟环境" class="headerlink" title="找到我们刚刚用Anaconda 新建的TensorFlow-seven的虚拟环境"></a>找到我们刚刚用Anaconda 新建的TensorFlow-seven的虚拟环境</h5><p><img src="/images/py/43.png" alt="1"></p></li><li><h5 id="保存并退出"><a href="#保存并退出" class="headerlink" title="保存并退出"></a>保存并退出</h5><p><img src="/images/py/44.png" alt="1"></p></li></ul></li></ul><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>上述方法是采用<strong>Anaconda</strong> 配置 的<strong>python</strong>的开发环境</p><p>如果你不用，自己配置，只需要把<strong>pycharm</strong>的解释器路径设置为你自己的<strong>python路径</strong>。</p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pycharm </tag>
            
            <tag> anaconda </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Anaconda python3.6及pycharm安装和简单使用--Ubuntu16.04</title>
      <link href="/2018/11/09/2018-11-9-anaconda-ubuntu/"/>
      <url>/2018/11/09/2018-11-9-anaconda-ubuntu/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="Ubuntu-安装Anaconda基于python3-6"><a href="#Ubuntu-安装Anaconda基于python3-6" class="headerlink" title="Ubuntu 安装Anaconda基于python3.6"></a>Ubuntu 安装Anaconda基于python3.6</h3><ul><li><p>anaconda是python发行的包的管理工具，其中自带python的版本，还带很多python的包.安装它比安装python好。可以省掉再安装python包的时间。推荐使用Anaconda,用Anaconda安装python的包是非常便捷高效的，比如安装scrapy框架，如果用原生python的pip安装，要安装很多依赖的包，还经常报错，但是用Anaconda直接输入conda install scrapy就可以了，其他的都不用管了，就是这么方便。另外它自带了 180多个python常用的包，简直就是好得没朋友。</p></li><li><p>要利用 Python 进行科学计算，就需要一一安装所需的模块， 而这些模块可能又依赖于其它的软件包或库，安装和使用起来相对麻烦。Anaconda 就是将科学计算所需要的模块都编译好，然后打包以发行版的形式供用户使用的一个常用的科学计算环境。</p></li></ul><h4 id="它包含了众多流行的科学、数学、工程、数据分析的-Python-包。"><a href="#它包含了众多流行的科学、数学、工程、数据分析的-Python-包。" class="headerlink" title="它包含了众多流行的科学、数学、工程、数据分析的 Python 包。"></a>它包含了众多流行的科学、数学、工程、数据分析的 Python 包。</h4><h3 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h3><h4 id="官网：https-www-anaconda-com"><a href="#官网：https-www-anaconda-com" class="headerlink" title="官网：https://www.anaconda.com/"></a><a href="https://www.anaconda.com/" target="_blank" rel="noopener">官网</a>：<a href="https://www.anaconda.com/" target="_blank" rel="noopener">https://www.anaconda.com/</a></h4><h4 id="最新版本下载地址：https-www-anaconda-com-download"><a href="#最新版本下载地址：https-www-anaconda-com-download" class="headerlink" title="最新版本下载地址：https://www.anaconda.com/download/"></a><a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">最新版本下载地址</a>：<a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">https://www.anaconda.com/download/</a></h4><h4 id="历史版本：https-repo-anaconda-com-archive"><a href="#历史版本：https-repo-anaconda-com-archive" class="headerlink" title="历史版本：https://repo.anaconda.com/archive/"></a><a href="https://repo.anaconda.com/archive/" target="_blank" rel="noopener">历史版本</a>：<a href="https://repo.anaconda.com/archive/" target="_blank" rel="noopener">https://repo.anaconda.com/archive/</a></h4><p>对应有python3.6和python2.7的版本，可自行选择。</p><p>我们这里下载的是<strong><a href="https://repo.anaconda.com/archive/Anaconda3-4.4.0-Linux-x86_64.sh" target="_blank" rel="noopener">Anaconda3-4.4.0-Linux-x86_64.sh</a>(对应是64位python3.6版本)</strong></p><h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><ul><li><h4 id="进入文件所在路径"><a href="#进入文件所在路径" class="headerlink" title="进入文件所在路径"></a>进入文件所在路径</h4><p><img src="/images/py/45.png" alt="1"></p></li><li><h4 id="开始安装-1"><a href="#开始安装-1" class="headerlink" title="开始安装"></a>开始安装</h4><ul><li><h5 id="出现欢迎信息，-阅读许可文件"><a href="#出现欢迎信息，-阅读许可文件" class="headerlink" title="出现欢迎信息， 阅读许可文件"></a>出现欢迎信息， 阅读许可文件</h5><p><img src="/images/py/46.png" alt="1"></p></li><li><h5 id="同意许可"><a href="#同意许可" class="headerlink" title="同意许可"></a>同意许可</h5><p><img src="/images/py/47.png" alt="1"></p></li><li><h5 id="选择安装目录-默认"><a href="#选择安装目录-默认" class="headerlink" title="选择安装目录-默认"></a>选择安装目录-默认</h5><p><img src="/images/py/48.png" alt="1"></p></li><li><h5 id="installing"><a href="#installing" class="headerlink" title="installing"></a>installing</h5><p><img src="/images/py/49.png" alt="1"></p></li><li><h5 id="添加系统环境"><a href="#添加系统环境" class="headerlink" title="添加系统环境"></a>添加系统环境</h5><p><img src="/images/py/50.png" alt="1"></p></li><li><h5 id="安装完成"><a href="#安装完成" class="headerlink" title="安装完成"></a>安装完成</h5><p><img src="/images/py/51.png" alt="1"></p></li><li><h5 id="使环境生效"><a href="#使环境生效" class="headerlink" title="使环境生效"></a>使环境生效</h5></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seven@ubuntu:~/Desktop$ source ~/.bashrc</span><br></pre></td></tr></table></figure></li></ul><h3 id="Conda的环境管理"><a href="#Conda的环境管理" class="headerlink" title="Conda的环境管理"></a>Conda的环境管理</h3><ul><li><p>创建一个名为python-seven的环境，指定Python版本是3.6（不用管是3.6.x，conda会为我们自动寻找3.6.x中的最新版本）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seven@ubuntu:~$ conda create --name python-seven python=3.6</span><br></pre></td></tr></table></figure></li><li><p>安装好后，使用activate激活某个环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">seven@ubuntu:~$ source activate python-seven</span><br><span class="line">(python-seven) seven@ubuntu:~$</span><br></pre></td></tr></table></figure></li><li><p>测试环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(python-seven) seven@ubuntu:~$ python --version</span><br><span class="line">Python 3.6.2 :: Continuum Analytics, Inc.</span><br><span class="line">(python-seven) seven@ubuntu:~$</span><br></pre></td></tr></table></figure></li><li><p>退出虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(python-seven) seven@ubuntu:~$ source deactivate python-seven</span><br><span class="line">seven@ubuntu:~$</span><br></pre></td></tr></table></figure></li><li><p>删除虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seven@ubuntu:~$ conda remove --name python-seven --all</span><br></pre></td></tr></table></figure></li></ul><h3 id="ubuntu16-04-安装pycharm"><a href="#ubuntu16-04-安装pycharm" class="headerlink" title="ubuntu16.04 安装pycharm"></a>ubuntu16.04 安装pycharm</h3><p>PyCharm是一种Python IDE，其带有一整套可以帮助用户在使用Python语言开发时提高其效率的工具，比如， 调试、语法高亮、Project管理、代码跳转、智能提示、自动完成、单元测试、版本控制等等。此外，该IDE提供了一些高级功能，以用于支持Django框架下的专业Web开发。同时支持Google App Engine，更酷的是，PyCharm支持IronPython！这些功能在先进代码分析程序的支持下，使 PyCharm 成为 Python 专业开发人员和刚起步人员使用的有力工具。</p><h3 id="下载地址-1"><a href="#下载地址-1" class="headerlink" title="下载地址"></a>下载地址</h3><p><a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">官网</a>：<a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/download/#section=windows</a></p><p><img src="/images/py/52.png" alt="1"></p><h3 id="开始安装-2"><a href="#开始安装-2" class="headerlink" title="开始安装"></a>开始安装</h3><ul><li><h4 id="进入文件所在路径-1"><a href="#进入文件所在路径-1" class="headerlink" title="进入文件所在路径"></a>进入文件所在路径</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seven@ubuntu:~$ cd Desktop/</span><br><span class="line">seven@ubuntu:~/Desktop$ ls</span><br><span class="line">Anaconda3-4.4.0-Linux-x86_64.sh  Mnist  pycharm-community-2018.2.4.tar.gz</span><br></pre></td></tr></table></figure></li><li><h4 id="解压文件"><a href="#解压文件" class="headerlink" title="解压文件"></a>解压文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seven@ubuntu:~/Desktop$ sudo tar -zxvf pycharm-community-2018.2.4.tar.gz -C /opt/</span><br></pre></td></tr></table></figure></li><li><h4 id="开始安装-3"><a href="#开始安装-3" class="headerlink" title="开始安装"></a>开始安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">seven@ubuntu:~/Desktop$ cd /opt/pycharm-community-2018.2.4/bin/</span><br><span class="line">seven@ubuntu:/opt/pycharm-community-2018.2.4/bin$ ./p</span><br><span class="line">printenv.py  pycharm.sh   </span><br><span class="line">seven@ubuntu:/opt/pycharm-community-2018.2.4/bin$ ./pycharm.sh</span><br></pre></td></tr></table></figure></li></ul><h3 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h3><ul><li><h4 id="是否导入设置"><a href="#是否导入设置" class="headerlink" title="是否导入设置"></a>是否导入设置</h4><p><img src="/images/py/54.png" alt="1"></p></li><li><h4 id="查看并接受协议"><a href="#查看并接受协议" class="headerlink" title="查看并接受协议"></a>查看并接受协议</h4><p><img src="/images/py/55.png" alt="1"></p></li><li><h4 id="选择主题"><a href="#选择主题" class="headerlink" title="选择主题"></a>选择主题</h4><p><img src="/images/py/56.png" alt="1"></p></li><li><h4 id="创建项目保存路径"><a href="#创建项目保存路径" class="headerlink" title="创建项目保存路径"></a>创建项目保存路径</h4><p><img src="/images/py/57.png" alt="1"></p></li><li><h4 id="选择是否下载插件"><a href="#选择是否下载插件" class="headerlink" title="选择是否下载插件"></a>选择是否下载插件</h4><p><img src="/images/py/58.png" alt="1"></p></li><li><h4 id="开始使用"><a href="#开始使用" class="headerlink" title="开始使用"></a>开始使用</h4><p><img src="/images/py/31.png" alt="1"></p></li><li><h4 id="创建项目或打开项目"><a href="#创建项目或打开项目" class="headerlink" title="创建项目或打开项目"></a>创建项目或打开项目</h4><p><img src="/images/py/32.png" alt="1"></p></li><li><h4 id="项目保存路径及解释器路径"><a href="#项目保存路径及解释器路径" class="headerlink" title="项目保存路径及解释器路径"></a>项目保存路径及解释器路径</h4><p><img src="/images/py/59.png" alt="1"></p><p><img src="/images/py/60.png" alt="1"></p><p><img src="/images/py/61.png" alt="1"></p><p><img src="/images/py/62.png" alt="1"></p></li><li><h4 id="创建代码-右键项目文件"><a href="#创建代码-右键项目文件" class="headerlink" title="创建代码-右键项目文件"></a>创建代码-右键项目文件</h4><p><img src="/images/py/37.png" alt="1"></p></li><li><h4 id="编写代码并执行"><a href="#编写代码并执行" class="headerlink" title="编写代码并执行"></a>编写代码并执行</h4><p><img src="/images/py/38.png" alt="1"></p></li><li><h4 id="修改字体–依次点击File-settings"><a href="#修改字体–依次点击File-settings" class="headerlink" title="修改字体–依次点击File-settings"></a>修改字体–依次点击File-settings</h4><p><img src="/images/py/39.png" alt="1"></p></li><li><h4 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h4><p><img src="/images/py/40.png" alt="1"></p></li><li><h4 id="切换python解释器"><a href="#切换python解释器" class="headerlink" title="切换python解释器"></a>切换python解释器</h4><ul><li><h5 id="依次点击File-settings"><a href="#依次点击File-settings" class="headerlink" title="依次点击File-settings"></a>依次点击File-settings</h5><p><img src="/images/py/63.png" alt="1"></p></li><li><h5 id="增加解释器-add"><a href="#增加解释器-add" class="headerlink" title="增加解释器-add"></a>增加解释器-add</h5><p><img src="/images/py/64.png" alt="1"></p></li><li><h5 id="找到我们刚刚用Anaconda-新建的python-seven的虚拟环境"><a href="#找到我们刚刚用Anaconda-新建的python-seven的虚拟环境" class="headerlink" title="找到我们刚刚用Anaconda 新建的python-seven的虚拟环境"></a>找到我们刚刚用Anaconda 新建的python-seven的虚拟环境</h5><p><img src="/images/py/65.png" alt="1"></p></li><li><h5 id="保存并退出"><a href="#保存并退出" class="headerlink" title="保存并退出"></a>保存并退出</h5><p><img src="/images/py/66.png" alt="1"></p></li></ul></li></ul><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>上述方法是采用<strong>Anaconda</strong> 配置 的<strong>python</strong>的开发环境</p><p>如果你不用，自己配置，只需要把<strong>pycharm</strong>的解释器路径设置为你自己的<strong>python路径</strong>。</p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pycharm </tag>
            
            <tag> anaconda </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Linux简介及简单操作命令</title>
      <link href="/2018/11/08/2018-11-8-linux/"/>
      <url>/2018/11/08/2018-11-8-linux/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="Linux-简介"><a href="#Linux-简介" class="headerlink" title="Linux 简介"></a>Linux 简介</h3><p>Linux内核最初只是由芬兰人林纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学上学时出于个人爱好而编写的。</p><p>Linux是一套免费使用和自由传播的类Unix操作系统，是一个基于POSIX和UNIX的多用户、多任务、支持多线程和多CPU的操作系统。</p><p>Linux能运行主要的UNIX工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。</p><h3 id="Linux的发行版"><a href="#Linux的发行版" class="headerlink" title="Linux的发行版"></a>Linux的发行版</h3><p>Linux的发行版说简单点就是将Linux内核与应用软件做一个打包。</p><p>目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debian、Fedora、SuSE、OpenSUSE、Arch Linux、SolusOS 等。</p><p>Linux具有如下优点：</p><p>Ø 稳定、免费或者花费少</p><p>Ø 安全性高</p><p>Ø 多任务，多用户</p><p>Ø 耗资源少</p><p>Ø 由于内核小，所以它可以支持多种电子产品，如：Android手机</p><h3 id="Linux应用领域"><a href="#Linux应用领域" class="headerlink" title="Linux应用领域"></a>Linux应用领域</h3><p>今天各种场合都有使用各种Linux发行版，从嵌入式设备到超级计算机，并且在服务器领域确定了地位，通常服务器使用LAMP（Linux + Apache + MySQL + PHP）或LNMP（Linux + Nginx+ MySQL + PHP）组合。</p><p>目前Linux不仅在家庭与企业中使用，并且在政府中也很受欢迎。</p><ul><li>巴西联邦政府由于支持Linux而世界闻名。</li><li>有新闻报道俄罗斯军队自己制造的Linux发布版的，做为G.H.ost项目已经取得成果.</li><li>印度的Kerala联邦计划在向全联邦的高中推广使用Linux。</li><li>中华人民共和国为取得技术独立，在龙芯过程中排他性地使用Linux。</li><li>在西班牙的一些地区开发了自己的Linux发布版，并且在政府与教育领域广泛使用，如Extremadura地区的gnuLinEx和Andalusia地区的Guadalinex。</li><li>葡萄牙同样使用自己的Linux发布版Caixa Mágica，用于Magalh?es笔记本电脑和e-escola政府软件。</li><li>法国和德国同样开始逐步采用Linux。</li></ul><h3 id="linux电源管理"><a href="#linux电源管理" class="headerlink" title="linux电源管理"></a>linux电源管理</h3><h4 id="shutdown"><a href="#shutdown" class="headerlink" title="shutdown"></a><code>shutdown</code></h4><ul><li><code>-c</code> 取消关机命令</li><li><code>-h</code> 关机</li><li><code>-r</code> 重启</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shutdown now # 关机</span><br><span class="line">shutdown -h 30 # 30分钟后关机 </span><br><span class="line">shutdown -r now # 重启</span><br><span class="line">shutdown -r 05:30 # 于05:30重启</span><br><span class="line">shutdown -c # 取消关机</span><br></pre></td></tr></table></figure><h4 id="不保存资料直接关机"><a href="#不保存资料直接关机" class="headerlink" title="不保存资料直接关机"></a><code>不保存资料直接关机</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> halt</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> poweroff</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> init 0</span></span><br></pre></td></tr></table></figure><h4 id="重启"><a href="#重启" class="headerlink" title="重启"></a><code>重启</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reboot # 重启电脑</span><br></pre></td></tr></table></figure><h4 id="init命令"><a href="#init命令" class="headerlink" title="init命令"></a><code>init命令</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">init 0 # 关机</span><br><span class="line">init 1 # 救援模式</span><br><span class="line">init 3 # 进入文字模式</span><br><span class="line">init 5 # 进入GUI模式</span><br><span class="line">init 6 # 重启</span><br></pre></td></tr></table></figure><h4 id="退出登录"><a href="#退出登录" class="headerlink" title="退出登录"></a><code>退出登录</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logout # 注销</span><br></pre></td></tr></table></figure><h3 id="Linux目录结构"><a href="#Linux目录结构" class="headerlink" title="Linux目录结构"></a>Linux目录结构</h3><ul><li><p><strong>/bin</strong>：<br>bin是Binary的缩写, 这个目录存放着最经常使用的命令。</p></li><li><p><strong>/boot：</strong><br>这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。</p></li><li><p><strong>/dev ：</strong><br>dev是Device(设备)的缩写, 该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。</p></li><li><p><strong>/etc：</strong><br>这个目录用来存放所有的系统管理所需要的配置文件和子目录。</p></li><li><p><strong>/home</strong>：<br>用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。</p></li><li><p><strong>/lib</strong>：<br>这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。</p></li><li><p><strong>/lost+found</strong>：<br>这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。</p></li><li><p><strong>/media</strong>：<br>linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。</p></li><li><p><strong>/mnt</strong>：<br>系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。</p></li><li><p><strong>/opt</strong>：<br>这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。</p></li><li><p><strong>/proc</strong>：<br>这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。<br>这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all</span><br></pre></td></tr></table></figure></li><li><p><strong>/root</strong>：<br>该目录为系统管理员，也称作超级权限者的用户主目录。</p></li><li><p><strong>/sbin</strong>：<br>s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。</p></li><li><p><strong>/selinux</strong>：<br>这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。</p></li><li><p><strong>/srv</strong>：<br>该目录存放一些服务启动之后需要提取的数据。</p></li><li><p><strong>/sys</strong>：<br>这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。</p><p>sysfs文件系统集成了下面3种文件系统的信息：</p></li></ul><blockquote><p>针对进程信息的proc文件系统</p><p>针对设备的devfs文件系统</p><p>针对伪终端的devpts文件系统。</p></blockquote><p>该文件系统是内核设备树的一个直观反映。</p><p>当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。</p><ul><li><strong>/tmp</strong>：<br>这个目录是用来存放一些临时文件的。</li><li><strong>/usr</strong>：<br>这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。</li><li><strong>/usr/bin：</strong><br>系统用户使用的应用程序。</li><li><strong>/usr/sbin：</strong><br>超级用户使用的比较高级的管理程序和系统守护程序。</li><li><strong>/usr/src：</strong>内核源代码默认的放置目录。</li><li><strong>/var</strong>：<br>这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。</li></ul><p>在linux系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。</p><p>/etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。</p><p>/bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在/bin/ls 目录下的。</p><p>值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给root使用的指令。</p><p>/var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在/var/log 目录下，另外mail的预设放置也是在这里。</p><h3 id="shell"><a href="#shell" class="headerlink" title="shell"></a><strong>shell</strong></h3><p>Shell基本上是一个命令<a href="https://baike.baidu.com/item/%E8%A7%A3%E9%87%8A%E5%99%A8" target="_blank" rel="noopener">解释器</a></p><p>Shell俗称壳（用来区别于核）</p><p>它接收用户命令，然后调用相应的应用程序。</p><h3 id="ls"><a href="#ls" class="headerlink" title="ls:"></a><code>ls</code>:</h3><p>ls是英文单词list的简写，其功能为列出目录的内容。这是用户最常用的一个命令，因为用户需要不时地查看某个目录的内容。该命令类似于DOS下的dir命令。 对于每个目录，该命令将列出其中的所有子目录与文件。对于每个文件，ls将输出其文件名以及所要求的其<br>他信息。默认情况下，输出条目按字母顺序排序。当未给出目录名或是文件名时，就显示当<br>前目录的信息。<br>主要的OPTION有：</p><p>-a 列出隐藏文件，文件中以“.”开头的均为隐藏文件，如：~/.bashrc</p><p>-l 列出文件的详细信息</p><p>-R 连同子目录中的内容一起列出</p><p>用ls -l命令显示的信息中，开头是由10个字符构成的字符串，其中第一个字符表示文</p><p>件类型，它可以是下述类型之一：</p><ul><li><p>普通文件</p><p>d 目录</p><p>l 符号链接</p><p>b 块设备文件</p><p>c 字符设备文件</p><p>s socket文件，网络套接字</p><p>p 管道</p><p>后面的9个字符表示文件的访问权限，分为3组，每组3位。第一组表示文件属主的权限，第二组表示同组用户的权限，第三组表示其他用户的权限。每一组的三个字符分别表示对文件的读、写和执行权限。</p><p>各权限如下所示：</p><p>r 读</p><p>w 写</p><p>x 可执行。对于目录，表示进入权限。</p><p>s 当文件被执行时，把该文件的UID或GID赋予执行进程的UID（用户ID）或GID（组 ID）。</p><p>t 设置标志位（sticky bit）。如果是有sticky bit的目录，在该目录下任何用户只要有适当的权限即可创建件，但文件只能被超级用户、目录拥有者或文件属主删除。</p><p>如果是有sticky bit的可执行文件，在该文件执行后，指向其正文段的指针仍留在内存。这样再次执行它时，系统就能更快地装入该文件。</p></li><li><p>没有相应位置的权限。</p><p>访问权限后面的数字表示与该文件共享inode的文件总数，即硬链接数(参见下面ln命令)。</p></li></ul><p>-a：显示所有档案及目录（ls内定将档案名或目录名称为“.”的视为影藏，不会列出）；</p><p>-A：显示除影藏文件“.”和“..”以外的所有文件列表；</p><p>-C：多列显示输出结果。这是默认选项；</p><p>-l：与“-C”选项功能相反，所有输出信息用单列格式输出，不输出为多列；</p><p>-F：在每个输出项后追加文件的类型标识符，具体含义：“*”表示具有可执行权限的普通文件，“/”表示目录，“@”表示符号链接，“|”表示命令管道FIFO，“=”表示sockets套接字。当文件为普通文件时，不输出任何标识符；</p><p>-b：将文件中的不可输出的字符以反斜线“”加字符编码的方式输出；</p><p>-c：与“-lt”选项连用时，按照文件状态时间排序输出目录内容，排序的依据是文件的索引节点中的ctime字段。与“-l”选项连用时，则排序的一句是文件的状态改变时间；</p><p>-d：仅显示目录名，而不显示目录下的内容列表。显示符号链接文件本身，而不显示其所指向的目录列表；</p><p>-f：此参数的效果和同时指定“aU”参数相同，并关闭“lst”参数的效果；</p><p>-i：显示文件索引节点号（inode）。一个索引节点代表一个文件；</p><p>–file-type：与“-F”选项的功能相同，但是不显示“*”；</p><p>-k：以KB（千字节）为单位显示文件大小；</p><p>-l：以长格式显示目录下的内容列表。输出的信息从左到右依次包括文件名，文件类型、权限模式、硬连接数、所有者、组、文件大小和文件的最后修改时间等；</p><p>-m：用“,”号区隔每个文件和目录的名称；</p><p>-n：以用户识别码和群组识别码替代其名称；</p><p>-r：以文件名反序排列并输出目录内容列表；</p><p>-s：显示文件和目录的大小，以区块为单位；</p><p>-t：用文件和目录的更改时间排序；</p><p>-L：如果遇到性质为符号链接的文件或目录，直接列出该链接所指向的原始文件或目录；</p><p>-R：递归处理，将指定目录下的所有文件及子目录一并处理；</p><p>–full-time：列出完整的日期与时间；</p><p>–color[=WHEN]：使用不同的颜色高亮显示不同类型的。</p><h3 id="文件权限"><a href="#文件权限" class="headerlink" title="文件权限"></a><code>文件权限</code></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-rwx-rwx-rwx </span><br><span class="line"><span class="meta">#</span><span class="bash"> 第一个代表文件类型</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 代表所有者的权限 </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 代表所属组的权限</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 代表其他人的权限</span></span><br></pre></td></tr></table></figure><h3 id="改变权限-chmod"><a href="#改变权限-chmod" class="headerlink" title="改变权限 - chmod"></a><code>改变权限</code> - <code>chmod</code></h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">r 读取权限  如果没有r 就不能 ls 查看里面的内容   对应数字 4</span><br><span class="line">w 写权限    如果没有w 就不能在目录下创建新的文件  对应数字 2</span><br><span class="line">x 执行权限  如果没有x 就不能cd进入这个目录 对应数字 1</span><br><span class="line">- 没权限  对应数字 0</span><br><span class="line">chmod 777 filename</span><br><span class="line">rwx-rwx-rwx</span><br></pre></td></tr></table></figure><h3 id="cd"><a href="#cd" class="headerlink" title="cd:"></a><code>cd</code>:</h3><p>切换文件夹</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd filename  # 进入文件-filename：文件名</span><br><span class="line"></span><br><span class="line">cd -  # 返回上一次进入的目录</span><br><span class="line"></span><br><span class="line">cd ~  # 进入根目录</span><br><span class="line"></span><br><span class="line">cd .. # 返回上级目录</span><br></pre></td></tr></table></figure><h3 id="pwd"><a href="#pwd" class="headerlink" title="pwd:"></a><code>pwd</code>:</h3><p>查看你的当前绝对路径</p><h3 id="mkdir"><a href="#mkdir" class="headerlink" title="mkdir:"></a><code>mkdir</code>:</h3><p>mkdir [OPTION] DIRECTORY…</p><p>创建目录DIRECTORY，可以一次创建多个。OPTION如果是-p，表示可以连同父目录一起创建。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir filename # 创建一个filename的文件</span><br></pre></td></tr></table></figure><h3 id="rmdir"><a href="#rmdir" class="headerlink" title="rmdir:"></a><code>rmdir</code>:</h3><p>删除一个空的目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rmdir filename # 删除一个空的filename的文件</span><br></pre></td></tr></table></figure><h3 id="cp"><a href="#cp" class="headerlink" title="cp:"></a><code>cp</code>:</h3><p><strong>复制文件或目录</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cp file1 file2</span><br><span class="line"></span><br><span class="line">cp file1 dir/</span><br><span class="line"></span><br><span class="line">cp file1 ../</span><br></pre></td></tr></table></figure><p><strong>拷贝目录</strong>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp dir1 dir2 -r</span><br><span class="line"></span><br><span class="line">cp dir1 ~/ -r</span><br></pre></td></tr></table></figure><h3 id="rm"><a href="#rm" class="headerlink" title="rm:"></a><code>rm</code>:</h3><p>移除文件或目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -r  # 递归删除文件</span><br><span class="line">rm -rf # 强制删除文件*****</span><br></pre></td></tr></table></figure><h3 id="cat"><a href="#cat" class="headerlink" title="cat:"></a><code>cat</code>:</h3><p>由第一行开始显示文件内容 <code>-b</code>显示行号</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat file  # 一次查看所有的文件</span><br><span class="line">cat file1  file2   # 一次查看两个命令</span><br></pre></td></tr></table></figure><h3 id="tac"><a href="#tac" class="headerlink" title="tac :"></a><code>tac</code> :</h3><p>从最后一行开始显示，可以看出 tac 是 cat 的倒著写！</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tac filename</span><br></pre></td></tr></table></figure><h3 id="nl"><a href="#nl" class="headerlink" title="nl:"></a><code>nl</code>:</h3><p>显示的时候，顺道输出行号！</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nl filename</span><br></pre></td></tr></table></figure><h3 id="more"><a href="#more" class="headerlink" title="more"></a><code>more</code></h3><p>一页一页的显示文件内容</p><p>按Space键：显示文本的下一屏内容。</p><p>按Enier键：只显示文本的下一行内容。</p><p>按斜线符/：接着输入一个模式，可以在文本中寻找下一个相匹配的模式。</p><p>按H键：显示帮助屏，该屏上有相关的帮助信息。</p><p>按B键：显示上一屏内容。</p><p>按Q键：退出more命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">more filename</span><br></pre></td></tr></table></figure><h3 id="less"><a href="#less" class="headerlink" title="less"></a><code>less</code></h3><p>用PageUp键向上翻页，用PageDown键向下翻页</p><p>f 往后 b 往前q 退出</p><p>-e：文件内容显示完毕后，自动退出；</p><p>-f：强制显示文件；</p><p>-g：不加亮显示搜索到的所有关键词，仅显示当前显示的关键字，以提高显示速度；</p><p>-l：搜索时忽略大小写的差异；</p><p>-N：每一行行首显示行号；</p><p>-s：将连续多个空行压缩成一行显示；</p><p>-S：在单行显示较长的内容，而不换行显示；</p><p>-x&lt;数字&gt;：将TAB字符显示为指定个数的空格字符。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">less -e filename</span><br></pre></td></tr></table></figure><h3 id="head"><a href="#head" class="headerlink" title="head:"></a><code>head</code>:</h3><p>-n&lt;数字&gt;：指定显示头部内容的行数；</p><p>-c&lt;字符数&gt;：指定显示头部内容的字符数；</p><p>-v：总是显示文件名的头信息；</p><p>-q：不显示文件名的头信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">head filename</span><br></pre></td></tr></table></figure><h3 id="tail："><a href="#tail：" class="headerlink" title="tail："></a><code>tail</code>：</h3><p>只看尾巴几行</p><p>–retry：即是在tail命令启动时，文件不可访问或者文件稍后变得不可访问，都始终尝试打开文件。使用此选项时需要与选项“——follow=name”连用；</p><p>-c<n>或——bytes=<n>：输出文件尾部的N（N为整数）个字节内容；</n></n></p><p>-f&lt;name/descriptor&gt;或；–follow<nameldescript>：显示文件最新追加的内容。“name”表示以文件名的方式监视文件的变化。“-f”与“-fdescriptor”等效；</nameldescript></p><p>-F：与选项“-follow=name”和“–retry”连用时功能相同；</p><p>-n<n>或——line=<n>：输出文件的尾部N（N位数字）行内容。</n></n></p><p>–pid=&lt;进程号&gt;：与“-f”选项连用，当指定的进程号的进程终止后，自动退出tail命令；</p><p>-q或——quiet或——silent：当有多个文件参数时，不输出各个文件名；</p><p>-s&lt;秒数&gt;或——sleep-interal=&lt;秒数&gt;：与“-f”选项连用，指定监视文件变化时间隔的秒数；</p><p>-v或——verbose：当有多个文件参数时，总是输出各个文件名；</p><p>–help：显示指令的帮助信息；</p><p>–version：显示指令的版本信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tail file # （显示文件file的最后10行）</span><br><span class="line"></span><br><span class="line">tail -c 10 file # （显示文件file的最后10个字符）</span><br></pre></td></tr></table></figure><h3 id="which-whereis-查找命令"><a href="#which-whereis-查找命令" class="headerlink" title="which / whereis 查找命令"></a><code>which / whereis</code> 查找命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">which command  # 查看    -二进制文件</span><br><span class="line">whereis 可执行文件    # 二进制文件 、man手册</span><br><span class="line"></span><br><span class="line">帮助文档：</span><br><span class="line">1.man手册  ，帮助文档    man  ls</span><br><span class="line">2.--help   , ls --help</span><br></pre></td></tr></table></figure><h3 id="find-查找文件"><a href="#find-查找文件" class="headerlink" title="find   查找文件"></a><code>find</code> 查找文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">find 路径  参数  </span><br><span class="line"><span class="meta">#</span><span class="bash"> 常用参数 </span></span><br><span class="line">-name   # 按照名字</span><br><span class="line">-size   # 按照大小</span><br><span class="line">find ./  -size +100k -size -10M  # 在当前目录下找大于100k 小于 10的文件</span><br></pre></td></tr></table></figure><h3 id="grep-文本搜索-筛选内容"><a href="#grep-文本搜索-筛选内容" class="headerlink" title="grep  文本搜索(筛选内容)"></a><code>grep</code> 文本搜索(筛选内容)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grep  'content'  filename</span><br><span class="line"><span class="meta">#</span><span class="bash"> 常用参数</span></span><br><span class="line">-v    显示不包含匹配文本的所有‘行’ (求反)</span><br><span class="line">-n    显示匹配行及行号</span><br><span class="line">-i    忽略大小写</span><br><span class="line"><span class="meta">#</span><span class="bash"> 内容参数</span></span><br><span class="line">^wu  行首 搜索以wu开头的行  </span><br><span class="line"><span class="meta">wh$</span><span class="bash">  行尾 索以wh结束的行</span></span><br></pre></td></tr></table></figure><h3 id="管道"><a href="#管道" class="headerlink" title="| 管道"></a><code>|</code> 管道</h3><h5 id="一个命令的输出，可以通过管道符，做为另一个命令的输入"><a href="#一个命令的输出，可以通过管道符，做为另一个命令的输入" class="headerlink" title="一个命令的输出，可以通过管道符，做为另一个命令的输入"></a>一个命令的输出，可以通过管道符，做为另一个<code>命令</code>的输入</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">命令输出  |  命令处理</span><br><span class="line"></span><br><span class="line">ls --help | less   # 将输出，放入翻页模式中</span><br><span class="line">ls --help | grep -n 'f'  #将输出，放入‘筛选’模式中</span><br><span class="line">ls --help | grep -n 'f' &gt;&gt; 3.txt  #将输出，放入‘筛选’模式中，然后将重定向到3.txt</span><br></pre></td></tr></table></figure><h3 id="ln-创建链接文件"><a href="#ln-创建链接文件" class="headerlink" title="ln   创建链接文件"></a><code>ln</code> 创建链接文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ln  file  hardlink # 硬链接</span><br><span class="line">ln -s  file softlink  # 软链接</span><br></pre></td></tr></table></figure><p>软链接: 相当于 <code>window</code>上的快捷方式 源文件删除则软链接失效</p><p>硬链接: 硬链接只能连接普通的文件 不能连接目录</p><p><strong>注意</strong> 如果软链接文件和源文件不在同一个目录 源文件要使用绝对路径 不能使用相对路径</p><h3 id="alias-创建别名"><a href="#alias-创建别名" class="headerlink" title="alias  创建别名"></a><code>alias</code> 创建别名</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">alias     # 查看所有别名   alias c4='cat 4.txt'</span><br><span class="line"></span><br><span class="line">unalias   # 删除别名</span><br></pre></td></tr></table></figure><p><strong>注意</strong> 这种定义别名的方式 只在当前登录有效 如果要永久定义生效 可以通过修改<code>~/.bashrc</code>文件 这个修改要下次登录才能生效 想要立即生效 可以输入<code>source ~/.bashrc</code></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> LINUX </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Linux-vim及python虚拟环境</title>
      <link href="/2018/11/08/2018-11-8-linux-vim/"/>
      <url>/2018/11/08/2018-11-8-linux-vim/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h2 id="linux常用命令"><a href="#linux常用命令" class="headerlink" title="linux常用命令"></a>linux常用命令</h2><h3 id="添加或删除用户："><a href="#添加或删除用户：" class="headerlink" title="添加或删除用户："></a><strong>添加或删除用户</strong>：</h3><h4 id="adduser"><a href="#adduser" class="headerlink" title="adduser:"></a><code>adduser</code>:</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">adduser username # 添加一个叫username的用户</span><br></pre></td></tr></table></figure><h4 id="userdel"><a href="#userdel" class="headerlink" title="userdel:"></a><code>userdel</code>:</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">userdel  username  # 删除用户  不删错home目录</span><br><span class="line">userdel -r  username # 删除用户 同时删除用户home目录</span><br></pre></td></tr></table></figure><h3 id="文件压缩解压"><a href="#文件压缩解压" class="headerlink" title="文件压缩解压"></a>文件压缩解压</h3><h4 id="tar格式"><a href="#tar格式" class="headerlink" title=".tar格式"></a><code>.tar格式</code></h4><p>在<code>Linux</code>中打包就是使用<code>tar</code>命令 命令有点特殊 前面的参数 可加可不加 <code>-</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 打包</span><br><span class="line"> tar -cvf  文件名.tar   # 要打包的文件</span><br><span class="line"></span><br><span class="line"># 解包</span><br><span class="line"> tar  -xvf  文件名.tar  </span><br><span class="line"></span><br><span class="line">#查看包里的内容</span><br><span class="line">tar -tvf 包的文件名.tar</span><br></pre></td></tr></table></figure><h4 id="gz格式"><a href="#gz格式" class="headerlink" title=".gz格式"></a><code>.gz格式</code></h4><p>或者说配合 <code>tar</code> 则是</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -zcvf  xxx.tar.gz  文件 # 压缩</span><br><span class="line">tar -zxvf  xxx.tar.gz 文件  # 解压</span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压到指定目录 </span></span><br><span class="line">tar -zxvf xxx.tar.gz -C dirname</span><br></pre></td></tr></table></figure><h4 id="bz2格式"><a href="#bz2格式" class="headerlink" title=".bz2格式"></a><code>.bz2格式</code></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -jcvf  xxx.tar.bz2  文件  # 压缩</span><br><span class="line">tar -jxvf  xxx.tar.bz2       # 解压</span><br></pre></td></tr></table></figure><h4 id="zip格式"><a href="#zip格式" class="headerlink" title=".zip格式"></a><code>.zip格式</code></h4><p>通过<code>zip</code> 压缩的话 不需要指定拓展名 默认拓展名为 <code>zip</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install zip</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">压缩文件</span><br><span class="line">zip  压缩文件  源文件</span><br><span class="line"></span><br><span class="line">解压 </span><br><span class="line">unzip  压缩文件</span><br><span class="line">-d 解压到指定目录   如果目录不存在 会自动创建新目录 并压缩进去</span><br><span class="line">unzip test.zip -d filename</span><br></pre></td></tr></table></figure><h3 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h3><h3 id="ps-aux-查看进程"><a href="#ps-aux-查看进程" class="headerlink" title="ps   aux 查看进程"></a><code>ps</code> aux 查看进程</h3><p>查看所有进程 ： <code>ps aux</code></p><h3 id="top-动态查看进程"><a href="#top-动态查看进程" class="headerlink" title="top 动态查看进程"></a><code>top</code> 动态查看进程</h3><p>查看所有进程 ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top</span><br></pre></td></tr></table></figure><p>top命令用来动态显示运行中的进程 top命令能够在运行后 在指定的时间间隔更新显示信息 可以在使用top命令时加上-d 来指定显示信息更新的时间间隔</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jobs # 查看所有在后台的进程</span><br></pre></td></tr></table></figure><p><code>ctrl＋Z</code> 　 暂停当前进程　比如你正运行一个命令　突然觉得有点问题想暂停一下　就可以使用这个快捷键　暂停后　可以使用<code>fg</code> 恢复它</p><p>###<code>kill</code> 杀死一个进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill -9  id号  强制杀死</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ps -aux </span><br><span class="line">查看正在内存中的程序 </span><br><span class="line">会配合 管道符 </span><br><span class="line">ps aux | grep ssh/python </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">### top动态查看</span></span></span><br><span class="line">默认3秒  </span><br><span class="line">-d 时间 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">### 结束进程</span></span></span><br><span class="line"></span><br><span class="line">kill -9  id号  强制杀死</span><br><span class="line"></span><br><span class="line">ctrl+z 可以让你正在运行的东西 暂停 </span><br><span class="line"></span><br><span class="line">jobs 查看所有在后台的进程</span><br></pre></td></tr></table></figure><h4 id="ctrl-z-暂停-。-fg-继续"><a href="#ctrl-z-暂停-。-fg-继续" class="headerlink" title="ctrl  z   暂停 。   fg   继续"></a>ctrl z 暂停 。 fg 继续</h4><h3 id="vim-编辑器"><a href="#vim-编辑器" class="headerlink" title="vim 编辑器"></a>vim <strong>编辑器</strong></h3><p>工作模式：命令模式、输入模式、末行模式</p><h4 id="模式之间切换"><a href="#模式之间切换" class="headerlink" title="模式之间切换"></a>模式之间切换</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">当打开一个文件时处于命令模式</span><br><span class="line">在命令模式下，按 i 进入输入模式</span><br><span class="line">在输入模式，按ESC回到命令模式。</span><br><span class="line">在命令模式下，按shift+; ，末行出现:冒号，则进入末行模式</span><br></pre></td></tr></table></figure><h4 id="进入与退出"><a href="#进入与退出" class="headerlink" title="进入与退出"></a>进入与退出</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">进入</span><br><span class="line">    vim   filename</span><br><span class="line">退出</span><br><span class="line">    :wq    末行模式，wq 保存退出</span><br><span class="line">    :q       末行模式，q 直接退出</span><br><span class="line">    :q!      末行模式，q! 强制退出，不保存</span><br></pre></td></tr></table></figure><h4 id="输入模式"><a href="#输入模式" class="headerlink" title="输入模式"></a><strong>输入模式</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">输入模式</span><br><span class="line"></span><br><span class="line">    i    从光标所在位置前面开始插入</span><br><span class="line">    I    在当前行首插入</span><br><span class="line">    a   从光标所在位置后面开始输入</span><br><span class="line">    A   在当前行尾插入</span><br><span class="line">    o   在光标所在行下方新增一行并进入输入模式</span><br><span class="line">    O  在当前上面一行插入</span><br><span class="line"></span><br><span class="line">进入输入模式后，在最后一行会出现--INSERT—的字样</span><br></pre></td></tr></table></figure><h4 id="移动光标"><a href="#移动光标" class="headerlink" title="移动光标"></a><strong>移动光标</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">移动光标</span><br><span class="line">    gg    到文件第一行</span><br><span class="line">    G      到文件最后一行   (Shift + g)</span><br><span class="line">    ^      非空格行首</span><br><span class="line">    0       行首(数字0)</span><br><span class="line">    $       行尾</span><br></pre></td></tr></table></figure><h4 id="复制和粘贴"><a href="#复制和粘贴" class="headerlink" title="复制和粘贴"></a><strong>复制和粘贴</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">复制和粘贴</span><br><span class="line">    yy    复制整行内容</span><br><span class="line">    3yy  复制3行内容</span><br><span class="line">    yw   复制当前光标到单词尾内容</span><br><span class="line"></span><br><span class="line">    p      粘贴</span><br></pre></td></tr></table></figure><h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a><strong>删除</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">删除</span><br><span class="line">    dd  删除光标所在行</span><br><span class="line">    dw  删除一个单词</span><br><span class="line">    x     删除光标所在字符</span><br><span class="line">    u    撤销上一次操作</span><br><span class="line">    s     替换</span><br><span class="line">    ctrl + r    撤销   u</span><br></pre></td></tr></table></figure><h4 id="块操作"><a href="#块操作" class="headerlink" title="块操作"></a><strong>块操作</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">块操作</span><br><span class="line">    v  + 方向键进行块选择</span><br><span class="line">    ctrl + v +方向键进行列块选择</span><br></pre></td></tr></table></figure><h4 id="查找"><a href="#查找" class="headerlink" title="查找"></a><strong>查找</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">查找</span><br><span class="line">    /    命令模式下输入：/   向前搜索     不能空格</span><br><span class="line">    ?    命令模式下输入：?   向后搜索</span><br><span class="line"><span class="meta">#</span><span class="bash"> / 方式</span></span><br><span class="line">    n    向下查找</span><br><span class="line">    N   向上查找</span><br><span class="line"><span class="meta">#</span><span class="bash"> ? 方式</span></span><br><span class="line">    n    向上查找</span><br><span class="line">    N   向下查找</span><br></pre></td></tr></table></figure><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在vim下千万不要按Ctrl + s 。 不然就会卡死</span><br><span class="line">如果按了Ctrl + s，按Ctrl + q 可以退出</span><br></pre></td></tr></table></figure><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a><strong>实例</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hello.py # 创建一个hello.py的python文件</span><br></pre></td></tr></table></figure><p>filename：hello.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'hello word'</span>) <span class="comment"># 打印hello word</span></span><br></pre></td></tr></table></figure><p>保存，退出</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ python hello.py </span><br><span class="line">hello word</span><br></pre></td></tr></table></figure><p><img src="/images/py/67.png" alt="1542693813625"></p><h3 id="python虚拟环境的使用"><a href="#python虚拟环境的使用" class="headerlink" title="python虚拟环境的使用"></a>python<strong>虚拟环境的使用</strong></h3><h4 id="安装虚拟环境"><a href="#安装虚拟环境" class="headerlink" title="安装虚拟环境"></a><strong>安装虚拟环境</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo pip install virtualenv</span></span><br></pre></td></tr></table></figure><h4 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a><strong>创建虚拟环境</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> virtualenv envname <span class="comment"># envname 自定义的虚拟环境名字</span></span></span><br></pre></td></tr></table></figure><h4 id="激活虚拟环境"><a href="#激活虚拟环境" class="headerlink" title="激活虚拟环境"></a><strong>激活虚拟环境</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> envname/bin/activate</span></span><br></pre></td></tr></table></figure><h4 id="退出虚拟环境"><a href="#退出虚拟环境" class="headerlink" title="退出虚拟环境"></a><strong>退出虚拟环境</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">(envname)$</span><span class="bash"> deactivate</span></span><br></pre></td></tr></table></figure><h4 id="删除虚拟环境"><a href="#删除虚拟环境" class="headerlink" title="删除虚拟环境"></a><strong>删除虚拟环境</strong></h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo rm -rf envname <span class="comment"># 只需删除建立的文件夹就行</span></span></span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> LINUX </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Docker安装-windows</title>
      <link href="/2018/10/29/2018-10-29-docker-install-win10/"/>
      <url>/2018/10/29/2018-10-29-docker-install-win10/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="windows安装Docker"><a href="#windows安装Docker" class="headerlink" title="windows安装Docker"></a>windows安装Docker</h3><p>Docker发布了Docker for Windows的正式版，于是就可以在Windows下运行Docker容器。</p><h3 id="版本要求"><a href="#版本要求" class="headerlink" title="版本要求"></a>版本要求</h3><p>要在Windows下运行Docker，需要满足以下先决条件：</p><ul><li>64位Windows 10 Pro、Enterprise或者Education版本（Build 10586以上版本，需要安装1511 November更新）</li><li>在系统中启用Hyper-V。如果没有启用，Docker for Windows在安装过程中会自动启用Hyper-V（这个过程需要重启系统）<br>不过，如果不是使用的Windows 10，也没有关系，可以使用<a href="https://link.jianshu.com?t=http%3A%2F%2Fwww.docker.com%2Fproducts%2Fdocker-toolbox" target="_blank" rel="noopener">Docker Toolbox</a>作为替代方案。</li></ul><h3 id="启用系统的Hper-V"><a href="#启用系统的Hper-V" class="headerlink" title="启用系统的Hper-V"></a>启用系统的Hper-V</h3><p>打开<strong>程序</strong>和<strong>功能</strong>，右击<strong>开始</strong>选择<strong>应用和功能</strong>。</p><p><img src="/images/docker/1.png" alt="1"></p><p>打开右边的<strong>程序</strong>和<strong>功能</strong></p><p><img src="/images/docker/2.png" alt="1"></p><p>打开左边的<strong>启用或关闭windows功能</strong></p><p><img src="/images/docker/3.png" alt="1"></p><p>启用<strong>Hper-V</strong>服务</p><p><img src="/images/docker/4.png" alt="1"></p><p>然后<strong>重启计算机</strong>，使服务生效</p><h4 id="Docker-for-Windows的安装"><a href="#Docker-for-Windows的安装" class="headerlink" title="Docker for Windows的安装"></a>Docker for Windows的安装</h4><p>在Windows 10中，请<a href="https://link.jianshu.com/?t=https%3A%2F%2Fdownload.docker.com%2Fwin%2Fstable%2FInstallDocker.msi" target="_blank" rel="noopener">点击此处</a>下载Docker for Windows的安装包，下载好之后双击 Docker for Windows Installer.exe 开始安装。安装好后，桌面会出现如下快捷方式，表明docker安装成功。</p><p><img src="/images/docker/5.png" alt="1"></p><p>Docker CE 启动之后会在 Windows 任务栏出现鲸鱼图标。</p><p>等待片刻，点击 Got it 开始使用 Docker CE。</p><p><img src="/images/docker/6.png" alt="1"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Docker安装-ubuntu</title>
      <link href="/2018/10/29/2018-10-29-docker-install/"/>
      <url>/2018/10/29/2018-10-29-docker-install/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="ubuntu安装Docker"><a href="#ubuntu安装Docker" class="headerlink" title="ubuntu安装Docker"></a>ubuntu安装Docker</h3><p>Docker CE 支持以下版本的 <a href="https://www.ubuntu.com/server" target="_blank" rel="noopener">Ubuntu</a> 操作系统：</p><ul><li>Bionic 18.04 (LTS)</li><li>Xenial 16.04 (LTS)</li><li>Trusty 14.04 (LTS)</li></ul><h3 id="卸载旧版本Docker"><a href="#卸载旧版本Docker" class="headerlink" title="卸载旧版本Docker"></a>卸载旧版本Docker</h3><p>旧版本的 Docker 称为 <code>docker</code> 或者 <code>docker-engine</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get remove docker \</span></span><br><span class="line">               docker-engine \</span><br><span class="line">               docker.io</span><br></pre></td></tr></table></figure><h3 id="安装可选内核模块–ubuntu14-04"><a href="#安装可选内核模块–ubuntu14-04" class="headerlink" title="安装可选内核模块–ubuntu14.04"></a>安装可选内核模块–ubuntu14.04</h3><p>从 Ubuntu 14.04 开始，一部分内核模块移到了可选内核模块包 (<code>linux-modules-extra-*</code>) ，以减少内核软件包的体积。正常安装的系统应该会包含可选内核模块包，而一些裁剪后的系统可能会将其精简掉。<code>AUFS</code> 内核驱动属于可选内核模块的一部分，作为推荐的 Docker 存储层驱动，一般建议安装可选内核模块包以使用 <code>AUFS</code>。</p><p><code>linux-image-generic</code>应该已经安装了相关的<code>linux-image-extra</code>包，但名称已更改为<code>linux-modules-extra</code>。</p><h4 id="升级到最新的内核："><a href="#升级到最新的内核：" class="headerlink" title="升级到最新的内核："></a>升级到最新的内核：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt update</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt upgrade</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt install --reinstall linux-image-generic</span></span><br></pre></td></tr></table></figure><p>如果系统没有安装可选内核模块的话，可以执行下面的命令来安装可选内核模块包：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt install linux-modules-extra-$(uname -r) linux-image-extra-virtual</span></span><br></pre></td></tr></table></figure><h3 id="ubuntu16-04"><a href="#ubuntu16-04" class="headerlink" title="ubuntu16.04+"></a>ubuntu16.04+</h3><p>Ubuntu 16.04 + 上的 Docker CE 默认使用 <code>overlay2</code> 存储层驱动,无需手动配置。</p><h3 id="使用APT安装"><a href="#使用APT安装" class="headerlink" title="使用APT安装"></a>使用APT安装</h3><p>由于 <code>apt</code> 源使用 HTTPS 以确保软件下载过程中不被篡改。因此，我们首先需要添加使用 HTTPS 传输的软件包以及 CA 证书。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get update</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install \</span></span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    software-properties-common</span><br></pre></td></tr></table></figure><p>鉴于国内网络问题，强烈建议使用国内源，为了确认所下载软件包的合法性，需要添加软件源的 <code>GPG</code> 密钥。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add -</span></span><br></pre></td></tr></table></figure><p>然后，我们需要向 <code>source.list</code> 中添加 Docker 软件源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo add-apt-repository \</span></span><br><span class="line">    "deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \</span><br><span class="line">    $(lsb_release -cs) \</span><br><span class="line">    stable"</span><br></pre></td></tr></table></figure><p>以上命令会添加稳定版本的 Docker CE APT 镜像源，如果需要测试或每日构建版本的 Docker CE 请将 stable 改为 test 或者 nightly。</p><h4 id="安装Docker-CE"><a href="#安装Docker-CE" class="headerlink" title="安装Docker CE"></a>安装Docker CE</h4><p>更新 apt 软件包缓存，并安装 <code>docker-ce</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get update</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install docker-ce</span></span><br></pre></td></tr></table></figure><h3 id="使用脚本自动安装"><a href="#使用脚本自动安装" class="headerlink" title="使用脚本自动安装"></a>使用脚本自动安装</h3><p>在测试或开发环境中 Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，Ubuntu 系统上可以使用这套脚本安装：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -fsSL get.docker.com -o get-docker.sh</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo sh get-docker.sh --mirror Aliyun</span></span><br></pre></td></tr></table></figure><p>执行这个命令后，脚本就会自动的将一切准备工作做好，并且把 Docker CE 的 Edge 版本安装在系统中。</p><h3 id="启动Docker-CE"><a href="#启动Docker-CE" class="headerlink" title="启动Docker CE"></a>启动Docker CE</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo systemctl <span class="built_in">enable</span> docker</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo systemctl start docker</span></span><br></pre></td></tr></table></figure><p>Ubuntu 14.04 请使用以下命令启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo service docker start</span></span><br></pre></td></tr></table></figure><h3 id="建立-docker-用户组"><a href="#建立-docker-用户组" class="headerlink" title="建立 docker 用户组"></a>建立 docker 用户组</h3><p>默认情况下，<code>docker</code> 命令会使用 <a href="https://en.wikipedia.org/wiki/Unix_domain_socket" target="_blank" rel="noopener">Unix socket</a> 与 Docker 引擎通讯。而只有 <code>root</code> 用户和 <code>docker</code> 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 <code>root</code> 用户。因此，更好地做法是将需要使用 <code>docker</code> 的用户加入 <code>docker</code> 用户组。</p><h4 id="建立-Docker组："><a href="#建立-Docker组：" class="headerlink" title="建立 Docker组："></a>建立 Docker组：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo groupadd docker</span></span><br></pre></td></tr></table></figure><h4 id="将当前用户加入-docker-组："><a href="#将当前用户加入-docker-组：" class="headerlink" title="将当前用户加入 docker 组："></a>将当前用户加入 <code>docker</code> 组：</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo usermod -aG docker <span class="variable">$USER</span></span></span><br></pre></td></tr></table></figure><p>退出当前终端并重新登录，进行如下测试。</p><h3 id="测试Docker是否安装正确"><a href="#测试Docker是否安装正确" class="headerlink" title="测试Docker是否安装正确"></a>测试Docker是否安装正确</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run hello-world</span></span><br><span class="line"></span><br><span class="line">Unable to find image 'hello-world:latest' locally</span><br><span class="line">latest: Pulling from library/hello-world</span><br><span class="line">d1725b59e92d: Pull complete</span><br><span class="line">Digest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788</span><br><span class="line">Status: Downloaded newer image for hello-world:latest</span><br><span class="line"></span><br><span class="line">Hello from Docker!</span><br><span class="line">This message shows that your installation appears to be working correctly.</span><br><span class="line"></span><br><span class="line">To generate this message, Docker took the following steps:</span><br><span class="line"> 1. The Docker client contacted the Docker daemon.</span><br><span class="line"> 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.</span><br><span class="line">    (amd64)</span><br><span class="line"> 3. The Docker daemon created a new container from that image which runs the</span><br><span class="line">    executable that produces the output you are currently reading.</span><br><span class="line"> 4. The Docker daemon streamed that output to the Docker client, which sent it</span><br><span class="line">    to your terminal.</span><br><span class="line"></span><br><span class="line">To try something more ambitious, you can run an Ubuntu container with:</span><br><span class="line"><span class="meta"> $</span><span class="bash"> docker run -it ubuntu bash</span></span><br><span class="line"></span><br><span class="line">Share images, automate workflows, and more with a free Docker ID:</span><br><span class="line"> https://hub.docker.com/</span><br><span class="line"></span><br><span class="line">For more examples and ideas, visit:</span><br><span class="line"> https://docs.docker.com/get-started/</span><br></pre></td></tr></table></figure><p>若能正常输出以上信息，则说明安装成功。<br>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Docker镜像加速器</title>
      <link href="/2018/10/29/2018-10-29-docker-speed/"/>
      <url>/2018/10/29/2018-10-29-docker-speed/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="镜像加速器"><a href="#镜像加速器" class="headerlink" title="镜像加速器"></a>镜像加速器</h3><p>国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如：</p><ul><li><a href="https://docs.docker.com/registry/recipes/mirror/#use-case-the-china-registry-mirror" target="_blank" rel="noopener">Docker 官方提供的中国 registry mirror <code>https://registry.docker-cn.com</code></a></li><li><a href="https://kirk-enterprise.github.io/hub-docs/#/user-guide/mirror" target="_blank" rel="noopener">七牛云加速器 <code>https://reg-mirror.qiniu.com/</code></a></li></ul><blockquote><p>当配置某一个加速器地址之后，若发现拉取不到镜像，请切换到另一个加速器地址。</p><p>国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务。</p></blockquote><p>我们以 Docker 官方加速器 <code>https://registry.docker-cn.com</code> 为例进行介绍。</p><h3 id="Ubuntu-14-04、Debian-7-Wheezy"><a href="#Ubuntu-14-04、Debian-7-Wheezy" class="headerlink" title="Ubuntu 14.04、Debian 7 Wheezy"></a>Ubuntu 14.04、Debian 7 Wheezy</h3><p>对于使用 <a href="http://upstart.ubuntu.com/" target="_blank" rel="noopener">upstart</a> 的系统而言，编辑 <code>/etc/default/docker</code> 文件，在其中的 <code>DOCKER_OPTS</code> 中配置加速器地址：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DOCKER_OPTS=<span class="string">"--registry-mirror=https://registry.docker-cn.com"</span></span><br></pre></td></tr></table></figure><p>重新启动服务。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo service docker restart</span><br></pre></td></tr></table></figure><h3 id="Ubuntu-16-04-、Debian-8-、CentOS-7"><a href="#Ubuntu-16-04-、Debian-8-、CentOS-7" class="headerlink" title="Ubuntu 16.04+、Debian 8+、CentOS 7"></a>Ubuntu 16.04+、Debian 8+、CentOS 7</h3><p>对于使用 <a href="https://www.freedesktop.org/wiki/Software/systemd/" target="_blank" rel="noopener">systemd</a> 的系统，请在 <code>/etc/docker/daemon.json</code> 中写入如下内容（如果文件不存在请新建该文件）</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"registry-mirrors"</span>: [</span><br><span class="line">    <span class="string">"https://registry.docker-cn.com"</span></span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意，一定要保证该文件符合 json 规范，否则 Docker 将不能启动。</p></blockquote><p>之后重新启动服务。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl daemon-reload</span><br><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure><blockquote><p>注意：如果您之前查看旧教程，修改了 <code>docker.service</code> 文件内容，请去掉您添加的内容（<code>--registry-mirror=https://registry.docker-cn.com</code>），这里不再赘述。</p></blockquote><h3 id="Windows-10"><a href="#Windows-10" class="headerlink" title="Windows 10"></a>Windows 10</h3><p>对于使用 Windows 10 的系统，在系统右下角托盘 Docker 图标内右键菜单选择 <code>Settings</code>，打开配置窗口后左侧导航菜单选择 <code>Daemon</code>。在 <code>Registry mirrors</code> 一栏中填写加速器地址 <code>https://registry.docker-cn.com</code>，之后点击 <code>Apply</code> 保存后 Docker 就会重启并应用配置的镜像地址了。</p><h3 id="macOS"><a href="#macOS" class="headerlink" title="macOS"></a>macOS</h3><p>对于使用 macOS 的用户，在任务栏点击 Docker for mac 应用图标 -&gt; Perferences… -&gt; Daemon -&gt; Registry mirrors。在列表中填写加速器地址 <code>https://registry.docker-cn.com</code>。修改完成之后，点击 <code>Apply &amp; Restart</code> 按钮，Docker 就会重启并应用配置的镜像地址了。</p><h3 id="检查加速器是否生效"><a href="#检查加速器是否生效" class="headerlink" title="检查加速器是否生效"></a>检查加速器是否生效</h3><p>配置加速器之后，如果拉取镜像仍然十分缓慢，请手动检查加速器配置是否生效，在命令行执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  docker info</span><br></pre></td></tr></table></figure><p>如果从结果中看到了如下内容，说明配置成功。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Registry Mirrors:</span><br><span class="line"> https://registry.docker-cn.com/</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Docker镜像管理</title>
      <link href="/2018/10/29/2018-10-29-docker-source/"/>
      <url>/2018/10/29/2018-10-29-docker-source/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="Docker镜像管理"><a href="#Docker镜像管理" class="headerlink" title="Docker镜像管理"></a>Docker镜像管理</h3><p>Docker镜像是一个不包含Linux内核而又精简的Linux的操作系统。</p><h3 id="Dock镜像下载"><a href="#Dock镜像下载" class="headerlink" title="Dock镜像下载"></a>Dock镜像下载</h3><p>Docker Hub 是由Docker公司负责维护的公共注册中心，包含大量的容器镜像，Docker工具默认从这个公共镜像库下载镜像：<a href="https://hub.docker.com/explore" target="_blank" rel="noopener">https://hub.docker.com/explore</a></p><p>默认是国外的源，下载会很慢，可以使用国内的源提供下载速度：参考上一节<a href="">镜像加速器</a></p><h3 id="镜像的工作原理"><a href="#镜像的工作原理" class="headerlink" title="镜像的工作原理"></a>镜像的工作原理</h3><p>当我们启动一个新的容器时，Docker会加载只读镜像，并在其之上添加一个读写层，并将镜像中的目录复制一份到/var/lib/docker/aufs/mnt/容器ID为目录下，我们可以使用chroot进入此目录。</p><p>如果运行中的容器修改一个已经存在的文件，那么会将该文件从下面的只读层复制到读写层，只读层的这个文件就会覆盖，但还存在，这就实现了文件系统隔离，当删除容器后，读写层的数据将会删除，只读镜像不变。</p><h3 id="镜像的文件存储结构"><a href="#镜像的文件存储结构" class="headerlink" title="镜像的文件存储结构"></a>镜像的文件存储结构</h3><p>docker相关文件存放在：/var/lib/docker目录下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/var/lib/docker/aufs/diff <span class="comment"># 每层与其父层之间的文件差异</span></span><br><span class="line">/var/lib/docker/aufs/layers/ <span class="comment"># 每层一个文件，记录其父层一直到根层之间的ID，大部分文件的最后一行都已，表示继承来自同一层</span></span><br><span class="line">/var/lib/docker/aufs/mnt <span class="comment"># 联合挂载点，从只读层复制到最上层可读写层的文件系统数据</span></span><br></pre></td></tr></table></figure><p>在建立镜像时，每次写操作，都被视作一种增量操作，即在原有的数据层上添加一个新层；所以一个镜像会有若干个层组成。每次commit提交就会对产生一个ID，就相当于在上一层有加了一层，可以通过这个ID对镜像回滚。</p><h3 id="镜像管理命令"><a href="#镜像管理命令" class="headerlink" title="镜像管理命令"></a>镜像管理命令</h3><p>列举一些常用的Docker镜像管理命令，完整内容请参考<a href="https://docs.docker.com/get-started/#docker-concepts" target="_blank" rel="noopener">官方文档</a></p><h3 id="Docker-search-命令"><a href="#Docker-search-命令" class="headerlink" title="Docker search 命令"></a>Docker search 命令</h3><p><strong>docker search :</strong> 从Docker Hub查找镜像</p><h4 id="语法："><a href="#语法：" class="headerlink" title="语法："></a>语法：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search [OPTIONS] TERM</span><br></pre></td></tr></table></figure><blockquote><p>OPTIONS说明：</p><p><strong>–automated :</strong>只列出 automated build类型的镜像；</p><p><strong>–no-trunc :</strong>显示完整的镜像描述；</p><p><strong>-s : </strong>列出star数不小于指定值的镜像。</p></blockquote><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>从Docker Hub 查找所有镜像包含MySQL， 并且star大于100</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker search mysql -s 100</span><br><span class="line">Flag --stars has been deprecated, use --filter=stars=3 instead</span><br><span class="line">NAME                         DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">mysql                        MySQL is a widely used, open-source relation…   7213                [OK]                </span><br><span class="line">mariadb                      MariaDB is a community-developed fork of MyS…   2311                [OK]                </span><br><span class="line">mysql/mysql-server           Optimized MySQL Server Docker images. Create…   531                                     [OK]</span><br><span class="line">percona                      Percona Server is a fork of the MySQL relati…   382                 [OK]                </span><br><span class="line">zabbix/zabbix-server-mysql   Zabbix Server with MySQL database support       136                                     [OK]</span><br></pre></td></tr></table></figure><h3 id="Docker-pull-命令"><a href="#Docker-pull-命令" class="headerlink" title="Docker pull 命令"></a>Docker pull 命令</h3><p><strong>docker pull :</strong> 从镜像仓库中拉取或者更新指定镜像</p><h4 id="语法：-1"><a href="#语法：-1" class="headerlink" title="语法："></a>语法：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull [OPTIONS] NAME[:TAG|@DIGEST]</span><br></pre></td></tr></table></figure><blockquote><p>OPTIONS说明：</p><p><strong>-a :</strong>拉取所有 tagged 镜像</p><p><strong>–disable-content-trust :</strong>忽略镜像的校验,默认开启</p></blockquote><h4 id="实例："><a href="#实例：" class="headerlink" title="实例："></a>实例：</h4><p>从Docker Hub下载mysql。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker pull mysql</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/mysql</span><br><span class="line">f17d81b4b692: Pull complete </span><br><span class="line">c691115e6ae9: Pull complete </span><br><span class="line">41544cb19235: Pull complete </span><br><span class="line">254d04f5f66d: Pull complete </span><br><span class="line">4fe240edfdc9: Pull complete </span><br><span class="line">0cd4fcc94b67: Pull complete </span><br><span class="line">8df36ec4b34a: Pull complete </span><br><span class="line">720bf9851f6a: Pull complete </span><br><span class="line">e933e0a4fddf: Pull complete </span><br><span class="line">9ffdbf5f677f: Pull complete </span><br><span class="line">a403e1df0389: Pull complete </span><br><span class="line">4669c5f285a6: Pull complete </span><br><span class="line">Digest: sha256:811483efcd38de17d93193b4b4bc4ba290a931215c4c8512cbff624e5967a7dd</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> mysql:latest</span><br></pre></td></tr></table></figure><h3 id="Docker-push-命令"><a href="#Docker-push-命令" class="headerlink" title="Docker push 命令"></a>Docker push 命令</h3><p><strong>docker push :</strong> 将本地的镜像上传到镜像仓库,要先登陆到镜像仓库</p><h4 id="语法：-2"><a href="#语法：-2" class="headerlink" title="语法："></a>语法：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push [OPTIONS] NAME[:TAG]</span><br></pre></td></tr></table></figure><blockquote><p>OPTIONS说明：</p><p><strong>–disable-content-trust :</strong>忽略镜像的校验,默认开启</p></blockquote><h3 id="Docker-images-命令"><a href="#Docker-images-命令" class="headerlink" title="Docker images 命令"></a>Docker images 命令</h3><p>Docker images: 查看本机的镜像</p><h4 id="语法：-3"><a href="#语法：-3" class="headerlink" title="语法："></a>语法：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images [OPTIONS] [REPOSITORY[:TAG]]</span><br></pre></td></tr></table></figure><blockquote><p>OPTIONS说明：</p><p><strong>-a :</strong>列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）；</p><p><strong>–digests :</strong>显示镜像的摘要信息；</p><p><strong>-f :</strong>显示满足条件的镜像；</p><p><strong>–format :</strong>指定返回值的模板文件；</p><p><strong>–no-trunc :</strong>显示完整的镜像信息；</p><p><strong>-q :</strong>只显示镜像ID。</p></blockquote><h3 id="实例：-1"><a href="#实例：-1" class="headerlink" title="实例："></a>实例：</h3><p>查看本地镜像列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker images </span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">mysql               latest              2dd01afbe8df        4 days ago          485MB</span><br><span class="line">ubuntu              latest              ea4c82dcd15a        10 days ago         85.8MB</span><br><span class="line">hello-world         latest              4ab4c602aa5e        7 weeks ago         1.84kB</span><br></pre></td></tr></table></figure><h3 id="Docker-rmi-命令"><a href="#Docker-rmi-命令" class="headerlink" title="Docker rmi 命令"></a>Docker rmi 命令</h3><p><strong>docker rmi: </strong>删除本地一个或多少镜像。</p><h4 id="语法：-4"><a href="#语法：-4" class="headerlink" title="语法："></a>语法：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rmi [OPTIONS] IMAGE [IMAGE...]</span><br></pre></td></tr></table></figure><blockquote><p>OPTIONS说明：</p><p><strong>-f :</strong>强制删除；</p><p><strong>–no-prune :</strong>不移除该镜像的过程镜像，默认移除；</p></blockquote><h3 id="Docker-export-命令"><a href="#Docker-export-命令" class="headerlink" title="Docker export 命令"></a>Docker export 命令</h3><p><strong>docker export :</strong>将文件系统作为一个tar归档文件导出到STDOUT。</p><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">export</span> [OPTIONS] CONTAINER</span><br></pre></td></tr></table></figure><blockquote><p>OPTIONS说明：</p><p><strong>-o :</strong>将输入内容写到文件。</p></blockquote><h3 id="Docker-import-命令"><a href="#Docker-import-命令" class="headerlink" title="Docker import 命令"></a>Docker import 命令</h3><p><strong>docker import :</strong> 从归档文件中创建镜像。</p><p>语法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker import [OPTIONS] file|URL|- [REPOSITORY[:TAG]]</span><br></pre></td></tr></table></figure><blockquote><p>OPTIONS说明：</p><p><strong>-c :</strong>应用docker 指令创建镜像；</p><p><strong>-m :</strong>提交时的说明文字；</p></blockquote><h3 id="Docker-save-命令"><a href="#Docker-save-命令" class="headerlink" title="Docker save 命令"></a>Docker save 命令</h3><p><strong>docker save :</strong> 将指定镜像保存成 tar 归档文件。</p><h4 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker save [OPTIONS] IMAGE [IMAGE...]</span><br></pre></td></tr></table></figure><blockquote><p>OPTIONS说明：</p><p><strong>-o :</strong>输出到的文件。</p></blockquote><h3 id="Docker-load命令"><a href="#Docker-load命令" class="headerlink" title="Docker load命令"></a>Docker load命令</h3><p>docker load:从归档文件中导入镜像。</p><h4 id="语法：-5"><a href="#语法：-5" class="headerlink" title="语法："></a>语法：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker load [OPTIONS]</span><br></pre></td></tr></table></figure><blockquote><p>Options:<br>-i: 从tar存档文件读取的输入字符串，而不是STDIN<br>-q: Suppress the load output</p></blockquote><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Docker容器管理</title>
      <link href="/2018/10/29/2018-10-29-docker-vessel/"/>
      <url>/2018/10/29/2018-10-29-docker-vessel/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h3><p>启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（<code>stopped</code>）的容器重新启动。</p><h3 id="新建并启动"><a href="#新建并启动" class="headerlink" title="新建并启动"></a>新建并启动</h3><p>所需要的命令主要为 <code>docker run</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker run -it ubuntu</span><br><span class="line">root@c964215eb3c2:/<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>其中，<code>-t</code> 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上， <code>-i</code> 则让容器的标准输入保持打开。</p><p>在交互模式下，用户可以通过所创建的终端来输入命令，例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@c964215eb3c2:/<span class="comment"># ls</span></span><br><span class="line">bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">root@c964215eb3c2:/<span class="comment"># dir</span></span><br><span class="line">bin  boot  devetc  home  liblib64  media  mnt  optproc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">root@c964215eb3c2:/<span class="comment"># pwd</span></span><br><span class="line">/</span><br></pre></td></tr></table></figure><p>当利用 <code>docker run</code> 来创建容器时，Docker 在后台运行的标准操作包括：</p><ul><li>检查本地是否存在指定的镜像，不存在就从公有仓库下载</li><li>利用镜像创建并启动一个容器</li><li>分配一个文件系统，并在只读的镜像层外面挂载一层可读写层</li><li>从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去</li><li>从地址池配置一个 ip 地址给容器</li><li>执行用户指定的应用程序</li><li>执行完毕后容器被终止</li></ul><h3 id="启动已终止容器"><a href="#启动已终止容器" class="headerlink" title="启动已终止容器"></a>启动已终止容器</h3><p>所需要的命令主要为：<code>docker container start</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker container start -i  trusting_boyd</span><br><span class="line">root@c964215eb3c2:/<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>其中： <code>-i</code> 则让容器的标准输入保持打开。</p><h3 id="终止容器"><a href="#终止容器" class="headerlink" title="终止容器"></a>终止容器</h3><p>可以使用 <code>docker container stop</code> 来终止一个运行中的容器。</p><p>此外，当 Docker 容器中指定的应用终结时，容器也自动终止。</p><p>当只启动了一个终端的容器，用户通过 <code>exit</code> 命令或 <code>Ctrl+d</code> 来退出终端时，所创建的容器立刻终止。</p><p>终止状态的容器可以用 <code>docker container ls -a</code> 命令看到。例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker container ls -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                          PORTS               NAMES</span><br><span class="line">c964215eb3c2        ubuntu              <span class="string">"/bin/bash"</span>         12 minutes ago      Exited (0) About a minute ago                       trusting_boyd</span><br></pre></td></tr></table></figure><p>处于终止状态的容器，可以通过 <code>docker container start</code> 命令来重新启动。</p><p>此外，<code>docker container restart</code> 命令会将一个运行态的容器终止，然后再重新启动它。</p><h3 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h3><p>在使用 <code>-d</code> 参数时，容器启动后会进入后台。</p><p>某些时候需要进入容器进行操作，包括使用 <code>docker attach</code> 命令或 <code>docker exec</code> 命令，推荐大家使用 <code>docker exec</code> 命令，原因会在下面说明。</p><h3 id="attach-命令"><a href="#attach-命令" class="headerlink" title="attach 命令"></a><code>attach</code> 命令</h3><p><code>docker attach</code> 是 Docker 自带的命令。下面示例如何使用该命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker run -dit ubuntu</span><br><span class="line">aaa535c389754179db273c48bafbe8b2c514cb0699f5e77b1249394715965f5f</span><br><span class="line">seven@seven:~$ sudo docker container ls</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span><br><span class="line">aaa535c38975        ubuntu              <span class="string">"/bin/bash"</span>         19 seconds ago      Up 17 seconds                           vigilant_cori</span><br><span class="line">seven@seven:~$ sudo docker attach vigilant_cori</span><br><span class="line">root@aaa535c38975:/<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>或者进入我们刚创建那个容器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker container start trusting_boyd</span><br><span class="line">trusting_boyd</span><br><span class="line">seven@seven:~$ sudo docker attach trusting_boyd</span><br><span class="line">root@c964215eb3c2:/<span class="comment">#</span></span><br></pre></td></tr></table></figure><h3 id="exec-命令"><a href="#exec-命令" class="headerlink" title="exec 命令"></a><code>exec</code> 命令</h3><h4 id="i-t-参数"><a href="#i-t-参数" class="headerlink" title="-i -t 参数"></a>-i -t 参数</h4><p><code>docker exec</code> 后边可以跟多个参数，这里主要说明 <code>-i</code> <code>-t</code> 参数。</p><p>只用 <code>-i</code> 参数时，由于没有分配伪终端，界面没有我们熟悉的 Linux 命令提示符，但命令执行结果仍然可以返回。</p><p>当 <code>-i</code> <code>-t</code> 参数一起使用时，则可以看到我们熟悉的 Linux 命令提示符。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker <span class="built_in">exec</span> -i trusting_boyd  ls </span><br><span class="line">bin</span><br><span class="line">boot</span><br><span class="line">dev</span><br><span class="line">etc</span><br><span class="line">home</span><br><span class="line">lib</span><br><span class="line">lib64</span><br><span class="line">media</span><br><span class="line">mnt</span><br><span class="line">opt</span><br><span class="line">proc</span><br><span class="line">root</span><br><span class="line">run</span><br><span class="line">sbin</span><br><span class="line">srv</span><br><span class="line">sys</span><br><span class="line">tmp</span><br><span class="line">usr</span><br><span class="line">var</span><br></pre></td></tr></table></figure><h3 id="导出容器"><a href="#导出容器" class="headerlink" title="导出容器"></a>导出容器</h3><p>如果要导出本地某个容器，可以使用 <code>docker export</code> 命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~/demo$ sudo docker <span class="built_in">export</span> trusting_boyd &gt; ubuntu.tar</span><br><span class="line">seven@seven:~/demo$ ls</span><br><span class="line">ubuntu.tar</span><br></pre></td></tr></table></figure><p>这样将导出容器快照到本地文件。</p><h3 id="导入容器"><a href="#导入容器" class="headerlink" title="导入容器"></a>导入容器</h3><p>可以使用 <code>docker import</code> 从容器快照文件中再导入为镜像，例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~/demo$ cat ubuntu.tar | sudo docker import - <span class="built_in">test</span>/ubuntu:v1.0</span><br><span class="line">sha256:2d78a6138d933a64fef619687dfb31f4323f607e35c980d1e70ec5a79a7cb39f</span><br><span class="line">seven@seven:~/demo$ sudo docker images </span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED              SIZE</span><br><span class="line"><span class="built_in">test</span>/ubuntu         v1.0                2d78a6138d93        About a minute ago   69.8MB</span><br><span class="line">mysql               latest              2dd01afbe8df        4 days ago           485MB</span><br><span class="line">ubuntu              latest              ea4c82dcd15a        10 days ago          85.8MB</span><br><span class="line">hello-world         latest              4ab4c602aa5e        7 weeks ago          1.84kB</span><br></pre></td></tr></table></figure><p>此外，也可以通过指定 URL 或者某个目录来导入，例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker import http://example.com/exampleimage.tgz example/imagerepo</span><br></pre></td></tr></table></figure><p><em>注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。</em></p><h3 id="删除容器"><a href="#删除容器" class="headerlink" title="删除容器"></a>删除容器</h3><p>可以使用 <code>docker container rm</code> 来删除一个处于终止状态的容器。例如</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker container rm  trusting_newton</span><br><span class="line">trusting_newton</span><br></pre></td></tr></table></figure><p>如果要删除一个运行中的容器，可以添加 <code>-f</code> 参数。Docker 会发送 <code>SIGKILL</code> 信号给容器。</p><h3 id="清理所有容器"><a href="#清理所有容器" class="headerlink" title="清理所有容器"></a>清理所有容器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~/demo$  sudo docker rm -f $(sudo docker ps -q -a)</span><br><span class="line">aaa535c38975</span><br><span class="line">c964215eb3c2</span><br></pre></td></tr></table></figure><h3 id="容器安装软件"><a href="#容器安装软件" class="headerlink" title="容器安装软件"></a>容器安装软件</h3><p>就比如安装vim软件， 在使用docker容器时，有时候里边没有安装vim，敲vim命令时提示说：vim: command not found，这个时候就需要安装vim：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install vim</span><br><span class="line"> Reading package lists... Done</span><br><span class="line"> Building dependency tree       </span><br><span class="line"> Reading state information... Done</span><br><span class="line"> E: Unable to locate package vim</span><br></pre></td></tr></table></figure><p>解决这个问题的方法：执行更新命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get update</span><br></pre></td></tr></table></figure><p>这个命令的作用是：<strong>同步 /etc/apt/sources.list 和 /etc/apt/sources.list.d 中列出的源的索引</strong>，这样才能获取到最新的软件包。</p><p>等更新完毕以后再敲命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">root@4c7345f38eab:~/.pip<span class="comment"># apt-get install vim</span></span><br><span class="line">Reading package lists... Done</span><br><span class="line">Building dependency tree       </span><br><span class="line">Reading state information... Done</span><br><span class="line">The following additional packages will be installed:</span><br><span class="line">  libgpm2 vim-common vim-runtime xxd</span><br><span class="line">Suggested packages:</span><br><span class="line">  gpm ctags vim-doc vim-scripts</span><br><span class="line">The following NEW packages will be installed:</span><br><span class="line">  libgpm2 vim vim-common vim-runtime xxd</span><br><span class="line">0 upgraded, 5 newly installed, 0 to remove and 1 not upgraded.</span><br><span class="line">Need to get 6766 kB of archives.</span><br><span class="line">After this operation, 31.2 MB of additional disk space will be used.</span><br><span class="line">Do you want to <span class="built_in">continue</span>? [Y/n] y</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>接下来你就可以在docker容器里面，安装你所需要的软件了。</p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Docker配置TensorFlow-CPU开发环境</title>
      <link href="/2018/10/29/2018-10-29-docker-TensorFlow-cpu/"/>
      <url>/2018/10/29/2018-10-29-docker-TensorFlow-cpu/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="查找TensorFlow镜像"><a href="#查找TensorFlow镜像" class="headerlink" title="查找TensorFlow镜像"></a>查找TensorFlow镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker search tensorflow  -s 10</span><br><span class="line">NAME                                DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED</span><br><span class="line">tensorflow/tensorflow               Official docker images <span class="keyword">for</span> deep learning fra…   1150                                    </span><br><span class="line">jupyter/tensorflow-notebook         Jupyter Notebook Scientific Python Stack w/ …   86                                      </span><br><span class="line">xblaster/tensorflow-jupyter         Dockerized Jupyter with tensorflow              50                                      [OK]</span><br><span class="line">tensorflow/serving                  Official images <span class="keyword">for</span> TensorFlow Serving (http…   22                                      </span><br><span class="line">floydhub/tensorflow                 tensorflow                                      14                                      [OK]</span><br><span class="line">bitnami/tensorflow-serving          Bitnami Docker Image <span class="keyword">for</span> TensorFlow Serving     13                                      [OK]</span><br></pre></td></tr></table></figure><h3 id="PULL-TensorFlow镜像"><a href="#PULL-TensorFlow镜像" class="headerlink" title="PULL TensorFlow镜像"></a>PULL TensorFlow镜像</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker run -it tensorflow/tensorflow</span><br><span class="line">Unable to find image <span class="string">'tensorflow/tensorflow:latest'</span> locally</span><br><span class="line">latest: Pulling from tensorflow/tensorflow</span><br><span class="line">3b37166ec614: Pull complete </span><br><span class="line">504facff238f: Pull complete </span><br><span class="line">ebbcacd28e10: Pull complete </span><br><span class="line">c7fb3351ecad: Pull complete </span><br><span class="line">2e3debadcbf7: Pull complete </span><br><span class="line">568ddecd541b: Pull complete </span><br><span class="line">cb5781b11958: Pull complete </span><br><span class="line">c6f383503f95: Pull complete </span><br><span class="line">71dcbb855dc9: Pull complete </span><br><span class="line">5cfd34784f24: Pull complete </span><br><span class="line">c26bfefb0572: Pull complete </span><br><span class="line">88302acc21c8: Pull complete </span><br><span class="line">4c2b848f6d49: Pull complete </span><br><span class="line">...</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> tensorflow/tensorflow:latest</span><br></pre></td></tr></table></figure><h3 id="创建TensorFlow容器–jupyter-notebook"><a href="#创建TensorFlow容器–jupyter-notebook" class="headerlink" title="创建TensorFlow容器–jupyter notebook"></a>创建TensorFlow容器–jupyter notebook</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ docker run --name seven-tensorflow -it -p 8888:8888 -v ~/Docker-tensorflow:/demo tensorflow/tensorflow</span><br></pre></td></tr></table></figure><blockquote><p><strong>–name</strong>：创建的容器名，即seven-tensorflow</p><p><strong>-it</strong>：保留命令行运行</p><p><strong>p 8888:8888</strong>：将本地的8888端口和<code>http://localhost:8888/</code>映射</p><p><strong>-v ~/Docker-tensorflow:/demo</strong>:将本地的~/Docker-tensorflow挂载到容器内的/demo下</p><p><strong>tensorflow/tensorflow</strong> ：默认是tensorflow/tensorflow:latest,指定使用的镜像</p></blockquote><p>输入以上命令后，默认容器就被启动了，命令行显示</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[I 13:16:07.331 NotebookApp] Writing notebook server cookie secret to /root/.<span class="built_in">local</span>/share/jupyter/runtime/notebook_cookie_secret</span><br><span class="line">[I 13:16:07.347 NotebookApp] Serving notebooks from <span class="built_in">local</span> directory: /notebooks</span><br><span class="line">[I 13:16:07.347 NotebookApp] The Jupyter Notebook is running at:</span><br><span class="line">[I 13:16:07.347 NotebookApp] http://(7191961747da or 127.0.0.1):8888/?token=2734a3a619e376f876eb72ba562852fa79efd94a5f3f871a</span><br><span class="line">[I 13:16:07.347 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span><br><span class="line">[C 13:16:07.347 NotebookApp] </span><br><span class="line">    </span><br><span class="line">    Copy/paste this URL into your browser when you connect <span class="keyword">for</span> the first time,</span><br><span class="line">    to login with a token:</span><br><span class="line">        http://(7191961747da or 127.0.0.1):8888/?token=2734a3a619e376f876eb72ba562852fa79efd94a5f3f871a</span><br></pre></td></tr></table></figure><p>拷贝带token的URL在浏览器打开</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://(7191961747da or 127.0.0.1):8888/?token=257ce32bf00cd16dee9019462f8753a3b06154618885d682</span><br></pre></td></tr></table></figure><p><img src="/images/docker/7.png" alt="2"></p><p>接下来，你就可以在jupyter notebook上运行你的TensorFlow代码啦</p><p><img src="/images/docker/8.png" alt="2"></p><h3 id="关闭容器"><a href="#关闭容器" class="headerlink" title="关闭容器"></a>关闭容器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker stop seven-tensorflow  </span><br><span class="line">seven-tensorflow</span><br></pre></td></tr></table></figure><h3 id="再次打开容器"><a href="#再次打开容器" class="headerlink" title="再次打开容器"></a>再次打开容器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker start seven-tensortflow</span><br></pre></td></tr></table></figure><p>如果不喜欢用Jupyter Notebook，我们也可以创建基于命令行的容器</p><h3 id="基于命令行的TensorFlow容器"><a href="#基于命令行的TensorFlow容器" class="headerlink" title="基于命令行的TensorFlow容器"></a>基于命令行的TensorFlow容器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ sudo docker run -it --name bash_tensorflow tensorflow/tensorflow /bin/bash </span><br><span class="line">root@cb158fab4040:/notebooks<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>这样我们就创建了名为bash_tensorflow的容器， 并已经进入到容器</p><h3 id="验证TensorFlow环境"><a href="#验证TensorFlow环境" class="headerlink" title="验证TensorFlow环境"></a>验证TensorFlow环境</h3><p>默认的是python2.x安装的TensorFlowCPU版</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@cb158fab4040:/notebooks<span class="comment"># python</span></span><br><span class="line">Python 2.7.12 (default, Dec  4 2017, 14:50:18) </span><br><span class="line">[GCC 5.4.0 20160609] on linux2</span><br><span class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> or <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br><span class="line">&gt;&gt;&gt; import tensorflow </span><br><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span> tf.__version__</span><br><span class="line">1.11.0</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>计算机视觉-openCV：基于Hog+SVM小狮子识别</title>
      <link href="/2018/10/27/2018-10-27-openCV-svm-hog/"/>
      <url>/2018/10/27/2018-10-27-openCV-svm-hog/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="openCV：基于Hog-SVM小狮子识别"><a href="#openCV：基于Hog-SVM小狮子识别" class="headerlink" title="openCV：基于Hog+SVM小狮子识别"></a>openCV：基于Hog+SVM小狮子识别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 1 par</span></span><br><span class="line">PosNum = <span class="number">820</span></span><br><span class="line">NegNum = <span class="number">1931</span></span><br><span class="line">winSize = (<span class="number">64</span>,<span class="number">128</span>)</span><br><span class="line">blockSize = (<span class="number">16</span>,<span class="number">16</span>)<span class="comment"># 105</span></span><br><span class="line">blockStride = (<span class="number">8</span>,<span class="number">8</span>)<span class="comment">#4 cell</span></span><br><span class="line">cellSize = (<span class="number">8</span>,<span class="number">8</span>)</span><br><span class="line">nBin = <span class="number">9</span><span class="comment">#9 bin 3780</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 hog create hog 1 win 2 block 3 blockStride 4 cell 5 bin</span></span><br><span class="line">hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nBin)</span><br><span class="line"><span class="comment"># 3 svm</span></span><br><span class="line">svm = cv2.ml.SVM_create()</span><br><span class="line"><span class="comment"># 4 computer hog</span></span><br><span class="line">featureNum = int(((<span class="number">128</span><span class="number">-16</span>)/<span class="number">8</span>+<span class="number">1</span>)*((<span class="number">64</span><span class="number">-16</span>)/<span class="number">8</span>+<span class="number">1</span>)*<span class="number">4</span>*<span class="number">9</span>) <span class="comment">#3780</span></span><br><span class="line">featureArray = np.zeros(((PosNum+NegNum),featureNum),np.float32)</span><br><span class="line">labelArray = np.zeros(((PosNum+NegNum),<span class="number">1</span>),np.int32)</span><br><span class="line"><span class="comment"># svm 监督学习 样本 标签 svm -》image hog  </span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,PosNum):</span><br><span class="line">    fileName = <span class="string">'pos/'</span>+str(i+<span class="number">1</span>)+<span class="string">'.jpg'</span></span><br><span class="line">    img = cv2.imread(fileName)</span><br><span class="line">    hist = hog.compute(img,(<span class="number">8</span>,<span class="number">8</span>))<span class="comment"># 3780</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,featureNum):</span><br><span class="line">        featureArray[i,j] = hist[j]</span><br><span class="line">    <span class="comment"># featureArray hog [1,:] hog1 [2,:]hog2 </span></span><br><span class="line">    labelArray[i,<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 正样本 label 1</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,NegNum):</span><br><span class="line">    fileName = <span class="string">'neg/'</span>+str(i+<span class="number">1</span>)+<span class="string">'.jpg'</span></span><br><span class="line">    img = cv2.imread(fileName)</span><br><span class="line">    hist = hog.compute(img,(<span class="number">8</span>,<span class="number">8</span>))<span class="comment"># 3780</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,featureNum):</span><br><span class="line">        featureArray[i+PosNum,j] = hist[j]</span><br><span class="line">    labelArray[i+PosNum,<span class="number">0</span>] = <span class="number">-1</span></span><br><span class="line"><span class="comment"># 负样本 label -1</span></span><br><span class="line">svm.setType(cv2.ml.SVM_C_SVC)</span><br><span class="line">svm.setKernel(cv2.ml.SVM_LINEAR)</span><br><span class="line">svm.setC(<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 6 train</span></span><br><span class="line">ret = svm.train(featureArray,cv2.ml.ROW_SAMPLE,labelArray)</span><br><span class="line"><span class="comment"># 7 myHog ：《-myDetect</span></span><br><span class="line"><span class="comment"># myDetect-《resultArray  rho</span></span><br><span class="line"><span class="comment"># myHog-》detectMultiScale</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 7 检测  核心：create Hog -》 myDetect—》array-》</span></span><br><span class="line"><span class="comment"># resultArray-》resultArray = -1*alphaArray*supportVArray</span></span><br><span class="line"><span class="comment"># rho-》svm-〉svm.train</span></span><br><span class="line">alpha = np.zeros((<span class="number">1</span>),np.float32)</span><br><span class="line">rho = svm.getDecisionFunction(<span class="number">0</span>,alpha)</span><br><span class="line">print(rho)</span><br><span class="line">print(alpha)</span><br><span class="line">alphaArray = np.zeros((<span class="number">1</span>,<span class="number">1</span>),np.float32)</span><br><span class="line">supportVArray = np.zeros((<span class="number">1</span>,featureNum),np.float32)</span><br><span class="line">resultArray = np.zeros((<span class="number">1</span>,featureNum),np.float32)</span><br><span class="line">alphaArray[<span class="number">0</span>,<span class="number">0</span>] = alpha</span><br><span class="line">resultArray = <span class="number">-1</span>*alphaArray*supportVArray</span><br><span class="line"><span class="comment"># detect</span></span><br><span class="line">myDetect = np.zeros((<span class="number">3781</span>),np.float32)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">3780</span>):</span><br><span class="line">    myDetect[i] = resultArray[<span class="number">0</span>,i]</span><br><span class="line">myDetect[<span class="number">3780</span>] = rho[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># rho svm （判决）</span></span><br><span class="line">myHog = cv2.HOGDescriptor()</span><br><span class="line">myHog.setSVMDetector(myDetect)</span><br><span class="line"><span class="comment"># load </span></span><br><span class="line">imageSrc = cv2.imread(<span class="string">'Test2.jpg'</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># (8,8) win </span></span><br><span class="line">objs = myHog.detectMultiScale(imageSrc,<span class="number">0</span>,(<span class="number">8</span>,<span class="number">8</span>),(<span class="number">32</span>,<span class="number">32</span>),<span class="number">1.05</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># xy wh 三维 最后一维</span></span><br><span class="line">x = int(objs[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">y = int(objs[<span class="number">0</span>][<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line">w = int(objs[<span class="number">0</span>][<span class="number">0</span>][<span class="number">2</span>])</span><br><span class="line">h = int(objs[<span class="number">0</span>][<span class="number">0</span>][<span class="number">3</span>])</span><br><span class="line"><span class="comment"># 绘制展示</span></span><br><span class="line">cv2.rectangle(imageSrc,(x,y),(x+w,y+h),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,imageSrc)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/48.png" alt="1"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>计算机视觉-openCV视频处理</title>
      <link href="/2018/10/27/2018-10-27-openCV-video/"/>
      <url>/2018/10/27/2018-10-27-openCV-video/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="openCV视频分解图片"><a href="#openCV视频分解图片" class="headerlink" title="openCV视频分解图片"></a>openCV视频分解图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">cap = cv2.VideoCapture(<span class="string">"1.mp4"</span>)<span class="comment"># 获取一个视频打开cap 1 file name</span></span><br><span class="line">isOpened = cap.isOpened<span class="comment"># 判断是否打开‘</span></span><br><span class="line">print(isOpened)</span><br><span class="line">fps = cap.get(cv2.CAP_PROP_FPS)<span class="comment">#帧率</span></span><br><span class="line">width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))<span class="comment">#w h</span></span><br><span class="line">height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))</span><br><span class="line">print(fps,width,height)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(isOpened):</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">10</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        i = i+<span class="number">1</span></span><br><span class="line">    (flag,frame) = cap.read()<span class="comment"># 读取每一张 flag frame </span></span><br><span class="line">    fileName = <span class="string">'image'</span>+str(i)+<span class="string">'.jpg'</span></span><br><span class="line">    print(fileName)</span><br><span class="line">    <span class="keyword">if</span> flag == <span class="keyword">True</span>:</span><br><span class="line">        cv2.imwrite(fileName,frame,[cv2.IMWRITE_JPEG_QUALITY,<span class="number">100</span>])</span><br><span class="line">print(<span class="string">'end!'</span>)</span><br></pre></td></tr></table></figure><h3 id="openCV图片合成视频"><a href="#openCV图片合成视频" class="headerlink" title="openCV图片合成视频"></a>openCV图片合成视频</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">img = cv2.imread(<span class="string">'image1.jpg'</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">size = (imgInfo[<span class="number">1</span>],imgInfo[<span class="number">0</span>])</span><br><span class="line">print(size)</span><br><span class="line">videoWrite = cv2.VideoWriter(<span class="string">'2.mp4'</span>,<span class="number">-1</span>,<span class="number">5</span>,size)<span class="comment"># 写入对象 1 file name</span></span><br><span class="line"><span class="comment"># 2 编码器 3 帧率 4 size</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>):</span><br><span class="line">    fileName = <span class="string">'image'</span>+str(i)+<span class="string">'.jpg'</span></span><br><span class="line">    img = cv2.imread(fileName)</span><br><span class="line">    videoWrite.write(img)<span class="comment"># 写入方法 1 jpg data</span></span><br><span class="line">print(<span class="string">'end!'</span>)</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>计算机视觉-openCV：SVM支持向量机</title>
      <link href="/2018/10/27/2018-10-27-openCV-svm/"/>
      <url>/2018/10/27/2018-10-27-openCV-svm/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="openCV：SVM支持向量机"><a href="#openCV：SVM支持向量机" class="headerlink" title="openCV：SVM支持向量机"></a>openCV：SVM支持向量机</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 身高体重 训练 预测 </span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#1 准备data</span></span><br><span class="line">rand1 = np.array([[<span class="number">155</span>,<span class="number">48</span>],[<span class="number">159</span>,<span class="number">50</span>],[<span class="number">164</span>,<span class="number">53</span>],[<span class="number">168</span>,<span class="number">56</span>],[<span class="number">172</span>,<span class="number">60</span>]])</span><br><span class="line">rand2 = np.array([[<span class="number">152</span>,<span class="number">53</span>],[<span class="number">156</span>,<span class="number">55</span>],[<span class="number">160</span>,<span class="number">56</span>],[<span class="number">172</span>,<span class="number">64</span>],[<span class="number">176</span>,<span class="number">65</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 label</span></span><br><span class="line">label = np.array([[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 data</span></span><br><span class="line">data = np.vstack((rand1,rand2))</span><br><span class="line">data = np.array(data,dtype=<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># svm 所有的数据都要有label</span></span><br><span class="line"><span class="comment"># [155,48] -- 0 女生 [152,53] ---1  男生</span></span><br><span class="line"><span class="comment"># 监督学习 0 负样本 1 正样本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 训练</span></span><br><span class="line">svm = cv2.ml.SVM_create() <span class="comment"># ml  机器学习模块 SVM_create() 创建</span></span><br><span class="line"><span class="comment"># 属性设置</span></span><br><span class="line">svm.setType(cv2.ml.SVM_C_SVC) <span class="comment"># svm type</span></span><br><span class="line">svm.setKernel(cv2.ml.SVM_LINEAR) <span class="comment"># line</span></span><br><span class="line">svm.setC(<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">result = svm.train(data,cv2.ml.ROW_SAMPLE,label)</span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">pt_data = np.vstack([[<span class="number">167</span>,<span class="number">55</span>],[<span class="number">162</span>,<span class="number">57</span>]]) <span class="comment">#0 女生 1男生</span></span><br><span class="line">pt_data = np.array(pt_data,dtype=<span class="string">'float32'</span>)</span><br><span class="line">print(pt_data)</span><br><span class="line">(par1,par2) = svm.predict(pt_data)</span><br><span class="line">print(par2)</span><br></pre></td></tr></table></figure><h3 id="预测结果："><a href="#预测结果：" class="headerlink" title="预测结果："></a>预测结果：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">167.</span>  <span class="number">55.</span>]</span><br><span class="line"> [<span class="number">162.</span>  <span class="number">57.</span>]]</span><br><span class="line">[[<span class="number">0.</span>]</span><br><span class="line"> [<span class="number">1.</span>]]</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>计算机视觉-openCV图像美化</title>
      <link href="/2018/10/27/2018-10-27-openCV-whitening/"/>
      <url>/2018/10/27/2018-10-27-openCV-whitening/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="openCV彩色图片直方图"><a href="#openCV彩色图片直方图" class="headerlink" title="openCV彩色图片直方图"></a>openCV彩色图片直方图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ImageHist</span><span class="params">(image,type)</span>:</span></span><br><span class="line">    color = (<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>)</span><br><span class="line">    windowName = <span class="string">'Gray'</span></span><br><span class="line">    <span class="keyword">if</span> type == <span class="number">31</span>:</span><br><span class="line">        color = (<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">        windowName = <span class="string">'B Hist'</span></span><br><span class="line">    <span class="keyword">elif</span> type == <span class="number">32</span>:</span><br><span class="line">        color = (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>)</span><br><span class="line">        windowName = <span class="string">'G Hist'</span></span><br><span class="line">    <span class="keyword">elif</span> type == <span class="number">33</span>:</span><br><span class="line">        color = (<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>)</span><br><span class="line">        windowName = <span class="string">'R Hist'</span></span><br><span class="line">    <span class="comment"># 1 image 2 [0] 3 mask None 4 256 5 0-255</span></span><br><span class="line">    hist = cv2.calcHist([image],[<span class="number">0</span>],<span class="keyword">None</span>,[<span class="number">256</span>],[<span class="number">0.0</span>,<span class="number">255.0</span>])</span><br><span class="line">    minV,maxV,minL,maxL = cv2.minMaxLoc(hist)  <span class="comment"># 获取像素的最大值和最小值以便后面归一化处理</span></span><br><span class="line">    histImg = np.zeros([<span class="number">256</span>,<span class="number">256</span>,<span class="number">3</span>],np.uint8)</span><br><span class="line">    <span class="comment"># 数据归一化处理</span></span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">256</span>):</span><br><span class="line">        intenNormal = int(hist[h]*<span class="number">256</span>/maxV)</span><br><span class="line">        cv2.line(histImg,(h,<span class="number">256</span>),(h,<span class="number">256</span>-intenNormal),color)</span><br><span class="line">    cv2.imshow(windowName,histImg)</span><br><span class="line">    <span class="keyword">return</span> histImg</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">channels = cv2.split(img)<span class="comment"># RGB - R G B</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">3</span>):</span><br><span class="line">    ImageHist(channels[i],<span class="number">31</span>+i)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/33.png" alt="1"></p><h3 id="openCV直方图均衡化-灰度"><a href="#openCV直方图均衡化-灰度" class="headerlink" title="openCV直方图均衡化-灰度"></a>openCV直方图均衡化-灰度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) <span class="comment"># 图片灰度化</span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,gray)</span><br><span class="line">dst = cv2.equalizeHist(gray) <span class="comment"># api 完成直方图均衡化</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/34.png" alt="1"></p><h3 id="openCV直方图均衡化-彩色"><a href="#openCV直方图均衡化-彩色" class="headerlink" title="openCV直方图均衡化-彩色"></a>openCV直方图均衡化-彩色</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">(b,g,r) = cv2.split(img) <span class="comment"># 通道分解</span></span><br><span class="line"><span class="comment"># 图片单通道处理</span></span><br><span class="line">bH = cv2.equalizeHist(b)</span><br><span class="line">gH = cv2.equalizeHist(g)</span><br><span class="line">rH = cv2.equalizeHist(r)</span><br><span class="line">result = cv2.merge((bH,gH,rH))<span class="comment"># 通道合成</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,result)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/35.png" alt="1"></p><h3 id="openCV直方图均衡化-YUV"><a href="#openCV直方图均衡化-YUV" class="headerlink" title="openCV直方图均衡化-YUV"></a>openCV直方图均衡化-YUV</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgYUV = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb) <span class="comment">#  </span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">channelYUV = cv2.split(imgYUV) <span class="comment"># 图片分解</span></span><br><span class="line">channelYUV[<span class="number">0</span>] = cv2.equalizeHist(channelYUV[<span class="number">0</span>]) <span class="comment"># 直方图均衡化</span></span><br><span class="line">channels = cv2.merge(channelYUV) <span class="comment"># 合成</span></span><br><span class="line">result = cv2.cvtColor(channels,cv2.COLOR_YCrCb2BGR)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,result)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/36.png" alt="1"></p><h3 id="openCV图片修补"><a href="#openCV图片修补" class="headerlink" title="openCV图片修补"></a>openCV图片修补</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'damaged.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">paint = np.zeros((height,width,<span class="number">1</span>),np.uint8)</span><br><span class="line"><span class="comment"># 描绘图片坏的数组</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">200</span>,<span class="number">300</span>):</span><br><span class="line">    paint[i,<span class="number">200</span>] = <span class="number">255</span></span><br><span class="line">    paint[i,<span class="number">200</span>+<span class="number">1</span>] = <span class="number">255</span></span><br><span class="line">    paint[i,<span class="number">200</span><span class="number">-1</span>] = <span class="number">255</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">150</span>,<span class="number">250</span>):</span><br><span class="line">    paint[<span class="number">250</span>,i] = <span class="number">255</span></span><br><span class="line">    paint[<span class="number">250</span>+<span class="number">1</span>,i] = <span class="number">255</span></span><br><span class="line">    paint[<span class="number">250</span><span class="number">-1</span>,i] = <span class="number">255</span></span><br><span class="line">cv2.imshow(<span class="string">'paint'</span>,paint)</span><br><span class="line"><span class="comment">#1 src 2 mask</span></span><br><span class="line">imgDst = cv2.inpaint(img,paint,<span class="number">3</span>,cv2.INPAINT_TELEA)</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'image'</span>,imgDst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/37.png" alt="2"></p><h3 id="openCV灰度直方图"><a href="#openCV灰度直方图" class="headerlink" title="openCV灰度直方图"></a>openCV灰度直方图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本质：统计每个像素灰度 出现的概率 0-255 p</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) <span class="comment"># 图片灰度化</span></span><br><span class="line">count = np.zeros(<span class="number">256</span>,np.float)</span><br><span class="line"><span class="comment"># 获取灰度像素</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        pixel = gray[i,j]</span><br><span class="line">        index = int(pixel)</span><br><span class="line">        count[index] = count[index]+<span class="number">1</span></span><br><span class="line"><span class="comment"># 灰度等级出现的概率</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">255</span>):</span><br><span class="line">    count[i] = count[i]/(height*width)</span><br><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">255</span>,<span class="number">256</span>)</span><br><span class="line">y = count</span><br><span class="line">plt.bar(x,y,<span class="number">0.9</span>,alpha=<span class="number">1</span>,color=<span class="string">'b'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/cv/38.png" alt="2"></p><h3 id="openCV彩色直方图"><a href="#openCV彩色直方图" class="headerlink" title="openCV彩色直方图"></a>openCV彩色直方图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">count_b = np.zeros(<span class="number">256</span>,np.float)</span><br><span class="line">count_g = np.zeros(<span class="number">256</span>,np.float)</span><br><span class="line">count_r = np.zeros(<span class="number">256</span>,np.float)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        (b,g,r) = img[i,j]</span><br><span class="line">        index_b = int(b)</span><br><span class="line">        index_g = int(g)</span><br><span class="line">        index_r = int(r)</span><br><span class="line">        count_b[index_b] = count_b[index_b]+<span class="number">1</span></span><br><span class="line">        count_g[index_g] = count_g[index_g]+<span class="number">1</span></span><br><span class="line">        count_r[index_r] = count_r[index_r]+<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">256</span>):</span><br><span class="line">    count_b[i] = count_b[i]/(height*width)</span><br><span class="line">    count_g[i] = count_g[i]/(height*width)</span><br><span class="line">    count_r[i] = count_r[i]/(height*width)</span><br><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">255</span>,<span class="number">256</span>)</span><br><span class="line">y1 = count_b</span><br><span class="line">plt.figure()</span><br><span class="line">plt.bar(x,y1,<span class="number">0.9</span>,alpha=<span class="number">1</span>,color=<span class="string">'b'</span>)</span><br><span class="line">y2 = count_g</span><br><span class="line">plt.figure()</span><br><span class="line">plt.bar(x,y2,<span class="number">0.9</span>,alpha=<span class="number">1</span>,color=<span class="string">'g'</span>)</span><br><span class="line">y3 = count_r</span><br><span class="line">plt.figure()</span><br><span class="line">plt.bar(x,y3,<span class="number">0.9</span>,alpha=<span class="number">1</span>,color=<span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/cv/39.png" alt="3"></p><h3 id="openCV灰度直方图均衡化"><a href="#openCV灰度直方图均衡化" class="headerlink" title="openCV灰度直方图均衡化"></a>openCV灰度直方图均衡化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,gray)</span><br><span class="line">count = np.zeros(<span class="number">256</span>,np.float)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        pixel = gray[i,j]</span><br><span class="line">        index = int(pixel)</span><br><span class="line">        count[index] = count[index]+<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">255</span>):</span><br><span class="line">    count[i] = count[i]/(height*width)</span><br><span class="line"><span class="comment">#计算累计概率</span></span><br><span class="line">sum1 = float(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">256</span>):</span><br><span class="line">    sum1 = sum1+count[i]</span><br><span class="line">    count[i] = sum1</span><br><span class="line"><span class="comment">#print(count)</span></span><br><span class="line"><span class="comment"># 计算映射表</span></span><br><span class="line">map1 = np.zeros(<span class="number">256</span>,np.uint16)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">256</span>):</span><br><span class="line">    map1[i] = np.uint16(count[i]*<span class="number">255</span>)</span><br><span class="line"><span class="comment"># 映射</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        pixel = gray[i,j]</span><br><span class="line">        gray[i,j] = map1[pixel]</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,gray)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/40.png" alt="4"></p><h3 id="openCV彩色直方图均衡化"><a href="#openCV彩色直方图均衡化" class="headerlink" title="openCV彩色直方图均衡化"></a>openCV彩色直方图均衡化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line"></span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">count_b = np.zeros(<span class="number">256</span>,np.float)</span><br><span class="line">count_g = np.zeros(<span class="number">256</span>,np.float)</span><br><span class="line">count_r = np.zeros(<span class="number">256</span>,np.float)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        (b,g,r) = img[i,j]</span><br><span class="line">        index_b = int(b)</span><br><span class="line">        index_g = int(g)</span><br><span class="line">        index_r = int(r)</span><br><span class="line">        count_b[index_b] = count_b[index_b]+<span class="number">1</span></span><br><span class="line">        count_g[index_g] = count_g[index_g]+<span class="number">1</span></span><br><span class="line">        count_r[index_r] = count_r[index_r]+<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">255</span>):</span><br><span class="line">    count_b[i] = count_b[i]/(height*width)</span><br><span class="line">    count_g[i] = count_g[i]/(height*width)</span><br><span class="line">    count_r[i] = count_r[i]/(height*width)</span><br><span class="line"><span class="comment">#计算累计概率</span></span><br><span class="line">sum_b = float(<span class="number">0</span>)</span><br><span class="line">sum_g = float(<span class="number">0</span>)</span><br><span class="line">sum_r = float(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">256</span>):</span><br><span class="line">    sum_b = sum_b+count_b[i]</span><br><span class="line">    sum_g = sum_g+count_g[i]</span><br><span class="line">    sum_r = sum_r+count_r[i]</span><br><span class="line">    count_b[i] = sum_b</span><br><span class="line">    count_g[i] = sum_g</span><br><span class="line">    count_r[i] = sum_r</span><br><span class="line"><span class="comment">#print(count)</span></span><br><span class="line"><span class="comment"># 计算映射表</span></span><br><span class="line">map_b = np.zeros(<span class="number">256</span>,np.uint16)</span><br><span class="line">map_g = np.zeros(<span class="number">256</span>,np.uint16)</span><br><span class="line">map_r = np.zeros(<span class="number">256</span>,np.uint16)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">256</span>):</span><br><span class="line">    map_b[i] = np.uint16(count_b[i]*<span class="number">255</span>)</span><br><span class="line">    map_g[i] = np.uint16(count_g[i]*<span class="number">255</span>)</span><br><span class="line">    map_r[i] = np.uint16(count_r[i]*<span class="number">255</span>)</span><br><span class="line"><span class="comment"># 映射</span></span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        (b,g,r) = img[i,j]</span><br><span class="line">        b = map_b[b]</span><br><span class="line">        g = map_g[g]</span><br><span class="line">        r = map_r[r]</span><br><span class="line">        dst[i,j] = (b,g,r)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/41.png" alt="4"></p><h3 id="openCV亮度增强"><a href="#openCV亮度增强" class="headerlink" title="openCV亮度增强"></a>openCV亮度增强</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        (b,g,r) = img[i,j]</span><br><span class="line">        bb = int(b)+<span class="number">40</span></span><br><span class="line">        gg = int(g)+<span class="number">40</span></span><br><span class="line">        rr = int(r)+<span class="number">40</span></span><br><span class="line">        <span class="keyword">if</span> bb&gt;<span class="number">255</span>:</span><br><span class="line">            bb = <span class="number">255</span></span><br><span class="line">        <span class="keyword">if</span> gg&gt;<span class="number">255</span>:</span><br><span class="line">            gg = <span class="number">255</span></span><br><span class="line">        <span class="keyword">if</span> rr&gt;<span class="number">255</span>:</span><br><span class="line">            rr = <span class="number">255</span></span><br><span class="line">        dst[i,j] = (bb,gg,rr)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/42.png" alt="4"></p><h3 id="openCV磨皮美白-双边滤波"><a href="#openCV磨皮美白-双边滤波" class="headerlink" title="openCV磨皮美白-双边滤波"></a>openCV磨皮美白-双边滤波</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">img = cv2.imread(<span class="string">'1.png'</span>,<span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">dst = cv2.bilateralFilter(img,<span class="number">15</span>,<span class="number">35</span>,<span class="number">35</span>) <span class="comment"># 滤波函数</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/43.png" alt="4"></p><h3 id="openCV高斯滤波"><a href="#openCV高斯滤波" class="headerlink" title="openCV高斯滤波"></a>openCV高斯滤波</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image11.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">dst = cv2.GaussianBlur(img,(<span class="number">5</span>,<span class="number">5</span>),<span class="number">1.5</span>)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/44.png" alt="4"></p><h3 id="openCV均值滤波"><a href="#openCV均值滤波" class="headerlink" title="openCV均值滤波"></a>openCV均值滤波</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#均值 6*6 1 。 * 【6*6】/36 = mean -》P</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image11.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>,height<span class="number">-3</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">3</span>,width<span class="number">-3</span>):</span><br><span class="line">        sum_b = int(<span class="number">0</span>)</span><br><span class="line">        sum_g = int(<span class="number">0</span>)</span><br><span class="line">        sum_r = int(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">-3</span>,<span class="number">3</span>):<span class="comment">#-3 -2 -1 0 1 2</span></span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-3</span>,<span class="number">3</span>):</span><br><span class="line">                (b,g,r) = img[i+m,j+n]</span><br><span class="line">                sum_b = sum_b+int(b)</span><br><span class="line">                sum_g = sum_g+int(g)</span><br><span class="line">                sum_r = sum_r+int(r)</span><br><span class="line">            </span><br><span class="line">        b = np.uint8(sum_b/<span class="number">36</span>)</span><br><span class="line">        g = np.uint8(sum_g/<span class="number">36</span>)</span><br><span class="line">        r = np.uint8(sum_r/<span class="number">36</span>)</span><br><span class="line">        dst[i,j] = (b,g,r)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/45.png" alt="4"></p><h3 id="openCV中值滤波"><a href="#openCV中值滤波" class="headerlink" title="openCV中值滤波"></a>openCV中值滤波</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image11.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line">collect = np.zeros(<span class="number">9</span>,np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,height<span class="number">-1</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,width<span class="number">-1</span>):</span><br><span class="line">        k = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">-1</span>,<span class="number">2</span>):</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-1</span>,<span class="number">2</span>):</span><br><span class="line">                gray = img[i+m,j+n]</span><br><span class="line">                collect[k] = gray</span><br><span class="line">                k = k+<span class="number">1</span></span><br><span class="line">        <span class="comment"># 0 1 2 3 4 5 6 7 8</span></span><br><span class="line">        <span class="comment">#   1 </span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">9</span>):</span><br><span class="line">            p1 = collect[k]</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(k+<span class="number">1</span>,<span class="number">9</span>):</span><br><span class="line">                <span class="keyword">if</span> p1&lt;collect[t]:</span><br><span class="line">                    mid = collect[t]</span><br><span class="line">                    collect[t] = p1</span><br><span class="line">                    p1 = mid</span><br><span class="line">        dst[i,j] = collect[<span class="number">4</span>]</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/46.png" alt="4"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>计算机视觉-openCV：基于Haar+Adaboost人脸识别</title>
      <link href="/2018/10/27/2018-10-27-openCV-face/"/>
      <url>/2018/10/27/2018-10-27-openCV-face/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="openCV基于Haar-Adaboost人脸识别"><a href="#openCV基于Haar-Adaboost人脸识别" class="headerlink" title="openCV基于Haar+Adaboost人脸识别"></a>openCV基于Haar+Adaboost人脸识别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># load xml 1 file name</span></span><br><span class="line">face_xml = cv2.CascadeClassifier(<span class="string">'haarcascade_frontalface_default.xml'</span>)</span><br><span class="line">eye_xml = cv2.CascadeClassifier(<span class="string">'haarcascade_eye.xml'</span>)</span><br><span class="line"><span class="comment"># load jpg</span></span><br><span class="line">img = cv2.imread(<span class="string">'face.jpg'</span>)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line"><span class="comment"># haar gray</span></span><br><span class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment"># detect faces 1 data 2 scale 3 5</span></span><br><span class="line">faces = face_xml.detectMultiScale(gray,<span class="number">1.3</span>,<span class="number">5</span>)</span><br><span class="line">print(<span class="string">'face='</span>,len(faces))</span><br><span class="line"><span class="comment"># draw</span></span><br><span class="line"><span class="keyword">for</span> (x,y,w,h) <span class="keyword">in</span> faces:</span><br><span class="line">    cv2.rectangle(img,(x,y),(x+w,y+h),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">    roi_face = gray[y:y+h,x:x+w]</span><br><span class="line">    roi_color = img[y:y+h,x:x+w]</span><br><span class="line">    <span class="comment"># 1 gray</span></span><br><span class="line">    eyes = eye_xml.detectMultiScale(roi_face)</span><br><span class="line">    print(<span class="string">'eye='</span>,len(eyes))</span><br><span class="line">    <span class="keyword">for</span> (e_x,e_y,e_w,e_h) <span class="keyword">in</span> eyes:</span><br><span class="line">        cv2.rectangle(roi_color,(e_x,e_y),(e_x+e_w,e_y+e_h),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,img)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/47.png" alt="1"></p><p>ps: 需要xml文件的，可留言邮箱<br>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>计算机视觉-openCV图片特效&amp;绘制线段文字</title>
      <link href="/2018/10/26/2018-10-26-openCV-special/"/>
      <url>/2018/10/26/2018-10-26-openCV-special/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="openCV图片灰度处理一"><a href="#openCV图片灰度处理一" class="headerlink" title="openCV图片灰度处理一"></a>openCV图片灰度处理一</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  <span class="comment"># 导入cv库</span></span><br><span class="line">img0 = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">0</span>)</span><br><span class="line">img1 = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">print(img0.shape)</span><br><span class="line">print(img1.shape)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img0)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/14.png" alt="1"></p><h3 id="openCV图片灰度处理二"><a href="#openCV图片灰度处理二" class="headerlink" title="openCV图片灰度处理二"></a>openCV图片灰度处理二</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">dst = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)<span class="comment"># 颜色空间转换 1 data 2 BGR gray</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/15.png" alt="1"></p><h3 id="openCV图片灰度处理三"><a href="#openCV图片灰度处理三" class="headerlink" title="openCV图片灰度处理三"></a>openCV图片灰度处理三</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># RGB R=G=B = gray  (R+G+B)/3</span></span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        (b,g,r) = img[i,j]</span><br><span class="line">        gray = (int(b)+int(g)+int(r))/<span class="number">3</span></span><br><span class="line">        dst[i,j] = np.uint8(gray)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/16.png" alt="1"></p><h3 id="openCV图片灰度处理四"><a href="#openCV图片灰度处理四" class="headerlink" title="openCV图片灰度处理四"></a>openCV图片灰度处理四</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方法4 gray = r*0.299+g*0.587+b*0.114</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        (b,g,r) = img[i,j]</span><br><span class="line">        b = int(b)</span><br><span class="line">        g = int(g)</span><br><span class="line">        r = int(r)</span><br><span class="line">        gray = r*<span class="number">0.299</span>+g*<span class="number">0.587</span>+b*<span class="number">0.114</span></span><br><span class="line">        dst[i,j] = np.uint8(gray)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/17.png" alt="1"></p><h3 id="openCV图片灰度处理五-算法优化版"><a href="#openCV图片灰度处理五-算法优化版" class="headerlink" title="openCV图片灰度处理五-算法优化版"></a>openCV图片灰度处理五-算法优化版</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># r*0.299+g*0.587+b*0.114</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># RGB R=G=B = gray  (R+G+B)/3</span></span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        (b,g,r) = img[i,j]</span><br><span class="line">        b = int(b)</span><br><span class="line">        g = int(g)</span><br><span class="line">        r = int(r)</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">#gray = (r*1+g*2+b*1)/4</span></span><br><span class="line">        gray = (r+(g&lt;&lt;<span class="number">1</span>)+b)&gt;&gt;<span class="number">2</span></span><br><span class="line">        dst[i,j] = np.uint8(gray)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/18.png" alt="2"></p><h3 id="openCV图片颜色反转-灰色"><a href="#openCV图片颜色反转-灰色" class="headerlink" title="openCV图片颜色反转-灰色"></a>openCV图片颜色反转-灰色</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) <span class="comment"># 原始图片，灰度api</span></span><br><span class="line">dst = np.zeros((height,width,<span class="number">1</span>),np.uint8) <span class="comment"># 反转矩阵</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        grayPixel = gray[i,j]  </span><br><span class="line">        dst[i,j] = <span class="number">255</span>-grayPixel</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/19.png" alt="2"></p><h3 id="openCV图片颜色反转-彩色"><a href="#openCV图片颜色反转-彩色" class="headerlink" title="openCV图片颜色反转-彩色"></a>openCV图片颜色反转-彩色</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        (b,g,r) = img[i,j]</span><br><span class="line">        dst[i,j] = (<span class="number">255</span>-b,<span class="number">255</span>-g,<span class="number">255</span>-r)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/20.png" alt="3"></p><h3 id="openCV图片马赛克"><a href="#openCV图片马赛克" class="headerlink" title="openCV图片马赛克"></a>openCV图片马赛克</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment"># 导入cv库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>) <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape  <span class="comment"># 获取图片的维度</span></span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">300</span>):</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">200</span>):</span><br><span class="line">        <span class="comment"># pixel -&gt;10*10</span></span><br><span class="line">        <span class="keyword">if</span> m%<span class="number">10</span> == <span class="number">0</span> <span class="keyword">and</span> n%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">10</span>):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">10</span>):</span><br><span class="line">                    (b,g,r) = img[m,n]</span><br><span class="line">                    img[i+m,j+n] = (b,g,r)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,img)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/21.png" alt="4"></p><h3 id="openCV图片毛玻璃"><a href="#openCV图片毛玻璃" class="headerlink" title="openCV图片毛玻璃"></a>openCV图片毛玻璃</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment"># 导入cv库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>) <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape  <span class="comment"># 获取图片的维度</span></span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line">mm = <span class="number">8</span></span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">0</span>,height-mm):</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">0</span>,width-mm):</span><br><span class="line">        index = int(random.random()*<span class="number">8</span>)<span class="comment">#0-8</span></span><br><span class="line">        (b,g,r) = img[m+index,n+index]</span><br><span class="line">        dst[m,n] = (b,g,r)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/22.png" alt="4"></p><h3 id="openCV图片融合"><a href="#openCV图片融合" class="headerlink" title="openCV图片融合"></a>openCV图片融合</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dst  = src1*a+src2*(1-a)</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img0 = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">img1 = cv2.imread(<span class="string">'image1.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img0.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># ROI</span></span><br><span class="line">roiH = int(height/<span class="number">2</span>)</span><br><span class="line">roiW = int(width/<span class="number">2</span>)</span><br><span class="line">img0ROI = img0[<span class="number">0</span>:roiH,<span class="number">0</span>:roiW]</span><br><span class="line">img1ROI = img1[<span class="number">0</span>:roiH,<span class="number">0</span>:roiW]</span><br><span class="line"><span class="comment"># dst</span></span><br><span class="line">dst = np.zeros((roiH,roiW,<span class="number">3</span>),np.uint8)</span><br><span class="line">dst = cv2.addWeighted(img0ROI,<span class="number">0.5</span>,img1ROI,<span class="number">0.5</span>,<span class="number">0</span>)<span class="comment">#add src1*a+src2*(1-a)</span></span><br><span class="line"><span class="comment"># 1 src1 2 a 3 src2 4 1-a</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/23.png" alt="4"></p><h3 id="openCV边缘检测一"><a href="#openCV边缘检测一" class="headerlink" title="openCV边缘检测一"></a>openCV边缘检测一</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line"><span class="comment">#canny 1 gray 2 高斯 3 canny </span></span><br><span class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line">imgG = cv2.GaussianBlur(gray,(<span class="number">3</span>,<span class="number">3</span>),<span class="number">0</span>)</span><br><span class="line">dst = cv2.Canny(img,<span class="number">50</span>,<span class="number">50</span>) <span class="comment">#图片卷积——》th</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/24.png" alt="4"></p><h3 id="openCV边缘检测二"><a href="#openCV边缘检测二" class="headerlink" title="openCV边缘检测二"></a>openCV边缘检测二</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line"><span class="comment"># sobel 1 算子模版 2 图片卷积 3 阈值判决 </span></span><br><span class="line"><span class="comment"># [1 2 1          [ 1 0 -1</span></span><br><span class="line"><span class="comment">#  0 0 0            2 0 -2</span></span><br><span class="line"><span class="comment"># -1 -2 -1 ]       1 0 -1 ]</span></span><br><span class="line">              </span><br><span class="line"><span class="comment"># [1 2 3 4] [a b c d] a*1+b*2+c*3+d*4 = dst</span></span><br><span class="line"><span class="comment"># sqrt(a*a+b*b) = f&gt;th</span></span><br><span class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line">dst = np.zeros((height,width,<span class="number">1</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height<span class="number">-2</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width<span class="number">-2</span>):</span><br><span class="line">        gy = gray[i,j]*<span class="number">1</span>+gray[i,j+<span class="number">1</span>]*<span class="number">2</span>+gray[i,j+<span class="number">2</span>]*<span class="number">1</span>-gray[i+<span class="number">2</span>,j]*<span class="number">1</span>-gray[i+<span class="number">2</span>,j+<span class="number">1</span>]*<span class="number">2</span>-gray[i+<span class="number">2</span>,j+<span class="number">2</span>]*<span class="number">1</span></span><br><span class="line">        gx = gray[i,j]+gray[i+<span class="number">1</span>,j]*<span class="number">2</span>+gray[i+<span class="number">2</span>,j]-gray[i,j+<span class="number">2</span>]-gray[i+<span class="number">1</span>,j+<span class="number">2</span>]*<span class="number">2</span>-gray[i+<span class="number">2</span>,j+<span class="number">2</span>]</span><br><span class="line">        grad = math.sqrt(gx*gx+gy*gy)</span><br><span class="line">        <span class="keyword">if</span> grad&gt;<span class="number">50</span>:</span><br><span class="line">            dst[i,j] = <span class="number">255</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dst[i,j] = <span class="number">0</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/25.png" alt="4"></p><h3 id="openCV图像浮雕风格"><a href="#openCV图像浮雕风格" class="headerlink" title="openCV图像浮雕风格"></a>openCV图像浮雕风格</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment"># newP = gray0-gray1+150</span></span><br><span class="line">dst = np.zeros((height,width,<span class="number">1</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width<span class="number">-1</span>):</span><br><span class="line">        grayP0 = int(gray[i,j])</span><br><span class="line">        grayP1 = int(gray[i,j+<span class="number">1</span>])</span><br><span class="line">        newP = grayP0-grayP1+<span class="number">150</span></span><br><span class="line">        <span class="keyword">if</span> newP &gt; <span class="number">255</span>:</span><br><span class="line">            newP = <span class="number">255</span></span><br><span class="line">        <span class="keyword">if</span> newP &lt; <span class="number">0</span>:</span><br><span class="line">            newP = <span class="number">0</span></span><br><span class="line">        dst[i,j] = newP</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/26.png" alt="4"></p><h3 id="openCV图片颜色风格"><a href="#openCV图片颜色风格" class="headerlink" title="openCV图片颜色风格"></a>openCV图片颜色风格</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#rgb -》RGB new “蓝色”</span></span><br><span class="line"><span class="comment"># b=b*1.5</span></span><br><span class="line"><span class="comment"># g = g*1.3</span></span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        (b,g,r) = img[i,j]</span><br><span class="line">        b = b*<span class="number">1.5</span></span><br><span class="line">        g = g*<span class="number">1.3</span></span><br><span class="line">        <span class="keyword">if</span> b&gt;<span class="number">255</span>:</span><br><span class="line">            b = <span class="number">255</span></span><br><span class="line">        <span class="keyword">if</span> g&gt;<span class="number">255</span>:</span><br><span class="line">            g = <span class="number">255</span></span><br><span class="line">        dst[i,j]=(b,g,r)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/27.png" alt="4"></p><h3 id="openCV油画特效"><a href="#openCV油画特效" class="headerlink" title="openCV油画特效"></a>openCV油画特效</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image00.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line">dst = np.zeros((height,width,<span class="number">3</span>),np.uint8)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>,height<span class="number">-4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>,width<span class="number">-4</span>):</span><br><span class="line">        array1 = np.zeros(<span class="number">8</span>,np.uint8)</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">-4</span>,<span class="number">4</span>):</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-4</span>,<span class="number">4</span>):</span><br><span class="line">                p1 = int(gray[i+m,j+n]/<span class="number">32</span>)</span><br><span class="line">                array1[p1] = array1[p1]+<span class="number">1</span></span><br><span class="line">        currentMax = array1[<span class="number">0</span>]</span><br><span class="line">        l = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">8</span>):</span><br><span class="line">            <span class="keyword">if</span> currentMax&lt;array1[k]:</span><br><span class="line">                currentMax = array1[k]</span><br><span class="line">                l = k</span><br><span class="line">        <span class="comment"># 简化 均值</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">-4</span>,<span class="number">4</span>):</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">-4</span>,<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">if</span> gray[i+m,j+n]&gt;=(l*<span class="number">32</span>) <span class="keyword">and</span> gray[i+m,j+n]&lt;=((l+<span class="number">1</span>)*<span class="number">32</span>):</span><br><span class="line">                    (b,g,r) = img[i+m,j+n]</span><br><span class="line">        dst[i,j] = (b,g,r)</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/28.png" alt="4"></p><h3 id="openCV线段绘制"><a href="#openCV线段绘制" class="headerlink" title="openCV线段绘制"></a>openCV线段绘制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">newImageInfo = (<span class="number">500</span>,<span class="number">500</span>,<span class="number">3</span>)</span><br><span class="line">dst = np.zeros(newImageInfo,np.uint8)</span><br><span class="line"><span class="comment"># line</span></span><br><span class="line"><span class="comment"># 绘制线段 1 dst 2 begin 3 end 4 color</span></span><br><span class="line">cv2.line(dst,(<span class="number">100</span>,<span class="number">100</span>),(<span class="number">400</span>,<span class="number">400</span>),(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>))</span><br><span class="line"><span class="comment"># 5 line w</span></span><br><span class="line">cv2.line(dst,(<span class="number">100</span>,<span class="number">200</span>),(<span class="number">400</span>,<span class="number">200</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">255</span>),<span class="number">20</span>)</span><br><span class="line"><span class="comment"># 6 line type</span></span><br><span class="line">cv2.line(dst,(<span class="number">100</span>,<span class="number">300</span>),(<span class="number">400</span>,<span class="number">300</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">20</span>,cv2.LINE_AA)</span><br><span class="line"></span><br><span class="line">cv2.line(dst,(<span class="number">200</span>,<span class="number">150</span>),(<span class="number">50</span>,<span class="number">250</span>),(<span class="number">25</span>,<span class="number">100</span>,<span class="number">255</span>))</span><br><span class="line">cv2.line(dst,(<span class="number">50</span>,<span class="number">250</span>),(<span class="number">400</span>,<span class="number">380</span>),(<span class="number">25</span>,<span class="number">100</span>,<span class="number">255</span>))</span><br><span class="line">cv2.line(dst,(<span class="number">400</span>,<span class="number">380</span>),(<span class="number">200</span>,<span class="number">150</span>),(<span class="number">25</span>,<span class="number">100</span>,<span class="number">255</span>))</span><br><span class="line"></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/29.png" alt="4"></p><h3 id="openCV矩形圆形绘制"><a href="#openCV矩形圆形绘制" class="headerlink" title="openCV矩形圆形绘制"></a>openCV矩形圆形绘制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">newImageInfo = (<span class="number">500</span>,<span class="number">500</span>,<span class="number">3</span>)</span><br><span class="line">dst = np.zeros(newImageInfo,np.uint8)</span><br><span class="line"><span class="comment">#  1 2 左上角 3 右下角 4 5 fill -1 &gt;0 line w</span></span><br><span class="line">cv2.rectangle(dst,(<span class="number">50</span>,<span class="number">100</span>),(<span class="number">200</span>,<span class="number">300</span>),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 2 center 3 r </span></span><br><span class="line">cv2.circle(dst,(<span class="number">250</span>,<span class="number">250</span>),(<span class="number">50</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 2 center 3 轴 4 angle 5 begin 6 end 7 </span></span><br><span class="line">cv2.ellipse(dst,(<span class="number">256</span>,<span class="number">256</span>),(<span class="number">150</span>,<span class="number">100</span>),<span class="number">0</span>,<span class="number">0</span>,<span class="number">180</span>,(<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">points = np.array([[<span class="number">150</span>,<span class="number">50</span>],[<span class="number">140</span>,<span class="number">140</span>],[<span class="number">200</span>,<span class="number">170</span>],[<span class="number">250</span>,<span class="number">250</span>],[<span class="number">150</span>,<span class="number">50</span>]],np.int32)</span><br><span class="line">print(points.shape)</span><br><span class="line">points = points.reshape((<span class="number">-1</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">print(points.shape)</span><br><span class="line">cv2.polylines(dst,[points],<span class="keyword">True</span>,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">255</span>))</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/30.png" alt="4"></p><h3 id="openCV文字绘制"><a href="#openCV文字绘制" class="headerlink" title="openCV文字绘制"></a>openCV文字绘制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">font = cv2.FONT_HERSHEY_SIMPLEX</span><br><span class="line">cv2.rectangle(img,(<span class="number">200</span>,<span class="number">100</span>),(<span class="number">500</span>,<span class="number">400</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 1 dst 2 文字内容 3 坐标 4 5 字体大小 6 color 7 粗细 8 line type</span></span><br><span class="line">cv2.putText(img,<span class="string">'this is flow'</span>,(<span class="number">100</span>,<span class="number">300</span>),font,<span class="number">1</span>,(<span class="number">200</span>,<span class="number">100</span>,<span class="number">255</span>),<span class="number">2</span>,cv2.LINE_AA)</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/31.png" alt="4"></p><h3 id="openCV图片绘制"><a href="#openCV图片绘制" class="headerlink" title="openCV图片绘制"></a>openCV图片绘制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)</span><br><span class="line">height = int(img.shape[<span class="number">0</span>]*<span class="number">0.2</span>)</span><br><span class="line">width = int(img.shape[<span class="number">1</span>]*<span class="number">0.2</span>)</span><br><span class="line">imgResize = cv2.resize(img,(width,height))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        img[i+<span class="number">200</span>,j+<span class="number">350</span>] = imgResize[i,j]</span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/32.png" alt="4"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>计算机视觉-openCV安装及入门</title>
      <link href="/2018/10/26/2018-10-26-openCV-install/"/>
      <url>/2018/10/26/2018-10-26-openCV-install/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="openCV-python的安装"><a href="#openCV-python的安装" class="headerlink" title="openCV-python的安装"></a>openCV-python的安装</h3><h4 id="windows安装：下载地址：opencv"><a href="#windows安装：下载地址：opencv" class="headerlink" title="windows安装：下载地址：opencv."></a>windows安装：下载地址：<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/" target="_blank" rel="noopener">opencv</a>.</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install  opencv_python-3.4.3-cp36-cp36m-win_amd64.whl</span><br></pre></td></tr></table></figure><h4 id="ubuntu安装"><a href="#ubuntu安装" class="headerlink" title="ubuntu安装"></a>ubuntu安装</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install opencv-python</span><br></pre></td></tr></table></figure><h3 id="openCV图片读取与展示"><a href="#openCV图片读取与展示" class="headerlink" title="openCV图片读取与展示"></a>openCV图片读取与展示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  <span class="comment"># 导入cv库</span></span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imshow(<span class="string">'image'</span>,img)  <span class="comment"># 显示图片</span></span><br></pre></td></tr></table></figure><p><img src="/images/cv/1.png" alt="1"></p><h3 id="openCV图片写入"><a href="#openCV图片写入" class="headerlink" title="openCV图片写入"></a>openCV图片写入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment"># 导入cv库</span></span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>) <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imwrite(<span class="string">'image1.jpg'</span>,img) <span class="comment"># 写入文件名字 ， 图片数据</span></span><br></pre></td></tr></table></figure><h3 id="openCV更改图像质量-有损压缩"><a href="#openCV更改图像质量-有损压缩" class="headerlink" title="openCV更改图像质量-有损压缩"></a>openCV更改图像质量-有损压缩</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment"># 导入cv库</span></span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>) <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imwrite(<span class="string">'imageTest.jpg'</span>,img,[cv2.IMWRITE_JPEG_QUALITY,<span class="number">50</span>]) <span class="comment"># 写入文件名字 ， 图片数据 ， 当前jpg图片保存的质量（范围0-100）</span></span><br><span class="line"><span class="comment">#1M 100k 10k 0-100 有损压缩</span></span><br></pre></td></tr></table></figure><p><img src="/images/cv/2.png" alt="2"></p><h3 id="openCV更改图像质量-无损压缩"><a href="#openCV更改图像质量-无损压缩" class="headerlink" title="openCV更改图像质量-无损压缩"></a>openCV更改图像质量-无损压缩</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 无损 2 透明度属性</span></span><br><span class="line"><span class="keyword">import</span> cv2  <span class="comment"># 导入cv库</span></span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imwrite(<span class="string">'imageTest.png'</span>,img,[cv2.IMWRITE_PNG_COMPRESSION,<span class="number">0</span>])  <span class="comment"># 写入文件名字 ， 图片数据 ， 当前jpg图片保存的质量（范围0-100）</span></span><br><span class="line"><span class="comment"># jpg 0 压缩比高0-100 png 0 压缩比低0-9</span></span><br></pre></td></tr></table></figure><p><img src="/images/cv/3.png" alt="3"></p><h3 id="openCV图片像素操作"><a href="#openCV图片像素操作" class="headerlink" title="openCV图片像素操作"></a>openCV图片像素操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  <span class="comment"># 导入cv库 </span></span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">(b,g,r) = img[<span class="number">100</span>,<span class="number">100</span>] <span class="comment"># 获取图片的（100,100）坐标的像素值，按照bgr的形式读取</span></span><br><span class="line">print(b,g,r)<span class="comment"># bgr</span></span><br><span class="line"><span class="comment">#10 100 --- 110 100</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">100</span>):  <span class="comment"># 总共一百个像素点</span></span><br><span class="line">    img[<span class="number">10</span>+i,<span class="number">100</span>] = (<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>)  <span class="comment"># 写入标准的蓝色</span></span><br><span class="line">cv2.imshow(<span class="string">'image'</span>,img)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/4.png" alt="4"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>计算机视觉-openCV图片几何变换</title>
      <link href="/2018/10/26/2018-10-26-openCV-geometry/"/>
      <url>/2018/10/26/2018-10-26-openCV-geometry/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="openCV图片缩放一"><a href="#openCV图片缩放一" class="headerlink" title="openCV图片缩放一"></a>openCV图片缩放一</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  <span class="comment"># 导入cv库</span></span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">imgInfo = img.shape <span class="comment"># 获取图片的维度</span></span><br><span class="line">print(imgInfo)</span><br><span class="line">height = imgInfo[<span class="number">0</span>] </span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">mode = imgInfo[<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 1 放大 缩小 2 等比例 非 2:3 </span></span><br><span class="line">dstHeight = int(height*<span class="number">0.5</span>)</span><br><span class="line">dstWidth = int(width*<span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#最近临域插值 双线性插值 像素关系重采样 立方插值</span></span><br><span class="line">dst = cv2.resize(img,(dstWidth,dstHeight))</span><br><span class="line">cv2.imshow(<span class="string">'image'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/5.png" alt="1"></p><h3 id="openCV图片缩放二"><a href="#openCV图片缩放二" class="headerlink" title="openCV图片缩放二"></a>openCV图片缩放二</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  <span class="comment"># 导入cv库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">imgInfo = img.shape <span class="comment"># 获取图片的维度</span></span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 定义目标图片的高度和宽度</span></span><br><span class="line">dstHeight = int(height/<span class="number">2</span>)</span><br><span class="line">dstWidth = int(width/<span class="number">2</span>)</span><br><span class="line">dstImage = np.zeros((dstHeight,dstWidth,<span class="number">3</span>),np.uint8) <span class="comment"># 0-255 准备好缩放后的图片数据维度</span></span><br><span class="line"><span class="comment"># 计算新的目标图片的坐标</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,dstHeight):<span class="comment">#行</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,dstWidth):<span class="comment">#列 </span></span><br><span class="line">        iNew = int(i*(height*<span class="number">1.0</span>/dstHeight)) </span><br><span class="line">        jNew = int(j*(width*<span class="number">1.0</span>/dstWidth))</span><br><span class="line">        dstImage[i,j] = img[iNew,jNew]</span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dstImage)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/6.png" alt="1"></p><h3 id="openCV图片剪切"><a href="#openCV图片剪切" class="headerlink" title="openCV图片剪切"></a>openCV图片剪切</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  <span class="comment"># 导入cv库</span></span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">imgInfo = img.shape</span><br><span class="line">dst = img[<span class="number">100</span>:<span class="number">200</span>,<span class="number">100</span>:<span class="number">300</span>] <span class="comment"># 获取宽度100-200， 高度100-300的图像</span></span><br><span class="line">cv2.imshow(<span class="string">'image'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/7.png" alt="1"></p><h3 id="openCV图片读取与展示"><a href="#openCV图片读取与展示" class="headerlink" title="openCV图片读取与展示"></a>openCV图片读取与展示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  <span class="comment"># 导入cv库</span></span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imshow(<span class="string">'image'</span>,img)  <span class="comment"># 显示图片</span></span><br></pre></td></tr></table></figure><p><img src="/images/cv/1.png" alt="1"></p><h3 id="openCV图片移位一"><a href="#openCV图片移位一" class="headerlink" title="openCV图片移位一"></a>openCV图片移位一</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment"># 导入cv库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>) <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape  <span class="comment"># 获取图片的维度</span></span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"><span class="comment">####</span></span><br><span class="line">matShift = np.float32([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">100</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">200</span>]])<span class="comment"># 2*3  设置平移的矩阵</span></span><br><span class="line">dst = cv2.warpAffine(img,matShift,(height,width))<span class="comment">#图片数据 ，移位矩阵 图片的维度信息</span></span><br><span class="line"><span class="comment"># 移位 矩阵</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/8.png" alt="2"></p><h3 id="openCV图片移位二"><a href="#openCV图片移位二" class="headerlink" title="openCV图片移位二"></a>openCV图片移位二</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment"># 导入cv库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>) <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape  <span class="comment"># 获取图片的维度</span></span><br><span class="line">dst = np.zeros(img.shape,np.uint8)</span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width<span class="number">-100</span>):</span><br><span class="line">        dst[i,j+<span class="number">100</span>]=img[i,j]</span><br><span class="line">cv2.imshow(<span class="string">'image'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/9.png" alt="2"></p><h3 id="openCV图片镜像"><a href="#openCV图片镜像" class="headerlink" title="openCV图片镜像"></a>openCV图片镜像</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment"># 导入cv库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>) <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape  <span class="comment"># 获取图片的维度</span></span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">deep = imgInfo[<span class="number">2</span>]</span><br><span class="line">newImgInfo = (height*<span class="number">2</span>,width,deep) <span class="comment"># 新图片的维度</span></span><br><span class="line">dst = np.zeros(newImgInfo,np.uint8)<span class="comment">#uint8 # 目标图片的数据维度</span></span><br><span class="line"><span class="comment"># 刷新图片的数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,height):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,width):</span><br><span class="line">        dst[i,j] = img[i,j]</span><br><span class="line">        <span class="comment">#x y = 2*h - y -1</span></span><br><span class="line">        dst[height*<span class="number">2</span>-i<span class="number">-1</span>,j] = img[i,j]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,width): <span class="comment"># 添加分割线</span></span><br><span class="line">    dst[height,i] = (<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>)<span class="comment">#BGR</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/10.png" alt="3"></p><h3 id="openCV图片缩放"><a href="#openCV图片缩放" class="headerlink" title="openCV图片缩放"></a>openCV图片缩放</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment"># 导入cv库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>) <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape  <span class="comment"># 获取图片的维度</span></span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line">matScale = np.float32([[<span class="number">0.5</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.5</span>,<span class="number">0</span>]]) <span class="comment"># 定义缩放矩阵</span></span><br><span class="line">dst = cv2.warpAffine(img,matScale,(int(width/<span class="number">2</span>),int(height/<span class="number">2</span>))) <span class="comment"># 原始数据，缩放矩阵，目标的宽高信息</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/11.png" alt="4"></p><h3 id="openCV图片仿射变换"><a href="#openCV图片仿射变换" class="headerlink" title="openCV图片仿射变换"></a>openCV图片仿射变换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment"># 导入cv库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>) <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape  <span class="comment"># 获取图片的维度</span></span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"><span class="comment">#src 3-&gt;dst 3 (左上角 左下角 右上角)</span></span><br><span class="line">matSrc = np.float32([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,height<span class="number">-1</span>],[width<span class="number">-1</span>,<span class="number">0</span>]]) <span class="comment"># 获取原图片三个点坐标</span></span><br><span class="line">matDst = np.float32([[<span class="number">50</span>,<span class="number">50</span>],[<span class="number">300</span>,height<span class="number">-200</span>],[width<span class="number">-300</span>,<span class="number">100</span>]]) <span class="comment"># 三个点的新坐标</span></span><br><span class="line"><span class="comment">#把两个矩阵组合</span></span><br><span class="line">matAffine = cv2.getAffineTransform(matSrc,matDst) <span class="comment"># 获取矩阵的组合，</span></span><br><span class="line">dst = cv2.warpAffine(img,matAffine,(width,height)) <span class="comment"># 仿射变换方法</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/12.png" alt="4"></p><h3 id="openCV图片旋转"><a href="#openCV图片旋转" class="headerlink" title="openCV图片旋转"></a>openCV图片旋转</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment"># 导入cv库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = cv2.imread(<span class="string">'image0.jpg'</span>,<span class="number">1</span>) <span class="comment"># 读取图片文件， 1：彩色， 0：灰色</span></span><br><span class="line">cv2.imshow(<span class="string">'src'</span>,img)</span><br><span class="line">imgInfo = img.shape  <span class="comment"># 获取图片的维度</span></span><br><span class="line">height = imgInfo[<span class="number">0</span>]</span><br><span class="line">width = imgInfo[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 2*3 定义旋转矩阵--旋转的中心点，旋转的角度， 缩放系数</span></span><br><span class="line">matRotate = cv2.getRotationMatrix2D((height*<span class="number">0.5</span>,width*<span class="number">0.5</span>),<span class="number">45</span>,<span class="number">1</span>)<span class="comment"># mat rotate 1 center 2 angle 3 scale</span></span><br><span class="line"><span class="comment">#100*100 25 </span></span><br><span class="line">dst = cv2.warpAffine(img,matRotate,(height,width)) <span class="comment"># 仿射方法</span></span><br><span class="line">cv2.imshow(<span class="string">'dst'</span>,dst)</span><br></pre></td></tr></table></figure><p><img src="/images/cv/13.png" alt="4"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ubuntu换源</title>
      <link href="/2018/10/15/2018-10-15-source/"/>
      <url>/2018/10/15/2018-10-15-source/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="查找国内的开源镜像提供的源地址"><a href="#查找国内的开源镜像提供的源地址" class="headerlink" title="查找国内的开源镜像提供的源地址"></a><strong>查找国内的开源镜像提供的源地址</strong></h3><blockquote><p>Ubuntu的源的list文件位于 /etc/apt/sources.list</p></blockquote><ul><li><strong>高校镜像源</strong><br><a href="https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/help/ubuntu/" target="_blank" rel="noopener">清华大学开源镜像站</a><br><a href="https://link.zhihu.com/?target=https%3A//mirrors.ustc.edu.cn/repogen/" target="_blank" rel="noopener">中科大开源镜像站</a></li><li><strong>企业镜像源</strong><br><a href="https://link.zhihu.com/?target=http%3A//mirrors.aliyun.com/help/ubuntu" target="_blank" rel="noopener">阿里云开源镜像站</a><br><a href="https://link.zhihu.com/?target=http%3A//mirrors.163.com/.help/ubuntu.html" target="_blank" rel="noopener">网易开源镜像站</a></li></ul><p><strong>这里以清华源为例</strong></p><p>先去查一下清华源的帮助文档，戳 <a href="https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/help/ubuntu/" target="_blank" rel="noopener">清华大学开源镜像站</a>，选择相匹配的Ubuntu的版本，会得到软件源镜像的地址。</p><p>接下来直接替换一下系统的source.list文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 备份一下</span><br><span class="line">sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</span><br><span class="line"><span class="meta">#</span> 修改</span><br><span class="line">sudo vim /etc/apt/sources.list</span><br></pre></td></tr></table></figure><p>将文档里面所有的内容删除，然后替换为清华镜像提供的软件源镜像的地址。<br>例如我用的 18.04 版， 替换为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line"><span class="meta">#</span> deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line"><span class="meta">#</span> deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line"><span class="meta">#</span> deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse</span><br><span class="line">deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br><span class="line"><span class="meta">#</span> deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse</span><br></pre></td></tr></table></figure><p>然后再update一下，你会发现速度还是有明显的提升的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br></pre></td></tr></table></figure><h3 id="一行命令搞定：阿里源"><a href="#一行命令搞定：阿里源" class="headerlink" title="一行命令搞定：阿里源"></a>一行命令搞定：阿里源</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo python3 -c <span class="string">"d='mirrors.aliyun.com';import re;from pathlib import Path;p=Path('/etc/apt/sources.list');s=p.read_text();bak=p.with_name(p.name+'.bak');bak.exists() or bak.write_text(s);p.write_text(re.sub(r'(cn.archive|security|archive)\.ubuntu\.com', d, s))"</span></span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> LINUX </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ubuntu </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow安装问题检测</title>
      <link href="/2018/10/09/2018-10-09-tensorflow-question/"/>
      <url>/2018/10/09/2018-10-09-tensorflow-question/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h4 id="当你安装TensorFlow后，import时需要未知的错误，以下代码会为你检测出来并提供正确的解决方式"><a href="#当你安装TensorFlow后，import时需要未知的错误，以下代码会为你检测出来并提供正确的解决方式" class="headerlink" title="当你安装TensorFlow后，import时需要未知的错误，以下代码会为你检测出来并提供正确的解决方式"></a>当你安装TensorFlow后，import时需要未知的错误，以下代码会为你检测出来并提供正确的解决方式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ctypes</span><br><span class="line"><span class="keyword">import</span> imp</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">    print(<span class="string">"TensorFlow successfully installed."</span>)</span><br><span class="line">    <span class="keyword">if</span> tf.test.is_built_with_cuda():</span><br><span class="line">      print(<span class="string">"The installed version of TensorFlow includes GPU support."</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      print(<span class="string">"The installed version of TensorFlow does not include GPU support."</span>)</span><br><span class="line">    sys.exit(<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">except</span> ImportError:</span><br><span class="line">    print(<span class="string">"ERROR: Failed to import the TensorFlow module."</span>)</span><br><span class="line"></span><br><span class="line">  print(<span class="string">"""</span></span><br><span class="line"><span class="string">WARNING! This script is no longer maintained! </span></span><br><span class="line"><span class="string">=============================================</span></span><br><span class="line"><span class="string">Since TensorFlow 1.4, the self-check has been integrated with TensorFlow itself,</span></span><br><span class="line"><span class="string">and any missing DLLs will be reported when you execute the `import tensorflow`</span></span><br><span class="line"><span class="string">statement. The error messages printed below refer to TensorFlow 1.3 and earlier,</span></span><br><span class="line"><span class="string">and are inaccurate for later versions of TensorFlow."""</span>)</span><br><span class="line">    </span><br><span class="line">  candidate_explanation = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">  python_version = sys.version_info.major, sys.version_info.minor</span><br><span class="line">  print(<span class="string">"\n- Python version is %d.%d."</span> % python_version)</span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> (python_version == (<span class="number">3</span>, <span class="number">5</span>) <span class="keyword">or</span> python_version == (<span class="number">3</span>, <span class="number">6</span>)):</span><br><span class="line">    candidate_explanation = <span class="keyword">True</span></span><br><span class="line">    print(<span class="string">"- The official distribution of TensorFlow for Windows requires "</span></span><br><span class="line">          <span class="string">"Python version 3.5 or 3.6."</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    _, pathname, _ = imp.find_module(<span class="string">"tensorflow"</span>)</span><br><span class="line">    print(<span class="string">"\n- TensorFlow is installed at: %s"</span> % pathname)</span><br><span class="line">  <span class="keyword">except</span> ImportError:</span><br><span class="line">    candidate_explanation = <span class="keyword">False</span></span><br><span class="line">    print(<span class="string">"""</span></span><br><span class="line"><span class="string">- No module named TensorFlow is installed in this Python environment. You may</span></span><br><span class="line"><span class="string">  install it using the command `pip install tensorflow`."""</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    msvcp140 = ctypes.WinDLL(<span class="string">"msvcp140.dll"</span>)</span><br><span class="line">  <span class="keyword">except</span> OSError:</span><br><span class="line">    candidate_explanation = <span class="keyword">True</span></span><br><span class="line">    print(<span class="string">"""</span></span><br><span class="line"><span class="string">- Could not load 'msvcp140.dll'. TensorFlow requires that this DLL be</span></span><br><span class="line"><span class="string">  installed in a directory that is named in your %PATH% environment</span></span><br><span class="line"><span class="string">  variable. You may install this DLL by downloading Microsoft Visual</span></span><br><span class="line"><span class="string">  C++ 2015 Redistributable Update 3 from this URL:</span></span><br><span class="line"><span class="string">  https://www.microsoft.com/en-us/download/details.aspx?id=53587"""</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    cudart64_80 = ctypes.WinDLL(<span class="string">"cudart64_80.dll"</span>)</span><br><span class="line">  <span class="keyword">except</span> OSError:</span><br><span class="line">    candidate_explanation = <span class="keyword">True</span></span><br><span class="line">    print(<span class="string">"""</span></span><br><span class="line"><span class="string">- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow</span></span><br><span class="line"><span class="string">  requires that this DLL be installed in a directory that is named in</span></span><br><span class="line"><span class="string">  your %PATH% environment variable. Download and install CUDA 8.0 from</span></span><br><span class="line"><span class="string">  this URL: https://developer.nvidia.com/cuda-toolkit"""</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    nvcuda = ctypes.WinDLL(<span class="string">"nvcuda.dll"</span>)</span><br><span class="line">  <span class="keyword">except</span> OSError:</span><br><span class="line">    candidate_explanation = <span class="keyword">True</span></span><br><span class="line">    print(<span class="string">"""</span></span><br><span class="line"><span class="string">- Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that</span></span><br><span class="line"><span class="string">  this DLL be installed in a directory that is named in your %PATH%</span></span><br><span class="line"><span class="string">  environment variable. Typically it is installed in 'C:\Windows\System32'.</span></span><br><span class="line"><span class="string">  If it is not present, ensure that you have a CUDA-capable GPU with the</span></span><br><span class="line"><span class="string">  correct driver installed."""</span>)</span><br><span class="line"></span><br><span class="line">  cudnn5_found = <span class="keyword">False</span></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    cudnn5 = ctypes.WinDLL(<span class="string">"cudnn64_5.dll"</span>)</span><br><span class="line">    cudnn5_found = <span class="keyword">True</span></span><br><span class="line">  <span class="keyword">except</span> OSError:</span><br><span class="line">    candidate_explanation = <span class="keyword">True</span></span><br><span class="line">    print(<span class="string">"""</span></span><br><span class="line"><span class="string">- Could not load 'cudnn64_5.dll'. The GPU version of TensorFlow</span></span><br><span class="line"><span class="string">  requires that this DLL be installed in a directory that is named in</span></span><br><span class="line"><span class="string">  your %PATH% environment variable. Note that installing cuDNN is a</span></span><br><span class="line"><span class="string">  separate step from installing CUDA, and it is often found in a</span></span><br><span class="line"><span class="string">  different directory from the CUDA DLLs. You may install the</span></span><br><span class="line"><span class="string">  necessary DLL by downloading cuDNN 5.1 from this URL:</span></span><br><span class="line"><span class="string">  https://developer.nvidia.com/cudnn"""</span>)</span><br><span class="line"></span><br><span class="line">  cudnn6_found = <span class="keyword">False</span></span><br><span class="line">  <span class="keyword">try</span>:</span><br><span class="line">    cudnn = ctypes.WinDLL(<span class="string">"cudnn64_6.dll"</span>)</span><br><span class="line">    cudnn6_found = <span class="keyword">True</span></span><br><span class="line">  <span class="keyword">except</span> OSError:</span><br><span class="line">    candidate_explanation = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> cudnn5_found <span class="keyword">or</span> <span class="keyword">not</span> cudnn6_found:</span><br><span class="line">    print()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> cudnn5_found <span class="keyword">and</span> <span class="keyword">not</span> cudnn6_found:</span><br><span class="line">      print(<span class="string">"- Could not find cuDNN."</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="keyword">not</span> cudnn5_found:</span><br><span class="line">      print(<span class="string">"- Could not find cuDNN 5.1."</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      print(<span class="string">"- Could not find cuDNN 6."</span>)</span><br><span class="line">      print(<span class="string">"""</span></span><br><span class="line"><span class="string">  The GPU version of TensorFlow requires that the correct cuDNN DLL be installed</span></span><br><span class="line"><span class="string">  in a directory that is named in your %PATH% environment variable. Note that</span></span><br><span class="line"><span class="string">  installing cuDNN is a separate step from installing CUDA, and it is often</span></span><br><span class="line"><span class="string">  found in a different directory from the CUDA DLLs. The correct version of</span></span><br><span class="line"><span class="string">  cuDNN depends on your version of TensorFlow:</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  * TensorFlow 1.2.1 or earlier requires cuDNN 5.1. ('cudnn64_5.dll')</span></span><br><span class="line"><span class="string">  * TensorFlow 1.3 or later requires cuDNN 6. ('cudnn64_6.dll')</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">  You may install the necessary DLL by downloading cuDNN from this URL:</span></span><br><span class="line"><span class="string">  https://developer.nvidia.com/cudnn"""</span>)</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> candidate_explanation:</span><br><span class="line">    print(<span class="string">"""</span></span><br><span class="line"><span class="string">- All required DLLs appear to be present. Please open an issue on the</span></span><br><span class="line"><span class="string">  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues"""</span>)</span><br><span class="line"></span><br><span class="line">  sys.exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  main()</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 安装 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pip换源</title>
      <link href="/2018/10/09/2018-10-09-pip/"/>
      <url>/2018/10/09/2018-10-09-pip/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="pip在windows和linux上换源的方法："><a href="#pip在windows和linux上换源的方法：" class="headerlink" title="pip在windows和linux上换源的方法："></a>pip在windows和linux上换源的方法：</h3><h4 id="pip国内的一些镜像"><a href="#pip国内的一些镜像" class="headerlink" title="pip国内的一些镜像"></a><strong>pip国内的一些镜像</strong></h4><ul><li>阿里云 <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a></li><li>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.mirrors.ustc.edu.cn/simple/</a></li><li>豆瓣(douban) <a href="http://pypi.douban.com/simple/" target="_blank" rel="noopener">http://pypi.douban.com/simple/</a></li><li>清华大学 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple/</a></li><li>中国科学技术大学 <a href="http://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">http://pypi.mirrors.ustc.edu.cn/simple/</a></li></ul><h3 id="修改源方法："><a href="#修改源方法：" class="headerlink" title="修改源方法："></a><strong>修改源方法：</strong></h3><h4 id="临时使用："><a href="#临时使用：" class="headerlink" title="临时使用："></a><strong>临时使用：</strong></h4><p>可以在使用pip的时候在后面加上-i参数，指定pip源</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install numpy -i &lt;https://pypi.tuna.tsinghua.edu.cn/simple&gt;</span><br></pre></td></tr></table></figure><h4 id="永久修改："><a href="#永久修改：" class="headerlink" title="永久修改："></a><strong>永久修改：</strong></h4><h4 id="linux"><a href="#linux" class="headerlink" title="linux:"></a><strong>linux:</strong></h4><p>修改或创建 ~/.pip/pip.conf ：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><h4 id="windows"><a href="#windows" class="headerlink" title="windows:"></a><strong>windows:</strong></h4><p>直接在user目录中创建一个pip目录，如：C:\Users\xx\pip，新建文件pip.ini，内容如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> LINUX </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pip </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch实现LeNet</title>
      <link href="/2018/09/15/2018-09-15-Pytorch-LeNet/"/>
      <url>/2018/09/15/2018-09-15-Pytorch-LeNet/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/13 20:26</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : LeNet.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line">        <span class="comment"># 32*32*3 --28*28*6--&gt; 14*14*6</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>,</span><br><span class="line">                      out_channels=<span class="number">6</span>,</span><br><span class="line">                      kernel_size=<span class="number">5</span>,</span><br><span class="line">                      stride=<span class="number">1</span>,</span><br><span class="line">                      padding=<span class="number">0</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 14*14*6 --10*10*16--&gt; 5*5*16</span></span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">6</span>,</span><br><span class="line">                      out_channels=<span class="number">16</span>,</span><br><span class="line">                      kernel_size=<span class="number">5</span>,</span><br><span class="line">                      stride=<span class="number">1</span>,</span><br><span class="line">                      padding=<span class="number">0</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 5*5*16 --&gt; 120</span></span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">5</span> * <span class="number">5</span> * <span class="number">16</span>, <span class="number">120</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.8</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 120 --&gt; 84</span></span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.8</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 84 --&gt; 10</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.conv1(inputs)</span><br><span class="line">        network = self.conv2(network)</span><br><span class="line">        network = network.view(network.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        network = self.fc1(network)</span><br><span class="line">        network = self.fc2(network)</span><br><span class="line">        out = self.out(network)</span><br><span class="line">        <span class="keyword">return</span> out, network</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch实现ResNet</title>
      <link href="/2018/09/15/2018-09-15-Pytorch-ResNet/"/>
      <url>/2018/09/15/2018-09-15-Pytorch-ResNet/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/14 21:57</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : ResNet.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    growth = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputs, outs, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(BasicBlock, self).__init__()</span><br><span class="line">        self.left = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inputs, outs, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outs),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(outs, outs, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outs)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> inputs != outs:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inputs, outs, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="keyword">False</span>),</span><br><span class="line">                nn.BatchNorm2d(outs)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.left(inputs)</span><br><span class="line">        network += self.shortcut(inputs)</span><br><span class="line">        out = F.relu(network)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpgradeBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    growth = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputs, outs, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(UpgradeBlock, self).__init__()</span><br><span class="line">        self.left = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inputs, outs, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outs),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(outs, outs, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outs),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(outs, self.growth*outs, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(self.growth*outs)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> inputs != self.growth * outs:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inputs, self.growth * outs, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="keyword">False</span>),</span><br><span class="line">                nn.BatchNorm2d(self.growth * outs)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.left(inputs)</span><br><span class="line">        network += self.shortcut(inputs)</span><br><span class="line">        out = F.relu(network)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, block, layers)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line">        self.inputs = <span class="number">64</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.conv2 = self._block(block, layers=layers[<span class="number">0</span>], channels=<span class="number">64</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = self._block(block, layers=layers[<span class="number">1</span>], channels=<span class="number">128</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv4 = self._block(block, layers=layers[<span class="number">2</span>], channels=<span class="number">256</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv5 = self._block(block, layers=layers[<span class="number">3</span>], channels=<span class="number">512</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(<span class="number">512</span>*block.growth, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.conv1(inputs)</span><br><span class="line">        network = self.conv2(network)</span><br><span class="line">        network = self.conv3(network)</span><br><span class="line">        network = self.conv4(network)</span><br><span class="line">        network = self.conv5(network)</span><br><span class="line">        network = F.avg_pool2d(network, kernel_size=network.shape[<span class="number">2</span>])</span><br><span class="line">        network = network.view(network.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = self.linear(network)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out, network</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_block</span><span class="params">(self, block, layers, channels, stride)</span>:</span></span><br><span class="line">        strides = [stride] + [<span class="number">1</span>] * (layers - <span class="number">1</span>)  <span class="comment"># strides=[1,1]</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> stride <span class="keyword">in</span> strides:</span><br><span class="line">            layers.append(block(self.inputs, channels, stride))</span><br><span class="line">            self.inputs = channels*block.growth</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet18</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(block=BasicBlock, layers=[<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet34</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(block=BasicBlock, layers=[<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(UpgradeBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet101</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(UpgradeBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet152</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(UpgradeBlock, [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch实现ResNextNet</title>
      <link href="/2018/09/15/2018-09-15-Pytorch-ResNextNet/"/>
      <url>/2018/09/15/2018-09-15-Pytorch-ResNextNet/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/15 20:48</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : ResnextNet.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    growth = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputs, cardinality=<span class="number">32</span>, block_width=<span class="number">4</span>, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(Block, self).__init__()</span><br><span class="line">        outs = cardinality*block_width</span><br><span class="line">        self.left = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inputs, outs, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outs),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(outs, outs, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, groups=cardinality, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outs),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(outs, self.growth * outs, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(self.growth * outs)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> inputs != self.growth * outs:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inputs, self.growth * outs, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="keyword">False</span>),</span><br><span class="line">                nn.BatchNorm2d(self.growth * outs)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.left(inputs)</span><br><span class="line">        network += self.shortcut(inputs)</span><br><span class="line">        out = F.relu(network)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResnextNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers, cardinality, block_width)</span>:</span></span><br><span class="line">        super(ResnextNet, self).__init__()</span><br><span class="line">        self.inputs = <span class="number">64</span></span><br><span class="line">        self.cardinality = cardinality</span><br><span class="line">        self.block_width = block_width</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.conv2 = self._block(layers=layers[<span class="number">0</span>], stride=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = self._block(layers=layers[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.conv4 = self._block(layers=layers[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.conv5 = self._block(layers=layers[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(<span class="number">8</span> * cardinality * block_width, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.conv1(inputs)</span><br><span class="line">        network = self.conv2(network)</span><br><span class="line">        network = self.conv3(network)</span><br><span class="line">        network = self.conv4(network)</span><br><span class="line">        network = self.conv5(network)</span><br><span class="line">        print(network.shape)</span><br><span class="line">        network = F.avg_pool2d(network, kernel_size=network.shape[<span class="number">2</span>]//<span class="number">2</span>)</span><br><span class="line">        print(network.shape)</span><br><span class="line">        network = network.view(network.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = self.linear(network)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out, network</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_block</span><span class="params">(self, layers, stride)</span>:</span></span><br><span class="line">        strides = [stride] + [<span class="number">1</span>] * (layers - <span class="number">1</span>)  <span class="comment"># strides=[1,1]</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> stride <span class="keyword">in</span> strides:</span><br><span class="line">            layers.append(Block(self.inputs, self.cardinality, self.block_width, stride))</span><br><span class="line">            self.inputs = self.block_width*self.cardinality*Block.growth</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNext50_32x4d</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ResnextNet(layers=[<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], cardinality=<span class="number">32</span>, block_width=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNext50_4x32d</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ResnextNet(layers=[<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], cardinality=<span class="number">4</span>, block_width=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNext50_64x4d</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ResnextNet(layers=[<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], cardinality=<span class="number">64</span>, block_width=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNext101_32x4d</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ResnextNet(layers=[<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], cardinality=<span class="number">32</span>, block_width=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNext101_64x4d</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ResnextNet(layers=[<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], cardinality=<span class="number">64</span>, block_width=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch实现VGGNet</title>
      <link href="/2018/09/15/2018-09-15-Pytorch-VGGNet/"/>
      <url>/2018/09/15/2018-09-15-Pytorch-VGGNet/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/14 13:33</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : VGG.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">model = &#123;</span><br><span class="line">    <span class="string">'VGG11'</span>: [<span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'VGG13'</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'VGG16'</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">    <span class="string">'VGG19'</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGG</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vgg_name)</span>:</span></span><br><span class="line">        super(VGG, self).__init__()</span><br><span class="line">        self.features = self._make_layers(model[vgg_name])</span><br><span class="line">        self.classifier = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        network = self.features(x)</span><br><span class="line">        network = network.view(network.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = self.classifier(network)</span><br><span class="line">        <span class="keyword">return</span> out, network</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layers</span><span class="params">(models)</span>:</span></span><br><span class="line">        layers = []</span><br><span class="line">        in_channels = <span class="number">3</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> models:</span><br><span class="line">            <span class="keyword">if</span> layer == <span class="string">'M'</span>:</span><br><span class="line">                layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layers += [nn.Conv2d(in_channels, layer, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                           nn.BatchNorm2d(layer),</span><br><span class="line">                           nn.ReLU(inplace=<span class="keyword">True</span>)]</span><br><span class="line">                in_channels = layer</span><br><span class="line">        layers += [nn.AvgPool2d(kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch实现CIFAR-10之数据预处理</title>
      <link href="/2018/09/15/2018-09-15-Pytorch-cifar10-data/"/>
      <url>/2018/09/15/2018-09-15-Pytorch-cifar10-data/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/13 19:45</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : train.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">print(<span class="string">'==&gt; Preparing data..'</span>)</span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">transform_test = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">DOWNLOAD_MNIST = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span>(os.path.exists(<span class="string">'./data/'</span>)) <span class="keyword">or</span> <span class="keyword">not</span> os.listdir(<span class="string">'./data/'</span>):  <span class="comment"># 判断数据是否存在</span></span><br><span class="line">    DOWNLOAD_MNIST = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>,</span><br><span class="line">                                        train=<span class="keyword">True</span>,</span><br><span class="line">                                        download=DOWNLOAD_MNIST,</span><br><span class="line">                                        transform=transform_train)</span><br><span class="line"></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset,</span><br><span class="line">                                          batch_size=<span class="number">128</span>,</span><br><span class="line">                                          shuffle=<span class="keyword">True</span>,</span><br><span class="line">                                          num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>,</span><br><span class="line">                                       train=<span class="keyword">False</span>,</span><br><span class="line">                                       download=DOWNLOAD_MNIST,</span><br><span class="line">                                       transform=transform_test)</span><br><span class="line"></span><br><span class="line">testloader = torch.utils.data.DataLoader(testset,</span><br><span class="line">                                         batch_size=<span class="number">100</span>,</span><br><span class="line">                                         shuffle=<span class="keyword">False</span>,</span><br><span class="line">                                         num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch实现AlexNet</title>
      <link href="/2018/09/15/2018-09-15-Pytorch-AlexNet/"/>
      <url>/2018/09/15/2018-09-15-Pytorch-AlexNet/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(AlexNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>,</span><br><span class="line">                      out_channels=<span class="number">96</span>,</span><br><span class="line">                      kernel_size=<span class="number">2</span>,</span><br><span class="line">                      stride=<span class="number">1</span>,</span><br><span class="line">                      padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.conv2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">96</span>,</span><br><span class="line">                      out_channels=<span class="number">256</span>,</span><br><span class="line">                      kernel_size=<span class="number">2</span>,</span><br><span class="line">                      stride=<span class="number">1</span>,</span><br><span class="line">                      padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.conv3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">256</span>,</span><br><span class="line">                      out_channels=<span class="number">384</span>,</span><br><span class="line">                      kernel_size=<span class="number">3</span>,</span><br><span class="line">                      stride=<span class="number">1</span>,</span><br><span class="line">                      padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line">        self.conv4 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">384</span>,</span><br><span class="line">                      out_channels=<span class="number">384</span>,</span><br><span class="line">                      kernel_size=<span class="number">3</span>,</span><br><span class="line">                      stride=<span class="number">1</span>,</span><br><span class="line">                      padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line">        self.conv5 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">384</span>,</span><br><span class="line">                      out_channels=<span class="number">256</span>,</span><br><span class="line">                      kernel_size=<span class="number">2</span>,</span><br><span class="line">                      stride=<span class="number">1</span>,</span><br><span class="line">                      padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">4</span> * <span class="number">4</span> * <span class="number">256</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.8</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.fc2 = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(p=<span class="number">0.8</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.out = nn.Linear(<span class="number">1024</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.conv1(inputs)</span><br><span class="line">        network = self.conv2(network)</span><br><span class="line">        network = self.conv3(network)</span><br><span class="line">        network = self.conv4(network)</span><br><span class="line">        network = self.conv5(network)</span><br><span class="line">        network = network.view(network.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        network = self.fc1(network)</span><br><span class="line">        network = self.fc2(network)</span><br><span class="line">        out = self.out(network)</span><br><span class="line">        <span class="keyword">return</span> out, network</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch实现GoogleNet</title>
      <link href="/2018/09/15/2018-09-15-Pytorch-GoogleNet/"/>
      <url>/2018/09/15/2018-09-15-Pytorch-GoogleNet/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/14 15:11</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : GoogleNet.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes)</span>:</span></span><br><span class="line">        super(Inception, self).__init__()</span><br><span class="line">        <span class="comment"># 1x1 conv branch</span></span><br><span class="line">        self.b1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_planes, n1x1, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(n1x1),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1x1 conv -&gt; 3x3 conv branch</span></span><br><span class="line">        self.b2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_planes, n3x3red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(n3x3red),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Conv2d(n3x3red, n3x3, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(n3x3),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1x1 conv -&gt; 5x5 conv branch</span></span><br><span class="line">        self.b3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_planes, n5x5red, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(n5x5red),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Conv2d(n5x5red, n5x5, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(n5x5),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">            nn.Conv2d(n5x5, n5x5, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(n5x5),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3x3 pool -&gt; 1x1 conv branch</span></span><br><span class="line">        self.b4 = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Conv2d(in_planes, pool_planes, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(pool_planes),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y1 = self.b1(x)</span><br><span class="line">        y2 = self.b2(x)</span><br><span class="line">        y3 = self.b3(x)</span><br><span class="line">        y4 = self.b4(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat([y1, y2, y3, y4], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GoogLeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(GoogLeNet, self).__init__()</span><br><span class="line">        self.pre_layers = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">192</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.a3 = Inception(<span class="number">192</span>,  <span class="number">64</span>,  <span class="number">96</span>, <span class="number">128</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.b3 = Inception(<span class="number">256</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">96</span>, <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">        self.maxpool = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.a4 = Inception(<span class="number">480</span>, <span class="number">192</span>,  <span class="number">96</span>, <span class="number">208</span>, <span class="number">16</span>,  <span class="number">48</span>,  <span class="number">64</span>)</span><br><span class="line">        self.b4 = Inception(<span class="number">512</span>, <span class="number">160</span>, <span class="number">112</span>, <span class="number">224</span>, <span class="number">24</span>,  <span class="number">64</span>,  <span class="number">64</span>)</span><br><span class="line">        self.c4 = Inception(<span class="number">512</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">24</span>,  <span class="number">64</span>,  <span class="number">64</span>)</span><br><span class="line">        self.d4 = Inception(<span class="number">512</span>, <span class="number">112</span>, <span class="number">144</span>, <span class="number">288</span>, <span class="number">32</span>,  <span class="number">64</span>,  <span class="number">64</span>)</span><br><span class="line">        self.e4 = Inception(<span class="number">528</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">        self.a5 = Inception(<span class="number">832</span>, <span class="number">256</span>, <span class="number">160</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        self.b5 = Inception(<span class="number">832</span>, <span class="number">384</span>, <span class="number">192</span>, <span class="number">384</span>, <span class="number">48</span>, <span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">        self.avgpool = nn.AvgPool2d(kernel_size=<span class="number">8</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">1024</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.pre_layers(inputs)</span><br><span class="line">        network = self.a3(network)</span><br><span class="line">        network = self.b3(network)</span><br><span class="line">        network = self.maxpool(network)</span><br><span class="line">        network = self.a4(network)</span><br><span class="line">        network = self.b4(network)</span><br><span class="line">        network = self.c4(network)</span><br><span class="line">        network = self.d4(network)</span><br><span class="line">        network = self.e4(network)</span><br><span class="line">        network = self.maxpool(network)</span><br><span class="line">        network = self.a5(network)</span><br><span class="line">        network = self.b5(network)</span><br><span class="line">        network = self.avgpool(network)</span><br><span class="line">        network = network.view(network.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = self.linear(network)</span><br><span class="line">        <span class="keyword">return</span> out, network</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch实现CIFAR10之读取模型训练本地图片</title>
      <link href="/2018/09/15/2018-09-15-Pytorch-Cifar10-test/"/>
      <url>/2018/09/15/2018-09-15-Pytorch-Cifar10-test/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/14 12:51</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : test.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取模型</span></span><br><span class="line">model = torch.load(<span class="string">'LeNet.pkl'</span>)</span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">local_photos</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># input</span></span><br><span class="line">    im = Image.open(<span class="string">'plane.jpg'</span>)</span><br><span class="line">    <span class="comment"># im = im.convert('L')</span></span><br><span class="line"></span><br><span class="line">    im = im.resize((<span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">    <span class="comment"># im.show()</span></span><br><span class="line">    im = np.array(im).astype(np.float32)</span><br><span class="line">    im = np.reshape(im, [<span class="number">-1</span>, <span class="number">32</span>*<span class="number">32</span>*<span class="number">3</span>])</span><br><span class="line">    im = (im - (<span class="number">255</span> / <span class="number">2.0</span>)) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">    batch_xs = np.reshape(im, [<span class="number">-1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">    batch_xs = torch.FloatTensor(batch_xs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    pred_y, _ = model(batch_xs)</span><br><span class="line">    pred_y = torch.max(pred_y, <span class="number">1</span>)[<span class="number">1</span>].data.numpy().squeeze()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"The predict is : "</span>, classes[pred_y])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">local_photos()</span><br></pre></td></tr></table></figure><h3 id="测试图片："><a href="#测试图片：" class="headerlink" title="测试图片："></a>测试图片：</h3><p><img src="/images/dl/127.jpg" alt="images"></p><h3 id="测试结果："><a href="#测试结果：" class="headerlink" title="测试结果："></a>测试结果：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The predict is :  plane</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch实现CIFAR10之训练模型</title>
      <link href="/2018/09/15/2018-09-15-Pytorch-Cifar10-train/"/>
      <url>/2018/09/15/2018-09-15-Pytorch-Cifar10-train/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">best_acc = <span class="number">0</span>  <span class="comment"># best test accuracy</span></span><br><span class="line">epochs = <span class="number">20</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line"><span class="comment"># Training</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        print(<span class="string">'\nEpoch: &#123;&#125;'</span>.format(epoch + <span class="number">1</span>))</span><br><span class="line">        train_loss = <span class="number">0</span></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> step, (inputs, targets) <span class="keyword">in</span> enumerate(trainloader):</span><br><span class="line"></span><br><span class="line">            outputs = net(inputs)[<span class="number">0</span>]</span><br><span class="line">            loss = criterion(outputs, targets)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            train_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            _, predicted = outputs.max(<span class="number">1</span>)</span><br><span class="line">            total += targets.size(<span class="number">0</span>)</span><br><span class="line">            correct += predicted.eq(targets).sum().item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"step:&#123;&#125; "</span>.format(step))</span><br><span class="line">                print(<span class="string">"Loss:%.4f "</span> % (train_loss / (step + <span class="number">1</span>)))</span><br><span class="line">                print(<span class="string">"train Accuracy: %4f"</span> % (<span class="number">100.</span>*correct/total))</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Finished Training'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span></span><br><span class="line">    net.eval()</span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> step, (inputs, targets) <span class="keyword">in</span> enumerate(testloader):</span><br><span class="line">            outputs, _ = net(inputs)</span><br><span class="line">            loss = criterion(outputs, targets)</span><br><span class="line"></span><br><span class="line">            test_loss += loss.item()</span><br><span class="line">            _, predicted = outputs.max(<span class="number">1</span>)</span><br><span class="line">            total += targets.size(<span class="number">0</span>)</span><br><span class="line">            correct += predicted.eq(targets).sum().item()</span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"step:&#123;&#125; "</span>.format(step))</span><br><span class="line">                print(<span class="string">"Loss:%.4f "</span> % (test_loss / (step + <span class="number">1</span>)))</span><br><span class="line">                print(<span class="string">"Test Accuracy: %4f"</span> % (<span class="number">100.</span>*correct/total))</span><br><span class="line">    print(<span class="string">"TEST Finished"</span>)</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pytorch实现DenseNet</title>
      <link href="/2018/09/15/2018-09-15-Pytorch-DenseNet/"/>
      <url>/2018/09/15/2018-09-15-Pytorch-DenseNet/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/14 16:02</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : DenseNet.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bn_act_conv_drop</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputs, outs, kernel_size, padding)</span>:</span></span><br><span class="line">        super(Bn_act_conv_drop, self).__init__()</span><br><span class="line">        self.bn = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(inputs),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=inputs,</span><br><span class="line">                out_channels=outs,</span><br><span class="line">                kernel_size=kernel_size,</span><br><span class="line">                padding=padding,</span><br><span class="line">                stride=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.bn(inputs)</span><br><span class="line">        network = self.conv(network)</span><br><span class="line">        <span class="keyword">return</span> network</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transition</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputs, outs)</span>:</span></span><br><span class="line">        super(Transition, self).__init__()</span><br><span class="line">        self.conv = Bn_act_conv_drop(inputs, outs, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.avgpool = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.conv(inputs)</span><br><span class="line">        network = self.avgpool(network)</span><br><span class="line">        <span class="keyword">return</span> network</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Block</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputs, growth)</span>:</span></span><br><span class="line">        super(Block, self).__init__()</span><br><span class="line">        self.conv1 = Bn_act_conv_drop(inputs, <span class="number">4</span>*growth, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.conv2 = Bn_act_conv_drop(<span class="number">4</span>*growth, growth, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.conv1(inputs)</span><br><span class="line">        network = self.conv2(network)</span><br><span class="line"></span><br><span class="line">        out = torch.cat([network, inputs], <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, blocks, growth)</span>:</span></span><br><span class="line">        super(DenseNet, self).__init__()</span><br><span class="line">        num_planes = <span class="number">2</span>*growth</span><br><span class="line">        inputs = <span class="number">3</span></span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels=inputs,</span><br><span class="line">                out_channels=num_planes,</span><br><span class="line">                kernel_size=<span class="number">3</span>,</span><br><span class="line">                <span class="comment"># stride=2,</span></span><br><span class="line">                padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            <span class="comment"># nn.MaxPool2d(kernel_size=2, stride=2)</span></span><br><span class="line">        )</span><br><span class="line">        self.block1 = self._block(blocks[<span class="number">0</span>], num_planes, growth)</span><br><span class="line">        num_planes += blocks[<span class="number">0</span>] * growth</span><br><span class="line">        out_planes = int(math.floor(num_planes * <span class="number">0.5</span>))</span><br><span class="line">        self.tran1 = Transition(inputs=num_planes, outs=out_planes)</span><br><span class="line">        num_planes = out_planes</span><br><span class="line"></span><br><span class="line">        self.block2 = self._block(blocks[<span class="number">1</span>], num_planes, growth)</span><br><span class="line">        num_planes += blocks[<span class="number">1</span>] * growth</span><br><span class="line">        out_planes = int(math.floor(num_planes * <span class="number">0.5</span>))</span><br><span class="line">        self.tran2 = Transition(inputs=num_planes, outs=out_planes)</span><br><span class="line">        num_planes = out_planes</span><br><span class="line"></span><br><span class="line">        self.block3 = self._block(blocks[<span class="number">2</span>], num_planes, growth)</span><br><span class="line">        num_planes += blocks[<span class="number">2</span>] * growth</span><br><span class="line">        out_planes = int(math.floor(num_planes * <span class="number">0.5</span>))</span><br><span class="line">        self.tran3 = Transition(inputs=num_planes, outs=out_planes)</span><br><span class="line">        num_planes = out_planes</span><br><span class="line"></span><br><span class="line">        self.block4 = self._block(blocks[<span class="number">3</span>], num_planes, growth)</span><br><span class="line">        num_planes += blocks[<span class="number">3</span>] * growth</span><br><span class="line"></span><br><span class="line">        self.bn = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(num_planes),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.avgpool = nn.AvgPool2d(kernel_size=<span class="number">4</span>)</span><br><span class="line">        self.linear = nn.Linear(num_planes, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        network = self.conv(inputs)</span><br><span class="line">        network = self.block1(network)</span><br><span class="line">        network = self.tran1(network)</span><br><span class="line"></span><br><span class="line">        network = self.block2(network)</span><br><span class="line">        network = self.tran2(network)</span><br><span class="line"></span><br><span class="line">        network = self.block3(network)</span><br><span class="line">        network = self.tran3(network)</span><br><span class="line"></span><br><span class="line">        network = self.block4(network)</span><br><span class="line">        network = self.bn(network)</span><br><span class="line"></span><br><span class="line">        network = self.avgpool(network)</span><br><span class="line">        network = network.view(network.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = self.linear(network)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out, network</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_block</span><span class="params">(layers, inputs, growth)</span>:</span></span><br><span class="line">        block_layer = []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> range(layers):</span><br><span class="line">            network = Block(inputs, growth)</span><br><span class="line">            block_layer.append(network)</span><br><span class="line">            inputs += growth</span><br><span class="line">        block_layer = nn.Sequential(*block_layer)</span><br><span class="line">        <span class="keyword">return</span> block_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DenseNet121</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> DenseNet(blocks=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">16</span>], growth=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DenseNet169</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> DenseNet(blocks=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">32</span>, <span class="number">32</span>], growth=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DenseNet201</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> DenseNet(blocks=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">48</span>, <span class="number">32</span>], growth=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DenseNet161</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> DenseNet(blocks=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">36</span>, <span class="number">24</span>], growth=<span class="number">48</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DenseNet_cifar</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> DenseNet(blocks=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">16</span>], growth=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow-cifar10-图像分类之数据预处理及配置</title>
      <link href="/2018/09/07/2018-09-07-TensorFlow-cifar/"/>
      <url>/2018/09/07/2018-09-07-TensorFlow-cifar/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="预处理数据："><a href="#预处理数据：" class="headerlink" title="预处理数据："></a><strong>预处理数据</strong>：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/9/6 15:31</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : Read_data.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 加载数据</span><br><span class="line">import pickle</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.preprocessing import MinMaxScaler, LabelBinarizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_cifar10_batch(path, batch_id):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载batch的数据</span><br><span class="line">    :param path: 数据存储的目录</span><br><span class="line">    :param batch_id:batch的编号</span><br><span class="line">    :return:features and labels</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    with open(path + &apos;/data_batch_&apos; + str(batch_id), mode=&apos;rb&apos;) as file:</span><br><span class="line">        batch = pickle.load(file, encoding=&apos;latin1&apos;)</span><br><span class="line"></span><br><span class="line">    # features and labels</span><br><span class="line">    features = batch[&apos;data&apos;].reshape((len(batch[&apos;data&apos;]), 3, 32, 32)).transpose(0, 2, 3, 1)</span><br><span class="line">    labels = batch[&apos;labels&apos;]</span><br><span class="line"></span><br><span class="line">    return features, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数据预处理</span><br><span class="line">def pre_processing_data(x_train, y_train, x_test, y_test):</span><br><span class="line">    # features</span><br><span class="line">    minmax = MinMaxScaler()</span><br><span class="line">    # 重塑数据</span><br><span class="line">    # (50000, 32, 32, 3) --&gt; (50000, 32*32*3)</span><br><span class="line">    x_train_rows = x_train.reshape(x_train.shape[0], 32*32*3)</span><br><span class="line">    # (10000, 32, 32, 3) --&gt; (10000, 32*32*3)</span><br><span class="line">    x_test_rows = x_test.reshape(x_test.shape[0], 32*32*3)</span><br><span class="line">    # 归一化</span><br><span class="line">    x_train_norm = minmax.fit_transform(x_train_rows)</span><br><span class="line">    x_test_norm = minmax.fit_transform(x_test_rows)</span><br><span class="line">    # 重塑数据</span><br><span class="line">    x_train = x_train_norm.reshape(x_train_norm.shape[0], 32, 32, 3)</span><br><span class="line">    x_test = x_test_norm.reshape(x_test_norm.shape[0], 32, 32, 3)</span><br><span class="line"></span><br><span class="line">    # labels</span><br><span class="line">    # 对标签进行one-hot</span><br><span class="line">    n_class = 10</span><br><span class="line">    label_binarizer = LabelBinarizer().fit(np.array(range(n_class)))</span><br><span class="line">    y_train = label_binarizer.transform(y_train)</span><br><span class="line">    y_test = label_binarizer.transform(y_test)</span><br><span class="line"></span><br><span class="line">    return x_train, y_train, x_test, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def cifar10_data():</span><br><span class="line">    # 加载训练数据</span><br><span class="line">    cifar10_path = &apos;data&apos;</span><br><span class="line">    # 一共是有5个batch的训练数据</span><br><span class="line">    x_train, y_train = load_cifar10_batch(cifar10_path, 1)</span><br><span class="line">    for n in range(2, 6):</span><br><span class="line">        features, labels = load_cifar10_batch(cifar10_path, n)</span><br><span class="line">        x_train = np.concatenate([x_train, features])</span><br><span class="line">        y_train = np.concatenate([y_train, labels])</span><br><span class="line"></span><br><span class="line">    # 加载测试数据</span><br><span class="line">    with open(cifar10_path + &apos;/test_batch&apos;, mode=&apos;rb&apos;) as file:</span><br><span class="line">        batch = pickle.load(file, encoding=&apos;latin1&apos;)</span><br><span class="line">        x_test = batch[&apos;data&apos;].reshape((len(batch[&apos;data&apos;]), 3, 32, 32)).transpose(0, 2, 3, 1)</span><br><span class="line">        y_test = batch[&apos;labels&apos;]</span><br><span class="line"></span><br><span class="line">    x_train, y_train, x_test, y_test = pre_processing_data(x_train, y_train, x_test, y_test)</span><br><span class="line"></span><br><span class="line">    return x_train, y_train, x_test, y_test</span><br></pre></td></tr></table></figure><h3 id="配置文件："><a href="#配置文件：" class="headerlink" title="配置文件："></a><strong>配置文件</strong>：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/9/6 16:02</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : config.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 初始化卷积神经网络参数</span><br><span class="line">keep_prob = 0.8</span><br><span class="line">epochs = 20</span><br><span class="line">batch_size = 128</span><br><span class="line">n_classes = 10  # 总共10类</span><br><span class="line"></span><br><span class="line"># 定义输入和标签的placeholder</span><br><span class="line">inputs = tf.placeholder(tf.float32, [None, 32, 32, 3], name=&apos;inputs&apos;)</span><br><span class="line">targets = tf.placeholder(tf.float32, [None, 10], name=&apos;logits&apos;)</span><br><span class="line"></span><br><span class="line">learning_rate = 0.001</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 显示图片</span><br><span class="line">def show_images(images):</span><br><span class="line">    fig, axes = plt.subplots(nrows=3, ncols=3, sharex=True, sharey=True, figsize=(9, 9))</span><br><span class="line">    img = images[: 60]</span><br><span class="line">    for image, row in zip([img[: 20], img[20: 40], img[40: 60]], axes):</span><br><span class="line">        for img, ax in zip(image, row):</span><br><span class="line">            ax.imshow(img)</span><br><span class="line">            ax.get_xaxis().set_visible(False)</span><br><span class="line">            ax.get_yaxis().set_visible(False)</span><br><span class="line"></span><br><span class="line">    fig.tight_layout(pad=0.1)</span><br><span class="line">    # plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 存储alexnet所有的网络参数</span><br><span class="line">weights = &#123;</span><br><span class="line">    &apos;wc1&apos;: tf.Variable(tf.random_normal(shape=[11, 11, 3, 96])),</span><br><span class="line">    &apos;wc2&apos;: tf.Variable(tf.random_normal(shape=[5, 5, 96, 256])),</span><br><span class="line">    &apos;wc3&apos;: tf.Variable(tf.random_normal(shape=[3, 3, 256, 384])),</span><br><span class="line">    &apos;wc4&apos;: tf.Variable(tf.random_normal(shape=[3, 3, 384, 384])),</span><br><span class="line">    &apos;wc5&apos;: tf.Variable(tf.random_normal(shape=[3, 3, 384, 256])),</span><br><span class="line">    &apos;wd1&apos;: tf.Variable(tf.random_normal(shape=[4*4*256, 4096])),</span><br><span class="line">    &apos;wd2&apos;: tf.Variable(tf.random_normal(shape=[4096, 1024])),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal(shape=[1024, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    &apos;bc1&apos;: tf.Variable(tf.random_normal([96])),</span><br><span class="line">    &apos;bc2&apos;: tf.Variable(tf.random_normal([256])),</span><br><span class="line">    &apos;bc3&apos;: tf.Variable(tf.random_normal([384])),</span><br><span class="line">    &apos;bc4&apos;: tf.Variable(tf.random_normal([384])),</span><br><span class="line">    &apos;bc5&apos;: tf.Variable(tf.random_normal([256])),</span><br><span class="line">    &apos;bd1&apos;: tf.Variable(tf.random_normal([4096])),</span><br><span class="line">    &apos;bd2&apos;: tf.Variable(tf.random_normal([1024])),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([n_classes]))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow数据可视化</title>
      <link href="/2018/09/07/2018-09-07-TensorFlow-visualization/"/>
      <url>/2018/09/07/2018-09-07-TensorFlow-visualization/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a><strong>代码</strong>：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/9/7 16:59</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    :</span><br><span class="line"># @File    : demo.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">n_observations = 100</span><br><span class="line">xs = np.linspace(-3, 3, n_observations)</span><br><span class="line">ys = 0.8*xs + 0.1 + np.random.uniform(-0.5, 0.5, n_observations)</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, name=&apos;X&apos;)</span><br><span class="line">Y = tf.placeholder(tf.float32, name=&apos;Y&apos;)</span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.random_normal([1]), name=&apos;weight&apos;)</span><br><span class="line">tf.summary.histogram(&apos;weight&apos;, W)</span><br><span class="line">b = tf.Variable(tf.random_normal([1]), name=&apos;bias&apos;)</span><br><span class="line">tf.summary.histogram(&apos;bias&apos;, b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Y_pred = tf.add(tf.multiply(X, W), b)</span><br><span class="line"></span><br><span class="line">loss = tf.square(Y - Y_pred, name=&apos;loss&apos;)</span><br><span class="line">tf.summary.scalar(&apos;loss&apos;, tf.reshape(loss, []))</span><br><span class="line"></span><br><span class="line">learning_rate = 0.01</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span><br><span class="line"></span><br><span class="line">n_samples = xs.shape[0]</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    # 记得初始化所有变量</span><br><span class="line">    sess.run(init)</span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line">    log_writer = tf.summary.FileWriter(&quot;./logs/linear_regression&quot;, sess.graph)</span><br><span class="line"></span><br><span class="line">    # 训练模型</span><br><span class="line">    for i in range(50):</span><br><span class="line">        total_loss = 0</span><br><span class="line">        for x, y in zip(xs, ys):</span><br><span class="line">            # 通过feed_dic把数据灌进去</span><br><span class="line">            _, loss_value, merged_summary = sess.run([optimizer, loss, merged], feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line">            total_loss += loss_value</span><br><span class="line"></span><br><span class="line">        if i % 5 == 0:</span><br><span class="line">            print(&apos;Epoch &#123;0&#125;: &#123;1&#125;&apos;.format(i, total_loss / n_samples))</span><br><span class="line">            log_writer.add_summary(merged_summary, i)</span><br><span class="line"></span><br><span class="line">    # 关闭writer</span><br><span class="line">    log_writer.close()</span><br><span class="line"></span><br><span class="line">    # 取出w和b的值</span><br><span class="line">    W, b = sess.run([W, b])</span><br><span class="line"></span><br><span class="line">print(W, b)</span><br><span class="line">print(&quot;W:&quot;+str(W[0]))</span><br><span class="line">print(&quot;b:&quot;+str(b[0]))</span><br></pre></td></tr></table></figure><h4 id="执行结果："><a href="#执行结果：" class="headerlink" title="执行结果："></a><strong>执行结果</strong>：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0: [0.5815637]</span><br><span class="line">Epoch 5: [0.08926834]</span><br><span class="line">Epoch 10: [0.08926827]</span><br><span class="line">Epoch 15: [0.08926827]</span><br><span class="line">Epoch 20: [0.08926827]</span><br><span class="line">Epoch 25: [0.08926827]</span><br><span class="line">Epoch 30: [0.08926827]</span><br><span class="line">Epoch 35: [0.08926827]</span><br><span class="line">Epoch 40: [0.08926827]</span><br><span class="line">Epoch 45: [0.08926827]</span><br><span class="line">[0.7907032] [0.10920969]</span><br><span class="line">W:0.7907032</span><br><span class="line">b:0.10920969</span><br></pre></td></tr></table></figure><h4 id="Tensoboard："><a href="#Tensoboard：" class="headerlink" title="Tensoboard："></a><strong>Tensoboard</strong>：</h4><h4 id="在终端执行代码："><a href="#在终端执行代码：" class="headerlink" title="在终端执行代码："></a>在终端执行代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir log (你保存文件所在位置)</span><br></pre></td></tr></table></figure><h4 id="输出："><a href="#输出：" class="headerlink" title="输出："></a>输出：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TensorBoard 0.4.0 at http://seven:6006 (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure><p>然后打开网页：<code>http://seven:6006</code>。</p><h4 id="显示结果："><a href="#显示结果：" class="headerlink" title="显示结果："></a>显示结果：</h4><p><img src="/images/dl/123.png" alt="image"></p><p><img src="/images/dl/124.png" alt="image"></p><p><img src="/images/dl/125.png" alt="image"></p><p><img src="/images/dl/126.png" alt="image"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow-cifar10-图像分类之训练模型及可视化数据</title>
      <link href="/2018/09/07/2018-09-07-TensorFlow-cifar-train/"/>
      <url>/2018/09/07/2018-09-07-TensorFlow-cifar-train/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="训练文件"><a href="#训练文件" class="headerlink" title="训练文件:"></a><strong>训练文件</strong>:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/7 14:07</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : TrainModel.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 模型训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：导入环境</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(network, X_train, y_train, X_test, y_test, augmentation=False)</span>:</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X_train shape:'</span>, X_train.shape)</span><br><span class="line">    print(<span class="string">'Y_train shape:'</span>, y_train.shape)</span><br><span class="line">    print(X_train.shape[<span class="number">0</span>], <span class="string">'x_training samples'</span>)</span><br><span class="line">    print(X_test.shape[<span class="number">0</span>], <span class="string">'validation samples'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 规一化处理</span></span><br><span class="line">    X_train = X_train.astype(<span class="string">'float32'</span>)</span><br><span class="line">    X_test = X_test.astype(<span class="string">'float32'</span>)</span><br><span class="line">    X_train /= <span class="number">255</span></span><br><span class="line">    X_test /= <span class="number">255</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO： 初始化模型</span></span><br><span class="line">    inputs, logits, name = network</span><br><span class="line">    model = Model(inputs=inputs, outputs=logits, name=<span class="string">'model'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 计算损失值并初始化optimizer</span></span><br><span class="line">    model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">                  optimizer=<span class="string">'adadelta'</span>,</span><br><span class="line">                  metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">    model.summary()</span><br><span class="line">    print(<span class="string">'FUNCTION READY!!'</span>)</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 开始训练</span></span><br><span class="line">    print(<span class="string">'TRAINING....'</span>)</span><br><span class="line">    epoch = <span class="number">100</span></span><br><span class="line">    batch_size = <span class="number">256</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="comment"># 数据增强</span></span><br><span class="line">    <span class="keyword">if</span> augmentation:</span><br><span class="line">        aug_gen = ImageDataGenerator(</span><br><span class="line">            featurewise_center=<span class="keyword">False</span>,  <span class="comment"># set input mean to 0 over the dataset</span></span><br><span class="line">            samplewise_center=<span class="keyword">False</span>,  <span class="comment"># set each sample mean to 0</span></span><br><span class="line">            featurewise_std_normalization=<span class="keyword">False</span>,  <span class="comment"># divide inputs by std of the dataset</span></span><br><span class="line">            samplewise_std_normalization=<span class="keyword">False</span>,  <span class="comment"># divide each input by its std</span></span><br><span class="line">            zca_whitening=<span class="keyword">False</span>,  <span class="comment"># apply ZCA whitening</span></span><br><span class="line">            rotation_range=<span class="number">0</span>,  <span class="comment"># randomly rotate images in the range (degrees, 0 to 180)</span></span><br><span class="line">            width_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images horizontally (fraction of total width)</span></span><br><span class="line">            height_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images vertically (fraction of total height)</span></span><br><span class="line">            horizontal_flip=<span class="keyword">True</span>,  <span class="comment"># randomly flip images</span></span><br><span class="line">            vertical_flip=<span class="keyword">False</span>,  <span class="comment"># randomly flip images</span></span><br><span class="line">        )</span><br><span class="line">        aug_gen.fit(X_train)</span><br><span class="line">        generator = aug_gen.flow(X_train, y_train, batch_size=batch_size)</span><br><span class="line">        out = model.fit_generator(generator=generator,</span><br><span class="line">                                  steps_per_epoch=<span class="number">50000</span> // batch_size,</span><br><span class="line">                                  epochs=epoch,</span><br><span class="line">                                  validation_data=(X_test, y_test))</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> 保存模型</span></span><br><span class="line">        model.save(<span class="string">'CIFAR10_model_with_data_augmentation_%s.h5'</span> % name)</span><br><span class="line">    <span class="comment"># 不使用数据增强</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        out = model.fit(x=X_train, y=y_train,</span><br><span class="line">                        batch_size=batch_size,</span><br><span class="line">                        epochs=epoch,</span><br><span class="line">                        validation_data=(X_test, y_test),</span><br><span class="line">                        shuffle=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> 保存模型</span></span><br><span class="line">        model.save(<span class="string">'CIFAR10_model_no_data_augmentation_%s.h5'</span> % name)</span><br><span class="line"></span><br><span class="line">    loss, accuracy = model.evaluate(X_train, y_train, verbose=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">"Training Accuracy = %.2f %%     loss = %f"</span> % (accuracy * <span class="number">100</span>, loss))</span><br><span class="line">    loss, accuracy = model.evaluate(X_test, y_test, verbose=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">"Testing Accuracy = %.2f %%    loss = %f"</span> % (accuracy * <span class="number">100</span>, loss))</span><br><span class="line">    print(<span class="string">'@ Total Time Spent: %.2f seconds'</span> % (time.time() - start))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, epoch, model</span><br></pre></td></tr></table></figure><h3 id="可视化数据："><a href="#可视化数据：" class="headerlink" title="可视化数据："></a><strong>可视化数据</strong>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/7 15:00</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : visualization.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 可视化数据</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_acc_loss</span><span class="params">(data, epoch)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    可视化数据</span></span><br><span class="line"><span class="string">    :param data: 数据</span></span><br><span class="line"><span class="string">    :param epoch: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    acc, loss, val_acc, val_loss = data.history[<span class="string">'acc'</span>], data.history[<span class="string">'loss'</span>], \</span><br><span class="line">                                   data.history[<span class="string">'val_acc'</span>], data.history[<span class="string">'val_loss'</span>]</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.plot(range(epoch), acc, label=<span class="string">'Train'</span>)</span><br><span class="line">    plt.plot(range(epoch), val_acc, label=<span class="string">'Test'</span>)</span><br><span class="line">    plt.title(<span class="string">'Accuracy over '</span> + str(epoch) + <span class="string">' Epochs'</span>, size=<span class="number">15</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(range(epoch), loss, label=<span class="string">'Train'</span>)</span><br><span class="line">    plt.plot(range(epoch), val_loss, label=<span class="string">'Test'</span>)</span><br><span class="line">    plt.title(<span class="string">'Loss over '</span> + str(epoch) + <span class="string">' Epochs'</span>, size=<span class="number">15</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid(<span class="keyword">True</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span><span class="params">(x_test, y_test, class_name, model)</span>:</span></span><br><span class="line">    rand_id = np.random.choice(range(<span class="number">10000</span>), size=<span class="number">10</span>)</span><br><span class="line">    X_pred = np.array([x_test[i] <span class="keyword">for</span> i <span class="keyword">in</span> rand_id])</span><br><span class="line">    y_true = [y_test[i] <span class="keyword">for</span> i <span class="keyword">in</span> rand_id]</span><br><span class="line">    y_true = np.argmax(y_true, axis=<span class="number">1</span>)</span><br><span class="line">    y_true = [class_name[name] <span class="keyword">for</span> name <span class="keyword">in</span> y_true]</span><br><span class="line">    y_pred = model.predict(X_pred)</span><br><span class="line">    y_pred = np.argmax(y_pred, axis=<span class="number">1</span>)</span><br><span class="line">    y_pred = [class_name[name] <span class="keyword">for</span> name <span class="keyword">in</span> y_pred]</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>, <span class="number">7</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        plt.subplot(<span class="number">2</span>, <span class="number">5</span>, i + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(X_pred[i].reshape(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), cmap=<span class="string">'gray'</span>)</span><br><span class="line">        plt.title(<span class="string">'True: %s \n Pred: %s'</span> % (y_true[i], y_pred[i]), size=<span class="number">15</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="验证本地图片："><a href="#验证本地图片：" class="headerlink" title="验证本地图片："></a><strong>验证本地图片</strong>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/7 19:18</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : TestModel.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> Read_data</span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">import</span> visualization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 加载模型</span></span><br><span class="line">    model = load_model(<span class="string">'CIFAR10_model_with_data_augmentation_VGG.h5'</span>)</span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    X_train, y_train, X_test, y_test = Read_data.load_data()</span><br><span class="line">    visualization.plot_image(X_test, y_test, config.class_name, model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">local_photos</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># input</span></span><br><span class="line">    im = Image.open(<span class="string">'image/dog-1.jpg'</span>)</span><br><span class="line">    <span class="comment"># im.show()</span></span><br><span class="line">    im = im.resize((<span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(im.size, im.mode)</span></span><br><span class="line"></span><br><span class="line">    im = np.array(im).astype(np.float32)</span><br><span class="line">    im = np.reshape(im, [<span class="number">-1</span>, <span class="number">32</span>*<span class="number">32</span>*<span class="number">3</span>])</span><br><span class="line">    im = (im - (<span class="number">255</span> / <span class="number">2.0</span>)) / <span class="number">255</span></span><br><span class="line">    batch_xs = np.reshape(im, [<span class="number">-1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    model = load_model(<span class="string">'CIFAR10_model_with_data_augmentation_VGG.h5'</span>)</span><br><span class="line">    output = model.predict(batch_xs)</span><br><span class="line"></span><br><span class="line">    print(output)</span><br><span class="line">    print(<span class="string">'the out put is :'</span>, config.class_name[np.argmax(output)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    local_photos()</span><br></pre></td></tr></table></figure><h3 id="输出："><a href="#输出：" class="headerlink" title="输出："></a><strong>输出：</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">the out put <span class="keyword">is</span> : dog</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow-cifar10-图像分类之网络结构</title>
      <link href="/2018/09/07/2018-09-07-TensorFlow-cifar-network/"/>
      <url>/2018/09/07/2018-09-07-TensorFlow-cifar-network/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="LeNet网络："><a href="#LeNet网络：" class="headerlink" title="LeNet网络："></a><strong>LeNet网络</strong>：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">def LeNet(inputs):</span><br><span class="line"></span><br><span class="line">    mu = 0</span><br><span class="line">    sigma = 0.1</span><br><span class="line">    print(inputs.shape)</span><br><span class="line">    # TODO: 第一层卷积：输入=32x32x3, 输出=28x28x6</span><br><span class="line">    conv1_w = tf.Variable(tf.truncated_normal(shape=[5, 5, 3, 6], mean=mu, stddev=sigma))</span><br><span class="line">    conv1_b = tf.Variable(tf.zeros(6))</span><br><span class="line"></span><br><span class="line">    conv1 = tf.nn.conv2d(inputs, conv1_w, strides=[1, 1, 1, 1], padding=&apos;VALID&apos;) + conv1_b</span><br><span class="line">    print(conv1.shape)</span><br><span class="line">    # 激活函数</span><br><span class="line">    conv1_out = tf.nn.relu(conv1)</span><br><span class="line"></span><br><span class="line">    # 池化层， 输入=28x28x6, 输出=14x14x6</span><br><span class="line">    pool_1 = tf.nn.max_pool(conv1_out, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;VALID&apos;)</span><br><span class="line">    print(pool_1.shape)</span><br><span class="line"></span><br><span class="line">    # TODO: 第二层卷积： 输入=14x14x6， 输出=10x10x16</span><br><span class="line">    conv2_w = tf.Variable(tf.truncated_normal(shape=[5, 5, 6, 16], mean=mu, stddev=sigma))</span><br><span class="line">    conv2_b = tf.Variable(tf.zeros(16))</span><br><span class="line"></span><br><span class="line">    conv2 = tf.nn.conv2d(pool_1, conv2_w, strides=[1, 1, 1, 1], padding=&apos;VALID&apos;) + conv2_b</span><br><span class="line">    print(conv2.shape)</span><br><span class="line">    # 激活函数</span><br><span class="line">    conv2_out = tf.nn.relu(conv2)</span><br><span class="line"></span><br><span class="line">    # 池化层， 输入=10x10x16, 输出=5x5x16</span><br><span class="line">    pool_2 = tf.nn.max_pool(conv2_out, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;VALID&apos;)</span><br><span class="line">    print(pool_2.shape)</span><br><span class="line">    # Flatten 输入=5x5x16， 输出=400</span><br><span class="line">    pool_2_flat = tf.reshape(pool_2, [-1, 400])</span><br><span class="line"></span><br><span class="line">    # TODO: 第三层全连接层， 输入=400， 输出=120</span><br><span class="line">    fc1_w = tf.Variable(tf.truncated_normal(shape=[400, 120], mean=mu, stddev=sigma))</span><br><span class="line">    fc1_b = tf.Variable(tf.zeros(120))</span><br><span class="line"></span><br><span class="line">    fc1 = tf.matmul(pool_2_flat, fc1_w) + fc1_b</span><br><span class="line"></span><br><span class="line">    # 激活函数</span><br><span class="line">    fc1_out = tf.nn.relu(fc1)</span><br><span class="line">    print(fc1_out.shape)</span><br><span class="line"></span><br><span class="line">    # TODO: 第四层全连接层： 输入=120， 输出=84</span><br><span class="line">    fc2_w = tf.Variable(tf.truncated_normal(shape=[120, 84], mean=mu, stddev=sigma))</span><br><span class="line">    fc2_b = tf.Variable(tf.zeros(84))</span><br><span class="line"></span><br><span class="line">    fc2 = tf.matmul(fc1_out, fc2_w) + fc2_b</span><br><span class="line"></span><br><span class="line">    # 激活函数</span><br><span class="line">    fc2_out = tf.nn.relu(fc2)</span><br><span class="line">    print(fc2_out.shape)</span><br><span class="line"></span><br><span class="line">    # TODO: 第五层全连接层： 输入=84， 输出=10</span><br><span class="line">    fc3_w = tf.Variable(tf.truncated_normal(shape=[84, 10], mean=mu, stddev=sigma))</span><br><span class="line">    fc3_b = tf.Variable(tf.zeros(10))</span><br><span class="line"></span><br><span class="line">    fc3_out = tf.matmul(fc2_out, fc3_w) + fc3_b</span><br><span class="line">    print(fc3_out.shape)</span><br><span class="line"></span><br><span class="line">    return fc3_out</span><br></pre></td></tr></table></figure><h3 id="AlexNet网络："><a href="#AlexNet网络：" class="headerlink" title="AlexNet网络："></a><strong>AlexNet网络</strong>：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"># 卷积操作</span><br><span class="line">def conv2d(name, l_input, w, b):</span><br><span class="line">    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;), b), name=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 最大下采样操作</span><br><span class="line">def max_pool(name, l_input, k):</span><br><span class="line">    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=&apos;SAME&apos;, name=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 归一化操作</span><br><span class="line">def norm(name, l_input, lsize=4):</span><br><span class="line">    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 定义整个网络</span><br><span class="line">def alex_net(_X, _weights, _biases, _dropout):</span><br><span class="line">    # 向量转为矩阵</span><br><span class="line">    # _X = tf.reshape(_X, shape=[-1, 28, 28, 3])</span><br><span class="line">    print(_X.shape)</span><br><span class="line">    # TODO: 第一层卷积：</span><br><span class="line">    conv1 = conv2d(&apos;conv1&apos;, _X, _weights[&apos;wc1&apos;], _biases[&apos;bc1&apos;])</span><br><span class="line">    # 下采样层</span><br><span class="line">    pool1 = max_pool(&apos;pool1&apos;, conv1, k=2)</span><br><span class="line">    # 归一化层</span><br><span class="line">    norm1 = norm(&apos;norm1&apos;, pool1, lsize=4)</span><br><span class="line">    print(norm1.shape)</span><br><span class="line">    # TODO: 第二层卷积：</span><br><span class="line">    conv2 = conv2d(&apos;conv2&apos;, norm1, _weights[&apos;wc2&apos;], _biases[&apos;bc2&apos;])</span><br><span class="line">    # 下采样</span><br><span class="line">    pool2 = max_pool(&apos;pool2&apos;, conv2, k=2)</span><br><span class="line">    # 归一化</span><br><span class="line">    norm2 = norm(&apos;norm2&apos;, pool2, lsize=4)</span><br><span class="line">    print(norm2.shape)</span><br><span class="line">    # TODO: 第三层卷积：</span><br><span class="line">    conv3 = conv2d(&apos;conv3&apos;, norm2, _weights[&apos;wc3&apos;], _biases[&apos;bc3&apos;])</span><br><span class="line">    # 归一化</span><br><span class="line">    norm3 = norm(&apos;norm3&apos;, conv3, lsize=4)</span><br><span class="line">    print(norm3.shape)</span><br><span class="line">    # TODO: 第四层卷积</span><br><span class="line">    # 卷积</span><br><span class="line">    conv4 = conv2d(&apos;conv4&apos;, norm3, _weights[&apos;wc4&apos;], _biases[&apos;bc4&apos;])</span><br><span class="line">    # 归一化</span><br><span class="line">    norm4 = norm(&apos;norm4&apos;, conv4, lsize=4)</span><br><span class="line">    print(norm4.shape)</span><br><span class="line">    # TODO: 第五层卷积</span><br><span class="line">    # 卷积</span><br><span class="line">    conv5 = conv2d(&apos;conv5&apos;, norm4, _weights[&apos;wc5&apos;], _biases[&apos;bc5&apos;])</span><br><span class="line">    # 下采样</span><br><span class="line">    pool5 = max_pool(&apos;pool5&apos;, conv5, k=2)</span><br><span class="line">    # 归一化</span><br><span class="line">    norm5 = norm(&apos;norm5&apos;, pool5, lsize=4)</span><br><span class="line">    print(norm5.shape)</span><br><span class="line">    # TODO: 第六层全连接层</span><br><span class="line">    # 先把特征图转为向量</span><br><span class="line">    dense1 = tf.reshape(norm5, [-1, _weights[&apos;wd1&apos;].get_shape().as_list()[0]])</span><br><span class="line">    dense1 = tf.nn.relu(tf.matmul(dense1, _weights[&apos;wd1&apos;]) + _biases[&apos;bd1&apos;], name=&apos;fc1&apos;)</span><br><span class="line">    dense1 = tf.nn.dropout(dense1, _dropout)</span><br><span class="line">    print(dense1.shape)</span><br><span class="line">    # TODO: 第七层全连接层：</span><br><span class="line">    dense2 = tf.nn.relu(tf.matmul(dense1, _weights[&apos;wd2&apos;]) + _biases[&apos;bd2&apos;], name=&apos;fc2&apos;)  # Relu activation</span><br><span class="line">    dense2 = tf.nn.dropout(dense2, _dropout)</span><br><span class="line">    print(dense2.shape)</span><br><span class="line">    # TODO: 第八层全连接层：</span><br><span class="line">    # 网络输出层</span><br><span class="line">    out = tf.matmul(dense2, _weights[&apos;out&apos;]) + _biases[&apos;out&apos;]</span><br><span class="line">    print(out.shape)</span><br><span class="line">    return out</span><br></pre></td></tr></table></figure><h3 id="VGG16Net网络："><a href="#VGG16Net网络：" class="headerlink" title="VGG16Net网络："></a><strong>VGG16Net网络</strong>：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line">def VGG16(inputs):</span><br><span class="line">    print(inputs.shape)</span><br><span class="line">    # (32x32x3) --&gt; (32x32x64)</span><br><span class="line">    with tf.name_scope(&apos;conv_1&apos;):</span><br><span class="line">         conv_1_out = tf.layers.conv2d(inputs, 64, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_1_out.shape)</span><br><span class="line">    # (32x32x64) --&gt; (32x32x64)</span><br><span class="line">    with tf.name_scope(&apos;conv_2&apos;):</span><br><span class="line">        conv_2_out = tf.layers.conv2d(conv_1_out, 64, [3, 3],</span><br><span class="line">                                      padding=&apos;same&apos;,</span><br><span class="line">                                      activation=tf.nn.relu,</span><br><span class="line">                                      kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_2_out.shape)</span><br><span class="line">    # (32x32x64) --&gt; (16x16x64)</span><br><span class="line">    with tf.name_scope(&apos;pool_1&apos;):</span><br><span class="line">        pool_1_out = tf.layers.max_pooling2d(conv_2_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;)</span><br><span class="line"></span><br><span class="line">    print(pool_1_out.shape)</span><br><span class="line">    # (16x16x64) --&gt; (16x16x128)</span><br><span class="line">    with tf.name_scope(&apos;conv_3&apos;):</span><br><span class="line">         conv_3_out = tf.layers.conv2d(pool_1_out, 128, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_3_out.shape)</span><br><span class="line">    # (16x16x128) --&gt; (16x16x128)</span><br><span class="line">    with tf.name_scope(&apos;conv_4&apos;):</span><br><span class="line">         conv_4_out = tf.layers.conv2d(conv_3_out, 128, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_4_out.shape)</span><br><span class="line">    # (16x16x128) --&gt; (8x8x128)</span><br><span class="line">    with tf.name_scope(&apos;pool_2&apos;):</span><br><span class="line">        pool_2_out = tf.layers.max_pooling2d(conv_4_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;)</span><br><span class="line"></span><br><span class="line">    print(pool_2_out.shape)</span><br><span class="line">    # (8x8x128) --&gt; (8x8x256)</span><br><span class="line">    with tf.name_scope(&apos;conv_5&apos;):</span><br><span class="line">         conv_5_out = tf.layers.conv2d(pool_2_out, 256, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_5_out.shape)</span><br><span class="line">    # (8x8x256) --&gt; (8x8x256)</span><br><span class="line">    with tf.name_scope(&apos;conv_6&apos;):</span><br><span class="line">         conv_6_out = tf.layers.conv2d(conv_5_out, 256, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_6_out.shape)</span><br><span class="line">    # (8x8x256) --&gt; (8x8x256)</span><br><span class="line">    with tf.name_scope(&apos;conv_7&apos;):</span><br><span class="line">        conv_7_out = tf.layers.conv2d(conv_6_out, 256, [3, 3],</span><br><span class="line">                                      padding=&apos;same&apos;,</span><br><span class="line">                                      activation=tf.nn.relu,</span><br><span class="line">                                      kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_7_out.shape)</span><br><span class="line">    # (8x8x256) --&gt; (4x4x256)</span><br><span class="line">    with tf.name_scope(&apos;pool_3&apos;):</span><br><span class="line">        pool_3_out = tf.layers.max_pooling2d(conv_7_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;)</span><br><span class="line"></span><br><span class="line">    print(pool_3_out.shape)</span><br><span class="line">    # (4x4x256) --&gt; (4x4x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_8&apos;):</span><br><span class="line">        conv_8_out = tf.layers.conv2d(pool_3_out, 512, [3, 3],</span><br><span class="line">                                      padding=&apos;same&apos;,</span><br><span class="line">                                      activation=tf.nn.relu,</span><br><span class="line">                                      kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_8_out.shape)</span><br><span class="line">    # (4x4x512) --&gt; (4x4x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_9&apos;):</span><br><span class="line">        conv_9_out = tf.layers.conv2d(conv_8_out, 512, [3, 3],</span><br><span class="line">                                      padding=&apos;same&apos;,</span><br><span class="line">                                      activation=tf.nn.relu,</span><br><span class="line">                                      kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_9_out.shape)</span><br><span class="line">    # (4x4x512) --&gt; (4x4x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_10&apos;):</span><br><span class="line">        conv_10_out = tf.layers.conv2d(conv_9_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_10_out.shape)</span><br><span class="line">    # (4x4x512) --&gt; (2x2x512)</span><br><span class="line">    with tf.name_scope(&apos;pool_4&apos;):</span><br><span class="line">        pool_4_out = tf.layers.max_pooling2d(conv_10_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;)</span><br><span class="line"></span><br><span class="line">    print(pool_4_out.shape)</span><br><span class="line">    # (2x2x512) --&gt; (2x2x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_11&apos;):</span><br><span class="line">        conv_11_out = tf.layers.conv2d(pool_4_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_11_out.shape)</span><br><span class="line">    # (2x2x512) --&gt; (2x2x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_12&apos;):</span><br><span class="line">        conv_12_out = tf.layers.conv2d(conv_11_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_12_out.shape)</span><br><span class="line">    # (2x2x512) --&gt; (2x2x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_13&apos;):</span><br><span class="line">        conv_13_out = tf.layers.conv2d(conv_12_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_13_out.shape)</span><br><span class="line">    # (2x2x512) --&gt; (1x1x512)</span><br><span class="line">    with tf.name_scope(&apos;pool_5&apos;):</span><br><span class="line">        pool_5_out = tf.layers.max_pooling2d(conv_13_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;)</span><br><span class="line"></span><br><span class="line">    print(pool_5_out.shape)</span><br><span class="line">    # (1x1x512) --&gt; 512</span><br><span class="line">    with tf.name_scope(&apos;fc_1&apos;):</span><br><span class="line">        pool_5_outz_flat = tf.layers.flatten(pool_5_out)</span><br><span class="line">        fc_1_out = tf.layers.dense(pool_5_outz_flat, 512, activation=tf.nn.relu)</span><br><span class="line">        fc_1_drop = tf.nn.dropout(fc_1_out, keep_prob=config.keep_prob)</span><br><span class="line">    print(fc_1_drop.shape)</span><br><span class="line">    # 512 --&gt; 512</span><br><span class="line">    with tf.name_scope(&apos;fc_2&apos;):</span><br><span class="line">        fc_2_out = tf.layers.dense(fc_1_drop, 512, activation=tf.nn.relu)</span><br><span class="line">        fc_2_drop = tf.nn.dropout(fc_2_out, keep_prob=config.keep_prob)</span><br><span class="line">    print(fc_2_drop.shape)</span><br><span class="line">    # 512 --&gt; 10</span><br><span class="line">    with tf.name_scope(&apos;fc_3&apos;):</span><br><span class="line">        fc_3_out = tf.layers.dense(fc_2_drop, 10, activation=None)</span><br><span class="line">    print(fc_3_out.shape)</span><br><span class="line"></span><br><span class="line">    return fc_3_out</span><br></pre></td></tr></table></figure><h3 id="VGG19Net"><a href="#VGG19Net" class="headerlink" title="VGG19Net:"></a><strong>VGG19Net</strong>:</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line">def VGG19(inputs):</span><br><span class="line">    print(inputs.shape)</span><br><span class="line">    # (32x32x3) --&gt; (32x32x64)</span><br><span class="line">    with tf.name_scope(&apos;conv_1&apos;):</span><br><span class="line">         conv_1_out = tf.layers.conv2d(inputs, 64, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_1_out.shape)</span><br><span class="line">    # (32x32x64) --&gt; (32x32x64)</span><br><span class="line">    with tf.name_scope(&apos;conv_2&apos;):</span><br><span class="line">        conv_2_out = tf.layers.conv2d(conv_1_out, 64, [3, 3],</span><br><span class="line">                                      padding=&apos;same&apos;,</span><br><span class="line">                                      activation=tf.nn.relu,</span><br><span class="line">                                      kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_2_out.shape)</span><br><span class="line">    # (32x32x64) --&gt; (16x16x64)</span><br><span class="line">    with tf.name_scope(&apos;pool_1&apos;):</span><br><span class="line">        pool_1_out = tf.layers.max_pooling2d(conv_2_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;)</span><br><span class="line"></span><br><span class="line">    print(pool_1_out.shape)</span><br><span class="line">    # (16x16x64) --&gt; (16x16x128)</span><br><span class="line">    with tf.name_scope(&apos;conv_3&apos;):</span><br><span class="line">         conv_3_out = tf.layers.conv2d(pool_1_out, 128, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_3_out.shape)</span><br><span class="line">    # (16x16x128) --&gt; (16x16x128)</span><br><span class="line">    with tf.name_scope(&apos;conv_4&apos;):</span><br><span class="line">         conv_4_out = tf.layers.conv2d(conv_3_out, 128, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_4_out.shape)</span><br><span class="line">    # (16x16x128) --&gt; (8x8x128)</span><br><span class="line">    with tf.name_scope(&apos;pool_2&apos;):</span><br><span class="line">        pool_2_out = tf.layers.max_pooling2d(conv_4_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;)</span><br><span class="line"></span><br><span class="line">    print(pool_2_out.shape)</span><br><span class="line">    # (8x8x128) --&gt; (8x8x256)</span><br><span class="line">    with tf.name_scope(&apos;conv_5&apos;):</span><br><span class="line">         conv_5_out = tf.layers.conv2d(pool_2_out, 256, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_5_out.shape)</span><br><span class="line">    # (8x8x256) --&gt; (8x8x256)</span><br><span class="line">    with tf.name_scope(&apos;conv_6&apos;):</span><br><span class="line">         conv_6_out = tf.layers.conv2d(conv_5_out, 256, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_6_out.shape)</span><br><span class="line">    # (8x8x256) --&gt; (8x8x256)</span><br><span class="line">    with tf.name_scope(&apos;conv_7&apos;):</span><br><span class="line">        conv_7_out = tf.layers.conv2d(conv_6_out, 256, [3, 3],</span><br><span class="line">                                      padding=&apos;same&apos;,</span><br><span class="line">                                      activation=tf.nn.relu,</span><br><span class="line">                                      kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_7_out.shape)</span><br><span class="line">    # (8x8x256) --&gt; (8x8x256)</span><br><span class="line">    with tf.name_scope(&apos;conv_8&apos;):</span><br><span class="line">        conv_8_out = tf.layers.conv2d(conv_7_out, 256, [3, 3],</span><br><span class="line">                                      padding=&apos;same&apos;,</span><br><span class="line">                                      activation=tf.nn.relu,</span><br><span class="line">                                      kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_8_out.shape)</span><br><span class="line">    # (8x8x256) --&gt; (4x4x256)</span><br><span class="line">    with tf.name_scope(&apos;pool_3&apos;):</span><br><span class="line">        pool_3_out = tf.layers.max_pooling2d(conv_8_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;)</span><br><span class="line"></span><br><span class="line">    print(pool_3_out.shape)</span><br><span class="line">    # (4x4x256) --&gt; (4x4x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_9&apos;):</span><br><span class="line">        conv_9_out = tf.layers.conv2d(pool_3_out, 512, [3, 3],</span><br><span class="line">                                      padding=&apos;same&apos;,</span><br><span class="line">                                      activation=tf.nn.relu,</span><br><span class="line">                                      kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_9_out.shape)</span><br><span class="line">    # (4x4x512) --&gt; (4x4x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_10&apos;):</span><br><span class="line">        conv_10_out = tf.layers.conv2d(conv_9_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_10_out.shape)</span><br><span class="line">    # (4x4x512) --&gt; (4x4x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_11&apos;):</span><br><span class="line">        conv_11_out = tf.layers.conv2d(conv_10_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_11_out.shape)</span><br><span class="line">    # (4x4x512) --&gt; (4x4x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_12&apos;):</span><br><span class="line">        conv_12_out = tf.layers.conv2d(conv_11_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_12_out.shape)</span><br><span class="line">    # (4x4x512) --&gt; (2x2x512)</span><br><span class="line">    with tf.name_scope(&apos;pool_4&apos;):</span><br><span class="line">        pool_4_out = tf.layers.max_pooling2d(conv_12_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;)</span><br><span class="line"></span><br><span class="line">    print(pool_4_out.shape)</span><br><span class="line">    # (2x2x512) --&gt; (2x2x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_13&apos;):</span><br><span class="line">        conv_13_out = tf.layers.conv2d(pool_4_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_13_out.shape)</span><br><span class="line">    # (2x2x512) --&gt; (2x2x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_14&apos;):</span><br><span class="line">        conv_14_out = tf.layers.conv2d(conv_13_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_14_out.shape)</span><br><span class="line">    # (2x2x512) --&gt; (2x2x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_15&apos;):</span><br><span class="line">        conv_15_out = tf.layers.conv2d(conv_14_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_15_out.shape)</span><br><span class="line">    # (2x2x512) --&gt; (2x2x512)</span><br><span class="line">    with tf.name_scope(&apos;conv_16&apos;):</span><br><span class="line">        conv_16_out = tf.layers.conv2d(conv_15_out, 512, [3, 3],</span><br><span class="line">                                       padding=&apos;same&apos;,</span><br><span class="line">                                       activation=tf.nn.relu,</span><br><span class="line">                                       kernel_initializer=tf.truncated_normal_initializer(mean=0., stddev=0.1))</span><br><span class="line">    print(conv_16_out.shape)</span><br><span class="line">    # (2x2x512) --&gt; (1x1x512)</span><br><span class="line">    with tf.name_scope(&apos;pool_5&apos;):</span><br><span class="line">        pool_5_out = tf.layers.max_pooling2d(conv_16_out, pool_size=[2, 2], strides=[2, 2], padding=&apos;same&apos;)</span><br><span class="line"></span><br><span class="line">    print(pool_5_out.shape)</span><br><span class="line">    # (1x1x512) --&gt; 512</span><br><span class="line">    with tf.name_scope(&apos;fc_1&apos;):</span><br><span class="line">        pool_5_outz_flat = tf.layers.flatten(pool_5_out)</span><br><span class="line">        fc_1_out = tf.layers.dense(pool_5_outz_flat, 512, activation=tf.nn.relu)</span><br><span class="line">        fc_1_drop = tf.nn.dropout(fc_1_out, keep_prob=config.keep_prob)</span><br><span class="line">    print(fc_1_drop.shape)</span><br><span class="line">    # 512 --&gt; 512</span><br><span class="line">    with tf.name_scope(&apos;fc_2&apos;):</span><br><span class="line">        fc_2_out = tf.layers.dense(fc_1_drop, 512, activation=tf.nn.relu)</span><br><span class="line">        fc_2_drop = tf.nn.dropout(fc_2_out, keep_prob=config.keep_prob)</span><br><span class="line">    print(fc_2_drop.shape)</span><br><span class="line">    # 512 --&gt; 10</span><br><span class="line">    with tf.name_scope(&apos;fc_3&apos;):</span><br><span class="line">        fc_3_out = tf.layers.dense(fc_2_drop, 10, activation=None)</span><br><span class="line">    print(fc_3_out.shape)</span><br><span class="line"></span><br><span class="line">    return fc_3_out</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Keras-cifar10-图像分类</title>
      <link href="/2018/09/07/2018-09-07-keras-cifar/"/>
      <url>/2018/09/07/2018-09-07-keras-cifar/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="预处理数据："><a href="#预处理数据：" class="headerlink" title="预处理数据："></a><strong>预处理数据</strong>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/6 15:31</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : Read_data.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 加载数据</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, LabelBinarizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_cifar10_batch</span><span class="params">(path, batch_id)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    加载batch的数据</span></span><br><span class="line"><span class="string">    :param path: 数据存储的目录</span></span><br><span class="line"><span class="string">    :param batch_id:batch的编号</span></span><br><span class="line"><span class="string">    :return:features and labels</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(path + <span class="string">'/data_batch_'</span> + str(batch_id), mode=<span class="string">'rb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        batch = pickle.load(file, encoding=<span class="string">'latin1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># features and labels</span></span><br><span class="line">    features = batch[<span class="string">'data'</span>].reshape((len(batch[<span class="string">'data'</span>]), <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)).transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    labels = batch[<span class="string">'labels'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pre_processing_data</span><span class="params">(x_train, y_train, x_test, y_test)</span>:</span></span><br><span class="line">    <span class="comment"># features</span></span><br><span class="line">    minmax = MinMaxScaler()</span><br><span class="line">    <span class="comment"># 重塑数据</span></span><br><span class="line">    <span class="comment"># (50000, 32, 32, 3) --&gt; (50000, 32*32*3)</span></span><br><span class="line">    x_train_rows = x_train.reshape(x_train.shape[<span class="number">0</span>], <span class="number">32</span>*<span class="number">32</span>*<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># (10000, 32, 32, 3) --&gt; (10000, 32*32*3)</span></span><br><span class="line">    x_test_rows = x_test.reshape(x_test.shape[<span class="number">0</span>], <span class="number">32</span>*<span class="number">32</span>*<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    x_train_norm = minmax.fit_transform(x_train_rows)</span><br><span class="line">    x_test_norm = minmax.fit_transform(x_test_rows)</span><br><span class="line">    <span class="comment"># 重塑数据</span></span><br><span class="line">    x_train = x_train_norm.reshape(x_train_norm.shape[<span class="number">0</span>], <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)</span><br><span class="line">    x_test = x_test_norm.reshape(x_test_norm.shape[<span class="number">0</span>], <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># labels</span></span><br><span class="line">    <span class="comment"># 对标签进行one-hot</span></span><br><span class="line">    n_class = <span class="number">10</span></span><br><span class="line">    label_binarizer = LabelBinarizer().fit(np.array(range(n_class)))</span><br><span class="line">    y_train = label_binarizer.transform(y_train)</span><br><span class="line">    y_test = label_binarizer.transform(y_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train, y_train, x_test, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cifar10_data</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 加载训练数据</span></span><br><span class="line">    cifar10_path = <span class="string">'data'</span></span><br><span class="line">    <span class="comment"># 一共是有5个batch的训练数据</span></span><br><span class="line">    x_train, y_train = load_cifar10_batch(cifar10_path, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">6</span>):</span><br><span class="line">        features, labels = load_cifar10_batch(cifar10_path, n)</span><br><span class="line">        x_train = np.concatenate([x_train, features])</span><br><span class="line">        y_train = np.concatenate([y_train, labels])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载测试数据</span></span><br><span class="line">    <span class="keyword">with</span> open(cifar10_path + <span class="string">'/test_batch'</span>, mode=<span class="string">'rb'</span>) <span class="keyword">as</span> file:</span><br><span class="line">        batch = pickle.load(file, encoding=<span class="string">'latin1'</span>)</span><br><span class="line">        x_test = batch[<span class="string">'data'</span>].reshape((len(batch[<span class="string">'data'</span>]), <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)).transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        y_test = batch[<span class="string">'labels'</span>]</span><br><span class="line"></span><br><span class="line">    x_train, y_train, x_test, y_test = pre_processing_data(x_train, y_train, x_test, y_test)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train, y_train, x_test, y_test</span><br></pre></td></tr></table></figure><h3 id="配置文件："><a href="#配置文件：" class="headerlink" title="配置文件："></a><strong>配置文件</strong>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/7 14:10</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : config.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 基本配置</span></span><br><span class="line"></span><br><span class="line">nb_classes = <span class="number">10</span></span><br><span class="line">class_name = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">'airplane'</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">'automobile'</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">'bird'</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">'cat'</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">'deer'</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">'dog'</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">'frog'</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">'horse'</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">'ship'</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">'truck'</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="类VGG16Net网络："><a href="#类VGG16Net网络：" class="headerlink" title="类VGG16Net网络："></a><strong>类VGG16Net网络</strong>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KerasVGG</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    模型采用类似于 VGG16 的结构：</span></span><br><span class="line"><span class="string">        使用固定尺寸的小卷积核 (3x3)</span></span><br><span class="line"><span class="string">        以2的幂次递增的卷积核数量 (64, 128, 256)</span></span><br><span class="line"><span class="string">        两层卷积搭配一层池化</span></span><br><span class="line"><span class="string">        全连接层没有采用 VGG16 庞大的三层结构，避免运算量过大，仅使用 128 个节点的单个FC</span></span><br><span class="line"><span class="string">        权重初始化采用He Normal</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    name = <span class="string">'VGG'</span></span><br><span class="line">    inputs = Input(shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>))</span><br><span class="line">    net = inputs</span><br><span class="line">    <span class="comment"># (32, 32, 3)--&gt;(32, 32, 64)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">64</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    <span class="comment"># (32, 32, 64)--&gt;(32, 32, 64)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">64</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    <span class="comment"># (32, 32, 64)--&gt;(16, 16, 64)</span></span><br><span class="line">    net = MaxPooling2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>, padding=<span class="string">'valid'</span>)(net)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (16, 16, 64)--&gt;(16, 16, 128)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    <span class="comment"># (16, 16, 64)--&gt;(16, 16, 128)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    <span class="comment"># (16, 16, 128)--&gt;(8, 8, 128)</span></span><br><span class="line">    net = MaxPooling2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>, padding=<span class="string">'valid'</span>)(net)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (8, 8, 128)--&gt;(8, 8, 256)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">256</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    <span class="comment"># (8, 8, 256)--&gt;(8, 8, 256)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">256</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    <span class="comment"># (8, 8, 256)--&gt;(4, 4, 256)</span></span><br><span class="line">    net = MaxPooling2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>, padding=<span class="string">'valid'</span>)(net)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (4, 4, 256) --&gt; 4*4*256=4096</span></span><br><span class="line">    net = Flatten()(net)</span><br><span class="line">    <span class="comment"># 4096 --&gt; 128</span></span><br><span class="line">    net = Dense(units=<span class="number">128</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    <span class="comment"># Dropout</span></span><br><span class="line">    net = Dropout(<span class="number">0.5</span>)(net)</span><br><span class="line">    <span class="comment"># 128 --&gt; 10</span></span><br><span class="line">    net = Dense(units=config.nb_classes, activation=<span class="string">'softmax'</span>,</span><br><span class="line">                kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    <span class="keyword">return</span> inputs, net, name</span><br></pre></td></tr></table></figure><h3 id="添加BN层"><a href="#添加BN层" class="headerlink" title="添加BN层:"></a><strong>添加BN层</strong>:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">KerasBN</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    添加batch norm 层</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    name = <span class="string">'BN'</span></span><br><span class="line">    inputs = Input(shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>))</span><br><span class="line">    net = inputs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (32, 32, 3)--&gt;(32, 32, 64)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">64</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    net = BatchNormalization()(net)</span><br><span class="line">    net = Activation(<span class="string">'relu'</span>)(net)</span><br><span class="line">    <span class="comment"># (32, 32, 64)--&gt;(32, 32, 64)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">64</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    net = BatchNormalization()(net)</span><br><span class="line">    net = Activation(<span class="string">'relu'</span>)(net)</span><br><span class="line">    <span class="comment"># (32, 32, 64)--&gt;(16, 16, 64)</span></span><br><span class="line">    net = MaxPooling2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>, padding=<span class="string">'valid'</span>)(net)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (16, 16, 64)--&gt;(16, 16, 128)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    net = BatchNormalization()(net)</span><br><span class="line">    net = Activation(<span class="string">'relu'</span>)(net)</span><br><span class="line">    <span class="comment"># (16, 16, 64)--&gt;(16, 16, 128)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    net = BatchNormalization()(net)</span><br><span class="line">    net = Activation(<span class="string">'relu'</span>)(net)</span><br><span class="line">    <span class="comment"># (16, 16, 128)--&gt;(8, 8, 128)</span></span><br><span class="line">    net = MaxPooling2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>, padding=<span class="string">'valid'</span>)(net)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (8, 8, 128)--&gt;(8, 8, 256)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">256</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    net = BatchNormalization()(net)</span><br><span class="line">    net = Activation(<span class="string">'relu'</span>)(net)</span><br><span class="line">    <span class="comment"># (8, 8, 128)--&gt;(8, 8, 256)</span></span><br><span class="line">    net = Convolution2D(filters=<span class="number">256</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>,</span><br><span class="line">                        padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                        kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    net = BatchNormalization()(net)</span><br><span class="line">    net = Activation(<span class="string">'relu'</span>)(net)</span><br><span class="line">    <span class="comment"># (8, 8, 256)--&gt;(4, 4, 256)</span></span><br><span class="line">    net = MaxPooling2D(pool_size=<span class="number">2</span>, strides=<span class="number">2</span>, padding=<span class="string">'valid'</span>)(net)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (4, 4, 256) --&gt; 4*4*256=4096</span></span><br><span class="line">    net = Flatten()(net)</span><br><span class="line">    <span class="comment"># 4096 --&gt; 128</span></span><br><span class="line">    net = Dense(units=<span class="number">128</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line">    <span class="comment"># Dropout</span></span><br><span class="line">    net = Dropout(<span class="number">0.5</span>)(net)</span><br><span class="line">    <span class="comment"># 128 --&gt; 10</span></span><br><span class="line">    net = Dense(units=config.nb_classes, activation=<span class="string">'softmax'</span>,</span><br><span class="line">                kernel_initializer=<span class="string">'he_normal'</span>)(net)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inputs, net, name</span><br></pre></td></tr></table></figure><h3 id="训练文件"><a href="#训练文件" class="headerlink" title="训练文件:"></a><strong>训练文件</strong>:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/7 14:07</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : TrainModel.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 模型训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TODO：导入环境</span></span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(network, X_train, y_train, X_test, y_test, augmentation=False)</span>:</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X_train shape:'</span>, X_train.shape)</span><br><span class="line">    print(<span class="string">'Y_train shape:'</span>, y_train.shape)</span><br><span class="line">    print(X_train.shape[<span class="number">0</span>], <span class="string">'x_training samples'</span>)</span><br><span class="line">    print(X_test.shape[<span class="number">0</span>], <span class="string">'validation samples'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 规一化处理</span></span><br><span class="line">    X_train = X_train.astype(<span class="string">'float32'</span>)</span><br><span class="line">    X_test = X_test.astype(<span class="string">'float32'</span>)</span><br><span class="line">    X_train /= <span class="number">255</span></span><br><span class="line">    X_test /= <span class="number">255</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># TODO： 初始化模型</span></span><br><span class="line">    inputs, logits, name = network</span><br><span class="line">    model = Model(inputs=inputs, outputs=logits, name=<span class="string">'model'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 计算损失值并初始化optimizer</span></span><br><span class="line">    model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">                  optimizer=<span class="string">'adadelta'</span>,</span><br><span class="line">                  metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">    model.summary()</span><br><span class="line">    print(<span class="string">'FUNCTION READY!!'</span>)</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 开始训练</span></span><br><span class="line">    print(<span class="string">'TRAINING....'</span>)</span><br><span class="line">    epoch = <span class="number">100</span></span><br><span class="line">    batch_size = <span class="number">256</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="comment"># 数据增强</span></span><br><span class="line">    <span class="keyword">if</span> augmentation:</span><br><span class="line">        aug_gen = ImageDataGenerator(</span><br><span class="line">            featurewise_center=<span class="keyword">False</span>,  <span class="comment"># set input mean to 0 over the dataset</span></span><br><span class="line">            samplewise_center=<span class="keyword">False</span>,  <span class="comment"># set each sample mean to 0</span></span><br><span class="line">            featurewise_std_normalization=<span class="keyword">False</span>,  <span class="comment"># divide inputs by std of the dataset</span></span><br><span class="line">            samplewise_std_normalization=<span class="keyword">False</span>,  <span class="comment"># divide each input by its std</span></span><br><span class="line">            zca_whitening=<span class="keyword">False</span>,  <span class="comment"># apply ZCA whitening</span></span><br><span class="line">            rotation_range=<span class="number">0</span>,  <span class="comment"># randomly rotate images in the range (degrees, 0 to 180)</span></span><br><span class="line">            width_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images horizontally (fraction of total width)</span></span><br><span class="line">            height_shift_range=<span class="number">0.1</span>,  <span class="comment"># randomly shift images vertically (fraction of total height)</span></span><br><span class="line">            horizontal_flip=<span class="keyword">True</span>,  <span class="comment"># randomly flip images</span></span><br><span class="line">            vertical_flip=<span class="keyword">False</span>,  <span class="comment"># randomly flip images</span></span><br><span class="line">        )</span><br><span class="line">        aug_gen.fit(X_train)</span><br><span class="line">        generator = aug_gen.flow(X_train, y_train, batch_size=batch_size)</span><br><span class="line">        out = model.fit_generator(generator=generator,</span><br><span class="line">                                  steps_per_epoch=<span class="number">50000</span> // batch_size,</span><br><span class="line">                                  epochs=epoch,</span><br><span class="line">                                  validation_data=(X_test, y_test))</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> 保存模型</span></span><br><span class="line">        model.save(<span class="string">'CIFAR10_model_with_data_augmentation_%s.h5'</span> % name)</span><br><span class="line">    <span class="comment"># 不使用数据增强</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        out = model.fit(x=X_train, y=y_train,</span><br><span class="line">                        batch_size=batch_size,</span><br><span class="line">                        epochs=epoch,</span><br><span class="line">                        validation_data=(X_test, y_test),</span><br><span class="line">                        shuffle=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> 保存模型</span></span><br><span class="line">        model.save(<span class="string">'CIFAR10_model_no_data_augmentation_%s.h5'</span> % name)</span><br><span class="line"></span><br><span class="line">    loss, accuracy = model.evaluate(X_train, y_train, verbose=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">"Training Accuracy = %.2f %%     loss = %f"</span> % (accuracy * <span class="number">100</span>, loss))</span><br><span class="line">    loss, accuracy = model.evaluate(X_test, y_test, verbose=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">"Testing Accuracy = %.2f %%    loss = %f"</span> % (accuracy * <span class="number">100</span>, loss))</span><br><span class="line">    print(<span class="string">'@ Total Time Spent: %.2f seconds'</span> % (time.time() - start))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, epoch, model</span><br></pre></td></tr></table></figure><h3 id="可视化数据："><a href="#可视化数据：" class="headerlink" title="可视化数据："></a><strong>可视化数据</strong>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/7 15:00</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : visualization.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 可视化数据</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_acc_loss</span><span class="params">(data, epoch)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    可视化数据</span></span><br><span class="line"><span class="string">    :param data: 数据</span></span><br><span class="line"><span class="string">    :param epoch: 迭代次数</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    acc, loss, val_acc, val_loss = data.history[<span class="string">'acc'</span>], data.history[<span class="string">'loss'</span>], \</span><br><span class="line">                                   data.history[<span class="string">'val_acc'</span>], data.history[<span class="string">'val_loss'</span>]</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">    plt.subplot(<span class="number">121</span>)</span><br><span class="line">    plt.plot(range(epoch), acc, label=<span class="string">'Train'</span>)</span><br><span class="line">    plt.plot(range(epoch), val_acc, label=<span class="string">'Test'</span>)</span><br><span class="line">    plt.title(<span class="string">'Accuracy over '</span> + str(epoch) + <span class="string">' Epochs'</span>, size=<span class="number">15</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">122</span>)</span><br><span class="line">    plt.plot(range(epoch), loss, label=<span class="string">'Train'</span>)</span><br><span class="line">    plt.plot(range(epoch), val_loss, label=<span class="string">'Test'</span>)</span><br><span class="line">    plt.title(<span class="string">'Loss over '</span> + str(epoch) + <span class="string">' Epochs'</span>, size=<span class="number">15</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid(<span class="keyword">True</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_image</span><span class="params">(x_test, y_test, class_name, model)</span>:</span></span><br><span class="line">    rand_id = np.random.choice(range(<span class="number">10000</span>), size=<span class="number">10</span>)</span><br><span class="line">    X_pred = np.array([x_test[i] <span class="keyword">for</span> i <span class="keyword">in</span> rand_id])</span><br><span class="line">    y_true = [y_test[i] <span class="keyword">for</span> i <span class="keyword">in</span> rand_id]</span><br><span class="line">    y_true = np.argmax(y_true, axis=<span class="number">1</span>)</span><br><span class="line">    y_true = [class_name[name] <span class="keyword">for</span> name <span class="keyword">in</span> y_true]</span><br><span class="line">    y_pred = model.predict(X_pred)</span><br><span class="line">    y_pred = np.argmax(y_pred, axis=<span class="number">1</span>)</span><br><span class="line">    y_pred = [class_name[name] <span class="keyword">for</span> name <span class="keyword">in</span> y_pred]</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>, <span class="number">7</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        plt.subplot(<span class="number">2</span>, <span class="number">5</span>, i + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(X_pred[i].reshape(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), cmap=<span class="string">'gray'</span>)</span><br><span class="line">        plt.title(<span class="string">'True: %s \n Pred: %s'</span> % (y_true[i], y_pred[i]), size=<span class="number">15</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="验证本地图片："><a href="#验证本地图片：" class="headerlink" title="验证本地图片："></a><strong>验证本地图片</strong>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2018/9/7 19:18</span></span><br><span class="line"><span class="comment"># @Author  : Seven</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : TestModel.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">import</span> Read_data</span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">import</span> visualization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 加载模型</span></span><br><span class="line">    model = load_model(<span class="string">'CIFAR10_model_with_data_augmentation_VGG.h5'</span>)</span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    X_train, y_train, X_test, y_test = Read_data.load_data()</span><br><span class="line">    visualization.plot_image(X_test, y_test, config.class_name, model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">local_photos</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># input</span></span><br><span class="line">    im = Image.open(<span class="string">'image/dog-1.jpg'</span>)</span><br><span class="line">    <span class="comment"># im.show()</span></span><br><span class="line">    im = im.resize((<span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(im.size, im.mode)</span></span><br><span class="line"></span><br><span class="line">    im = np.array(im).astype(np.float32)</span><br><span class="line">    im = np.reshape(im, [<span class="number">-1</span>, <span class="number">32</span>*<span class="number">32</span>*<span class="number">3</span>])</span><br><span class="line">    im = (im - (<span class="number">255</span> / <span class="number">2.0</span>)) / <span class="number">255</span></span><br><span class="line">    batch_xs = np.reshape(im, [<span class="number">-1</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    model = load_model(<span class="string">'CIFAR10_model_with_data_augmentation_VGG.h5'</span>)</span><br><span class="line">    output = model.predict(batch_xs)</span><br><span class="line"></span><br><span class="line">    print(output)</span><br><span class="line">    print(<span class="string">'the out put is :'</span>, config.class_name[np.argmax(output)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    local_photos()</span><br></pre></td></tr></table></figure><h3 id="输出："><a href="#输出：" class="headerlink" title="输出："></a><strong>输出：</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line">the out put <span class="keyword">is</span> : dog</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GAN </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow--保存神经网络参数和加载神经网络参数</title>
      <link href="/2018/09/04/2018-09-04-TensorFlow-save-restore/"/>
      <url>/2018/09/04/2018-09-04-TensorFlow-save-restore/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="保存神经网络参数"><a href="#保存神经网络参数" class="headerlink" title="保存神经网络参数"></a>保存神经网络参数</h3><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a><strong>代码</strong>：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/9/4 16:11</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : save.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 保存神经网络参数</span><br><span class="line">def save_para():</span><br><span class="line">    # 定义权重参数</span><br><span class="line">    W = tf.Variable([[1, 2, 3], [4, 5, 6]], dtype = tf.float32, name = &apos;weights&apos;)</span><br><span class="line">    # 定义偏置参数</span><br><span class="line">    b = tf.Variable([[1, 2, 3]], dtype = tf.float32, name = &apos;biases&apos;)</span><br><span class="line">    # 参数初始化</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    # 定义保存参数的saver</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line">        # 保存session中的数据</span><br><span class="line">        save_path = saver.save(sess, &apos;./save_net.ckpt&apos;)</span><br><span class="line">        # 输出保存路径</span><br><span class="line">        print(&apos;Save to path: &apos;, save_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">save_para()</span><br></pre></td></tr></table></figure><h4 id="执行结果："><a href="#执行结果：" class="headerlink" title="执行结果："></a><strong>执行结果</strong>：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Save to path:  ./save_net.ckpt</span><br></pre></td></tr></table></figure><h3 id="加载神经网络参数"><a href="#加载神经网络参数" class="headerlink" title="加载神经网络参数"></a>加载神经网络参数</h3><h4 id="代码：-1"><a href="#代码：-1" class="headerlink" title="代码："></a><strong>代码</strong>：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/9/4 16:14</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : restore.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 恢复神经网络参数</span><br><span class="line">def restore_para():</span><br><span class="line">    # 定义权重参数</span><br><span class="line">    W = tf.Variable(np.arange(6).reshape((2, 3)), dtype = tf.float32, name = &apos;weights&apos;)</span><br><span class="line">    # 定义偏置参数</span><br><span class="line">    b = tf.Variable(np.arange(3).reshape((1, 3)), dtype = tf.float32, name = &apos;biases&apos;)</span><br><span class="line">    # 定义提取参数的saver</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        # 加载文件中的参数数据，会根据name加载数据并保存到变量W和b中</span><br><span class="line">        save_path = saver.restore(sess, &apos;save_net.ckpt&apos;)</span><br><span class="line">        # 输出保存路径</span><br><span class="line">        print(&apos;Weights: &apos;, sess.run(W))</span><br><span class="line">        print(&apos;biases:  &apos;, sess.run(b))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">restore_para()</span><br></pre></td></tr></table></figure><h4 id="执行结果：-1"><a href="#执行结果：-1" class="headerlink" title="执行结果："></a><strong>执行结果</strong>：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Weights:  [[1. 2. 3.]</span><br><span class="line"> [4. 5. 6.]]</span><br><span class="line">biases:   [[1. 2. 3.]]</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络之深度卷积生成对抗网络</title>
      <link href="/2018/09/04/2018-09-04-DL-DCGAN/"/>
      <url>/2018/09/04/2018-09-04-DL-DCGAN/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="深度卷积生成对抗网络"><a href="#深度卷积生成对抗网络" class="headerlink" title="深度卷积生成对抗网络"></a>深度卷积生成对抗网络</h3><p>深度卷积生成对抗网络（Deep Convolutional Generative Adversarial Networks, DCGANs）由Alec Radford等于2015年提出。在了解<a href="https://sevenold.github.io/2018/09/DL-GAN/" target="_blank" rel="noopener">GAN</a>的架构后，我们可以较为容易地了解DCGAN的原理。</p><h3 id="稳定的DCGAN架构指南"><a href="#稳定的DCGAN架构指南" class="headerlink" title="稳定的DCGAN架构指南"></a>稳定的DCGAN架构指南</h3><ol><li>将<strong>pooling layer</strong>替换成<strong>convolutions layer</strong><ol><li>对于<strong>生成模型</strong>：允许网络学习自己的<strong>空间下采样</strong>。<br>. 对于<strong>判别模型</strong>：允许网络学习自己的<strong>空间上采样</strong>。</li></ol></li><li>除了<strong>生成器的输出层</strong>和<strong>判别器的输入层</strong>之外，使用<strong>批量标准化–batchnorm</strong>。<ol><li>解决初始化差的问题。</li><li>帮助梯度传播到每一层。</li><li>防止<strong>生成器</strong>把所有的样本都收敛到同一个点。</li></ol></li><li>在CNN中移除<strong>全连接层</strong>。</li><li>在<strong>生成器</strong>的除了<strong>输出层</strong>外的所有层使用<strong>ReLU</strong>，<strong>输出层</strong>采用<strong>tanh</strong>。</li><li>在<strong>判别器</strong>的所有层中使用<strong>LeakyReLU</strong>。</li></ol><h3 id="DCGAN生成器"><a href="#DCGAN生成器" class="headerlink" title="DCGAN生成器"></a>DCGAN生成器</h3><p>在DCGAN中，生成式模型$G(z)$使用一个比较特殊的深度卷积网络来实现，如下图所示：</p><p><img src="/images/dl/119.png" alt="images"></p><h3 id="DCGAN判别器"><a href="#DCGAN判别器" class="headerlink" title="DCGAN判别器"></a>DCGAN判别器</h3><p>而判别式模型$D(x)$则仍是一个传统的深度卷积网络，如下图所示:</p><p><img src="/images/dl/120.png" alt="images"></p><h3 id="反卷积"><a href="#反卷积" class="headerlink" title="反卷积"></a>反卷积</h3><p>从前面两幅图中可以看出，DCGAN的生成式模型$G(z)$中出现了上采样（upsampling）。</p><p>卷积神经网络的下采样很好理解，加入polling层即可，然而这里的上采样要如何实现呢？</p><p>这里，DCGAN通过“微步幅卷积”（fractionally-strided convolution）进行上采样。</p><p>假设有一个3×3的输入，希望输出的尺寸比这要大，那么可以把这个3×3的输入通过在像素之间插入0的方式来进行扩展，如下图所示。当扩展到7×7的尺寸后，再进行卷积，就可以得到尺寸比原来大的输出。</p><p><img src="/images/dl/121.gif" alt="images"></p><h3 id="DCGAN的特点："><a href="#DCGAN的特点：" class="headerlink" title="DCGAN的特点："></a>DCGAN的特点：</h3><ul><li>判别模型：使用带步长的卷积（strided convolutions）取代了的空间池化（spatial pooling），容许网络学习自己的空间下采样（spatial downsampling）。</li><li>生成模型：使用微步幅卷积（fractional strided），容许它学习自己的空间上采样（spatial upsampling）。</li><li>激活函数： LeakyReLU</li><li>Batch Normalization 批标准化：解决因糟糕的初始化引起的训练问题，使得梯度能传播更深层次。 Batch Normalization证明了生成模型初始化的重要性，避免生成模型崩溃：生成的所有样本都在一个点上（样本相同），这是训练GANs经常遇到的失败现象。</li></ul><h3 id="DCGAN调优技巧"><a href="#DCGAN调优技巧" class="headerlink" title="DCGAN调优技巧"></a>DCGAN调优技巧</h3><ul><li><p>所有模型均采用小批量随机梯度下降（SGD）进行训练，最小批量为128。</p></li><li><p>所有权重均从零中心正态分布初始化，标准偏差为0.02。</p></li><li><p>在LeakyReLU中，所有model的leak斜率均设为0.2。</p></li><li><p>虽然之前的GAN工作已经使用了<strong>momentum</strong>来加速训练，但我们使用了<strong>Adam</strong>优化器和调整的超参数。</p></li><li><p>建议的学习率为0.001，过高，使用0.0002代替。</p></li><li><p>建议将<strong>momentum</strong>项$β_1$保持在0.9，会导致训练振荡和不稳定，所以将其降低到0.5有助于稳定训练。</p></li></ul><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow实现深度卷积生成对抗网络-DCGAN</title>
      <link href="/2018/09/04/2018-09-04-TensorFlow-DCGAN/"/>
      <url>/2018/09/04/2018-09-04-TensorFlow-DCGAN/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a><strong>代码</strong>：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/9/4 16:29</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : DCGAN.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"># TODO: 导入环境</span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line"># TODO：数据准备</span><br><span class="line">mnist = input_data.read_data_sets(&apos;data&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO:获得输入数据</span><br><span class="line">def get_inputs(noise_dim, image_height, image_width, image_depth):</span><br><span class="line">    # 真实数据</span><br><span class="line">    inputs_real = tf.placeholder(tf.float32, [None, image_height, image_width, image_depth], name=&apos;inputs_real&apos;)</span><br><span class="line">    # 噪声数据</span><br><span class="line">    inputs_noise = tf.placeholder(tf.float32, [None, noise_dim], name=&apos;inputs_noise&apos;)</span><br><span class="line"></span><br><span class="line">    return inputs_real, inputs_noise</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO:生成器</span><br><span class="line">def get_generator(noise_img, output_dim, is_train=True, alpha=0.01):</span><br><span class="line">    with tf.variable_scope(&quot;generator&quot;, reuse=(not is_train)):</span><br><span class="line">        # 100 x 1 to 4 x 4 x 512</span><br><span class="line">        # 全连接层</span><br><span class="line">        layer1 = tf.layers.dense(noise_img, 4 * 4 * 512)</span><br><span class="line">        layer1 = tf.reshape(layer1, [-1, 4, 4, 512])</span><br><span class="line">        # batch normalization</span><br><span class="line">        layer1 = tf.layers.batch_normalization(layer1, training=is_train)</span><br><span class="line">        # Leaky ReLU</span><br><span class="line">        layer1 = tf.maximum(alpha * layer1, layer1)</span><br><span class="line">        # dropout</span><br><span class="line">        layer1 = tf.nn.dropout(layer1, keep_prob=0.8)</span><br><span class="line"></span><br><span class="line">        # 4 x 4 x 512 to 7 x 7 x 256</span><br><span class="line">        layer2 = tf.layers.conv2d_transpose(layer1, 256, 4, strides=1, padding=&apos;valid&apos;)</span><br><span class="line">        layer2 = tf.layers.batch_normalization(layer2, training=is_train)</span><br><span class="line">        layer2 = tf.maximum(alpha * layer2, layer2)</span><br><span class="line">        layer2 = tf.nn.dropout(layer2, keep_prob=0.8)</span><br><span class="line"></span><br><span class="line">        # 7 x 7 256 to 14 x 14 x 128</span><br><span class="line">        layer3 = tf.layers.conv2d_transpose(layer2, 128, 3, strides=2, padding=&apos;same&apos;)</span><br><span class="line">        layer3 = tf.layers.batch_normalization(layer3, training=is_train)</span><br><span class="line">        layer3 = tf.maximum(alpha * layer3, layer3)</span><br><span class="line">        layer3 = tf.nn.dropout(layer3, keep_prob=0.8)</span><br><span class="line"></span><br><span class="line">        # 14 x 14 x 128 to 28 x 28 x 1</span><br><span class="line">        logits = tf.layers.conv2d_transpose(layer3, output_dim, 3, strides=2, padding=&apos;same&apos;)</span><br><span class="line">        # MNIST原始数据集的像素范围在0-1，这里的生成图片范围为(-1,1)</span><br><span class="line">        # 因此在训练时，记住要把MNIST像素范围进行resize</span><br><span class="line">        outputs = tf.tanh(logits)</span><br><span class="line"></span><br><span class="line">        return outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO:判别器</span><br><span class="line">def get_discriminator(inputs_img, reuse=False, alpha=0.01):</span><br><span class="line">    with tf.variable_scope(&quot;discriminator&quot;, reuse=reuse):</span><br><span class="line">        # 28 x 28 x 1 to 14 x 14 x 128</span><br><span class="line">        # 第一层不加入BN</span><br><span class="line">        layer1 = tf.layers.conv2d(inputs_img, 128, 3, strides=2, padding=&apos;same&apos;)</span><br><span class="line">        layer1 = tf.maximum(alpha * layer1, layer1)</span><br><span class="line">        layer1 = tf.nn.dropout(layer1, keep_prob=0.8)</span><br><span class="line"></span><br><span class="line">        # 14 x 14 x 128 to 7 x 7 x 256</span><br><span class="line">        layer2 = tf.layers.conv2d(layer1, 256, 3, strides=2, padding=&apos;same&apos;)</span><br><span class="line">        layer2 = tf.layers.batch_normalization(layer2, training=True)</span><br><span class="line">        layer2 = tf.maximum(alpha * layer2, layer2)</span><br><span class="line">        layer2 = tf.nn.dropout(layer2, keep_prob=0.8)</span><br><span class="line"></span><br><span class="line">        # 7 x 7 x 256 to 4 x 4 x 512</span><br><span class="line">        layer3 = tf.layers.conv2d(layer2, 512, 3, strides=2, padding=&apos;same&apos;)</span><br><span class="line">        layer3 = tf.layers.batch_normalization(layer3, training=True)</span><br><span class="line">        layer3 = tf.maximum(alpha * layer3, layer3)</span><br><span class="line">        layer3 = tf.nn.dropout(layer3, keep_prob=0.8)</span><br><span class="line"></span><br><span class="line">        # 4 x 4 x 512 to 4*4*512 x 1</span><br><span class="line">        flatten = tf.reshape(layer3, (-1, 4 * 4 * 512))</span><br><span class="line">        logits = tf.layers.dense(flatten, 1)</span><br><span class="line">        outputs = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">        return logits, outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 目标函数</span><br><span class="line">def get_loss(inputs_real, inputs_noise, image_depth, smooth=0.1):</span><br><span class="line">    g_outputs = get_generator(inputs_noise, image_depth, is_train=True)</span><br><span class="line">    d_logits_real, d_outputs_real = get_discriminator(inputs_real)</span><br><span class="line">    d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, reuse=True)</span><br><span class="line"></span><br><span class="line">    # 计算Loss</span><br><span class="line">    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,</span><br><span class="line">                                                                    labels=tf.ones_like(d_outputs_fake) * (1 - smooth)))</span><br><span class="line"></span><br><span class="line">    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,</span><br><span class="line">                                                                         labels=tf.ones_like(d_outputs_real) * (</span><br><span class="line">                                                                                     1 - smooth)))</span><br><span class="line">    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,</span><br><span class="line">                                                                         labels=tf.zeros_like(d_outputs_fake)))</span><br><span class="line">    d_loss = tf.add(d_loss_real, d_loss_fake)</span><br><span class="line"></span><br><span class="line">    return g_loss, d_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO:优化器</span><br><span class="line">def get_optimizer(g_loss, d_loss, learning_rate=0.001):</span><br><span class="line">    train_vars = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line">    g_vars = [var for var in train_vars if var.name.startswith(&quot;generator&quot;)]</span><br><span class="line">    d_vars = [var for var in train_vars if var.name.startswith(&quot;discriminator&quot;)]</span><br><span class="line"></span><br><span class="line">    # Optimizer</span><br><span class="line">    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):</span><br><span class="line">        g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)</span><br><span class="line">        d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)</span><br><span class="line"></span><br><span class="line">    return g_opt, d_opt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 显示图片</span><br><span class="line">def plot_images(samples):</span><br><span class="line">    fig, axes = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True, figsize=(7, 7))</span><br><span class="line">    for img, ax in zip(samples, axes.flatten()):</span><br><span class="line">        ax.imshow(img.reshape((28, 28)), cmap=&apos;Greys_r&apos;)</span><br><span class="line">        ax.get_xaxis().set_visible(False)</span><br><span class="line">        ax.get_yaxis().set_visible(False)</span><br><span class="line">    fig.tight_layout(pad=0)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def show_generator_output(sess, n_images, inputs_noise, output_dim):</span><br><span class="line">    noise_shape = inputs_noise.get_shape().as_list()[-1]</span><br><span class="line">    # 生成噪声图片</span><br><span class="line">    examples_noise = np.random.uniform(-1, 1, size=[n_images, noise_shape])</span><br><span class="line"></span><br><span class="line">    samples = sess.run(get_generator(inputs_noise, output_dim, False),</span><br><span class="line">                       feed_dict=&#123;inputs_noise: examples_noise&#125;)</span><br><span class="line"></span><br><span class="line">    result = np.squeeze(samples, -1)</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO:开始训练</span><br><span class="line"># 定义参数</span><br><span class="line">batch_size = 64</span><br><span class="line">noise_size = 100</span><br><span class="line">epochs = 5</span><br><span class="line">n_samples = 25</span><br><span class="line">learning_rate = 0.001</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train(noise_size, data_shape, batch_size, n_samples):</span><br><span class="line">    # 存储loss</span><br><span class="line">    losses = []</span><br><span class="line">    steps = 0</span><br><span class="line"></span><br><span class="line">    inputs_real, inputs_noise = get_inputs(noise_size, data_shape[1], data_shape[2], data_shape[3])</span><br><span class="line">    g_loss, d_loss = get_loss(inputs_real, inputs_noise, data_shape[-1])</span><br><span class="line">    print(&quot;FUNCTION READY!!&quot;)</span><br><span class="line">    g_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, learning_rate)</span><br><span class="line">    print(&quot;TRAINING....&quot;)</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        # 迭代epoch</span><br><span class="line">        for e in range(epochs):</span><br><span class="line">            for batch_i in range(mnist.train.num_examples // batch_size):</span><br><span class="line">                steps += 1</span><br><span class="line">                batch = mnist.train.next_batch(batch_size)</span><br><span class="line"></span><br><span class="line">                batch_images = batch[0].reshape((batch_size, data_shape[1], data_shape[2], data_shape[3]))</span><br><span class="line">                # scale to -1, 1</span><br><span class="line">                batch_images = batch_images * 2 - 1</span><br><span class="line"></span><br><span class="line">                # noise</span><br><span class="line">                batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_size))</span><br><span class="line"></span><br><span class="line">                # run optimizer</span><br><span class="line">                sess.run(g_train_opt, feed_dict=&#123;inputs_real: batch_images,</span><br><span class="line">                                                 inputs_noise: batch_noise&#125;)</span><br><span class="line">                sess.run(d_train_opt, feed_dict=&#123;inputs_real: batch_images,</span><br><span class="line">                                                 inputs_noise: batch_noise&#125;)</span><br><span class="line"></span><br><span class="line">                if steps % 101 == 0:</span><br><span class="line">                    train_loss_d = d_loss.eval(&#123;inputs_real: batch_images,</span><br><span class="line">                                                inputs_noise: batch_noise&#125;)</span><br><span class="line">                    train_loss_g = g_loss.eval(&#123;inputs_real: batch_images,</span><br><span class="line">                                                inputs_noise: batch_noise&#125;)</span><br><span class="line">                    losses.append((train_loss_d, train_loss_g))</span><br><span class="line">                    print(&quot;Epoch &#123;&#125;/&#123;&#125;....&quot;.format(e + 1, epochs),</span><br><span class="line">                          &quot;Discriminator Loss: &#123;:.4f&#125;....&quot;.format(train_loss_d),</span><br><span class="line">                          &quot;Generator Loss: &#123;:.4f&#125;....&quot;.format(train_loss_g))</span><br><span class="line"></span><br><span class="line">            if e % 1 == 0:</span><br><span class="line">                # 显示图片</span><br><span class="line">                samples = show_generator_output(sess, n_samples, inputs_noise, data_shape[-1])</span><br><span class="line">                plot_images(samples)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with tf.Graph().as_default():</span><br><span class="line">    train(noise_size, [-1, 28, 28, 1], batch_size, n_samples)</span><br><span class="line">    print(&quot;OPTIMIZER END!!&quot;)</span><br></pre></td></tr></table></figure><h4 id="执行结果："><a href="#执行结果：" class="headerlink" title="执行结果："></a><strong>执行结果</strong>：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Epoch 4/5.... Discriminator Loss: 0.4584.... Generator Loss: 4.8776....</span><br><span class="line">Epoch 4/5.... Discriminator Loss: 0.4315.... Generator Loss: 2.9278....</span><br><span class="line">Epoch 4/5.... Discriminator Loss: 0.5317.... Generator Loss: 3.4315....</span><br><span class="line">Epoch 4/5.... Discriminator Loss: 0.6342.... Generator Loss: 2.7376....</span><br><span class="line">Epoch 4/5.... Discriminator Loss: 0.4312.... Generator Loss: 3.9016....</span><br><span class="line">Epoch 4/5.... Discriminator Loss: 0.5498.... Generator Loss: 2.3418....</span><br><span class="line">Epoch 4/5.... Discriminator Loss: 0.4807.... Generator Loss: 3.4065....</span><br><span class="line">Epoch 4/5.... Discriminator Loss: 0.5360.... Generator Loss: 2.2658....</span><br><span class="line">Epoch 4/5.... Discriminator Loss: 0.4612.... Generator Loss: 2.8166....</span><br><span class="line">Epoch 5/5.... Discriminator Loss: 0.4784.... Generator Loss: 2.9896....</span><br><span class="line">Epoch 5/5.... Discriminator Loss: 0.7368.... Generator Loss: 2.1130....</span><br><span class="line">Epoch 5/5.... Discriminator Loss: 0.4192.... Generator Loss: 3.9733....</span><br><span class="line">Epoch 5/5.... Discriminator Loss: 0.4998.... Generator Loss: 2.1577....</span><br><span class="line">Epoch 5/5.... Discriminator Loss: 0.4693.... Generator Loss: 3.1395....</span><br><span class="line">Epoch 5/5.... Discriminator Loss: 0.3946.... Generator Loss: 4.0385....</span><br><span class="line">Epoch 5/5.... Discriminator Loss: 0.4807.... Generator Loss: 3.8975....</span><br><span class="line">Epoch 5/5.... Discriminator Loss: 0.4703.... Generator Loss: 3.6105....</span><br><span class="line">OPTIMIZER END!!</span><br></pre></td></tr></table></figure><h4 id="生成的图像："><a href="#生成的图像：" class="headerlink" title="生成的图像："></a><strong>生成的图像</strong>：</h4><p><img src="/images/dl/122.png" alt="images"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow实现简单的生成对抗网络-GAN</title>
      <link href="/2018/09/03/2018-09-03-TensorFlow-GAN/"/>
      <url>/2018/09/03/2018-09-03-TensorFlow-GAN/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h4 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a><strong>示例代码</strong>：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/9/1 16:38</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : GAN.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"># TODO: 0.导入环境</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 1：读取数据</span><br><span class="line">mnist = input_data.read_data_sets(&apos;data&apos;)</span><br><span class="line"></span><br><span class="line"># TODO: 2：初始化参数</span><br><span class="line">img_size = mnist.train.images[0].shape[0]</span><br><span class="line">noise_size = 100</span><br><span class="line">g_units = 128</span><br><span class="line">d_units = 128</span><br><span class="line">learning_rate = 0.001</span><br><span class="line">alpha = 0.01</span><br><span class="line"></span><br><span class="line"># 真实数据和噪音数据的placeholder</span><br><span class="line">real_img = tf.placeholder(tf.float32, [None, img_size])</span><br><span class="line">noise_img = tf.placeholder(tf.float32, [None, noise_size])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 显示生成的图像</span><br><span class="line">def view_samples(epoch, samples):</span><br><span class="line">    fig, axes = plt.subplots(figsize=(7, 7), nrows=5, ncols=5, sharey=True, sharex=True)</span><br><span class="line">    for ax, img in zip(axes.flatten(), samples[epoch][1]):  # 这里samples[epoch][1]代表生成的图像结果，而[0]代表对应的logits</span><br><span class="line">        ax.xaxis.set_visible(False)</span><br><span class="line">        ax.yaxis.set_visible(False)</span><br><span class="line">        ax.imshow(img.reshape((28, 28)), cmap=&apos;Greys_r&apos;)</span><br><span class="line">    plt.show()</span><br><span class="line">    return fig, axes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 4.生成器</span><br><span class="line">def get_generator(noise_img, n_units, out_dim, reuse=False, alpha=0.01):</span><br><span class="line">    with tf.variable_scope(&quot;generator&quot;, reuse=reuse):</span><br><span class="line">        # hidden layer</span><br><span class="line">        hidden1 = tf.layers.dense(noise_img, n_units)</span><br><span class="line">        # leaky ReLU</span><br><span class="line">        hidden1 = tf.maximum(alpha * hidden1, hidden1)</span><br><span class="line">        # dropout</span><br><span class="line">        hidden1 = tf.layers.dropout(hidden1, rate=0.2)</span><br><span class="line"></span><br><span class="line">        # logits &amp; outputs</span><br><span class="line">        logits = tf.layers.dense(hidden1, out_dim)</span><br><span class="line">        outputs = tf.tanh(logits)</span><br><span class="line"></span><br><span class="line">        return logits, outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 生成器生成数据</span><br><span class="line">g_logits, g_outputs = get_generator(noise_img, g_units, img_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 5.判别器</span><br><span class="line">def get_discriminator(img, n_units, reuse=False, alpha=0.01):</span><br><span class="line">    with tf.variable_scope(&quot;discriminator&quot;, reuse=reuse):</span><br><span class="line">        # hidden layer</span><br><span class="line">        hidden1 = tf.layers.dense(img, n_units)</span><br><span class="line">        hidden1 = tf.maximum(alpha * hidden1, hidden1)</span><br><span class="line"></span><br><span class="line">        # logits &amp; outputs</span><br><span class="line">        logits = tf.layers.dense(hidden1, 1)</span><br><span class="line">        outputs = tf.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">        return logits, outputs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 判别真实的数据</span><br><span class="line">d_logits_real, d_outputs_real = get_discriminator(real_img, d_units)</span><br><span class="line"># 判别生成器生成的数据</span><br><span class="line">d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, d_units, reuse=True)</span><br><span class="line"></span><br><span class="line"># TODO: 6.损失值的计算</span><br><span class="line"># 判别器的损失值</span><br><span class="line"># 识别真实图片的损失值</span><br><span class="line">d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,</span><br><span class="line">                                                                     labels=tf.ones_like(d_logits_real)))</span><br><span class="line"># 识别生成的图片的损失值</span><br><span class="line">d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,</span><br><span class="line">                                                                     labels=tf.zeros_like(d_logits_fake)))</span><br><span class="line"># 总体loss</span><br><span class="line">d_loss = tf.add(d_loss_real, d_loss_fake)</span><br><span class="line"># generator的loss</span><br><span class="line">g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,</span><br><span class="line">                                                                labels=tf.ones_like(d_logits_fake)))</span><br><span class="line"></span><br><span class="line"># TODO:7.始化optimizer</span><br><span class="line">train_vars = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line"># generator</span><br><span class="line">g_vars = [var for var in train_vars if var.name.startswith(&quot;generator&quot;)]</span><br><span class="line"># discriminator</span><br><span class="line">d_vars = [var for var in train_vars if var.name.startswith(&quot;discriminator&quot;)]</span><br><span class="line"></span><br><span class="line"># optimizer</span><br><span class="line">d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)</span><br><span class="line">g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)</span><br><span class="line"></span><br><span class="line">print(&quot;FUNCTION READY!!!&quot;)</span><br><span class="line">print(&quot;TRAINING.....&quot;)</span><br><span class="line"></span><br><span class="line"># TODO:8.开始训练</span><br><span class="line"></span><br><span class="line">batch_size = 64</span><br><span class="line"># 训练迭代轮数</span><br><span class="line">epochs = 300</span><br><span class="line"># 抽取样本数</span><br><span class="line">n_sample = 25</span><br><span class="line"># 存储测试样例</span><br><span class="line">samples = []</span><br><span class="line"># 存储loss</span><br><span class="line">losses = []</span><br><span class="line"># 初始化所有变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">show_imgs = []</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        for batch_i in range(mnist.train.num_examples // batch_size):</span><br><span class="line"></span><br><span class="line">            batch = mnist.train.next_batch(batch_size)</span><br><span class="line">            batch_images = batch[0].reshape((batch_size, 784))</span><br><span class="line"></span><br><span class="line">            # 对图像像素进行缩放，这是因为tanh输出的结果介于(-1,1),real和fake图片共享discriminator的参数</span><br><span class="line">            batch_images = batch_images * 2 - 1</span><br><span class="line"></span><br><span class="line">            # generator的输入噪声</span><br><span class="line">            batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_size))</span><br><span class="line"></span><br><span class="line">            # Run optimizers</span><br><span class="line">            sess.run(d_train_opt, feed_dict=&#123;real_img: batch_images, noise_img: batch_noise&#125;)</span><br><span class="line">            sess.run(g_train_opt, feed_dict=&#123;noise_img: batch_noise&#125;)</span><br><span class="line"></span><br><span class="line">        if (epoch+1) % 30 == 0:</span><br><span class="line">            # 每一轮结束计算loss</span><br><span class="line">            train_loss_d = sess.run(d_loss,</span><br><span class="line">                                    feed_dict=&#123;real_img: batch_images,</span><br><span class="line">                                               noise_img: batch_noise&#125;)</span><br><span class="line">            # real img loss</span><br><span class="line">            train_loss_d_real = sess.run(d_loss_real,</span><br><span class="line">                                         feed_dict=&#123;real_img: batch_images,</span><br><span class="line">                                                    noise_img: batch_noise&#125;)</span><br><span class="line">            # fake img loss</span><br><span class="line">            train_loss_d_fake = sess.run(d_loss_fake,</span><br><span class="line">                                         feed_dict=&#123;real_img: batch_images,</span><br><span class="line">                                                    noise_img: batch_noise&#125;)</span><br><span class="line">            # generator loss</span><br><span class="line">            train_loss_g = sess.run(g_loss,</span><br><span class="line">                                    feed_dict=&#123;noise_img: batch_noise&#125;)</span><br><span class="line"></span><br><span class="line">            print(&quot;Epoch &#123;&#125;/&#123;&#125;...\n&quot;.format(epoch + 1, epochs),</span><br><span class="line">                  &quot;判别器损失: &#123;:.4f&#125;--&gt;(判别真实的: &#123;:.4f&#125; + 判别生成的: &#123;:.4f&#125;)...\n&quot;.format(train_loss_d, train_loss_d_real,</span><br><span class="line">                                                                                train_loss_d_fake),</span><br><span class="line">                  &quot;生成器损失: &#123;:.4f&#125;&quot;.format(train_loss_g))</span><br><span class="line"></span><br><span class="line">            losses.append((train_loss_d, train_loss_d_real, train_loss_d_fake, train_loss_g))</span><br><span class="line">            # 抽取样本后期进行观察</span><br><span class="line">            sample_noise = np.random.uniform(-1, 1, size=(n_sample, noise_size))</span><br><span class="line">            gen_samples = sess.run(get_generator(noise_img, g_units, img_size, reuse=True),</span><br><span class="line">                                   feed_dict=&#123;noise_img: sample_noise&#125;)</span><br><span class="line">            samples.append(gen_samples)</span><br><span class="line"></span><br><span class="line">    # 显示生成的图像</span><br><span class="line">    view_samples(-1, samples)</span><br><span class="line"></span><br><span class="line">print(&quot;OPTIMIZER END&quot;)</span><br></pre></td></tr></table></figure><h4 id="执行结果："><a href="#执行结果：" class="headerlink" title="执行结果："></a><strong>执行结果</strong>：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"> 判别器损失: 0.8938--&gt;(判别真实的: 0.4657 + 判别生成的: 0.4281)...</span><br><span class="line"> 生成器损失: 1.9035</span><br><span class="line">Epoch 60/300...</span><br><span class="line"> 判别器损失: 0.9623--&gt;(判别真实的: 0.5577 + 判别生成的: 0.4046)...</span><br><span class="line"> 生成器损失: 1.7722</span><br><span class="line">Epoch 90/300...</span><br><span class="line"> 判别器损失: 0.9523--&gt;(判别真实的: 0.3698 + 判别生成的: 0.5825)...</span><br><span class="line"> 生成器损失: 1.3028</span><br><span class="line">Epoch 120/300...</span><br><span class="line"> 判别器损失: 0.8671--&gt;(判别真实的: 0.3948 + 判别生成的: 0.4723)...</span><br><span class="line"> 生成器损失: 1.5518</span><br><span class="line">Epoch 150/300...</span><br><span class="line"> 判别器损失: 1.0439--&gt;(判别真实的: 0.3626 + 判别生成的: 0.6813)...</span><br><span class="line"> 生成器损失: 1.1374</span><br><span class="line">Epoch 180/300...</span><br><span class="line"> 判别器损失: 1.3034--&gt;(判别真实的: 0.6210 + 判别生成的: 0.6824)...</span><br><span class="line"> 生成器损失: 1.3377</span><br><span class="line">Epoch 210/300...</span><br><span class="line"> 判别器损失: 0.8368--&gt;(判别真实的: 0.4397 + 判别生成的: 0.3971)...</span><br><span class="line"> 生成器损失: 1.7115</span><br><span class="line">Epoch 240/300...</span><br><span class="line"> 判别器损失: 1.0776--&gt;(判别真实的: 0.5503 + 判别生成的: 0.5273)...</span><br><span class="line"> 生成器损失: 1.4761</span><br><span class="line">Epoch 270/300...</span><br><span class="line"> 判别器损失: 0.9964--&gt;(判别真实的: 0.5351 + 判别生成的: 0.4612)...</span><br><span class="line"> 生成器损失: 1.8451</span><br><span class="line">Epoch 300/300...</span><br><span class="line"> 判别器损失: 0.9810--&gt;(判别真实的: 0.5085 + 判别生成的: 0.4725)...</span><br><span class="line"> 生成器损失: 1.5440</span><br><span class="line">OPTIMIZER END</span><br></pre></td></tr></table></figure><h4 id="生成的图像："><a href="#生成的图像：" class="headerlink" title="生成的图像："></a>生成的图像：</h4><p><img src="/images/dl/118.png" alt="images"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络之生成对抗网络</title>
      <link href="/2018/09/01/2018-09-01-DL-GAN/"/>
      <url>/2018/09/01/2018-09-01-DL-GAN/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="生成对抗网络"><a href="#生成对抗网络" class="headerlink" title="生成对抗网络"></a>生成对抗网络</h3><p>生成式对抗网络（GAN, Generative Adversarial Networks ）是一种<strong>深度学习</strong>模型，是近年来复杂分布上无监督学习最具前景的方法之一。</p><p>模型通过框架中（至少）两个模块：<strong>生成模型（Generative Model）和判别模型（Discriminative Model）</strong>的互相<strong>博弈</strong>学习产生相当好的输出。</p><p>原始 GAN 理论中，并不要求 G 和 D 都是神经网络，只需要是能拟合相应生成和判别的函数即可。</p><p>但实用中一般均使用深度神经网络作为 G 和 D 。一个优秀的GAN应用需要有良好的训练方法，否则可能由于神经网络模型的自由性而导致输出不理想。</p><h3 id="GAN的基本框架"><a href="#GAN的基本框架" class="headerlink" title="GAN的基本框架"></a>GAN的基本框架</h3><p>GAN所建立的一个学习框架，实际上我们可以看成<strong>生成模型和判别模型</strong>之间的一个模拟对抗游戏。我们可以把<strong>生成模型</strong>看作一个<strong>伪装者</strong>，而把<strong>判别模型</strong>看成一个<strong>警察</strong>。<strong>生成模型</strong>通过不断地学习来提高自己的<strong>伪装能力</strong>，从而使得生成出来的数据能够更好地“欺骗”<strong>判别模型</strong>。而<strong>判别模型</strong>则通过不断的训练来提高自己的判别能力，能够更准确地判断出数据的来源。GAN就是这样一个不断对抗的网络。GAN的架构如下图所示：</p><p><img src="/images/dl/110.png" alt="images"></p><p><strong>生成模型</strong>以<strong>随机变量</strong>作为输入，其输出是对真实数据分布的一个估计。</p><p>生成数据和真实数据的采样都由<strong>判别模型</strong>进行判别，并给出真假性的判断和当前的损失。</p><p>利用<strong>反向传播</strong>，GAN对生成模型和判别模型进行交替优化。</p><p><img src="/images/dl/113.png" alt="images"></p><h3 id="GAN的优化目标"><a href="#GAN的优化目标" class="headerlink" title="GAN的优化目标"></a>GAN的优化目标</h3><p>在对抗生成网络中，有两个博弈的角色分别为<strong>生成式模型(generative model)和判别式模型(discriminative model)</strong>。具体方式为：</p><ul><li>生成模型G捕捉样本数据的分布，判别模型D时一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率。</li></ul><p>在博弈的过程中我们需要提高两个模型的能力，所以通过不断调整<strong>生成模型G</strong>和<strong>判别模型D</strong>，直到<strong>判别模型D</strong>不能把数据的真假判别出来为止。在调整优化的过程中，我们需要：</p><ul><li>优化<strong>生成模型G</strong>，使得<strong>判别模型D</strong>无法判别出来事件的真假。</li><li>优化<strong>判别模型D</strong>，使得它尽可能的判别出事件的真假。</li></ul><h3 id="举个栗子："><a href="#举个栗子：" class="headerlink" title="举个栗子："></a>举个栗子：</h3><p>假设数据的概率分布为M，但是我们不知道具体的分布和构造是什么样的，就好像是一个黑盒子。为了了解这个黑盒子，我们就可以构建一个对抗生成网络：</p><ul><li><strong>生成模型G</strong>：使用一种我们完全知道的概率分布N来不断学习成为我们不知道的概率分布M.</li><li><strong>判别模型D</strong>：用来判别这个不断学习的概率是我们知道的概率分布N还是我们不知道的概率分布M。</li></ul><p>我们用图像来体现：</p><p><img src="/images/dl/111.png" alt="images"></p><p><img src="/images/dl/112.png" alt="images"></p><p>由上图所示：</p><ul><li>黑点所组成的数据分布是我们所不知道的概率分布M所形成的</li><li>绿色的线表示<strong>生成模型G</strong>使用已知的数据和判别模型不断对抗生成的数据分布。</li><li>蓝色的线表示<strong>判断模型D</strong></li><li>a图：初始状态</li><li>b图：生成模型不变，优化判别模型，直到判别的准确率最高</li><li>c图：判别模型不变。优化生成模型。直到生成的数据的真实性越高</li><li>d图：多次迭代后，生成模型产生的数据和概率部分M的数据基本一致，从而判别模型认为生成模型生成的数据就是概率分布M的数据分布。</li></ul><h3 id="GAN的数学推导"><a href="#GAN的数学推导" class="headerlink" title="GAN的数学推导"></a>GAN的数学推导</h3><p>符号定义:</p><ul><li><h4 id="P-data-x-：真实数据的分布"><a href="#P-data-x-：真实数据的分布" class="headerlink" title="$P_{data}(x)$：真实数据的分布"></a>$P_{data}(x)$：真实数据的分布</h4></li><li><h4 id="P-z-Z-：噪声数据"><a href="#P-z-Z-：噪声数据" class="headerlink" title="$P_z(Z)$：噪声数据"></a>$P_z(Z)$：噪声数据</h4></li><li><h4 id="P-g-x-：生成模型生成的数据分布"><a href="#P-g-x-：生成模型生成的数据分布" class="headerlink" title="$P_g(x)$：生成模型生成的数据分布"></a>$P_g(x)$：生成模型生成的数据分布</h4></li><li><h4 id="D-X-：判别器"><a href="#D-X-：判别器" class="headerlink" title="$D(X)$：判别器"></a>$D(X)$：判别器</h4></li><li><h4 id="G-x-：生成器"><a href="#G-x-：生成器" class="headerlink" title="$G(x)$：生成器"></a>$G(x)$：生成器</h4></li></ul><h3 id="定义判别模型和生成模型："><a href="#定义判别模型和生成模型：" class="headerlink" title="定义判别模型和生成模型："></a><strong>定义判别模型和生成模型</strong>：</h3><h4 id="E-x-sim-P-data-x-cdot-logD-x"><a href="#E-x-sim-P-data-x-cdot-logD-x" class="headerlink" title="$E_{x \sim P_{data}}(x) \cdot logD(x)$"></a>$E_{x \sim P_{data}}(x) \cdot logD(x)$</h4><p>由上式可知：当$x \sim P_{data}(x) , D(x)=1 $的时，$E_{x \sim P_{data}}(x)$取得最大值。</p><h4 id="E-x-sim-P-z-z-cdot-log-1-D-G-z"><a href="#E-x-sim-P-z-z-cdot-log-1-D-G-z" class="headerlink" title="$E_{x \sim P_{z}}(z) \cdot log(1-D(G(z)))$"></a>$E_{x \sim P_{z}}(z) \cdot log(1-D(G(z)))$</h4><p>由上式可知：当$x \sim P_{z}(z) , D(G(z))=0 $的时，$E_{x \sim P_{z}}(z)$取得最大值。</p><p>所以为了我们的<strong>判别模型</strong>越来越好，能力越来越强大，定义目标函数为：</p><h4 id="V-G-D-logD-x-log-1-D-G-z"><a href="#V-G-D-logD-x-log-1-D-G-z" class="headerlink" title="$V(G,D)=  logD(x) +  log(1-D(G(z)))$"></a>$V(G,D)= logD(x) + log(1-D(G(z)))$</h4><p>要使<strong>判别模型</strong>取得最好，所以需要使$V(G,D)$取得最大，即：</p><h4 id="D-agrmax-DV-G-D"><a href="#D-agrmax-DV-G-D" class="headerlink" title="$D = agrmax_DV(G,D)$"></a>$D = agrmax_DV(G,D)$</h4><p>当<strong>判别模型</strong>最好的时候，最好的<strong>生成模型</strong>就是<strong>目标函数取得最小</strong>的时候：</p><h4 id="G-argmin-G-aggmax-D-V-G-D"><a href="#G-argmin-G-aggmax-D-V-G-D" class="headerlink" title="$G=argmin_G(aggmax_D(V(G, D)))$"></a>$G=argmin_G(aggmax_D(V(G, D)))$</h4><p>所以经过这一系列的讨论，这个问题就变成了最大最小的问题，即：</p><h4 id="min-Gmax-DV-G-D-E-x-sim-P-data-x-cdot-logD-x-E-x-sim-P-z-z-cdot-log-1-D-G-z"><a href="#min-Gmax-DV-G-D-E-x-sim-P-data-x-cdot-logD-x-E-x-sim-P-z-z-cdot-log-1-D-G-z" class="headerlink" title="$min_Gmax_DV(G, D)=E_{x \sim P_{data}}(x) \cdot logD(x)+ E_{x \sim P_{z}}(z) \cdot log(1-D(G(z)))$"></a>$min_Gmax_DV(G, D)=E_{x \sim P_{data}}(x) \cdot logD(x)+ E_{x \sim P_{z}}(z) \cdot log(1-D(G(z)))$</h4><h3 id="最优判别模型："><a href="#最优判别模型：" class="headerlink" title="最优判别模型："></a><strong>最优判别模型</strong>：</h3><p>最终的目标函数：</p><p>$V(G,D)= \int_x P_{data}(x) \cdot logD(x) + P_g(x)log(1-D(G(z))) d(x)$</p><p>令：$V(G,D)=f(y), P_{data}(x)=a, P_g(x)=b$</p><p>所以：$f(y)=alogy+blog(1-y)$</p><p>因为:$a+b \ne 0$</p><p>所以最大值：$\frac{a}{a+b}$</p><p>最后，我们得到的<strong>最优判别模型</strong>就是：</p><h4 id="D-x-frac-P-data-X-P-data-X-P-g-x"><a href="#D-x-frac-P-data-X-P-data-X-P-g-x" class="headerlink" title="$D(x)=\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}$"></a>$D(x)=\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}$</h4><p>由于<strong>生成对抗网络</strong>的目的是：得到<strong>生成模型</strong>可以生成非常逼真的数据，也就是说是和真实数据的分布是一样的。因此最优的判别模型的输出为：</p><h4 id="D-x-frac-P-data-P-data-P-g-frac12"><a href="#D-x-frac-P-data-P-data-P-g-frac12" class="headerlink" title="$D(x)=\frac{P_{data}}{P_{data}+P_g}=\frac12$"></a>$D(x)=\frac{P_{data}}{P_{data}+P_g}=\frac12$</h4><p>其中：$P_g和P_{data}$的数据分布是一样的。</p><p>也就是说当D输出为0.5时，说明鉴别模型已经完全分不清真实数据和GAN生成的数据了，此时就是得到了最优生成模型了。</p><h3 id="证明生成模型："><a href="#证明生成模型：" class="headerlink" title="证明生成模型："></a><strong>证明生成模型</strong>：</h3><h4 id="充分性："><a href="#充分性：" class="headerlink" title="充分性："></a>充分性：</h4><p>前面我们已经得到了<strong>最优的判别模型</strong>，我们直接把数据带进目标函数：</p><h4 id="V-G-int-x-P-data-x-cdot-log-frac-12-P-g-x-log-frac-12-d-x-log4"><a href="#V-G-int-x-P-data-x-cdot-log-frac-12-P-g-x-log-frac-12-d-x-log4" class="headerlink" title="$V(G)=\int_x [P_{data}(x) \cdot log(\frac 12) +  P_g(x)log(\frac 12) ]d(x) =-log4$"></a>$V(G)=\int_x [P_{data}(x) \cdot log(\frac 12) + P_g(x)log(\frac 12) ]d(x) =-log4$</h4><h4 id="必要性："><a href="#必要性：" class="headerlink" title="必要性："></a>必要性：</h4><h4 id="V-G-int-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-P-g-x-P-g-x-log-1-frac-P-data-X-P-data-X-P-g-x-d-x"><a href="#V-G-int-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-P-g-x-P-g-x-log-1-frac-P-data-X-P-data-X-P-g-x-d-x" class="headerlink" title="$V(G)=\int_x [P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) +  P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) ] d(x) $"></a>$V(G)=\int_x [P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) + P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) ] d(x) $</h4><h4 id="V-G-int-x-log2-log2-cdot-P-data-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-P-g-x-log2-log2-cdot-P-g-x-P-g-x-log-1-frac-P-data-X-P-data-X-P-g-x-d-x"><a href="#V-G-int-x-log2-log2-cdot-P-data-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-P-g-x-log2-log2-cdot-P-g-x-P-g-x-log-1-frac-P-data-X-P-data-X-P-g-x-d-x" class="headerlink" title="$V(G)=\int_x [(log2-log2)\cdot P_{data}(x)+P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) + (log2-log2)\cdot P_g(x) + P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) ]d(x) $"></a>$V(G)=\int_x [(log2-log2)\cdot P_{data}(x)+P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) + (log2-log2)\cdot P_g(x) + P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) ]d(x) $</h4><h4 id="V-G-log2-int-x-P-g-x-P-data-x-d-x-int-x-P-data-x-cdot-log2-log-frac-P-data-X-P-data-X-P-g-x-P-g-x-cdot-log2-log-1-frac-P-data-X-P-data-X-P-g-x-d-x"><a href="#V-G-log2-int-x-P-g-x-P-data-x-d-x-int-x-P-data-x-cdot-log2-log-frac-P-data-X-P-data-X-P-g-x-P-g-x-cdot-log2-log-1-frac-P-data-X-P-data-X-P-g-x-d-x" class="headerlink" title="$V(G)=-log2 \int_x [P_g(x) +P_{data}(x)]d(x)+\int_x [P_{data}(x) \cdot(log2+ log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}))+  P_g(x) \cdot (log2+log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) )] d(x)$"></a>$V(G)=-log2 \int_x [P_g(x) +P_{data}(x)]d(x)+\int_x [P_{data}(x) \cdot(log2+ log(\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}))+ P_g(x) \cdot (log2+log(1-\frac{P_{data}(X)}{P_{data}(X)+P_g(x)}) )] d(x)$</h4><h4 id="V-G-log4-int-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-frac-P-g-x-2-P-g-x-log-1-frac-P-data-X-P-data-X-frac-P-g-x-2-d-x"><a href="#V-G-log4-int-x-P-data-x-cdot-log-frac-P-data-X-P-data-X-frac-P-g-x-2-P-g-x-log-1-frac-P-data-X-P-data-X-frac-P-g-x-2-d-x" class="headerlink" title="$V(G)=-log4+\int_x [P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+\frac{P_g(x)}{2}}) +  P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+\frac{P_g(x)}{2}}) ] d(x) $"></a>$V(G)=-log4+\int_x [P_{data}(x) \cdot log(\frac{P_{data}(X)}{P_{data}(X)+\frac{P_g(x)}{2}}) + P_g(x)log(1-\frac{P_{data}(X)}{P_{data}(X)+\frac{P_g(x)}{2}}) ] d(x) $</h4><h4 id="我们把最终结果转换为KL散度："><a href="#我们把最终结果转换为KL散度：" class="headerlink" title="我们把最终结果转换为KL散度："></a>我们把最终结果转换为<strong>KL散度</strong>：</h4><h4 id="V-G-log4-KL-P-data-mid-frac-P-data-P-g-2-KL-P-g-mid-frac-P-data-P-g-2"><a href="#V-G-log4-KL-P-data-mid-frac-P-data-P-g-2-KL-P-g-mid-frac-P-data-P-g-2" class="headerlink" title="$V(G)=-log4+KL(P_{data} \mid \frac{P_{data}+P_g}{2})+KL(P_g \mid  \frac{P_{data}+P_g}{2} )$"></a>$V(G)=-log4+KL(P_{data} \mid \frac{P_{data}+P_g}{2})+KL(P_g \mid \frac{P_{data}+P_g}{2} )$</h4><p>因为：KL散度永远大于等于0，所以可以知道目标函数最终最优值为-log4。</p><h3 id="GAN的特性"><a href="#GAN的特性" class="headerlink" title="GAN的特性"></a>GAN的特性</h3><p>优点：</p><ul><li>模型优化只用到了反向传播，而不需要马尔科夫链。</li><li>训练时不需要对隐变量做推断。</li><li>理论上，只要是可微分函数都能用于构建生成模型G和判别模型D，因而能够与深度神经网络结合–&gt;深度产生式模型。</li><li>生成模型G的参数更新不是直接来自于数据样本，而是使用来自判别模型D的反向传播梯度。</li></ul><p>缺点：</p><ul><li>可解释性差，生成模型的分布没有显示的表达。它只是一个黑盒子一样的映射函数：输入是一个随机变量，输出是我们想要的一个数据分布。</li><li>比较难训练，生成模型D和判别模型G之间需要很好的同步。例如，在实际中我们常常需要 D 更新 K次， G 才能更新 1 次，如果没有很好地平衡这两个部件的优化，那么G最后就极大可能会坍缩到一个鞍点。</li></ul><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow实现简单的递归神经网络-RNN</title>
      <link href="/2018/08/31/2018-08-31-TensorFlow-RNN/"/>
      <url>/2018/08/31/2018-08-31-TensorFlow-RNN/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="递归神经网络实现MNIST手写数字识别"><a href="#递归神经网络实现MNIST手写数字识别" class="headerlink" title="递归神经网络实现MNIST手写数字识别"></a>递归神经网络实现MNIST手写数字识别</h3><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a><code>代码</code></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/8/31 13:00</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : RNN.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"># TODO: 0.导入环境</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import os</span><br><span class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos;</span><br><span class="line">print(&quot;Packages imported&quot;)</span><br><span class="line"></span><br><span class="line"># TODO: 1.数据准备</span><br><span class="line">mnist = input_data.read_data_sets(&quot;data/&quot;, one_hot=True)</span><br><span class="line"></span><br><span class="line"># TODO: 2.数据处理</span><br><span class="line">x_train, y_train = mnist.train.images, mnist.train.labels</span><br><span class="line">x_test, y_test = mnist.test.images, mnist.test.labels</span><br><span class="line"></span><br><span class="line">train_number = x_train.shape[0]</span><br><span class="line">test_number = x_test.shape[0]</span><br><span class="line">dim = y_train.shape[1]</span><br><span class="line">classes_number = y_test.shape[1]</span><br><span class="line">print(&quot;MNIST LOADED&quot;)</span><br><span class="line"></span><br><span class="line"># TODO: 初始化权重参数</span><br><span class="line">input_dim = 28</span><br><span class="line">hidden_dim = 128</span><br><span class="line">output_dim = classes_number</span><br><span class="line">steps = 28</span><br><span class="line"></span><br><span class="line">weights = &#123;</span><br><span class="line">    &apos;hidden&apos;: tf.Variable(tf.random_normal([input_dim, hidden_dim])),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([hidden_dim, output_dim]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">biases = &#123;</span><br><span class="line">    &apos;hidden&apos;: tf.Variable(tf.random_normal([hidden_dim])),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([output_dim]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># TODO: 构建网络计算图</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def _RNN(_x, _w, _b, _step, _name):</span><br><span class="line">    # 1.把输入进行转换</span><br><span class="line">    # [batchsize, steps, input_dim]==&gt;[steps, batchsize, input_dim]</span><br><span class="line">    _x = tf.transpose(_x, [1, 0, 2])</span><br><span class="line">    # [steps, batchsize, input_dim]==&gt;[steps*batchsize, input_dim]</span><br><span class="line">    _x = tf.reshape(_x, [-1, input_dim])</span><br><span class="line"></span><br><span class="line">    # 2. input layer ==&gt; hidden layer</span><br><span class="line">    hidden_layer = tf.add(tf.matmul(_x, _w[&apos;hidden&apos;]), _b[&apos;hidden&apos;])</span><br><span class="line">    # 把数据进行分割</span><br><span class="line">    hidden_layer_data = tf.split(hidden_layer, _step, 0)</span><br><span class="line">    # LSTM</span><br><span class="line">    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_dim, forget_bias=0.8)</span><br><span class="line">    lstm_out, lstm_s = tf.nn.static_rnn(lstm_cell, hidden_layer_data, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    # 3. output</span><br><span class="line">    output = tf.add(tf.matmul(lstm_out[-1], _w[&apos;out&apos;]), _b[&apos;out&apos;])</span><br><span class="line"></span><br><span class="line">    return_data = &#123;</span><br><span class="line">        &apos;x&apos;: _x, &apos;hidden&apos;: hidden_layer, &apos;hidden_data&apos;: hidden_layer_data,</span><br><span class="line">        &apos;lstm_out&apos;: lstm_out, &apos;lstm_s&apos;: lstm_s, &apos;output&apos;: output</span><br><span class="line">    &#125;</span><br><span class="line">    return return_data</span><br><span class="line"></span><br><span class="line"># TODO: 5.计算损失值并初始化optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">learn_rate = 0.01</span><br><span class="line">x = tf.placeholder(tf.float32, [None, steps, input_dim])</span><br><span class="line">y = tf.placeholder(tf.float32, [None, output_dim])</span><br><span class="line">MyRNN = _RNN(x, weights, biases, steps, &apos;basic&apos;)</span><br><span class="line"># 预测值</span><br><span class="line">prediction = MyRNN[&apos;output&apos;]</span><br><span class="line"># 计算损失值</span><br><span class="line">cross = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))</span><br><span class="line"># 初始化优化器</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learn_rate).minimize(cross)</span><br><span class="line"># 模型评估</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1)), tf.float32))</span><br><span class="line"></span><br><span class="line">print(&quot;NETWORK READY!!&quot;)</span><br><span class="line"></span><br><span class="line"># TODO: 6.初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 7.模型训练</span><br><span class="line">print(&quot;Start optimization&quot;)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    training_epochs = 50</span><br><span class="line">    batch_size = 128</span><br><span class="line">    display_step = 10</span><br><span class="line">    for epoch in range(training_epochs):</span><br><span class="line">        avg_cost = 0.</span><br><span class="line">        total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            batch_xs = batch_xs.reshape((batch_size, steps, input_dim))</span><br><span class="line"></span><br><span class="line">            feeds = &#123;x: batch_xs, y: batch_ys&#125;</span><br><span class="line">            sess.run(optimizer, feed_dict=feeds)</span><br><span class="line"></span><br><span class="line">            avg_cost += sess.run(cross, feed_dict=feeds) / total_batch</span><br><span class="line"></span><br><span class="line">        if epoch % display_step == 0:</span><br><span class="line">            print(&quot;Epoch: %03d/%03d cost: %.9f&quot; % (epoch+1, training_epochs, avg_cost))</span><br><span class="line">            feeds = &#123;x: batch_xs, y: batch_ys&#125;</span><br><span class="line">            train_acc = sess.run(accuracy, feed_dict=feeds)</span><br><span class="line">            print(&quot; Training accuracy: %.3f&quot; % train_acc)</span><br><span class="line">            batch_x_test = mnist.test.images</span><br><span class="line">            batch_y_test = mnist.test.labels</span><br><span class="line">            batch_x_test = batch_x_test.reshape([-1, steps, input_dim])</span><br><span class="line">            test_acc = sess.run(accuracy, feed_dict=&#123;x: batch_x_test, y: batch_y_test&#125;)</span><br><span class="line">            print(&quot; Test accuracy: %.3f&quot; % test_acc)</span><br><span class="line"></span><br><span class="line">print(&quot;Optimization Finished.&quot;)</span><br></pre></td></tr></table></figure><h4 id="执行结果"><a href="#执行结果" class="headerlink" title="执行结果"></a><code>执行结果</code></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">NETWORK READY!!</span><br><span class="line">Start optimization</span><br><span class="line">Epoch: 001/050 cost: 2.033586812</span><br><span class="line"> Training accuracy: 0.422</span><br><span class="line"> Test accuracy: 0.511</span><br><span class="line">Epoch: 011/050 cost: 0.101458139</span><br><span class="line"> Training accuracy: 0.961</span><br><span class="line"> Test accuracy: 0.961</span><br><span class="line">Epoch: 021/050 cost: 0.096797398</span><br><span class="line"> Training accuracy: 0.977</span><br><span class="line"> Test accuracy: 0.956</span><br><span class="line">Epoch: 031/050 cost: 0.105833270</span><br><span class="line"> Training accuracy: 0.992</span><br><span class="line"> Test accuracy: 0.958</span><br><span class="line">Epoch: 041/050 cost: 0.117593214</span><br><span class="line"> Training accuracy: 0.977</span><br><span class="line"> Test accuracy: 0.959</span><br><span class="line">Optimization Finished.</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>递归神经网络之LSTM</title>
      <link href="/2018/08/30/2018-08-30-DL-RNN-LSTM/"/>
      <url>/2018/08/30/2018-08-30-DL-RNN-LSTM/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="LSTM模型"><a href="#LSTM模型" class="headerlink" title="LSTM模型"></a>LSTM模型</h3><p>由于RNN也有梯度消失的问题，因此很难处理长序列的数据，大牛们对RNN做了改进，得到了RNN的特例LSTM（Long Short-Term Memory），它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。</p><h3 id="从RNN到LSTM"><a href="#从RNN到LSTM" class="headerlink" title="从RNN到LSTM"></a>从RNN到LSTM</h3><p>在RNN模型中，我们总结了RNN具有如下结构：</p><p><img src="/images/dl/97.png" alt="images"></p><p>RNN的模型可以简化成如下图的形式，所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层：</p><p><img src="/images/dl/98.png" alt="images"></p><p>由于RNN梯度消失的问题，大牛们对于序列索引位置t的隐藏结构做了改进，可以说通过一些技巧让隐藏结构复杂了起来，来避免梯度消失的问题，这样的特殊RNN就是我们的LSTM。由于LSTM有很多的变种，这里我们以最常见的LSTM为例讲述。LSTM的结构如下图：</p><p><img src="/images/dl/99.png" alt="images"></p><p><img src="/images/dl/100.png" alt="images"></p><h3 id="LSTM模型结构剖析"><a href="#LSTM模型结构剖析" class="headerlink" title="LSTM模型结构剖析"></a>LSTM模型结构剖析</h3><p>上面我们给出了LSTM的模型结构，下面我们就一点点的剖析LSTM模型在每个序列索引位置t时刻的内部结构。</p><h3 id="LSTM关键–：“细胞状态”"><a href="#LSTM关键–：“细胞状态”" class="headerlink" title="LSTM关键–：“细胞状态”"></a>LSTM关键–：“细胞状态”</h3><p>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传会很容易保持不变。</p><p><img src="/images/dl/101.png" alt="images"></p><h4 id="LSTM控制“细胞状态”的方式："><a href="#LSTM控制“细胞状态”的方式：" class="headerlink" title="LSTM控制“细胞状态”的方式："></a><strong>LSTM</strong>控制“细胞状态”的方式：</h4><ul><li>通过“门”让信息选择性通过，来去除或者增加信息到细胞状态。</li><li>包含一个<code>SIGMOD</code>神经元层和一个<code>pointwise</code>乘法操作。</li><li><code>SIGMOD</code>层输出0到1之间的概率值，描述每个部分有多少量可以通过。0代表“不许任何量通过”，1就表示“允许任意量通过”。</li></ul><p><img src="/images/dl/102.png" alt="images"></p><p>除了细胞状态，LSTM图中还有了很多奇怪的结构，这些结构一般称之为门控结构(Gate)。</p><p>LSTM在在每个序列索引位置t的门一般包括<strong>遗忘门</strong>，<strong>输入门</strong>和<strong>输出门</strong>三种。</p><h3 id="LSTM之遗忘门"><a href="#LSTM之遗忘门" class="headerlink" title="LSTM之遗忘门"></a>LSTM之遗忘门</h3><p>遗忘门（forget gate）顾名思义，是控制是否遗忘的，在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。遗忘门子结构如下图所示：</p><p><img src="/images/dl/103.png" alt="images"></p><p>图中输入的有上一序列的隐藏状态$h(t-1)$和本序列数据$x_{(t)}$，通过一个激活函数，一般情况下是<code>SIGMOD</code>，得到遗忘门的输出$f(t)$。由于SIGMOD的输出$f(t)$在[0,1]之间，因此这里的输出$f(t)$代表了遗忘上一层隐藏细胞的概率。</p><p><strong>数学表达式</strong>：</p><h4 id="f-t-sigma-W-fh-t-1-U-fx-t-b-f"><a href="#f-t-sigma-W-fh-t-1-U-fx-t-b-f" class="headerlink" title="$f(t)=\sigma(W_fh(t-1)+U_fx(t)+b_f)$"></a>$f(t)=\sigma(W_fh(t-1)+U_fx(t)+b_f)$</h4><p>其中：$W_f、U_f、b_f$为线性关系的权重项和偏置项，$\sigma$为SIGMOD激活函数。</p><h3 id="LSTM之输入门"><a href="#LSTM之输入门" class="headerlink" title="LSTM之输入门"></a>LSTM之输入门</h3><p>输入门（input gate）负责处理当前序列位置的输入，它的子结构如下图：</p><p><img src="/images/dl/104.png" alt="images"></p><p>从图中可以看到输入门由两部分组成，第一部分使用了sigmoid激活函数，输出为$i_{(t)}$,第二部分使用了tanh激活函数，输出为$\tilde c_{(t)}$, 两者的结果后面会相乘再去更新细胞状态。</p><ul><li>SIGMOD层决定什么值需要更新。</li><li>Tanh层创建一个新的候选值向量$\tilde c_{(t)}$</li><li>第二步还是为状态更新做准备。</li></ul><p><strong>数学表达式</strong>：</p><h4 id="i-t-sigma-W-ih-t-1-U-ix-t-b-i"><a href="#i-t-sigma-W-ih-t-1-U-ix-t-b-i" class="headerlink" title="$i{(t)} = \sigma(W_ih{(t-1)} + U_ix^{(t)} + b_i)$"></a>$i{(t)} = \sigma(W_ih{(t-1)} + U_ix^{(t)} + b_i)$</h4><h4 id="tilde-c-t-tanh-W-ah-t-1-U-ax-t-b-a"><a href="#tilde-c-t-tanh-W-ah-t-1-U-ax-t-b-a" class="headerlink" title="$\tilde c{(t)} =tanh(W_ah{(t-1)} + U_ax^{(t)} + b_a)$"></a>$\tilde c{(t)} =tanh(W_ah{(t-1)} + U_ax^{(t)} + b_a)$</h4><p>其中$W_i, U_i, b_i, W_a, U_a, b_a$，为线性关系的权重项和偏置项，$\sigma$为SIGMOD激活函数。</p><h3 id="LSTM之更新“细胞状态”"><a href="#LSTM之更新“细胞状态”" class="headerlink" title="LSTM之更新“细胞状态”"></a>LSTM之更新“细胞状态”</h3><p>在研究LSTM输出门之前，我们要先看看LSTM之细胞状态。前面的遗忘门和输入门的结果都会作用于细胞状态 $C_{(t)}$，我们来看看细胞如何从$C_{(t-1)}$到$C_{(t)}$:</p><p><img src="/images/dl/105.png" alt="images"></p><p>由图可知：细胞状态$C_{(t)}$由两部分组成；第一部分是$C_{(t-1)}$和遗忘门输出$f(t)$的乘积，第二部分是输入门的$i_{(t)}$和$\tilde c_{(t)}$的乘积，总结为如下三点：</p><ul><li>更新$C_{(t-1)}$为$C_{(t)}$。</li><li>把就状态和$f(t)$相乘，丢弃掉我们确定需要丢弃的信息。</li><li>加上$i(t) * \tilde c_{(t)}$。最后得到新的候选值，根据我们决定更新每个状态的程度进行变化。</li></ul><p><strong>数学表达式</strong>：</p><h4 id="C-t-C-t-1-odot-f-t-i-t-odot-tilde-c-t"><a href="#C-t-C-t-1-odot-f-t-i-t-odot-tilde-c-t" class="headerlink" title="$C_{(t)} = C_{(t-1)} \odot f{(t)} + i_{(t)} \odot \tilde c_{(t)}$"></a>$C_{(t)} = C_{(t-1)} \odot f{(t)} + i_{(t)} \odot \tilde c_{(t)}$</h4><p>其中，$\bigodot$为Hadamard积.</p><h3 id="LSTM之输出门"><a href="#LSTM之输出门" class="headerlink" title="LSTM之输出门"></a>LSTM之输出门</h3><p>有了新的隐藏细胞状态$C_{(t)}$，我们就可以来看输出门了，子结构如下：</p><p><img src="/images/dl/106.png" alt="images"></p><p>从图中可以看出：隐藏状态$h(t)$的更新由两个部分组成：第一部分是$o_{(t)}$，它是由上一序列的隐藏状态$h_{(t-1)}$和本序列的$x_{(t)}$，以及激活函数SIGMOD得到的，第二部分是由隐藏状态$C_{(t)}$和$Tanh$激活函数组成，即：</p><ul><li>最开始先运行一个SIGMOD层来确定细胞状态的那个部分将输出。</li><li>接着用tanh处理细胞状态（得到一个-1到1之间的值），再讲它和SIGMOD门的输出相乘。输出我们确定输出的那部分值。</li></ul><p><strong>数学表达式</strong>：</p><h4 id="o-t-sigma-W-o-h-t-1-x-t-b-o"><a href="#o-t-sigma-W-o-h-t-1-x-t-b-o" class="headerlink" title="$o_t=\sigma(W_o[h_{t-1},x_t]+b_o)$"></a>$o_t=\sigma(W_o[h_{t-1},x_t]+b_o)$</h4><h4 id="h-t-o-t-tanh-C-t"><a href="#h-t-o-t-tanh-C-t" class="headerlink" title="$h_t=o_t*tanh(C_t)$"></a>$h_t=o_t*tanh(C_t)$</h4><h3 id="LSTM的变体"><a href="#LSTM的变体" class="headerlink" title="LSTM的变体"></a>LSTM的变体</h3><ul><li>增加<code>peephole connection</code></li><li>让门层也会接受细胞状态的输入。</li></ul><p><img src="/images/dl/107.png" alt="images"></p><p><strong>数学表达式</strong>：</p><h4 id="f-t-sigma-W-f-cdot-C-t-1-h-t-1-x-t-b-f"><a href="#f-t-sigma-W-f-cdot-C-t-1-h-t-1-x-t-b-f" class="headerlink" title="$f_t=\sigma(W_f \cdot[C_{t-1}, h_{t-1},x_t]+b_f)$"></a>$f_t=\sigma(W_f \cdot[C_{t-1}, h_{t-1},x_t]+b_f)$</h4><h4 id="i-t-sigma-W-i-cdot-C-t-1-h-t-1-x-t-b-i"><a href="#i-t-sigma-W-i-cdot-C-t-1-h-t-1-x-t-b-i" class="headerlink" title="$i_t=\sigma(W_i \cdot[C_{t-1}, h_{t-1},x_t]+b_i)$"></a>$i_t=\sigma(W_i \cdot[C_{t-1}, h_{t-1},x_t]+b_i)$</h4><h4 id="o-t-sigma-W-o-cdot-C-t-1-h-t-1-x-t-b-o"><a href="#o-t-sigma-W-o-cdot-C-t-1-h-t-1-x-t-b-o" class="headerlink" title="$o_t=\sigma(W_o \cdot[C_{t-1}, h_{t-1},x_t]+b_o)$"></a>$o_t=\sigma(W_o \cdot[C_{t-1}, h_{t-1},x_t]+b_o)$</h4><ul><li>通过使用<code>coupled</code>忘记和输入门</li><li>之前是分开确定需要忘记和添加的信息，然后一同做出决定。</li></ul><p><img src="/images/dl/108.png" alt="images"></p><p><strong>数学表达式</strong>：</p><h4 id="C-t-f-t-C-t-1-1-f-t-tilde-C-t"><a href="#C-t-f-t-C-t-1-1-f-t-tilde-C-t" class="headerlink" title="$C_t=f_t  C_{t-1}+(1-f_t)    \tilde C_t$"></a>$C_t=f_t <em>C_{t-1}+(1-f_t) </em>\tilde C_t$</h4><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><p><strong>Gatad Reacurrent Unit (GRU)</strong>，2014年提出。</p><ul><li>将忘记门和输入门合成了一个单一的<strong>更新门</strong></li><li>混合了细胞状态和隐藏状态</li><li>比标准的LSTM简单</li></ul><p><img src="/images/dl/109.png" alt="images"></p><p><strong>数学表达式</strong>：</p><h4 id="z-t-sigma-W-z-cdot-h-t-1-x-t"><a href="#z-t-sigma-W-z-cdot-h-t-1-x-t" class="headerlink" title="$z_t=\sigma(W_z \cdot [h_{t-1},x_t])$"></a>$z_t=\sigma(W_z \cdot [h_{t-1},x_t])$</h4><h4 id="r-t-sigma-W-r-cdot-h-t-1-x-t"><a href="#r-t-sigma-W-r-cdot-h-t-1-x-t" class="headerlink" title="$r_t=\sigma(W_r \cdot [h_{t-1},x_t])$"></a>$r_t=\sigma(W_r \cdot [h_{t-1},x_t])$</h4><h4 id="tilde-h-t-tanh-W-cdot-r-t-h-t-1-x-t"><a href="#tilde-h-t-tanh-W-cdot-r-t-h-t-1-x-t" class="headerlink" title="$\tilde h_t= tanh(W \cdot [r_t*h_{t-1},x_t])$"></a>$\tilde h_t= tanh(W \cdot [r_t*h_{t-1},x_t])$</h4><h4 id="h-t-1-z-t-h-t-1-z-t-tilde-h-t"><a href="#h-t-1-z-t-h-t-1-z-t-tilde-h-t" class="headerlink" title="$h_t=(1-z_t)   h_{t-1} + z_t    \tilde h_t$"></a>$h_t=(1-z_t) <em>h_{t-1} + z_t </em>\tilde h_t$</h4><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>递归神经网络之python实现RNN算法</title>
      <link href="/2018/08/29/2018-08-29-DL-RNN-hand/"/>
      <url>/2018/08/29/2018-08-29-DL-RNN-hand/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="PYTHON实现RNN算法"><a href="#PYTHON实现RNN算法" class="headerlink" title="PYTHON实现RNN算法"></a>PYTHON实现RNN算法</h3><h4 id="实现学习二进制的加法。"><a href="#实现学习二进制的加法。" class="headerlink" title="实现学习二进制的加法。"></a>实现学习二进制的加法。</h4><p><img src="/images/dl/90.png" alt="images"></p><p>如上图所示，我们来手写RNN算法：</p><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/8/29 13:51</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    :</span><br><span class="line"># @File    : demo.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"># TODO:导入环境</span><br><span class="line">import copy</span><br><span class="line">import numpy as np</span><br><span class="line">np.random.seed(0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 使用SIGMOD函数转换成非线性</span><br><span class="line">def sigmoid(inputs):</span><br><span class="line">    output = 1 / (1 + np.exp(-inputs))</span><br><span class="line">    return output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 将SIGMOD函数的输出转换成其导数</span><br><span class="line">def sigmoid_output_to_derivative(output):</span><br><span class="line">    return output * (1 - output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 生成训练数据集--&gt;把十进制转换为8位二进制</span><br><span class="line">int2binary = &#123;&#125;</span><br><span class="line">binary_dim = 8</span><br><span class="line"></span><br><span class="line">largest_number = pow(2, binary_dim)  # 2**8=256</span><br><span class="line"></span><br><span class="line"># 把0-256转换成对应的二进制编码并储存起来</span><br><span class="line">binary = np.unpackbits(</span><br><span class="line">    np.array([range(largest_number)], dtype=np.uint8).T, axis=1)</span><br><span class="line"></span><br><span class="line">for i in range(largest_number):</span><br><span class="line">    int2binary[i] = binary[i]</span><br><span class="line"></span><br><span class="line"># 参数定义</span><br><span class="line">alpha = 0.1</span><br><span class="line">input_dim = 2</span><br><span class="line">hidden_dim = 16</span><br><span class="line">output_dim = 1</span><br><span class="line"></span><br><span class="line"># 初始化权重参数--在-1到1之间</span><br><span class="line">U = 2 * np.random.random((input_dim, hidden_dim)) - 1</span><br><span class="line">V = 2 * np.random.random((hidden_dim, output_dim)) - 1</span><br><span class="line">W = 2 * np.random.random((hidden_dim, hidden_dim)) - 1</span><br><span class="line"></span><br><span class="line"># 更新权重参数</span><br><span class="line">U_update = np.zeros_like(U)</span><br><span class="line">V_update = np.zeros_like(V)</span><br><span class="line">W_update = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line"># TODO:训练</span><br><span class="line">for step in range(10000):</span><br><span class="line"></span><br><span class="line">    # 随机生成一个加法a+b=c（因为最大是8位，所以a最大是largest_number的1/2）</span><br><span class="line">    a_int = np.random.randint(largest_number / 2)  # int version</span><br><span class="line">    a = int2binary[a_int]  # binary encoding</span><br><span class="line"></span><br><span class="line">    b_int = np.random.randint(largest_number / 2)  # int version</span><br><span class="line">    b = int2binary[b_int]  # binary encoding</span><br><span class="line"></span><br><span class="line">    # true answer</span><br><span class="line">    c_int = a_int + b_int</span><br><span class="line">    c = int2binary[c_int]</span><br><span class="line"></span><br><span class="line">    # 存储概率最大的二进制--预测值</span><br><span class="line">    d = np.zeros_like(c)</span><br><span class="line"></span><br><span class="line">    overallError = 0</span><br><span class="line"></span><br><span class="line">    out_deltas = list()</span><br><span class="line">    layer_1_values = list()  # 保存’记忆‘</span><br><span class="line">    layer_1_values.append(np.zeros(hidden_dim))</span><br><span class="line"></span><br><span class="line">    # 沿着二进制的位置移动--前向传播</span><br><span class="line">    for position in range(binary_dim):</span><br><span class="line">        # 生成输入和输出的值</span><br><span class="line">        X = np.array([[a[binary_dim - position - 1], b[binary_dim - position - 1]]])</span><br><span class="line">        y = np.array([c[binary_dim - position - 1]]).T</span><br><span class="line"></span><br><span class="line">        # hidden layer --&gt; UX+layer1·W</span><br><span class="line">        layer_1 = sigmoid(np.dot(X, U) + np.dot(layer_1_values[-1], W))</span><br><span class="line"></span><br><span class="line">        # output layer</span><br><span class="line">        layer_2 = sigmoid(np.dot(layer_1, V))</span><br><span class="line"></span><br><span class="line">        # 计算损失值</span><br><span class="line">        layer_2_error = y - layer_2</span><br><span class="line">        out_deltas.append(layer_2_error * sigmoid_output_to_derivative(layer_2))</span><br><span class="line">        overallError += np.abs(layer_2_error[0])</span><br><span class="line"></span><br><span class="line">        # 解码成十进制</span><br><span class="line">        d[binary_dim - position - 1] = np.round(layer_2[0][0])</span><br><span class="line"></span><br><span class="line">        # 保存记忆，以便下一时刻使用</span><br><span class="line">        layer_1_values.append(copy.deepcopy(layer_1))</span><br><span class="line"></span><br><span class="line">    future_layer_1_delta = np.zeros(hidden_dim)</span><br><span class="line"></span><br><span class="line">    # 反向传播</span><br><span class="line">    for position in range(binary_dim):</span><br><span class="line">        X = np.array([[a[position], b[position]]])</span><br><span class="line">        layer_1 = layer_1_values[-position - 1]</span><br><span class="line">        prev_layer_1 = layer_1_values[-position - 2]</span><br><span class="line"></span><br><span class="line">        # error at output layer</span><br><span class="line">        layer_2_delta = out_deltas[-position - 1]</span><br><span class="line">        # error at hidden layer</span><br><span class="line">        layer_1_delta = (future_layer_1_delta.dot(W.T) + layer_2_delta.dot(</span><br><span class="line">            V.T)) * sigmoid_output_to_derivative(layer_1)</span><br><span class="line"></span><br><span class="line">        # 更新权重参数</span><br><span class="line">        V_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)</span><br><span class="line">        W_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)</span><br><span class="line">        U_update += X.T.dot(layer_1_delta)</span><br><span class="line"></span><br><span class="line">        future_layer_1_delta = layer_1_delta</span><br><span class="line"></span><br><span class="line">    U += U_update * alpha</span><br><span class="line">    V += V_update * alpha</span><br><span class="line">    W += W_update * alpha</span><br><span class="line"></span><br><span class="line">    U_update *= 0</span><br><span class="line">    V_update *= 0</span><br><span class="line">    W_update *= 0</span><br><span class="line"></span><br><span class="line">    # print out progress</span><br><span class="line">    if step % 1000 == 0:</span><br><span class="line">        print(&quot;错误值:&quot; + str(overallError))</span><br><span class="line">        print(&quot;预测值:&quot; + str(d))</span><br><span class="line">        print(&quot;正确值:&quot; + str(c))</span><br><span class="line">        out = 0</span><br><span class="line">        for index, x in enumerate(reversed(d)):</span><br><span class="line">            out += x * pow(2, index)</span><br><span class="line">        print(str(a_int) + &quot; + &quot; + str(b_int) + &quot; = &quot; + str(out))</span><br><span class="line">        print(&quot;------------&quot;)</span><br></pre></td></tr></table></figure><h4 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">损失值:[3.45638663]</span><br><span class="line">预测值:[0 0 0 0 0 0 0 1]</span><br><span class="line">正确值:[0 1 0 0 0 1 0 1]</span><br><span class="line">9 + 60 = 1</span><br><span class="line">------------</span><br><span class="line">损失值:[3.63389116]</span><br><span class="line">预测值:[1 1 1 1 1 1 1 1]</span><br><span class="line">正确值:[0 0 1 1 1 1 1 1]</span><br><span class="line">28 + 35 = 255</span><br><span class="line">------------</span><br><span class="line">损失值:[3.91366595]</span><br><span class="line">预测值:[0 1 0 0 1 0 0 0]</span><br><span class="line">正确值:[1 0 1 0 0 0 0 0]</span><br><span class="line">116 + 44 = 72</span><br><span class="line">------------</span><br><span class="line">损失值:[3.72191702]</span><br><span class="line">预测值:[1 1 0 1 1 1 1 1]</span><br><span class="line">正确值:[0 1 0 0 1 1 0 1]</span><br><span class="line">4 + 73 = 223</span><br><span class="line">------------</span><br><span class="line">损失值:[3.5852713]</span><br><span class="line">预测值:[0 0 0 0 1 0 0 0]</span><br><span class="line">正确值:[0 1 0 1 0 0 1 0]</span><br><span class="line">71 + 11 = 8</span><br><span class="line">------------</span><br><span class="line">损失值:[2.53352328]</span><br><span class="line">预测值:[1 0 1 0 0 0 1 0]</span><br><span class="line">正确值:[1 1 0 0 0 0 1 0]</span><br><span class="line">81 + 113 = 162</span><br><span class="line">------------</span><br><span class="line">损失值:[0.57691441]</span><br><span class="line">预测值:[0 1 0 1 0 0 0 1]</span><br><span class="line">正确值:[0 1 0 1 0 0 0 1]</span><br><span class="line">81 + 0 = 81</span><br><span class="line">------------</span><br><span class="line">损失值:[1.42589952]</span><br><span class="line">预测值:[1 0 0 0 0 0 0 1]</span><br><span class="line">正确值:[1 0 0 0 0 0 0 1]</span><br><span class="line">4 + 125 = 129</span><br><span class="line">------------</span><br><span class="line">损失值:[0.47477457]</span><br><span class="line">预测值:[0 0 1 1 1 0 0 0]</span><br><span class="line">正确值:[0 0 1 1 1 0 0 0]</span><br><span class="line">39 + 17 = 56</span><br><span class="line">------------</span><br><span class="line">损失值:[0.21595037]</span><br><span class="line">预测值:[0 0 0 0 1 1 1 0]</span><br><span class="line">正确值:[0 0 0 0 1 1 1 0]</span><br><span class="line">11 + 3 = 14</span><br><span class="line">------------</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>递归神经网络之BackPropagation Through Time</title>
      <link href="/2018/08/29/2018-08-29-DL-RNN-BPTT/"/>
      <url>/2018/08/29/2018-08-29-DL-RNN-BPTT/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="循环神经网络的训练算法：BPTT"><a href="#循环神经网络的训练算法：BPTT" class="headerlink" title="循环神经网络的训练算法：BPTT"></a>循环神经网络的训练算法：BPTT</h3><p><strong>BackPropagation Through Time (BPTT)</strong>经常出现用于学习递归神经网络（RNN）。</p><p>与前馈神经网络相比，RNN的特点是可以处理过去很长的信息。因此特别适合顺序模型。</p><p>BPTT扩展了普通的BP算法来适应递归神经网络。</p><p>BPTT算法是针对<strong>循环层</strong>的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：</p><ol><li>前向计算每个神经元的输出值；</li><li>反向计算每个神经元的<strong>误差项</strong>$E$值，它是误差函数<code>E</code>对神经元的<strong>加权输入</strong>的偏导数；</li><li>计算每个权重的梯度。</li></ol><p>最后再用<strong>随机梯度下降</strong>算法更新权重。</p><h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><p>网络结构：</p><p><img src="/images/dl/90.png" alt="images"></p><p>由图可知：</p><ul><li><h4 id="INPUT-LAYER-gt-HIDDEN-LAYER：-S-t-tanh-U-cdot-X-t-W-cdot-S-t-1"><a href="#INPUT-LAYER-gt-HIDDEN-LAYER：-S-t-tanh-U-cdot-X-t-W-cdot-S-t-1" class="headerlink" title="INPUT LAYER --&gt;HIDDEN LAYER：$S_t=tanh(U \cdot X_t+W \cdot S_{t-1}) $"></a><code>INPUT LAYER --&gt;HIDDEN LAYER</code>：$S_t=tanh(U \cdot X_t+W \cdot S_{t-1}) $</h4></li><li><h4 id="HIDDEN-LAYER-gt-OUTPUT-LAYER：-y-t-softmax-V-cdot-S-t"><a href="#HIDDEN-LAYER-gt-OUTPUT-LAYER：-y-t-softmax-V-cdot-S-t" class="headerlink" title="HIDDEN LAYER --&gt; OUTPUT LAYER：$y_t = softmax(V \cdot S_t)$"></a><code>HIDDEN LAYER --&gt; OUTPUT LAYER</code>：$y_t = softmax(V \cdot S_t)$</h4></li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>我们将损失或者误差定义为<strong>交叉熵损失</strong>：</p><p>$E_t(y_t,\widehat y_t) = -y_t \cdot log\widehat y_t$</p><h4 id="E-t-y-t-widehat-y-t-y-t-cdot-log-widehat-y-t"><a href="#E-t-y-t-widehat-y-t-y-t-cdot-log-widehat-y-t" class="headerlink" title="$E_t(y_t,\widehat y_t) = -y_t \cdot log\widehat y_t$"></a>$E_t(y_t,\widehat y_t) = -y_t \cdot log\widehat y_t$</h4><h4 id="E-y-t-widehat-y-t-sum-t-y-t-cdot-log-widehat-y-t"><a href="#E-y-t-widehat-y-t-sum-t-y-t-cdot-log-widehat-y-t" class="headerlink" title="$E(y_t,\widehat y_t) = -\sum_t y_t \cdot log\widehat y_t$"></a>$E(y_t,\widehat y_t) = -\sum_t y_t \cdot log\widehat y_t$</h4><p><code>注意</code>:</p><ul><li>$y_t$：是<code>t</code>时刻的正确值</li><li>$\widehat y_t$：是我们的预测值</li><li>总误差是每个时刻的误差之和</li></ul><h3 id="反向计算"><a href="#反向计算" class="headerlink" title="反向计算"></a>反向计算</h3><p>循环层如下图所示：</p><p><img src="/images/dl/95.png" alt="images"></p><p>接下来，我们就需要使用链式法则进行反向梯度的参数更新：</p><ul><li><h4 id="bigtriangleup-U-frac-partial-E-partial-U-sum-t-frac-partial-E-t-partial-U"><a href="#bigtriangleup-U-frac-partial-E-partial-U-sum-t-frac-partial-E-t-partial-U" class="headerlink" title="$\bigtriangleup U=\frac{ \partial E}{\partial U} = \sum_t \frac{\partial E_t}{\partial U}$"></a>$\bigtriangleup U=\frac{ \partial E}{\partial U} = \sum_t \frac{\partial E_t}{\partial U}$</h4></li><li><h4 id="bigtriangleup-W-frac-partial-E-partial-W-sum-t-frac-partial-E-t-partial-W"><a href="#bigtriangleup-W-frac-partial-E-partial-W-sum-t-frac-partial-E-t-partial-W" class="headerlink" title="$\bigtriangleup W=\frac{ \partial E}{\partial W} = \sum_t \frac{\partial E_t}{\partial W}$"></a>$\bigtriangleup W=\frac{ \partial E}{\partial W} = \sum_t \frac{\partial E_t}{\partial W}$</h4></li><li><h4 id="bigtriangleup-V-frac-partial-E-partial-V-sum-t-frac-partial-E-t-partial-V"><a href="#bigtriangleup-V-frac-partial-E-partial-V-sum-t-frac-partial-E-t-partial-V" class="headerlink" title="$\bigtriangleup V=\frac{ \partial E}{\partial V} = \sum_t \frac{\partial E_t}{\partial V}$"></a>$\bigtriangleup V=\frac{ \partial E}{\partial V} = \sum_t \frac{\partial E_t}{\partial V}$</h4></li></ul><p>我们令$t=3$为栗子：</p><h4 id="bigtriangleup-V-frac-partial-E-3-partial-V-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-net-3-cdot-frac-partial-net-3-partial-V-widehat-y-3-y-3-otimes-S-3"><a href="#bigtriangleup-V-frac-partial-E-3-partial-V-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-net-3-cdot-frac-partial-net-3-partial-V-widehat-y-3-y-3-otimes-S-3" class="headerlink" title="$\bigtriangleup V=\frac{ \partial E_3}{\partial V} = \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial net_3} \cdot \frac{\partial net_3}{\partial V} = (\widehat y_3 - y_3) \otimes S_3$"></a>$\bigtriangleup V=\frac{ \partial E_3}{\partial V} = \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial net_3} \cdot \frac{\partial net_3}{\partial V} = (\widehat y_3 - y_3) \otimes S_3$</h4><p><code>注意</code>：$net_3=V \cdot S_3$，$\otimes$是外积，V只和当前的时间有关，所以计算非常简单。</p><h4 id="bigtriangleup-W-frac-partial-E-3-partial-W-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-W"><a href="#bigtriangleup-W-frac-partial-E-3-partial-W-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-W" class="headerlink" title="$\bigtriangleup W=\frac{ \partial E_3}{\partial W} = \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial W} $"></a>$\bigtriangleup W=\frac{ \partial E_3}{\partial W} = \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial W} $</h4><p><code>因为</code>：</p><h4 id="S-3-tanh-U-cdot-X-t-W-cdot-S-2-，取决于-S-2-，而-S-2-又依赖于-W、S-1-。"><a href="#S-3-tanh-U-cdot-X-t-W-cdot-S-2-，取决于-S-2-，而-S-2-又依赖于-W、S-1-。" class="headerlink" title="$S_3 = tanh(U \cdot X_t+W \cdot S_2)$，取决于$S_2$，而$S_2$又依赖于$W、S_1$。"></a>$S_3 = tanh(U \cdot X_t+W \cdot S_2)$，取决于$S_2$，而$S_2$又依赖于$W、S_1$。</h4><p><code>所以</code>:</p><h4 id="bigtriangleup-W-frac-partial-E-3-partial-W-sum-k-0-3-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-S-k-cdot-frac-partial-S-k-partial-net-k-cdot-frac-partial-net-k-partial-W"><a href="#bigtriangleup-W-frac-partial-E-3-partial-W-sum-k-0-3-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-S-k-cdot-frac-partial-S-k-partial-net-k-cdot-frac-partial-net-k-partial-W" class="headerlink" title="$\bigtriangleup W=\frac{ \partial E_3}{\partial W} =\sum_{k=0}^3 \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial S_k} \cdot \frac{\partial S_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial W} $"></a>$\bigtriangleup W=\frac{ \partial E_3}{\partial W} =\sum_{k=0}^3 \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial S_k} \cdot \frac{\partial S_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial W} $</h4><p><code>总结</code>：W 在每一步中都有使用，所以我们需要$t=3$通过网络反向传播到$t=0$：</p><p><img src="/images/dl/96.png" alt="images"></p><p><code>注意</code>：这种方式是和我们前面所推导的<a href="https://sevenold.github.io/2018/08/DL-back-propagation/" target="_blank" rel="noopener">深度神经网络的反向传播算法</a>和<a href="https://sevenold.github.io/2018/08/CNN-back-propagation/" target="_blank" rel="noopener">卷积神经网络的反向传播算法</a>是完全相同的。关键的区别就是我们总结了W的每个时刻的渐变，在传统的神经网络中，我们不跨层共享参数，因此我们不需要总结任何东西。</p><h4 id="bigtriangleup-U-frac-partial-E-3-partial-W-sum-k-0-3-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-S-k-cdot-frac-partial-S-k-partial-net-k-cdot-frac-partial-net-k-partial-U"><a href="#bigtriangleup-U-frac-partial-E-3-partial-W-sum-k-0-3-frac-partial-E-3-partial-widehat-y-3-cdot-frac-partial-widehat-y-3-partial-S-3-cdot-frac-partial-S-3-partial-S-k-cdot-frac-partial-S-k-partial-net-k-cdot-frac-partial-net-k-partial-U" class="headerlink" title="$\bigtriangleup U=\frac{ \partial E_3}{\partial W} =\sum_{k=0}^3 \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial S_k} \cdot \frac{\partial S_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial U} $"></a>$\bigtriangleup U=\frac{ \partial E_3}{\partial W} =\sum_{k=0}^3 \frac{\partial E_3}{\partial \widehat y_3} \cdot \frac{\partial \widehat y_3}{\partial S_3} \cdot \frac{\partial S_3}{\partial S_k} \cdot \frac{\partial S_k}{\partial net_k} \cdot \frac{\partial net_k}{\partial U} $</h4><p><code>总结</code>：U参数W参数的传递过程基本一致。</p><h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><ul><li><h4 id="U-new-U-old-eta-bigtriangleup-U"><a href="#U-new-U-old-eta-bigtriangleup-U" class="headerlink" title="$U_{new}=U_{old}-\eta \bigtriangleup U$"></a>$U_{new}=U_{old}-\eta \bigtriangleup U$</h4></li><li><h4 id="V-new-V-old-eta-bigtriangleup-V"><a href="#V-new-V-old-eta-bigtriangleup-V" class="headerlink" title="$V_{new}=V_{old}-\eta \bigtriangleup V$"></a>$V_{new}=V_{old}-\eta \bigtriangleup V$</h4></li><li><h4 id="W-new-W-old-eta-bigtriangleup-W"><a href="#W-new-W-old-eta-bigtriangleup-W" class="headerlink" title="$W_{new}=W_{old}-\eta \bigtriangleup W$"></a>$W_{new}=W_{old}-\eta \bigtriangleup W$</h4></li></ul><p>$\eta​$：学习率</p><h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>RNN在训练中很容易发生<strong>梯度爆炸</strong>和<strong>梯度消失</strong>，这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。</p><p>通常来说，<strong>梯度爆炸</strong>更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。</p><p><strong>梯度消失</strong>更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：</p><ol><li>合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。</li><li>使用relu代替sigmoid和tanh作为激活函数。</li><li>使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。</li></ol><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow-slim之CNN实现手写数字识别</title>
      <link href="/2018/08/27/2018-08-27-TensorFlow-slim/"/>
      <url>/2018/08/27/2018-08-27-TensorFlow-slim/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="TensorFlow四种写法之三：Slim"><a href="#TensorFlow四种写法之三：Slim" class="headerlink" title="TensorFlow四种写法之三：Slim"></a>TensorFlow四种写法之三：Slim</h3><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/8/23 19:40</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : CNN-slim.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"># 0.导入环境</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos;</span><br><span class="line"></span><br><span class="line"># 1.数据准备</span><br><span class="line"></span><br><span class="line"># 使用tensorflow自带的工具加载MNIST手写数字集合</span><br><span class="line">mnist = input_data.read_data_sets(&apos;data&apos;, one_hot=True)</span><br><span class="line"># 查看数据的维度和target的维度</span><br><span class="line">print(mnist.train.images.shape)</span><br><span class="line">print(mnist.train.labels.shape)</span><br><span class="line"></span><br><span class="line"># 2.准备好palceholder</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784])</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10])</span><br><span class="line">learnRate = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"># 3.构建网络计算图结构</span><br><span class="line"></span><br><span class="line"># 把输入数据reshape--28x28=784, 单通道， -1表示None</span><br><span class="line">with tf.name_scope(&apos;reshape&apos;):</span><br><span class="line">    x_image = tf.reshape(x, [-1, 28, 28, 1])</span><br><span class="line">    print(x_image.shape)</span><br><span class="line"></span><br><span class="line"># 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5</span><br><span class="line">with tf.name_scope(&apos;conv1&apos;):</span><br><span class="line">    h_conv1 = tf.contrib.slim.conv2d(x_image, 32, [5, 5],</span><br><span class="line">                                     padding=&apos;SAME&apos;,</span><br><span class="line">                                     activation_fn=tf.nn.relu)</span><br><span class="line">    print(h_conv1.shape)</span><br><span class="line"># 构建池化层--采用最大池化</span><br><span class="line">with tf.name_scope(&apos;pool1&apos;):</span><br><span class="line">    h_pool1 = tf.contrib.slim.max_pool2d(h_conv1, [2, 2], stride=2, padding=&apos;VALID&apos;)</span><br><span class="line">    print(h_pool1.shape)</span><br><span class="line"># 构建第二层卷积计算层--maps 32 feature maps to 64.</span><br><span class="line">with tf.name_scope(&apos;conv2&apos;):</span><br><span class="line">    h_conv2 = tf.contrib.slim.conv2d(h_pool1, 64, [5, 5],</span><br><span class="line">                                     padding=&apos;SAME&apos;,</span><br><span class="line">                                     activation_fn=tf.nn.relu)</span><br><span class="line">    print(h_conv2.shape)</span><br><span class="line"># 构建第二个池化层</span><br><span class="line">with tf.name_scope(&apos;pool2&apos;):</span><br><span class="line">    h_pool2 = tf.contrib.slim.max_pool2d(h_conv2, [2, 2], stride=[2, 2], padding=&apos;VALID&apos;)</span><br><span class="line">    print(h_pool2.shape)</span><br><span class="line"># 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征</span><br><span class="line">with tf.name_scope(&apos;fc1&apos;):</span><br><span class="line">    h_pool2_flat = tf.contrib.slim.avg_pool2d(h_pool2, h_pool2.shape[1:3],</span><br><span class="line">                                              stride=[1, 1], padding=&apos;VALID&apos;)</span><br><span class="line">    h_fc1 = tf.contrib.slim.conv2d(h_pool2_flat, 1024, [1, 1],</span><br><span class="line">                                   activation_fn=tf.nn.relu)</span><br><span class="line">    print(h_fc1.shape)</span><br><span class="line"># Dropout--防止过拟合</span><br><span class="line">with tf.name_scope(&apos;dropout&apos;):</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob=keep_prob)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 构建全连接层--将1024个特性映射到10个类，每个类对应一个数字</span><br><span class="line">with tf.name_scope(&apos;fc2&apos;):</span><br><span class="line">    out = tf.squeeze(tf.contrib.slim.conv2d(h_fc1_drop, 10, [1, 1], activation_fn=None))</span><br><span class="line">    print(out.shape)</span><br><span class="line"></span><br><span class="line"># 4.计算损失值并初始化optimizer</span><br><span class="line">cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))</span><br><span class="line"></span><br><span class="line">l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])</span><br><span class="line"></span><br><span class="line">total_loss = cross_entropy + 7e-5*l2_loss</span><br><span class="line"></span><br><span class="line">train_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss)</span><br><span class="line"># 5.初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"># 6.在会话中执行网络定义的运算</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    for step in range(3000):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(100)</span><br><span class="line">        lr = 0.01</span><br><span class="line"></span><br><span class="line">        _, loss, l2_loss_value, total_loss_value = sess.run(</span><br><span class="line">            [train_step, cross_entropy, l2_loss, total_loss],</span><br><span class="line">            feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr, keep_prob: 0.5&#125;)</span><br><span class="line"></span><br><span class="line">        if (step+1) % 100 == 0:</span><br><span class="line">            print(&quot;step %d, entropy loss: %f, l2_loss: %f, total loss: %f&quot; %</span><br><span class="line">                  (step+1, loss, l2_loss_value, total_loss_value))</span><br><span class="line"></span><br><span class="line">            # 验证训练的模型</span><br><span class="line">            correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))</span><br><span class="line">            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">            print(&quot;Train accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob:0.5&#125;))</span><br><span class="line"></span><br><span class="line">        if (step + 1) % 1000 == 0:</span><br><span class="line">            print(&quot;Text accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: 0.5&#125;))</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow-原生写法之CNN实现手写数字识别</title>
      <link href="/2018/08/27/2018-08-27-TensorFlow-native/"/>
      <url>/2018/08/27/2018-08-27-TensorFlow-native/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="TensorFlow四种写法之一：原生写法"><a href="#TensorFlow四种写法之一：原生写法" class="headerlink" title="TensorFlow四种写法之一：原生写法"></a>TensorFlow四种写法之一：原生写法</h3><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/8/23 16:49</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : CNN-native.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"># 0.导入环境</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos;</span><br><span class="line"></span><br><span class="line"># 1.数据准备</span><br><span class="line"></span><br><span class="line"># 使用tensorflow自带的工具加载MNIST手写数字集合</span><br><span class="line">mnist = input_data.read_data_sets(&apos;data&apos;, one_hot=True)</span><br><span class="line"># 查看数据的维度和target的维度</span><br><span class="line">print(mnist.train.images.shape)</span><br><span class="line">print(mnist.train.labels.shape)</span><br><span class="line"></span><br><span class="line"># 2.准备好palceholder</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784])</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10])</span><br><span class="line">learnRate = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"># 3.构建网络计算图结构</span><br><span class="line"></span><br><span class="line"># 把输入数据reshape--28x28=784, 单通道， -1表示None</span><br><span class="line">with tf.name_scope(&apos;reshape&apos;):</span><br><span class="line">    x_image = tf.reshape(x, [-1, 28, 28, 1])</span><br><span class="line"></span><br><span class="line"># 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5</span><br><span class="line">with tf.name_scope(&apos;conv1&apos;):</span><br><span class="line">    shape = [5, 5, 1, 32]</span><br><span class="line">    W_conv1 = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1),</span><br><span class="line">                          collections=[tf.GraphKeys.GLOBAL_VARIABLES, &apos;WEIGHTS&apos;])</span><br><span class="line"></span><br><span class="line">    shape = [32]</span><br><span class="line">    b_conv1 = tf.Variable(tf.constant(0.1, shape=shape))</span><br><span class="line"></span><br><span class="line">    net_conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1],</span><br><span class="line">                             padding=&apos;SAME&apos;) + b_conv1</span><br><span class="line"></span><br><span class="line">    out_conv1 = tf.nn.relu(net_conv1)</span><br><span class="line"></span><br><span class="line"># 构建池化层--采用最大池化</span><br><span class="line">with tf.name_scope(&apos;pool1&apos;):</span><br><span class="line">    h_pool1 = tf.nn.max_pool(out_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],</span><br><span class="line">                             padding=&apos;VALID&apos;)</span><br><span class="line"></span><br><span class="line"># 构建第二层卷积计算层--maps 32 feature maps to 64.</span><br><span class="line">with tf.name_scope(&apos;conv2&apos;):</span><br><span class="line">    shape = [5, 5, 32, 64]</span><br><span class="line">    W_conv2 = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1),</span><br><span class="line">                          collections=[tf.GraphKeys.GLOBAL_VARIABLES, &apos;WEIGHTS&apos;])</span><br><span class="line"></span><br><span class="line">    shape = [64]</span><br><span class="line">    b_conv2 = tf.Variable(tf.constant(0.1, shape=shape))</span><br><span class="line"></span><br><span class="line">    net_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1],</span><br><span class="line">                             padding=&apos;SAME&apos;) + b_conv2</span><br><span class="line"></span><br><span class="line">    out_conv2 = tf.nn.relu(net_conv2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 构建第二层池化层--采用最大池化</span><br><span class="line">with tf.name_scope(&apos;pool2&apos;):</span><br><span class="line">    h_pool2 = tf.nn.max_pool(out_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],</span><br><span class="line">                             padding=&apos;VALID&apos;)</span><br><span class="line"></span><br><span class="line"># 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征</span><br><span class="line">with tf.name_scope(&apos;fc1&apos;):</span><br><span class="line">    shape = [7*7*64, 1024]</span><br><span class="line">    W_fc1 = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1),</span><br><span class="line">                        collections=[tf.GraphKeys.GLOBAL_VARIABLES, &apos;WEIGHTS&apos;])</span><br><span class="line"></span><br><span class="line">    shape = [1024]</span><br><span class="line">    b_fc1 = tf.Variable(tf.constant(0.1, shape=shape))</span><br><span class="line"></span><br><span class="line">    shape = [-1, 7*7*64]</span><br><span class="line">    h_pool2_flat = tf.reshape(h_pool2, shape)</span><br><span class="line"></span><br><span class="line">    out_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"># Dropout--防止过拟合</span><br><span class="line">with tf.name_scope(&apos;dropout&apos;):</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">    out_fc1_drop = tf.nn.dropout(out_fc1, keep_prob=keep_prob)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 构建第二层全连接层--将1024个特性映射到10个类，每个类对应一个数字</span><br><span class="line">with tf.name_scope(&apos;fc2&apos;):</span><br><span class="line">    shape = [1024, 10]</span><br><span class="line">    W_fc2 = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1),</span><br><span class="line">                        collections=[tf.GraphKeys.GLOBAL_VARIABLES, &apos;WEIGHTS&apos;])</span><br><span class="line"></span><br><span class="line">    shape = [10]</span><br><span class="line">    b_fc2 = tf.Variable(tf.constant(0.1, shape=shape))</span><br><span class="line"></span><br><span class="line">    out = tf.matmul(out_fc1_drop, W_fc2) + b_fc2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 4.计算损失值并初始化optimizer</span><br><span class="line">print(y.shape, out.shape)</span><br><span class="line">cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))</span><br><span class="line"></span><br><span class="line">l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])</span><br><span class="line"></span><br><span class="line">total_loss = cross_entropy + 7e-5*l2_loss</span><br><span class="line"></span><br><span class="line">train_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss)</span><br><span class="line"># 5.初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">print(&quot;FUNCTION READY!!&quot;)</span><br><span class="line"># 6.在会话中执行网络定义的运算</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    for step in range(3000):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(100)</span><br><span class="line">        lr = 0.01</span><br><span class="line"></span><br><span class="line">        _, loss, l2_loss_value, total_loss_value = sess.run(</span><br><span class="line">            [train_step, cross_entropy, l2_loss, total_loss],</span><br><span class="line">            feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr, keep_prob: 0.5&#125;)</span><br><span class="line"></span><br><span class="line">        if (step + 1) % 100 == 0:</span><br><span class="line">            print(&quot;step %d, entropy loss: %f, l2_loss: %f, total loss: %f&quot; %</span><br><span class="line">                  (step + 1, loss, l2_loss_value, total_loss_value))</span><br><span class="line"></span><br><span class="line">            # 验证训练的模型</span><br><span class="line">            correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))</span><br><span class="line">            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">            print(&quot;Train accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: 0.5&#125;))</span><br><span class="line"></span><br><span class="line">        if (step + 1) % 1000 == 0:</span><br><span class="line">            print(&quot;Text accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: 0.5&#125;))</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow-keras之CNN实现手写数字识别</title>
      <link href="/2018/08/27/2018-08-27-TensorFlow-keras/"/>
      <url>/2018/08/27/2018-08-27-TensorFlow-keras/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="TensorFlow四种写法之四：keras"><a href="#TensorFlow四种写法之四：keras" class="headerlink" title="TensorFlow四种写法之四：keras"></a>TensorFlow四种写法之四：keras</h3><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/8/23 19:14</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : CNN-keras.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"># 0.导入环境</span><br><span class="line">import os</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from keras.layers.core import Dense, Flatten</span><br><span class="line">from keras.layers.convolutional import Conv2D</span><br><span class="line">from keras.layers.pooling import MaxPooling2D</span><br><span class="line">from keras.objectives import categorical_crossentropy</span><br><span class="line">from keras import backend as K</span><br><span class="line">K.image_data_format()</span><br><span class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos;</span><br><span class="line"></span><br><span class="line"># 1.数据准备</span><br><span class="line"></span><br><span class="line"># 使用tensorflow自带的工具加载MNIST手写数字集合</span><br><span class="line">mnist = input_data.read_data_sets(&apos;data&apos;, one_hot=True)</span><br><span class="line"># 查看数据的维度和target的维度</span><br><span class="line">print(mnist.train.images.shape)</span><br><span class="line">print(mnist.train.labels.shape)</span><br><span class="line"></span><br><span class="line"># 2.准备好palceholder</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784])</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10])</span><br><span class="line">learnRate = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"># 3.构建网络计算图结构</span><br><span class="line"></span><br><span class="line"># 把输入数据reshape--28x28=784, 单通道， -1表示None</span><br><span class="line">with tf.name_scope(&apos;reshape&apos;):</span><br><span class="line">    x_image = tf.reshape(x, [-1, 28, 28, 1])</span><br><span class="line"></span><br><span class="line"># 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5</span><br><span class="line">net = Conv2D(32, kernel_size=[5, 5], strides=[1, 1],</span><br><span class="line">             activation=&apos;relu&apos;, padding=&apos;same&apos;,</span><br><span class="line">             input_shape=[28, 28, 1])(x_image)</span><br><span class="line"></span><br><span class="line"># 构建池化层--采用最大池化</span><br><span class="line">net = MaxPooling2D(pool_size=[2, 2])(net)</span><br><span class="line"></span><br><span class="line"># 构建第二层卷积计算层--maps 32 feature maps to 64.</span><br><span class="line">net = Conv2D(64, kernel_size=[5, 5], strides=[1, 1],</span><br><span class="line">             activation=&apos;relu&apos;, padding=&apos;same&apos;)(net)</span><br><span class="line"></span><br><span class="line"># 构建第二层池化层--采用最大池化</span><br><span class="line">net = MaxPooling2D(pool_size=[2, 2])(net)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征</span><br><span class="line">net = Flatten()(net)</span><br><span class="line">net = Dense(1024, activation=&apos;relu&apos;)(net)</span><br><span class="line"></span><br><span class="line"># 构建第二层全连接层--将1024个特性映射到10个类，每个类对应一个数字</span><br><span class="line">net = Dense(10, activation=&apos;softmax&apos;)(net)</span><br><span class="line"></span><br><span class="line"># 4.计算损失值并初始化optimizer</span><br><span class="line">cross_entropy = tf.reduce_mean(categorical_crossentropy(y, net))</span><br><span class="line"></span><br><span class="line">l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])</span><br><span class="line"></span><br><span class="line">total_loss = cross_entropy + 7e-5*l2_loss</span><br><span class="line"></span><br><span class="line">train_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss)</span><br><span class="line"># 5.初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">print(&quot;FUNCTION READY!!&quot;)</span><br><span class="line"></span><br><span class="line"># 6.在会话中执行网络定义的运算</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    for step in range(3000):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(100)</span><br><span class="line">        lr = 0.01</span><br><span class="line"></span><br><span class="line">        _, loss, l2_loss_value, total_loss_value = sess.run(</span><br><span class="line">            [train_step, cross_entropy, l2_loss, total_loss],</span><br><span class="line">            feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr&#125;)</span><br><span class="line"></span><br><span class="line">        if (step + 1) % 100 == 0:</span><br><span class="line">            print(&quot;step %d, entropy loss: %f, l2_loss: %f, total loss: %f&quot; %</span><br><span class="line">                  (step + 1, loss, l2_loss_value, total_loss_value))</span><br><span class="line"></span><br><span class="line">            # 验证训练的模型</span><br><span class="line">            correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))</span><br><span class="line">            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">            print(&quot;Train accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;))</span><br><span class="line"></span><br><span class="line">        if (step + 1) % 1000 == 0:</span><br><span class="line">            print(&quot;Text accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys&#125;))</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络之递归神经网络</title>
      <link href="/2018/08/27/2018-08-27-DL-RNN/"/>
      <url>/2018/08/27/2018-08-27-DL-RNN/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="递归神经网络"><a href="#递归神经网络" class="headerlink" title="递归神经网络"></a>递归神经网络</h3><p><strong>递归神经网络（Recurrent Neural Networks，RNN）</strong>是两种人工神经网络的总称：</p><ul><li><p><strong>时间递归神经网络（recurrent neural network）</strong>：时间递归神经网络的神经元间连接构成有向图</p></li><li><p><strong>结构递归神经网络（recursive neural network)</strong>：结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。</p></li></ul><p><strong>递归神经网络（RNN）</strong>(也叫循环神经网络）是一类神经网络，包括一层内的加权连接，与传统前馈神经网络相比，加权连接仅反馈到后续层。因为<strong>RNN</strong>包含循环，所以<strong>RNN</strong>就可以在<strong>处理输入信息的时候同时储存信息</strong>。这种记忆使得<strong>RNN</strong>非常适合处理必须考虑事先输入的任务（比如时序数据）。所以循环神经网络在自然语言处理领域非常适合。</p><p><strong>RNN</strong>一般指代<strong>时间递归神经网络</strong>。单纯递归神经网络因为无法处理随着递归，<strong>权重指数级爆炸或消失的问题（Vanishing gradient problem）</strong>，难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。时间递归神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。</p><p><img src="/images/dl/89.png" alt="images"></p><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>RNN是在<strong>自然语言处理</strong>领域中最先被用起来的，比如，RNN可以为<strong>语言模型</strong>来建模。</p><p>我们小时候都做过语文的填空题吧</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">饿死了，我要吃__。</span><br></pre></td></tr></table></figure><p>如果是我，肯定是填<code>肉</code>。哈哈</p><p>但是我不是今天的主角，如果这个问题我们让计算机来回答呢？</p><p>我们给计算机展示这段话，让计算机给出答案。而这个答案最有可能的就是<code>饭</code>，而太可能是<code>睡觉</code>或者<code>做作业</code>.</p><p><strong>语言模型</strong>就是这样的东西：给定一个一句话前面的部分，预测接下来最有可能的一个词是什么。</p><p><strong>语言模型</strong>是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要<strong>语言模型</strong>来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。</p><p>使用RNN之前，语言模型主要是采用N-Gram。N可以是一个自然数，比如2或者3。它的含义是，假设一个词出现的概率只与前面N个词相关。我们以2-Gram为例。首先，对前面的一句话进行切词：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">饿死了，我要____。</span><br></pre></td></tr></table></figure><p>如果用2-Gram进行建模，那么电脑在预测的时候，只会看到前面的『要』，然后，电脑会在语料库中，搜索『要』后面最可能的一个词。不管最后电脑选的是不是『饭』，我们都知道这个模型是不靠谱的，因为『要』前面说了那么一大堆实际上是没有用到的。如果是3-Gram模型呢，会搜索『我要吃』后面最可能的词，感觉上比2-Gram靠谱了不少，但还是远远不够的。因为这句话最关键的信息『饿』，远在5个词之前！</p><p>现在读者可能会想，可以提升继续提升N的值呀，比如4-Gram、5-Gram…….。实际上，这个想法是没有实用性的。因为我们想处理任意长度的句子，N设为多少都不合适；另外，模型的大小和N的关系是指数级的，4-Gram模型就会占用海量的存储空间。</p><p>所以，我们的RNN就出场了，RNN理论上<strong>可以往前看(往后看)任意多个词</strong>。</p><h3 id="基本递归神经网络"><a href="#基本递归神经网络" class="headerlink" title="基本递归神经网络"></a>基本递归神经网络</h3><p>下图是一个简单的递归神经网络，它是由输入层、一个隐层和一个输出层组成的。</p><p><img src="/images/dl/90.png" alt="images"></p><p>由图可知：</p><ul><li><code>x</code>：输入层的值</li><li><code>U</code>：输入层到隐层的权重参数</li><li><code>s</code>：隐层的值</li><li><code>v</code>：隐层到输出层的权重参数</li><li><code>o</code>：输出层的值</li><li><code>W</code>：<strong>递归神经网络</strong>的<strong>隐藏层</strong>的值<code>s</code>不仅仅取决于当前这次的输入<code>x</code>，还取决于上一次<strong>隐藏层</strong>的值<code>s</code>。<strong>权重参数</strong><code>W</code>就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。</li></ul><p>现在我们把上面的图展开，<strong>递归神经网络</strong>也可以画出这个结构：</p><p><img src="/images/dl/91.png" alt="images"></p><p>从上图中，我们可以很直观的看出，这个网络在<code>t</code>时刻接收到$X_t$后，隐层的值是$S_t$，输出值是$O_t$。</p><p><code>关键点</code>：$S_t$的值不仅仅取决于$X_t$，还取决于$S_{t-1}$(就是上一状态的隐层的值)。</p><p><code>公式</code>：循环神经网络的计算公式：</p><ul><li><h4 id="O-t-f-V-cdot-S-t-quad-1"><a href="#O-t-f-V-cdot-S-t-quad-1" class="headerlink" title="$O_t=f(V \cdot S_t) \quad (1)$"></a><strong>$O_t=f(V \cdot S_t) \quad (1)$</strong></h4><ul><li><code>输出层</code>的计算公式，由于输出层是一个<strong>全连接层</strong>，所以说它每个节点都和隐层的节点相连。<code>V</code>是输出层的权重参数，<code>f</code>是激活函数。</li></ul></li><li><h4 id="S-t-f-U-cdot-X-t-W-cdot-S-t-1-quad-2"><a href="#S-t-f-U-cdot-X-t-W-cdot-S-t-1-quad-2" class="headerlink" title="$S_t=f(U \cdot X_t+W \cdot S_{t-1}) \quad (2)$"></a><strong>$S_t=f(U \cdot X_t+W \cdot S_{t-1}) \quad (2)$</strong></h4><ul><li><code>隐层</code>的计算公式，它是一个<code>循环层</code>，<code>U</code>是输入<code>x</code>的权重参数，<code>W</code>是上一次的值<strong>$S_{t-1}$</strong>作为这一次输入的权重参数，<code>f</code>是激活函数。</li></ul></li></ul><p><code>总结</code>：从上面的公式中，我们可以看出，<strong>循环层</strong>和<strong>全连接层</strong>的区别就是<strong>循环层</strong>多了一个<strong>权重参数</strong><code>w</code>。</p><p><code>扩展</code>：如果反复的把（1）式带入 （2）式：</p><h4 id="o-t-f-V-cdot-S-t"><a href="#o-t-f-V-cdot-S-t" class="headerlink" title="${o}_t=f(V\cdot{S}_t) $"></a>${o}_t=f(V\cdot{S}_t) $</h4><p>$ = V \cdot f(U \cdot X_t + W \cdot S_{t-1}) $</p><p>$= V \cdot f(U \cdot X_t+W \cdot f(U \cdot X_{t-1}+W \cdot S_{t-2}))$</p><p>$= V \cdot f(U \cdot X_t+W \cdot f(U \cdot X_{t-1}+W \cdot f(U \cdot X_{t-2}+W \cdot S_{t-3}))) $</p><p>$= V \cdot f(U \cdot X_t+W \cdot f(U \cdot X_{t-1}+W \cdot f(U \cdot X_{t-2}+W \cdot f(U \cdot X_{t-3}+…))))$</p><p><code>总结</code>：从上面可以看出，<strong>递归神经网络</strong>的输出值$o_t$，是受前面几次输入值$X_t、X_{t-1}、X_{t-2}、X_{t-3}$…影响的，这也就是为什么<strong>递归神经网络</strong>可以往前看任意多个<strong>输入值</strong>的原因。</p><h3 id="双向递归神经网络"><a href="#双向递归神经网络" class="headerlink" title="双向递归神经网络"></a>双向递归神经网络</h3><p><code>介绍</code>：它们只是两个堆叠在一起的RNN。然后基于两个RNN的隐藏状态计算输出。</p><p>对于<strong>语言模型</strong>来说，很多时候光看前面的词是不够的，比如下面这句话：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">肚子好饿啊。我打算___一份外卖。</span><br></pre></td></tr></table></figure><p>可以想象，如果我们只是看横线前面的词，肚子好饿啊，我打算做饭，去吃饭还是什么什么的。这些都是无法确定的。但是如果我也先看横线后面的词【一份外卖】，那么横线上填【点】的概率就大多了。</p><p>但是我们前面所讲的<strong>基本递归神经网络</strong>是无法解决这个问题的，这时我们就需要<strong>双向递归神经网络</strong>，如下图所示：</p><p><img src="/images/dl/92.png" alt="images"></p><p>呃~~，有点复杂，我们先尝试分析一个特殊的场景，然后再总结规律。我们先看看上图的$y_2$的计算：</p><p>从上图可以看出，<strong>双向递归神经网络</strong>的隐层是需要保持两个值：</p><ul><li>$A$：参与正向计算</li><li>$A’$：参与反向计算</li></ul><p>所以$y_2$的值就取决于$A_2$和$A_2’$。计算方法：</p><ul><li><h4 id="y-2-f-V-cdot-A-2-V’-cdot-A-2’"><a href="#y-2-f-V-cdot-A-2-V’-cdot-A-2’" class="headerlink" title="$y_2=f(V \cdot A_2+V’ \cdot A_2’)$"></a>$y_2=f(V \cdot A_2+V’ \cdot A_2’)$</h4></li></ul><p>$A_2$和$A_2’$则分别计算：</p><ul><li><h4 id="A-2-f-W-cdot-A-1-U-cdot-X-2"><a href="#A-2-f-W-cdot-A-1-U-cdot-X-2" class="headerlink" title="$A_2 = f(W \cdot A_1+U \cdot X_2)$"></a>$A_2 = f(W \cdot A_1+U \cdot X_2)$</h4></li><li><h4 id="A-2’-f-W’-cdot-A-3’-U’-cdot-X-2"><a href="#A-2’-f-W’-cdot-A-3’-U’-cdot-X-2" class="headerlink" title="$A_2’=f(W’ \cdot A_3’+U’ \cdot X_2)$"></a>$A_2’=f(W’ \cdot A_3’+U’ \cdot X_2)$</h4></li></ul><p><code>总结</code>：</p><ul><li>正向计算时：隐层的值$S_t$和$S_{t-1}$有关。</li><li>反向计算时：隐层的值$S_t’$和$S_{t-1}’$有关。</li><li>最终的输出取决于正向和反向计算的<strong>加和</strong>。</li></ul><p><code>扩展</code>：我们仿照（1）和（2）那种方式：</p><h4 id="o-t-f-V-cdot-S-t-V’-cdot-S-t’"><a href="#o-t-f-V-cdot-S-t-V’-cdot-S-t’" class="headerlink" title="$o_t =f(V \cdot S_t+V’ \cdot S_t’) $"></a>$o_t =f(V \cdot S_t+V’ \cdot S_t’) $</h4><h4 id="S-t-f-U-cdot-X-t-W-cdot-S-t-1"><a href="#S-t-f-U-cdot-X-t-W-cdot-S-t-1" class="headerlink" title="$S_t =f(U \cdot X_t+W \cdot S_{t-1}) $"></a>$S_t =f(U \cdot X_t+W \cdot S_{t-1}) $</h4><h4 id="S-t’-f-U’-cdot-X-t-W’-cdot-S-t-1-’"><a href="#S-t’-f-U’-cdot-X-t-W’-cdot-S-t-1-’" class="headerlink" title="$ S_t’=f(U’ \cdot X_t+W’ \cdot S_{t+1}’)$"></a>$ S_t’=f(U’ \cdot X_t+W’ \cdot S_{t+1}’)$</h4><p><code>注意</code>：从上面三个公式我们可以看到，正向计算和反向计算<strong>不共享权重</strong>，也就是说U和U’、W和W’、V和V’都是不同的<strong>权重矩阵</strong>。</p><h3 id="深度递归神经网络"><a href="#深度递归神经网络" class="headerlink" title="深度递归神经网络"></a>深度递归神经网络</h3><p>前面我们介绍的<strong>递归神经网络</strong>只有一个隐藏层，我们当然也可以堆叠两个以上的隐藏层，这样就得到了<strong>深度递归神经网络</strong>。如下图所示：</p><p><img src="/images/dl/93.png" alt="images"></p><p>我们把第$i$个隐层的值表示为$S_t^{(i)}、S_t’^{(i)}$ ,则<strong>深度递归神经网络</strong>的计算方式就可以表示为：</p><ul><li><h4 id="o-t-f-cdot-V-i-cdot-S-t-i-V’-i-cdot-S-t’-i"><a href="#o-t-f-cdot-V-i-cdot-S-t-i-V’-i-cdot-S-t’-i" class="headerlink" title="${o}_t=f \cdot (V^{(i)} \cdot S_t^{(i)}+V’^{(i)} \cdot S_t’^{(i)})$"></a>${o}_t=f \cdot (V^{(i)} \cdot S_t^{(i)}+V’^{(i)} \cdot S_t’^{(i)})$</h4></li><li><h4 id="S-t-i-f-U-i-cdot-S-t-i-1-W-i-cdot-S-t-1"><a href="#S-t-i-f-U-i-cdot-S-t-i-1-W-i-cdot-S-t-1" class="headerlink" title="$S_t^{(i)}=f(U^{(i)}\cdot S_t^{(i-1)}+W^{(i)}\cdot S_{t-1})$"></a>$S_t^{(i)}=f(U^{(i)}\cdot S_t^{(i-1)}+W^{(i)}\cdot S_{t-1})$</h4></li><li><h4 id="S-t’-i-f-U’-i-cdot-S-t’-i-1-W’-i-cdot-S-t-1-’"><a href="#S-t’-i-f-U’-i-cdot-S-t’-i-1-W’-i-cdot-S-t-1-’" class="headerlink" title="$S_t’^{(i)}=f(U’^{(i)}\cdot S_t’^{(i-1)}+W’^{(i)}\cdot S_{t+1}’)$"></a>$S_t’^{(i)}=f(U’^{(i)}\cdot S_t’^{(i-1)}+W’^{(i)}\cdot S_{t+1}’)$</h4></li><li><p>$…$</p></li><li><h4 id="S-t-1-f-U-1-cdot-X-t-W-1-cdot-S-t-1"><a href="#S-t-1-f-U-1-cdot-X-t-W-1-cdot-S-t-1" class="headerlink" title="$S_t^{(1)}=f(U^{(1)} \cdot X_t+W^{(1)}\cdot S_{t-1})$"></a>$S_t^{(1)}=f(U^{(1)} \cdot X_t+W^{(1)}\cdot S_{t-1})$</h4></li><li><h4 id="S-t’-1-f-U’-1-cdot-X-t-W’-1-cdot-S-t-1-’"><a href="#S-t’-1-f-U’-1-cdot-X-t-W’-1-cdot-S-t-1-’" class="headerlink" title="$S_t’^{(1)}=f(U’^{(1)}\cdot X_t+W’^{(1)}\cdot S_{t+1}’)$"></a>$S_t’^{(1)}=f(U’^{(1)}\cdot X_t+W’^{(1)}\cdot S_{t+1}’)$</h4></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>递归神经网络的形式很自由：不同的递归神经网络可以实现不同的功能</p><p><img src="/images/dl/94.png" alt="images"></p><p>从上图我们可以总结出：</p><ul><li><code>one to one</code>：一个输入（单一标签）对应一个输出（单一标签）</li><li><code>one to many</code>：一个输入对应多个输出，即这个架构多用于图片的对象识别，即输入一个图片，输出一个文本序列。</li><li><code>many to one</code>： 多个输入对应一个输出，多用于文本分类或视频分类，即输入一段文本或视频片段，输出类别。</li><li><code>many to many</code>：这种结构广泛的用于机器翻译，输入一个文本，输出另一种语言的文本。</li><li><code>many to many</code>：这种广泛的用于序列标注。</li></ul><p>在众多的深度学习网络中，RNN由于能够接收序列输入，也能得到序列输出，在自然语言处理中取得了巨大的成功，并得到广泛的应用。</p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow-Layer之CNN实现手写数字识别</title>
      <link href="/2018/08/27/2018-08-27-TensorFlow-layer/"/>
      <url>/2018/08/27/2018-08-27-TensorFlow-layer/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="TensorFlow四种写法之二：layer"><a href="#TensorFlow四种写法之二：layer" class="headerlink" title="TensorFlow四种写法之二：layer"></a>TensorFlow四种写法之二：layer</h3><h4 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/8/22 21:01</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : CNN-layers.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"># 0.导入环境</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos;</span><br><span class="line"></span><br><span class="line"># 1.数据准备</span><br><span class="line"></span><br><span class="line"># 使用tensorflow自带的工具加载MNIST手写数字集合</span><br><span class="line">mnist = input_data.read_data_sets(&apos;data&apos;, one_hot=True)</span><br><span class="line"># 查看数据的维度和target的维度</span><br><span class="line">print(mnist.train.images.shape)</span><br><span class="line">print(mnist.train.labels.shape)</span><br><span class="line"></span><br><span class="line"># 2.准备好palceholder</span><br><span class="line">x = tf.placeholder(tf.float32, [None, 784])</span><br><span class="line">y = tf.placeholder(tf.float32, [None, 10])</span><br><span class="line">learnRate = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"># 3.构建网络计算图结构</span><br><span class="line"></span><br><span class="line"># 把输入数据reshape--28x28=784, 单通道， -1表示None</span><br><span class="line">with tf.name_scope(&apos;reshape&apos;):</span><br><span class="line">    x_image = tf.reshape(x, [-1, 28, 28, 1])</span><br><span class="line"></span><br><span class="line"># 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5</span><br><span class="line">with tf.name_scope(&apos;conv1&apos;):</span><br><span class="line">    h_conv1 = tf.layers.conv2d(x_image, 32, [5, 5], padding=&apos;SAME&apos;, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line"># 构建池化层--采用最大池化</span><br><span class="line">with tf.name_scope(&apos;pool1&apos;):</span><br><span class="line">    h_pool1 = tf.layers.max_pooling2d(h_conv1, pool_size=[2, 2], strides=[2, 2], padding=&apos;VALID&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 构建第二层卷积计算层--maps 32 feature maps to 64.</span><br><span class="line">with tf.name_scope(&apos;conv2&apos;):</span><br><span class="line">    h_conv2 = tf.layers.conv2d(h_pool1, 64, [5, 5], padding=&apos;SAME&apos;, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line"># 构建第二个池化层</span><br><span class="line">with tf.name_scope(&apos;pool2&apos;):</span><br><span class="line">    h_pool2 = tf.layers.max_pooling2d(h_conv2, pool_size=[2, 2], strides=[2, 2], padding=&apos;VALID&apos;)</span><br><span class="line"></span><br><span class="line"># 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征</span><br><span class="line">with tf.name_scope(&apos;fc1&apos;):</span><br><span class="line">    h_pool2_flat = tf.layers.flatten(h_pool2)</span><br><span class="line">    h_fc1 = tf.layers.dense(h_pool2_flat, 1024, activation=tf.nn.relu)</span><br><span class="line"></span><br><span class="line"># Dropout--防止过拟合</span><br><span class="line">with tf.name_scope(&apos;dropout&apos;):</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">    h_fc_drop = tf.nn.dropout(h_fc1, keep_prob=keep_prob)</span><br><span class="line"></span><br><span class="line"># 构建全连接层--将1024个特性映射到10个类，每个类对应一个数字</span><br><span class="line">with tf.name_scope(&apos;fc2&apos;):</span><br><span class="line">    out = tf.layers.dense(h_fc_drop, 10, activation=None)</span><br><span class="line"></span><br><span class="line"># 4.计算损失值并初始化optimizer</span><br><span class="line">print(y.shape, out.shape)</span><br><span class="line">cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))</span><br><span class="line"></span><br><span class="line">l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])</span><br><span class="line"></span><br><span class="line">total_loss = cross_entropy + 7e-5*l2_loss</span><br><span class="line"></span><br><span class="line">train_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss)</span><br><span class="line"># 5.初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"># 6.在会话中执行网络定义的运算</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    for step in range(3000):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(100)</span><br><span class="line">        lr = 0.01</span><br><span class="line"></span><br><span class="line">        _, loss, l2_loss_value, total_loss_value = sess.run(</span><br><span class="line">            [train_step, cross_entropy, l2_loss, total_loss],</span><br><span class="line">            feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr, keep_prob: 0.5&#125;)</span><br><span class="line"></span><br><span class="line">        if (step+1) % 100 == 0:</span><br><span class="line">            print(&quot;step %d, entropy loss: %f, l2_loss: %f, total loss: %f&quot; %</span><br><span class="line">                  (step+1, loss, l2_loss_value, total_loss_value))</span><br><span class="line"></span><br><span class="line">            # 验证训练的模型</span><br><span class="line">            correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))</span><br><span class="line">            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">            print(&quot;Train accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob:0.5&#125;))</span><br><span class="line"></span><br><span class="line">        if (step + 1) % 1000 == 0:</span><br><span class="line">            print(&quot;Text accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: 0.5&#125;))</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>卷积神经网络之AlexNet</title>
      <link href="/2018/08/25/2018-08-25-DL-CNN-AlexNet/"/>
      <url>/2018/08/25/2018-08-25-DL-CNN-AlexNet/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="AlexNet背景"><a href="#AlexNet背景" class="headerlink" title="AlexNet背景"></a>AlexNet背景</h3><p>AlexNet是在2012年被发表的一个经典之作，并在当年取得了ImageNet最好成绩，也是在那年之后，更多的更深的神经网路被提出，比如优秀的vgg,GoogleLeNet.</p><p>其官方提供的数据模型，准确率达到57.1%,top 1-5 达到80.2%. 这项对于传统的机器学习分类算法而言，已经相当的出色。</p><h3 id="框架介绍"><a href="#框架介绍" class="headerlink" title="框架介绍:"></a>框架介绍:</h3><p>AlexNet的结构模型如下：</p><p><img src="/images/dl/88.png" alt="images"></p><h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><h4 id="AlexNet实现Minist手写数字识别"><a href="#AlexNet实现Minist手写数字识别" class="headerlink" title="AlexNet实现Minist手写数字识别"></a><code>AlexNet实现Minist手写数字识别</code></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/8/24 22:11</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : AlexNet.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"># TODO: 0.导入环境</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import os</span><br><span class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos;</span><br><span class="line"></span><br><span class="line"># TODO: 1.数据准备</span><br><span class="line"></span><br><span class="line"># 使用tensorflow自带的工具加载MNIST手写数字集合</span><br><span class="line">mnist = input_data.read_data_sets(&apos;data&apos;, one_hot=True)</span><br><span class="line"># 查看数据的维度和target的维度</span><br><span class="line">print(mnist.train.images.shape)</span><br><span class="line">print(mnist.train.labels.shape)</span><br><span class="line"></span><br><span class="line"># TODO: 2.初始化变量</span><br><span class="line"></span><br><span class="line"># 定义网络超参数</span><br><span class="line">learning_rate = 0.001</span><br><span class="line">epochs = 200000</span><br><span class="line">batch_size = 128</span><br><span class="line">display_step = 5</span><br><span class="line"></span><br><span class="line"># 定义网络参数</span><br><span class="line">n_input = 784  # 输入的维度</span><br><span class="line">n_classes = 10  # 标签的维度</span><br><span class="line">dropout = 0.8  # Dropout 的概率</span><br><span class="line"></span><br><span class="line"># TODO: 3.准备好placeholder</span><br><span class="line"></span><br><span class="line"># 占位符输入</span><br><span class="line">x = tf.placeholder(tf.float32, [None, n_input])</span><br><span class="line">y = tf.placeholder(tf.float32, [None, n_classes])</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"># TODO: 4.构建网络计算图结构</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 卷积操作</span><br><span class="line">def conv2d(name, l_input, w, b):</span><br><span class="line">    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;), b), name=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 最大下采样操作</span><br><span class="line">def max_pool(name, l_input, k):</span><br><span class="line">    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding=&apos;SAME&apos;, name=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 归一化操作</span><br><span class="line">def norm(name, l_input, lsize=4):</span><br><span class="line">    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 存储所有的网络参数</span><br><span class="line">weights = &#123;</span><br><span class="line">    &apos;wc1&apos;: tf.Variable(tf.random_normal(shape=[11, 11, 1, 96])),</span><br><span class="line">    &apos;wc2&apos;: tf.Variable(tf.random_normal(shape=[5, 5, 96, 256])),</span><br><span class="line">    &apos;wc3&apos;: tf.Variable(tf.random_normal(shape=[3, 3, 256, 384])),</span><br><span class="line">    &apos;wc4&apos;: tf.Variable(tf.random_normal(shape=[3, 3, 384, 384])),</span><br><span class="line">    &apos;wc5&apos;: tf.Variable(tf.random_normal(shape=[3, 3, 384, 256])),</span><br><span class="line">    &apos;wd1&apos;: tf.Variable(tf.random_normal(shape=[4*4*256, 4096])),</span><br><span class="line">    &apos;wd2&apos;: tf.Variable(tf.random_normal(shape=[4096, 1024])),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal(shape=[1024, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    &apos;bc1&apos;: tf.Variable(tf.random_normal([96])),</span><br><span class="line">    &apos;bc2&apos;: tf.Variable(tf.random_normal([256])),</span><br><span class="line">    &apos;bc3&apos;: tf.Variable(tf.random_normal([384])),</span><br><span class="line">    &apos;bc4&apos;: tf.Variable(tf.random_normal([384])),</span><br><span class="line">    &apos;bc5&apos;: tf.Variable(tf.random_normal([256])),</span><br><span class="line">    &apos;bd1&apos;: tf.Variable(tf.random_normal([4096])),</span><br><span class="line">    &apos;bd2&apos;: tf.Variable(tf.random_normal([1024])),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 定义整个网络</span><br><span class="line">def alex_net(_X, _weights, _biases, _dropout):</span><br><span class="line">    # 向量转为矩阵</span><br><span class="line">    _X = tf.reshape(_X, shape=[-1, 28, 28, 1])</span><br><span class="line">    print(_X.shape)</span><br><span class="line">    # TODO: 第一层卷积：</span><br><span class="line">    conv1 = conv2d(&apos;conv1&apos;, _X, _weights[&apos;wc1&apos;], _biases[&apos;bc1&apos;])</span><br><span class="line">    # 下采样层</span><br><span class="line">    pool1 = max_pool(&apos;pool1&apos;, conv1, k=2)</span><br><span class="line">    # 归一化层</span><br><span class="line">    norm1 = norm(&apos;norm1&apos;, pool1, lsize=4)</span><br><span class="line">    print(norm1.shape)</span><br><span class="line">    # TODO: 第二层卷积：</span><br><span class="line">    conv2 = conv2d(&apos;conv2&apos;, norm1, _weights[&apos;wc2&apos;], _biases[&apos;bc2&apos;])</span><br><span class="line">    # 下采样</span><br><span class="line">    pool2 = max_pool(&apos;pool2&apos;, conv2, k=2)</span><br><span class="line">    # 归一化</span><br><span class="line">    norm2 = norm(&apos;norm2&apos;, pool2, lsize=4)</span><br><span class="line">    print(norm2.shape)</span><br><span class="line">    # TODO: 第三层卷积：</span><br><span class="line">    conv3 = conv2d(&apos;conv3&apos;, norm2, _weights[&apos;wc3&apos;], _biases[&apos;bc3&apos;])</span><br><span class="line">    # 归一化</span><br><span class="line">    norm3 = norm(&apos;norm3&apos;, conv3, lsize=4)</span><br><span class="line">    print(norm3.shape)</span><br><span class="line">    # TODO: 第四层卷积</span><br><span class="line">    # 卷积</span><br><span class="line">    conv4 = conv2d(&apos;conv4&apos;, norm3, _weights[&apos;wc4&apos;], _biases[&apos;bc4&apos;])</span><br><span class="line">    # 归一化</span><br><span class="line">    norm4 = norm(&apos;norm4&apos;, conv4, lsize=4)</span><br><span class="line">    print(norm4.shape)</span><br><span class="line">    # TODO: 第五层卷积</span><br><span class="line">    # 卷积</span><br><span class="line">    conv5 = conv2d(&apos;conv5&apos;, norm4, _weights[&apos;wc5&apos;], _biases[&apos;bc5&apos;])</span><br><span class="line">    # 下采样</span><br><span class="line">    pool5 = max_pool(&apos;pool5&apos;, conv5, k=2)</span><br><span class="line">    # 归一化</span><br><span class="line">    norm5 = norm(&apos;norm5&apos;, pool5, lsize=4)</span><br><span class="line">    print(norm5.shape)</span><br><span class="line">    # TODO: 第六层全连接层</span><br><span class="line">    # 先把特征图转为向量</span><br><span class="line">    dense1 = tf.reshape(norm5, [-1, _weights[&apos;wd1&apos;].get_shape().as_list()[0]])</span><br><span class="line">    dense1 = tf.nn.relu(tf.matmul(dense1, _weights[&apos;wd1&apos;]) + _biases[&apos;bd1&apos;], name=&apos;fc1&apos;)</span><br><span class="line">    dense1 = tf.nn.dropout(dense1, _dropout)</span><br><span class="line">    print(dense1.shape)</span><br><span class="line">    # TODO: 第七层全连接层：</span><br><span class="line">    dense2 = tf.nn.relu(tf.matmul(dense1, _weights[&apos;wd2&apos;]) + _biases[&apos;bd2&apos;], name=&apos;fc2&apos;)  # Relu activation</span><br><span class="line">    dense2 = tf.nn.dropout(dense2, _dropout)</span><br><span class="line">    print(dense2.shape)</span><br><span class="line">    # TODO: 第八层全连接层：</span><br><span class="line">    # 网络输出层</span><br><span class="line">    out = tf.matmul(dense2, _weights[&apos;out&apos;]) + _biases[&apos;out&apos;]</span><br><span class="line">    print(out.shape)</span><br><span class="line">    return out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 5.计算损失值并初始化optimizer</span><br><span class="line"></span><br><span class="line"># 构建模型</span><br><span class="line">logits = alex_net(x, weights, biases, keep_prob)</span><br><span class="line"># 定义损失函数和学习步骤</span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))</span><br><span class="line">training_operation = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">print(&quot;FUNCTION READY!!&quot;)</span><br><span class="line"></span><br><span class="line"># TODO: 6.模型评估</span><br><span class="line"># 测试网络</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))</span><br><span class="line">accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"># TODO: 7.初始化变量</span><br><span class="line"># 初始化所有的共享变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"># TODO: 8.训练模型</span><br><span class="line"># 开启一个训练</span><br><span class="line">print(&quot;Training....&quot;)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    # Keep training until reach max iterations</span><br><span class="line">    for n in range(epochs):</span><br><span class="line">        total_epochs = int(mnist.train.num_examples/batch_size)</span><br><span class="line">        # 遍历所有epoch</span><br><span class="line">        for epoch in range(total_epochs):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            # 获取批数据</span><br><span class="line">            sess.run(training_operation, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: dropout&#125;)</span><br><span class="line">        if n % display_step == 0:</span><br><span class="line">            # 计算精度和损失值</span><br><span class="line">            loss, acc = sess.run([cost, accuracy_operation], feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: 1.&#125;)</span><br><span class="line">            print(&quot;epoch :&quot;, (n+1))</span><br><span class="line">            print(&quot;Minibatch Loss= &#123;:.6f&#125;&quot;.format(loss))</span><br><span class="line">            print(&quot;Training Accuracy= &#123;:.5f&#125;&quot;.format(acc))</span><br><span class="line"></span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br><span class="line">    # 计算测试精度</span><br><span class="line">    print(&quot;Testing Accuracy:&quot;,</span><br><span class="line">          sess.run(accuracy_operation, feed_dict=&#123;x: mnist.test.images[:256],</span><br><span class="line">                                                  y: mnist.test.labels[:256],</span><br><span class="line">                                                  keep_prob: 1.&#125;))</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>卷积神经网络之LeNet</title>
      <link href="/2018/08/24/2018-08-24-DL-CNN-LeNet/"/>
      <url>/2018/08/24/2018-08-24-DL-CNN-LeNet/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="LeNet5"><a href="#LeNet5" class="headerlink" title="LeNet5"></a><strong>LeNet5</strong></h3><p>LeNet5 诞生于 1994 年，是最早的卷积神经网络之一，并且推动了深度学习领域的发展。自从 1988 年开始，在许多次成功的迭代后，这项由 Yann LeCun 完成的开拓性成果被命名为 LeNet5（参见：Gradient-Based Learning Applied to Document Recognition）。</p><p><img src="/images/dl/87.png" alt="images"></p><p>LeNet5 的架构基于这样的观点：图像的特征分布在整张图像上，以及带有可学习参数的卷积是一种用少量参数在多个位置上提取相似特征的有效方式。</p><p>在那时候，没有 GPU 帮助训练，甚至 CPU 的速度也很慢。因此，能够保存参数以及计算过程是一个关键进展。这和将每个像素用作一个大型多层神经网络的单独输入相反。</p><p>LeNet5 阐述了那些像素不应该被使用在第一层，因为图像具有很强的空间相关性，而使用图像中独立的像素作为不同的输入特征则利用不到这些相关性。</p><h3 id="LeNet5特点"><a href="#LeNet5特点" class="headerlink" title="LeNet5特点"></a><strong>LeNet5特点</strong></h3><p>LeNet5特征能够总结为如下几点：</p><p>1）卷积神经网络使用三个层作为一个系列： 卷积，池化，非线性</p><p>2） 使用卷积提取空间特征</p><p>3）使用映射到空间均值下采样（subsample）</p><p>4）双曲线（tanh）或S型（sigmoid）形式的非线性</p><p>5）多层神经网络（MLP）作为最后的分类器</p><p>6）层与层之间的稀疏连接矩阵避免大的计算成本</p><h3 id="LeNet5实现"><a href="#LeNet5实现" class="headerlink" title="LeNet5实现"></a><strong>LeNet5实现</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/8/24 16:35</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    :</span><br><span class="line"># @File    : LeNet.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"># TODO: 0.导入环境</span><br><span class="line"></span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import random</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.utils import shuffle</span><br><span class="line">import os</span><br><span class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos;</span><br><span class="line"></span><br><span class="line"># TODO: 1.数据准备</span><br><span class="line"></span><br><span class="line"># 使用tensorflow自带的工具加载MNIST手写数字集合</span><br><span class="line">mnist = input_data.read_data_sets(&apos;data&apos;, reshape=False)</span><br><span class="line"></span><br><span class="line"># TODO: 2.数据处理</span><br><span class="line">x_train, y_train = mnist.train.images, mnist.train.labels</span><br><span class="line">x_validation, y_validation = mnist.validation.images, mnist.validation.labels</span><br><span class="line">x_test, y_test = mnist.test.images, mnist.test.labels</span><br><span class="line"></span><br><span class="line"># 预先判断下维度是否一致</span><br><span class="line">assert(len(x_train) == len(y_train))</span><br><span class="line">assert(len(x_validation) == len(y_validation))</span><br><span class="line">assert(len(x_test) == len(y_test))</span><br><span class="line"></span><br><span class="line">print(&quot;Image Shape: &#123;&#125;&quot;.format(x_train[0].shape))</span><br><span class="line">print(&quot;Training Set:   &#123;&#125; samples&quot;.format(len(x_train)))</span><br><span class="line">print(&quot;Validation Set: &#123;&#125; samples&quot;.format(len(x_validation)))</span><br><span class="line">print(&quot;Test Set:       &#123;&#125; samples&quot;.format(len(x_test)))</span><br><span class="line"></span><br><span class="line"># TensorFlow预加载的MNIST数据为28x28x1图像。</span><br><span class="line"># 但是，LeNet架构只接受32x32xC图像，其中C是颜色通道的数量。</span><br><span class="line"># 为了将MNIST数据重新格式化为LeNet将接受的形状，我们在顶部和底部填充两行零，并在左侧和右侧填充两列零（28 + 2 + 2 = 32）</span><br><span class="line">x_train = np.pad(x_train, ((0, 0), (2, 2), (2, 2), (0, 0)), &apos;constant&apos;)</span><br><span class="line">x_validation = np.pad(x_validation, ((0, 0), (2, 2), (2, 2), (0, 0)), &apos;constant&apos;)</span><br><span class="line">x_test = np.pad(x_test, ((0, 0), (2, 2), (2, 2), (0, 0)), &apos;constant&apos;)</span><br><span class="line">print(&quot;Updated Image Shape: &#123;&#125;&quot;.format(x_train[0].shape))</span><br><span class="line"></span><br><span class="line">#  查看下灰度图</span><br><span class="line">index = random.randint(0, len(x_train))</span><br><span class="line">image = x_train[index].squeeze()</span><br><span class="line">plt.figure(figsize=(1, 1))</span><br><span class="line">plt.imshow(image, cmap=&quot;gray&quot;)</span><br><span class="line"># plt.show()</span><br><span class="line">print(y_train[index])</span><br><span class="line"></span><br><span class="line"># Shuffle the training data.</span><br><span class="line">x_train, y_train = shuffle(x_train, y_train)</span><br><span class="line"></span><br><span class="line"># TODO: 3.准备好placeholder</span><br><span class="line">x = tf.placeholder(tf.float32, (None, 32, 32, 1))</span><br><span class="line">y = tf.placeholder(tf.int32, None)</span><br><span class="line">one_hot_y = tf.one_hot(y, 10)</span><br><span class="line"></span><br><span class="line"># TODO: 4.构建网络计算图结构</span><br><span class="line"></span><br><span class="line">epochs = 10</span><br><span class="line">batch_size = 128</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def LeNet(input):</span><br><span class="line"></span><br><span class="line">    mu = 0</span><br><span class="line">    sigma = 0.1</span><br><span class="line"></span><br><span class="line">    # 层深度定义</span><br><span class="line">    layer_depth = &#123;</span><br><span class="line">        &apos;layer1&apos;: 6,</span><br><span class="line">        &apos;layer2&apos;: 16,</span><br><span class="line">        &apos;layer3&apos;: 120,</span><br><span class="line">        &apos;layer_fc1&apos;: 84</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    # TODO: 第一层卷积：输入=32x32x1, 输出=28x28x6</span><br><span class="line">    conv1_w = tf.Variable(tf.truncated_normal(shape=[5, 5, 1, 6], mean=mu, stddev=sigma))</span><br><span class="line">    conv1_b = tf.Variable(tf.zeros(6))</span><br><span class="line"></span><br><span class="line">    conv1 = tf.nn.conv2d(input, conv1_w, strides=[1, 1, 1, 1], padding=&apos;VALID&apos;) + conv1_b</span><br><span class="line"></span><br><span class="line">    # 激活函数</span><br><span class="line">    conv1_out = tf.nn.relu(conv1)</span><br><span class="line"></span><br><span class="line">    # 池化层， 输入=28x28x6, 输出=14x14x6</span><br><span class="line">    pool_1 = tf.nn.max_pool(conv1_out, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;VALID&apos;)</span><br><span class="line">    print(pool_1.shape)</span><br><span class="line"></span><br><span class="line">    # TODO: 第二层卷积： 输入=14x14x6， 输出=10x10x16</span><br><span class="line">    conv2_w = tf.Variable(tf.truncated_normal(shape=[5, 5, 6, 16], mean=mu, stddev=sigma))</span><br><span class="line">    conv2_b = tf.Variable(tf.zeros(16))</span><br><span class="line"></span><br><span class="line">    conv2 = tf.nn.conv2d(pool_1, conv2_w, strides=[1, 1, 1, 1], padding=&apos;VALID&apos;) + conv2_b</span><br><span class="line"></span><br><span class="line">    # 激活函数</span><br><span class="line">    conv2_out = tf.nn.relu(conv2)</span><br><span class="line"></span><br><span class="line">    # 池化层， 输入=10x10x16, 输出=5x5x16</span><br><span class="line">    pool_2 = tf.nn.max_pool(conv2_out, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;VALID&apos;)</span><br><span class="line"></span><br><span class="line">    # Flatten 输入=5x5x16， 输出=400</span><br><span class="line">    pool_2_flat = tf.reshape(pool_2, [-1, 400])</span><br><span class="line">    print(pool_2_flat.shape)</span><br><span class="line"></span><br><span class="line">    # TODO: 第三层全连接层， 输入=400， 输出=120</span><br><span class="line">    fc1_w = tf.Variable(tf.truncated_normal(shape=[400, 120], mean=mu, stddev=sigma))</span><br><span class="line">    fc1_b = tf.Variable(tf.zeros(120))</span><br><span class="line"></span><br><span class="line">    fc1 = tf.matmul(pool_2_flat, fc1_w) + fc1_b</span><br><span class="line"></span><br><span class="line">    # 激活函数</span><br><span class="line">    fc1_out = tf.nn.relu(fc1)</span><br><span class="line">    print(fc1_out.shape)</span><br><span class="line"></span><br><span class="line">    # TODO: 第四层全连接层： 输入=120， 输出=84</span><br><span class="line">    fc2_w = tf.Variable(tf.truncated_normal(shape=[120, 84], mean=mu, stddev=sigma))</span><br><span class="line">    fc2_b = tf.Variable(tf.zeros(84))</span><br><span class="line"></span><br><span class="line">    fc2 = tf.matmul(fc1_out, fc2_w) + fc2_b</span><br><span class="line"></span><br><span class="line">    # 激活函数</span><br><span class="line">    fc2_out = tf.nn.relu(fc2)</span><br><span class="line">    print(fc2_out.shape)</span><br><span class="line"></span><br><span class="line">    # TODO: 第五层全连接层： 输入=84， 输出=10</span><br><span class="line">    fc3_w = tf.Variable(tf.truncated_normal(shape=[84, 10], mean=mu, stddev=sigma))</span><br><span class="line">    fc3_b = tf.Variable(tf.zeros(10))</span><br><span class="line"></span><br><span class="line">    fc3_out = tf.matmul(fc2_out, fc3_w) + fc3_b</span><br><span class="line">    print(fc3_out.shape)</span><br><span class="line"></span><br><span class="line">    return fc3_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 5.计算损失值并初始化optimizer</span><br><span class="line">learning_rate = 0.001</span><br><span class="line"></span><br><span class="line">logits = LeNet(x)</span><br><span class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)</span><br><span class="line">loss_operation = tf.reduce_mean(cross_entropy)</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_operation = optimizer.minimize(loss_operation)</span><br><span class="line"></span><br><span class="line">print(&quot;FUNCTION READY!!&quot;)</span><br><span class="line"></span><br><span class="line"># TODO: 6.初始化变量</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"># TODO: 7.模型评估</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))</span><br><span class="line">accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def evaluate(X_data, y_data):</span><br><span class="line">    num_examples = len(X_data)</span><br><span class="line">    total_accuracy = 0</span><br><span class="line">    sess = tf.get_default_session()</span><br><span class="line">    for offset in range(0, num_examples, batch_size):</span><br><span class="line">        batch_x, batch_y = X_data[offset:offset+batch_size], y_data[offset:offset+batch_size]</span><br><span class="line"></span><br><span class="line">        accuracy = sess.run(accuracy_operation, feed_dict=&#123;x: batch_x, y: batch_y&#125;)</span><br><span class="line">        total_accuracy += (accuracy * len(batch_x))</span><br><span class="line">    return total_accuracy / num_examples</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 保存模型</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: 8.训练模型</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    num_examples = len(x_train)</span><br><span class="line"></span><br><span class="line">    print(&quot;Training.....&quot;)</span><br><span class="line"></span><br><span class="line">    for n in range(epochs):</span><br><span class="line">        for offset in range(0, num_examples, batch_size):</span><br><span class="line">            batch_x, batch_y = x_train[offset:offset+batch_size], y_train[offset:offset+batch_size]</span><br><span class="line">            sess.run(training_operation, feed_dict=&#123;x: batch_x, y: batch_y&#125;)</span><br><span class="line"></span><br><span class="line">        validation_accuracy = evaluate(x_validation, y_validation)</span><br><span class="line">        print(&quot;EPOCH &#123;&#125; ...&quot;.format(n + 1))</span><br><span class="line">        print(&quot;Validation Accuracy = &#123;:.3f&#125;&quot;.format(validation_accuracy))</span><br><span class="line"></span><br><span class="line">    saver.save(sess, &apos;./model/LeNet.model&apos;)</span><br><span class="line">    print(&quot;Model saved&quot;)</span><br><span class="line"></span><br><span class="line">    print(&quot;Evaluate The Model&quot;)</span><br><span class="line">    # TODO: 读取模型</span><br><span class="line">    saver.restore(sess, &apos;./model/LeNet.model&apos;)</span><br><span class="line">    test_accuracy = evaluate(x_test, y_test)</span><br><span class="line">    print(&quot;Test Accuracy = &#123;:.3f&#125;&quot;.format(test_accuracy))</span><br></pre></td></tr></table></figure><h3 id="执行结果"><a href="#执行结果" class="headerlink" title="执行结果"></a>执行结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">FUNCTION READY!!</span><br><span class="line">Training.....</span><br><span class="line">EPOCH 1 ...</span><br><span class="line">Validation Accuracy = 0.969</span><br><span class="line">EPOCH 2 ...</span><br><span class="line">Validation Accuracy = 0.974</span><br><span class="line">EPOCH 3 ...</span><br><span class="line">Validation Accuracy = 0.979</span><br><span class="line">EPOCH 4 ...</span><br><span class="line">Validation Accuracy = 0.981</span><br><span class="line">EPOCH 5 ...</span><br><span class="line">Validation Accuracy = 0.984</span><br><span class="line">EPOCH 6 ...</span><br><span class="line">Validation Accuracy = 0.986</span><br><span class="line">EPOCH 7 ...</span><br><span class="line">Validation Accuracy = 0.986</span><br><span class="line">EPOCH 8 ...</span><br><span class="line">Validation Accuracy = 0.986</span><br><span class="line">EPOCH 9 ...</span><br><span class="line">Validation Accuracy = 0.986</span><br><span class="line">EPOCH 10 ...</span><br><span class="line">Validation Accuracy = 0.984</span><br><span class="line">Model saved</span><br><span class="line">Evaluate The Model</span><br><span class="line">Test Accuracy = 0.988</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow实现简单的卷积神经网络-CNN</title>
      <link href="/2018/08/23/2018-08-23-DL-CNN-python/"/>
      <url>/2018/08/23/2018-08-23-DL-CNN-python/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="环境设定"><a href="#环境设定" class="headerlink" title="环境设定"></a>环境设定</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos;</span><br></pre></td></tr></table></figure><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 使用tensorflow自带的工具加载MNIST手写数字集合</span><br><span class="line">mnist = input_data.read_data_sets(&apos;data/mnist&apos;, one_hot=True)</span><br><span class="line"></span><br><span class="line"># 查看数据的维度和target的维度</span><br><span class="line">print(mnist.train.images.shape)</span><br><span class="line">print(mnist.train.labels.shape)</span><br></pre></td></tr></table></figure><h3 id="准备好placeholder"><a href="#准备好placeholder" class="headerlink" title="准备好placeholder"></a>准备好placeholder</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = tf.placeholder(tf.float32, [None, 784])</span><br><span class="line">Y = tf.placeholder(tf.float32, [None, 10])</span><br><span class="line">learnRate = tf.placeholder(tf.float32)</span><br></pre></td></tr></table></figure><h3 id="构建网络计算图结构"><a href="#构建网络计算图结构" class="headerlink" title="构建网络计算图结构"></a>构建网络计算图结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"># 把输入数据reshape--28x28=784, 单通道， -1表示None</span><br><span class="line">with tf.name_scope(&apos;reshape&apos;):</span><br><span class="line">    x_image = tf.reshape(x, [-1, 28, 28, 1])</span><br><span class="line"></span><br><span class="line"># 构建第一层卷积计算层--将一个灰度图像映射到32个feature maps, 卷积核为5x5</span><br><span class="line">with tf.name_scope(&apos;conv1&apos;):</span><br><span class="line">    shape = [5, 5, 1, 32]</span><br><span class="line">    W_conv1 = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1),</span><br><span class="line">                          collections=[tf.GraphKeys.GLOBAL_VARIABLES, &apos;WEIGHTS&apos;])</span><br><span class="line"></span><br><span class="line">    shape = [32]</span><br><span class="line">    b_conv1 = tf.Variable(tf.constant(0.1, shape=shape))</span><br><span class="line"></span><br><span class="line">    net_conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1],</span><br><span class="line">                             padding=&apos;SAME&apos;) + b_conv1</span><br><span class="line"></span><br><span class="line">    out_conv1 = tf.nn.relu(net_conv1)</span><br><span class="line"></span><br><span class="line"># 构建池化层--采用最大池化</span><br><span class="line">with tf.name_scope(&apos;pool1&apos;):</span><br><span class="line">    h_pool1 = tf.nn.max_pool(out_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],</span><br><span class="line">                             padding=&apos;VALID&apos;)</span><br><span class="line"></span><br><span class="line"># 构建第二层卷积计算层--maps 32 feature maps to 64.</span><br><span class="line">with tf.name_scope(&apos;conv2&apos;):</span><br><span class="line">    shape = [5, 5, 32, 64]</span><br><span class="line">    W_conv2 = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1),</span><br><span class="line">                          collections=[tf.GraphKeys.GLOBAL_VARIABLES, &apos;WEIGHTS&apos;])</span><br><span class="line"></span><br><span class="line">    shape = [64]</span><br><span class="line">    b_conv2 = tf.Variable(tf.constant(0.1, shape=shape))</span><br><span class="line"></span><br><span class="line">    net_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1],</span><br><span class="line">                             padding=&apos;SAME&apos;) + b_conv2</span><br><span class="line"></span><br><span class="line">    out_conv2 = tf.nn.relu(net_conv2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 构建第二层池化层--采用最大池化</span><br><span class="line">with tf.name_scope(&apos;pool2&apos;):</span><br><span class="line">    h_pool2 = tf.nn.max_pool(out_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],</span><br><span class="line">                             padding=&apos;VALID&apos;)</span><br><span class="line"></span><br><span class="line"># 构建全连接层--经过的两层的下采样（池化），28x28x1的图像--&gt;7x7x64，然后映射到1024个特征</span><br><span class="line">with tf.name_scope(&apos;fc1&apos;):</span><br><span class="line">    shape = [7*7*64, 1024]</span><br><span class="line">    W_fc1 = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1),</span><br><span class="line">                        collections=[tf.GraphKeys.GLOBAL_VARIABLES, &apos;WEIGHTS&apos;])</span><br><span class="line"></span><br><span class="line">    shape = [1024]</span><br><span class="line">    b_fc1 = tf.Variable(tf.constant(0.1, shape=shape))</span><br><span class="line"></span><br><span class="line">    shape = [-1, 7*7*64]</span><br><span class="line">    h_pool2_flat = tf.reshape(h_pool2, shape)</span><br><span class="line"></span><br><span class="line">    out_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"># Dropout--防止过拟合</span><br><span class="line">with tf.name_scope(&apos;dropout&apos;):</span><br><span class="line">    keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">    out_fc1_drop = tf.nn.dropout(out_fc1, keep_prob=keep_prob)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 构建第二层全连接层--将1024个特性映射到10个类，每个类对应一个数字</span><br><span class="line">with tf.name_scope(&apos;fc2&apos;):</span><br><span class="line">    shape = [1024, 10]</span><br><span class="line">    W_fc2 = tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1),</span><br><span class="line">                        collections=[tf.GraphKeys.GLOBAL_VARIABLES, &apos;WEIGHTS&apos;])</span><br><span class="line"></span><br><span class="line">    shape = [10]</span><br><span class="line">    b_fc2 = tf.Variable(tf.constant(0.1, shape=shape))</span><br><span class="line"></span><br><span class="line">    out = tf.matmul(out_fc1_drop, W_fc2) + b_fc2</span><br></pre></td></tr></table></figure><h3 id="计算损失函数并初始化optimizer"><a href="#计算损失函数并初始化optimizer" class="headerlink" title="计算损失函数并初始化optimizer"></a>计算损失函数并初始化optimizer</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">print(y.shape, out.shape)</span><br><span class="line">cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out))</span><br><span class="line"></span><br><span class="line">l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])</span><br><span class="line"></span><br><span class="line">total_loss = cross_entropy + 7e-5*l2_loss</span><br><span class="line"></span><br><span class="line">train_step = tf.train.AdamOptimizer(learnRate).minimize(total_loss)</span><br><span class="line">print(&quot;FUNCTIONS READY!!&quot;)</span><br></pre></td></tr></table></figure><h3 id="初始化变量"><a href="#初始化变量" class="headerlink" title="初始化变量"></a>初始化变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h3 id="在session中执行graph定义的运算"><a href="#在session中执行graph定义的运算" class="headerlink" title="在session中执行graph定义的运算"></a>在session中执行graph定义的运算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    for step in range(3000):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(100)</span><br><span class="line">        lr = 0.01</span><br><span class="line"></span><br><span class="line">        _, loss, l2_loss_value, total_loss_value = sess.run(</span><br><span class="line">            [train_step, cross_entropy, l2_loss, total_loss],</span><br><span class="line">            feed_dict=&#123;x: batch_xs, y: batch_ys, learnRate: lr, keep_prob: 0.5&#125;)</span><br><span class="line"></span><br><span class="line">        if (step + 1) % 100 == 0:</span><br><span class="line">            print(&quot;step %d, entropy loss: %f, l2_loss: %f, total loss: %f&quot; %</span><br><span class="line">                  (step + 1, loss, l2_loss_value, total_loss_value))</span><br><span class="line"></span><br><span class="line">            # 验证训练的模型</span><br><span class="line">            correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))</span><br><span class="line">            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">            print(&quot;Train accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: 0.5&#125;))</span><br><span class="line"></span><br><span class="line">        if (step + 1) % 1000 == 0:</span><br><span class="line">            print(&quot;Text accuracy:&quot;, sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: 0.5&#125;))</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">step 2100, entropy loss: 0.127328, l2_loss: 1764.112305, total loss: 0.250816</span><br><span class="line">Train accuracy: 0.99</span><br><span class="line">step 2200, entropy loss: 0.275415, l2_loss: 1819.110107, total loss: 0.402753</span><br><span class="line">Train accuracy: 0.97</span><br><span class="line">step 2300, entropy loss: 0.147261, l2_loss: 1773.890869, total loss: 0.271434</span><br><span class="line">Train accuracy: 0.99</span><br><span class="line">step 2400, entropy loss: 0.100819, l2_loss: 1699.657349, total loss: 0.219795</span><br><span class="line">Train accuracy: 0.97</span><br><span class="line">step 2500, entropy loss: 0.148775, l2_loss: 1701.602661, total loss: 0.267887</span><br><span class="line">Train accuracy: 0.96</span><br><span class="line">step 2600, entropy loss: 0.142768, l2_loss: 1800.301880, total loss: 0.268789</span><br><span class="line">Train accuracy: 0.96</span><br><span class="line">step 2700, entropy loss: 0.134457, l2_loss: 2041.165161, total loss: 0.277339</span><br><span class="line">Train accuracy: 0.96</span><br><span class="line">step 2800, entropy loss: 0.071112, l2_loss: 2003.971924, total loss: 0.211390</span><br><span class="line">Train accuracy: 0.99</span><br><span class="line">step 2900, entropy loss: 0.217092, l2_loss: 1879.004272, total loss: 0.348622</span><br><span class="line">Train accuracy: 0.98</span><br><span class="line">step 3000, entropy loss: 0.207697, l2_loss: 1841.294556, total loss: 0.336588</span><br><span class="line">Train accuracy: 0.97</span><br><span class="line">Text accuracy: 0.95</span><br><span class="line">Optimization Finished</span><br></pre></td></tr></table></figure><h4 id="结论：简单写了个卷积神经网络，没有进行参数调试。"><a href="#结论：简单写了个卷积神经网络，没有进行参数调试。" class="headerlink" title="结论：简单写了个卷积神经网络，没有进行参数调试。"></a>结论：简单写了个卷积神经网络，没有进行参数调试。</h4><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络之卷积神经网络反向传播算法</title>
      <link href="/2018/08/22/2018-08-22-CNN-back-propagation/"/>
      <url>/2018/08/22/2018-08-22-CNN-back-propagation/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="回顾DNN的反向传播算法"><a href="#回顾DNN的反向传播算法" class="headerlink" title="回顾DNN的反向传播算法"></a>回顾DNN的反向传播算法</h3><p>需要在理解深度神经网络的反向传播算来做预先理解，见<a href="https://sevenold.github.io/2018/08/DL-back-propagation/" target="_blank" rel="noopener">神经网络入门-反向传播</a>所总结的反向传播四部曲：</p><h4 id="反向传播的四项基本原则："><a href="#反向传播的四项基本原则：" class="headerlink" title="反向传播的四项基本原则："></a><strong>反向传播的四项基本原则</strong>：</h4><h4 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式:"></a>基本形式:</h4><ul><li><h4 id="delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L"><a href="#delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L" class="headerlink" title="$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)}) $"></a>$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times \sigma^{\prime}(net_i^{(L)}) $</h4></li><li><h4 id="delta-l-sum-j-delta-j-l-1-w-ji-l-1-sigma-prime-net-i-l"><a href="#delta-l-sum-j-delta-j-l-1-w-ji-l-1-sigma-prime-net-i-l" class="headerlink" title="$\delta^{(l)} = \sum_j \delta_j^{(l+1)} w_{ji}^{(l+1)}  \sigma^{\prime}(net_i^{(l)})$"></a>$\delta^{(l)} = \sum_j \delta_j^{(l+1)} w_{ji}^{(l+1)} \sigma^{\prime}(net_i^{(l)})$</h4></li><li><h4 id="frac-partial-E-total-partial-bias-i-l-delta-i-l"><a href="#frac-partial-E-total-partial-bias-i-l-delta-i-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l"><a href="#frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$</h4></li></ul><h4 id="矩阵形式："><a href="#矩阵形式：" class="headerlink" title="矩阵形式："></a>矩阵形式：</h4><ul><li><h4 id="delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L-，-bigodot-是Hadamard乘积（对应位置相乘）"><a href="#delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L-，-bigodot-是Hadamard乘积（对应位置相乘）" class="headerlink" title="$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot   \sigma^{\prime}(net_i^{(L)}) $ ， $\bigodot$是Hadamard乘积（对应位置相乘）"></a>$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot \sigma^{\prime}(net_i^{(L)}) $ ， $\bigodot$是Hadamard乘积（对应位置相乘）</h4></li><li><h4 id="delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l"><a href="#delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l" class="headerlink" title="$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$"></a>$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot \sigma^{\prime}(net^{(l)})$</h4></li><li><h4 id="frac-partial-E-total-partial-bias-l-delta-l"><a href="#frac-partial-E-total-partial-bias-l-delta-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias^{(l)}}=\delta^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias^{(l)}}=\delta^{(l)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T"><a href="#frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T" class="headerlink" title="$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$"></a>$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$</h4></li></ul><p>现在我们想把同样的思想用到卷积神经网络中，但是很明显，CNN有很多不同的地方（卷积计算层，池化层），不能直接去套用DNN的反向传播公式。</p><h3 id="卷积神经网络的反向传播算法"><a href="#卷积神经网络的反向传播算法" class="headerlink" title="卷积神经网络的反向传播算法"></a>卷积神经网络的反向传播算法</h3><p>卷积神经网络相比于多层感知机，增加了两种新的层次——卷积层与池化层。由于反向传播链的存在，要求出这两种层结构的梯度，仅需要解决输出对权值的梯度即可。 接下来我们根据卷积神经网络的网络结构来更新反向传播四部曲。</p><p>我们在推导DNN的反向传播的时候，推导过程中使用了：</p><h4 id="output-l-1-sigma-W-cdot-output-l-b"><a href="#output-l-1-sigma-W-cdot-output-l-b" class="headerlink" title="$output^{(l+1)}=\sigma(W \cdot output^{(l)}+b)$"></a>$output^{(l+1)}=\sigma(W \cdot output^{(l)}+b)$</h4><p>而对于卷积神经网络，我们使用卷积和池化的公式进行重新推导：</p><h4 id="卷积层：-output-l-1-sigma-output-l-W-b"><a href="#卷积层：-output-l-1-sigma-output-l-W-b" class="headerlink" title="卷积层：$output^{(l+1)}=\sigma( output^{(l)}*W+b)$"></a>卷积层：$output^{(l+1)}=\sigma( output^{(l)}*W+b)$</h4><h4 id="池化层：-output-l-1-subsampling-output-l-b"><a href="#池化层：-output-l-1-subsampling-output-l-b" class="headerlink" title="池化层：$output^{(l+1)}=subsampling( output^{(l)}+b)$"></a>池化层：$output^{(l+1)}=subsampling( output^{(l)}+b)$</h4><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>我们在回忆下：</p><h4 id="delta-l-frac-partial-E-total-partial-net-l"><a href="#delta-l-frac-partial-E-total-partial-net-l" class="headerlink" title="$\delta^{(l)}=\frac{\partial E_{total}}{\partial net^{(l)}} $"></a>$\delta^{(l)}=\frac{\partial E_{total}}{\partial net^{(l)}} $</h4><h4 id="frac-partial-E-total-partial-net-l-1-cdot-frac-partial-net-l-1-partial-net-l"><a href="#frac-partial-E-total-partial-net-l-1-cdot-frac-partial-net-l-1-partial-net-l" class="headerlink" title="$= \frac{\partial E_{total}}{\partial net^{(l+1)}} \cdot \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$"></a>$= \frac{\partial E_{total}}{\partial net^{(l+1)}} \cdot \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-times-frac-partial-（output-l-W-b）-partial-net-l"><a href="#delta-l-1-times-frac-partial-（output-l-W-b）-partial-net-l" class="headerlink" title="$= \delta^{(l+1)} \times  \frac{\partial （output^{(l)}*W+b）}{\partial net^{(l)}}$"></a>$= \delta^{(l+1)} \times \frac{\partial （output^{(l)}*W+b）}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-times-frac-partial-（output-l-W-b）-partial-out-l-times-frac-partial-out-l-partial-net-l"><a href="#delta-l-1-times-frac-partial-（output-l-W-b）-partial-out-l-times-frac-partial-out-l-partial-net-l" class="headerlink" title="$= \delta^{(l+1)} \times   \frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}} \times \frac{\partial out^{(l)}}{\partial net^{(l)}}$"></a>$= \delta^{(l+1)} \times \frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}} \times \frac{\partial out^{(l)}}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-cdot-frac-partial-（output-l-W-b）-partial-out-l-cdot-sigma-prime-net-i-L"><a href="#delta-l-1-cdot-frac-partial-（output-l-W-b）-partial-out-l-cdot-sigma-prime-net-i-L" class="headerlink" title="$= \delta^{(l+1)} \cdot  \frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}}  \cdot \sigma^{\prime}(net_i^{(L)})$"></a>$= \delta^{(l+1)} \cdot \frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}} \cdot \sigma^{\prime}(net_i^{(L)})$</h4><p>接下来，就是计算卷积运算的求导：</p><h4 id="frac-partial-（output-l-W-b）-partial-out-l"><a href="#frac-partial-（output-l-W-b）-partial-out-l" class="headerlink" title="$\frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}}$"></a>$\frac{\partial （output^{(l)}*W+b）}{\partial out^{(l)}}$</h4><p>假如我们要处理如下的卷积操作：</p><h4 id="left-begin-array-ccc-a-11-amp-a-12-amp-a-13-a-21-amp-a-22-amp-a-23-a-31-amp-a-32-amp-a-33-end-array-right-left-begin-array-ccc-w-11-amp-w-12-w-21-amp-w-22-end-array-right-left-begin-array-ccc-z-11-amp-z-12-z-21-amp-z-22-end-array-right"><a href="#left-begin-array-ccc-a-11-amp-a-12-amp-a-13-a-21-amp-a-22-amp-a-23-a-31-amp-a-32-amp-a-33-end-array-right-left-begin-array-ccc-w-11-amp-w-12-w-21-amp-w-22-end-array-right-left-begin-array-ccc-z-11-amp-z-12-z-21-amp-z-22-end-array-right" class="headerlink" title="$\left( \begin{array}{ccc} a_{11}&amp;a_{12}&amp;a_{13} \\ a_{21}&amp;a_{22}&amp;a_{23} \\ a_{31}&amp;a_{32}&amp;a_{33} \end{array} \right)    *  \left( \begin{array}{ccc} w_{11}&amp;w_{12} \\ w_{21}&amp;w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&amp;z_{12} \\ z_{21}&amp;z_{22} \end{array} \right)$"></a>$\left( \begin{array}{ccc} a_{11}&amp;a_{12}&amp;a_{13} \\ a_{21}&amp;a_{22}&amp;a_{23} \\ a_{31}&amp;a_{32}&amp;a_{33} \end{array} \right) * \left( \begin{array}{ccc} w_{11}&amp;w_{12} \\ w_{21}&amp;w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&amp;z_{12} \\ z_{21}&amp;z_{22} \end{array} \right)$</h4><p>从这步操作来看，套用DNN反向传播的思想是行不通的，所以我们需要把卷积操作表示成如下等式。</p><h4 id="z-11-a-11-w-11-a-12-w-12-a-21-w-21-a-22-w-22-z-12-a-12-w-11-a-13-w-12-a-22-w-21-a-23-w-22-z-21-a-21-w-11-a-22-w-12-a-31-w-21-a-32-w-22-z-22-a-22-w-11-a-23-w-12-a-32-w-21-a-33-w-22"><a href="#z-11-a-11-w-11-a-12-w-12-a-21-w-21-a-22-w-22-z-12-a-12-w-11-a-13-w-12-a-22-w-21-a-23-w-22-z-21-a-21-w-11-a-22-w-12-a-31-w-21-a-32-w-22-z-22-a-22-w-11-a-23-w-12-a-32-w-21-a-33-w-22" class="headerlink" title="$z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} +   a_{22}w_{22} \\ z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} +   a_{23}w_{22} \\ z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} +   a_{32}w_{22} \\ z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} +   a_{33}w_{22}$"></a>$z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22} \\ z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} + a_{23}w_{22} \\ z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} + a_{32}w_{22} \\ z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} + a_{33}w_{22}$</h4><p>对每个元素进行求导：</p><h4 id="begin-align-nabla-a-11-amp-frac-partial-C-partial-z-11-frac-partial-z-11-partial-a-11-frac-partial-C-partial-z-12-frac-partial-z-12-partial-a-11-frac-partial-C-partial-z-21-frac-partial-z-21-partial-a-11-frac-partial-C-partial-z-22-frac-partial-z-22-partial-a-11-notag-amp-w-11-notag-end-align"><a href="#begin-align-nabla-a-11-amp-frac-partial-C-partial-z-11-frac-partial-z-11-partial-a-11-frac-partial-C-partial-z-12-frac-partial-z-12-partial-a-11-frac-partial-C-partial-z-21-frac-partial-z-21-partial-a-11-frac-partial-C-partial-z-22-frac-partial-z-22-partial-a-11-notag-amp-w-11-notag-end-align" class="headerlink" title="$\begin{align} \nabla a_{11} = &amp; \frac{\partial C}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{11}}+  \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial a_{11}}+ \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial a_{11}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial a_{11}} \notag \\ =&amp;w_{11} \notag \end{align}$"></a>$\begin{align} \nabla a_{11} = &amp; \frac{\partial C}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{11}}+ \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial a_{11}}+ \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial a_{11}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial a_{11}} \notag \\ =&amp;w_{11} \notag \end{align}$</h4><h4 id="begin-align-nabla-a-12-amp-frac-partial-C-partial-z-11-frac-partial-z-11-partial-a-12-frac-partial-C-partial-z-12-frac-partial-z-12-partial-a-12-frac-partial-C-partial-z-21-frac-partial-z-21-partial-a-12-frac-partial-C-partial-z-22-frac-partial-z-22-partial-a-12-notag-amp-w-12-w-11-notag-end-align"><a href="#begin-align-nabla-a-12-amp-frac-partial-C-partial-z-11-frac-partial-z-11-partial-a-12-frac-partial-C-partial-z-12-frac-partial-z-12-partial-a-12-frac-partial-C-partial-z-21-frac-partial-z-21-partial-a-12-frac-partial-C-partial-z-22-frac-partial-z-22-partial-a-12-notag-amp-w-12-w-11-notag-end-align" class="headerlink" title="$\begin{align} \nabla a_{12} =&amp; \frac{\partial C}{\partial z_{11}}\frac{\partial z_{11}}{\partial a_{12}} + \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial a_{12}} + \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial a_{12}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial a_{12}} \notag \\ =&amp;w_{12} +w_{11} \notag \end{align}$"></a>$\begin{align} \nabla a_{12} =&amp; \frac{\partial C}{\partial z_{11}}\frac{\partial z_{11}}{\partial a_{12}} + \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial a_{12}} + \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial a_{12}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial a_{12}} \notag \\ =&amp;w_{12} +w_{11} \notag \end{align}$</h4><p>同理可得其他的$\nabla a_{ij}$，当你求出全部的结果后，你会发现我们可以用一个卷积运算来解决(实际就是把原来的卷积核做了180°的转换)：</p><h4 id="left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-1-amp-1-amp-0-0-amp-1-amp-1-amp-0-0-amp-0-amp-0-amp-0-end-array-right-left-begin-array-ccc-w-22-amp-w-21-w-12-amp-w-11-end-array-right-left-begin-array-ccc-nabla-a-11-amp-nabla-a-12-amp-nabla-a-13-nabla-a-21-amp-nabla-a-22-amp-nabla-a-23-nabla-a-31-amp-nabla-a-32-amp-nabla-a-33-end-array-right"><a href="#left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-1-amp-1-amp-0-0-amp-1-amp-1-amp-0-0-amp-0-amp-0-amp-0-end-array-right-left-begin-array-ccc-w-22-amp-w-21-w-12-amp-w-11-end-array-right-left-begin-array-ccc-nabla-a-11-amp-nabla-a-12-amp-nabla-a-13-nabla-a-21-amp-nabla-a-22-amp-nabla-a-23-nabla-a-31-amp-nabla-a-32-amp-nabla-a-33-end-array-right" class="headerlink" title="$\left( \begin{array}{ccc} 0&amp;0&amp;0&amp;0 \\ 0&amp;\ 1&amp; 1&amp;0 \\ 0&amp;1 &amp;1&amp;0 \\ 0&amp;0&amp;0&amp;0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&amp;w_{21}\\ w_{12}&amp;w_{11} \end{array} \right)  = \left( \begin{array}{ccc} \nabla a_{11}&amp;\nabla a_{12}&amp;\nabla a_{13} \\ \nabla a_{21}&amp;\nabla a_{22}&amp;\nabla a_{23}\\ \nabla a_{31}&amp;\nabla a_{32}&amp;\nabla a_{33} \end{array} \right)$"></a>$\left( \begin{array}{ccc} 0&amp;0&amp;0&amp;0 \\ 0&amp;\ 1&amp; 1&amp;0 \\ 0&amp;1 &amp;1&amp;0 \\ 0&amp;0&amp;0&amp;0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&amp;w_{21}\\ w_{12}&amp;w_{11} \end{array} \right) = \left( \begin{array}{ccc} \nabla a_{11}&amp;\nabla a_{12}&amp;\nabla a_{13} \\ \nabla a_{21}&amp;\nabla a_{22}&amp;\nabla a_{23}\\ \nabla a_{31}&amp;\nabla a_{32}&amp;\nabla a_{33} \end{array} \right)$</h4><h4 id="最终我们就可以得到："><a href="#最终我们就可以得到：" class="headerlink" title="最终我们就可以得到："></a>最终我们就可以得到：</h4><h4 id="frac-partial-（output-l-W-b）-partial-out-l-left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-1-amp-1-amp-0-0-amp-1-amp-1-amp-0-0-amp-0-amp-0-amp-0-end-array-right-left-begin-array-ccc-w-22-amp-w-21-w-12-amp-w-11-end-array-right"><a href="#frac-partial-（output-l-W-b）-partial-out-l-left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-1-amp-1-amp-0-0-amp-1-amp-1-amp-0-0-amp-0-amp-0-amp-0-end-array-right-left-begin-array-ccc-w-22-amp-w-21-w-12-amp-w-11-end-array-right" class="headerlink" title="$\frac{\partial （output^{(l)}W+b）}{\partial out^{(l)}} =\left( \begin{array}{ccc} 0&amp;0&amp;0&amp;0 \\ 0&amp;\ 1&amp; 1&amp;0 \\ 0&amp;1 &amp;1&amp;0 \\ 0&amp;0&amp;0&amp;0 \end{array} \right)  \left( \begin{array}{ccc} w_{22}&amp;w_{21}\\ w_{12}&amp;w_{11} \end{array} \right)  $"></a>$\frac{\partial （output^{(l)}<em>W+b）}{\partial out^{(l)}} =\left( \begin{array}{ccc} 0&amp;0&amp;0&amp;0 \\ 0&amp;\ 1&amp; 1&amp;0 \\ 0&amp;1 &amp;1&amp;0 \\ 0&amp;0&amp;0&amp;0 \end{array} \right) </em>\left( \begin{array}{ccc} w_{22}&amp;w_{21}\\ w_{12}&amp;w_{11} \end{array} \right) $</h4><h4 id="所以原来的-delta-l-w-l-1-delta-l-1-bigodot-sigma-prime-net-l-就变为："><a href="#所以原来的-delta-l-w-l-1-delta-l-1-bigodot-sigma-prime-net-l-就变为：" class="headerlink" title="所以原来的$\delta^{(l)} = (w^{(l+1)}) \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$就变为："></a>所以原来的$\delta^{(l)} = (w^{(l+1)}) \delta^{(l+1)} \bigodot \sigma^{\prime}(net^{(l)})$就变为：</h4><h4 id="delta-l-w-l-1-rot180-W-bigodot-sigma-prime-net-l"><a href="#delta-l-w-l-1-rot180-W-bigodot-sigma-prime-net-l" class="headerlink" title="$\delta^{(l)} = (w^{(l+1)}) *rot180(W) \bigodot  \sigma^{\prime}(net^{(l)})$"></a>$\delta^{(l)} = (w^{(l+1)}) *rot180(W) \bigodot \sigma^{\prime}(net^{(l)})$</h4><h4 id="对于偏置项："><a href="#对于偏置项：" class="headerlink" title="对于偏置项："></a>对于偏置项：</h4><h4 id="原来的是：-frac-partial-E-total-partial-bias-i-l-delta-i-l"><a href="#原来的是：-frac-partial-E-total-partial-bias-i-l-delta-i-l" class="headerlink" title="原来的是：$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$"></a>原来的是：$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$</h4><h4 id="因为对于全连接网络：-b-delta-l-in-R-n-out-l"><a href="#因为对于全连接网络：-b-delta-l-in-R-n-out-l" class="headerlink" title="因为对于全连接网络：$b, \delta^{(l)} \in R(n_{out}^{(l)})$"></a>因为对于全连接网络：$b, \delta^{(l)} \in R(n_{out}^{(l)})$</h4><h4 id="而对于卷积神经网络：-b-in-R-n-out-l-delta-l-in-R-H-out-l-W-out-l-n-out-l"><a href="#而对于卷积神经网络：-b-in-R-n-out-l-delta-l-in-R-H-out-l-W-out-l-n-out-l" class="headerlink" title="而对于卷积神经网络：$b \in R(n_{out}^{(l)}), \delta^{(l)} \in R(H_{out}^{(l)}, W_{out}^{(l)}, n_{out}^{(l)})$"></a>而对于卷积神经网络：$b \in R(n_{out}^{(l)}), \delta^{(l)} \in R(H_{out}^{(l)}, W_{out}^{(l)}, n_{out}^{(l)})$</h4><h4 id="我们对应的处理方式就是："><a href="#我们对应的处理方式就是：" class="headerlink" title="我们对应的处理方式就是："></a>我们对应的处理方式就是：</h4><h4 id="将-delta-l-的H和W维度求和"><a href="#将-delta-l-的H和W维度求和" class="headerlink" title="将$\delta^{(l)}$的H和W维度求和"></a>将$\delta^{(l)}$的H和W维度求和</h4><h4 id="所以："><a href="#所以：" class="headerlink" title="所以："></a>所以：</h4><h4 id="frac-partial-E-total-partial-bias-i-l-sum-H-sum-W-delta-H-W-i-l"><a href="#frac-partial-E-total-partial-bias-i-l-sum-H-sum-W-delta-H-W-i-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\sum_H \sum_W \delta_{(H,W,i)}^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\sum_H \sum_W \delta_{(H,W,i)}^{(l)}$</h4><h3 id="卷积神经网络反向传播的四部曲"><a href="#卷积神经网络反向传播的四部曲" class="headerlink" title="卷积神经网络反向传播的四部曲"></a>卷积神经网络反向传播的四部曲</h3><ul><li><h4 id="delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L-1"><a href="#delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L-1" class="headerlink" title="$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)}) $"></a>$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times \sigma^{\prime}(net_i^{(L)}) $</h4></li><li><h4 id="delta-l-w-l-1-rot180-W-bigodot-sigma-prime-net-l-1"><a href="#delta-l-w-l-1-rot180-W-bigodot-sigma-prime-net-l-1" class="headerlink" title="$\delta^{(l)} = (w^{(l+1)}) *rot180(W) \bigodot  \sigma^{\prime}(net^{(l)})$"></a>$\delta^{(l)} = (w^{(l+1)}) *rot180(W) \bigodot \sigma^{\prime}(net^{(l)})$</h4></li><li><h4 id="frac-partial-E-total-partial-bias-i-l-sum-H-sum-W-delta-H-W-i-l-1"><a href="#frac-partial-E-total-partial-bias-i-l-sum-H-sum-W-delta-H-W-i-l-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\sum_H \sum_W \delta_{(H,W,i)}^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\sum_H \sum_W \delta_{(H,W,i)}^{(l)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-l-rot180-delta-l-outh-l-1"><a href="#frac-partial-E-total-partial-w-l-rot180-delta-l-outh-l-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial w^{(l)}}=rot180(\delta^{(l)}*(outh^{(l-1)}))$"></a>$\frac{\partial E_{total}}{\partial w^{(l)}}=rot180(\delta^{(l)}*(outh^{(l-1)}))$</h4></li></ul><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>跟卷积层一样，我们先把 pooling 层也放回网络连接的形式中：</p><p><img src="/images/dl/86.png" alt="images"></p><p>红色神经元是前一层的响应结果，一般是卷积后再用激活函数处理。绿色的神经元表示 pooling 层。</p><p>很明显，pooling 主要是起到<code>降维</code>的作用，而且，由于 pooling 时没有参数需要学习，</p><p>因此，当得到 pooling 层的误差项 $δ^l $后，我们只需要计算上一层的误差项 $δ^{l−1}$即可。</p><p>要注意的一点是，由于 pooling 一般会降维，因此传回去的误差矩阵要调整维度，即 upsample。这样，误差传播的公式原型大概是：</p><h4 id="delta-l-1-upsample-delta-l-odot-sigma’-z-l-1"><a href="#delta-l-1-upsample-delta-l-odot-sigma’-z-l-1" class="headerlink" title="$\delta^{l-1}=upsample(\delta^l) \odot \sigma’(z^{l-1})$"></a>$\delta^{l-1}=upsample(\delta^l) \odot \sigma’(z^{l-1})$</h4><p>下面以最常用的 <strong>average pooling</strong> 和 <strong>max pooling</strong> 为例，讲讲 $upsample(δ^l)$具体要怎么处理。</p><p>假设 pooling 层的区域大小为 2×22×2，pooling 这一层的误差项为：</p><h4 id="delta-l-left-begin-array-ccc-2-amp-8-4-amp-6-end-array-right"><a href="#delta-l-left-begin-array-ccc-2-amp-8-4-amp-6-end-array-right" class="headerlink" title="$\delta^l= \left( \begin{array}{ccc} 2 &amp; 8 \\ 4 &amp; 6 \end{array} \right)$"></a>$\delta^l= \left( \begin{array}{ccc} 2 &amp; 8 \\ 4 &amp; 6 \end{array} \right)$</h4><p>首先，我们先把维度还原到上一层的维度：</p><h4 id="left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-2-amp-8-amp-0-0-amp-4-amp-6-amp-0-0-amp-0-amp-0-amp-0-end-array-right"><a href="#left-begin-array-ccc-0-amp-0-amp-0-amp-0-0-amp-2-amp-8-amp-0-0-amp-4-amp-6-amp-0-0-amp-0-amp-0-amp-0-end-array-right" class="headerlink" title="$\left( \begin{array}{ccc} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 8 &amp; 0  \\ 0 &amp; 4 &amp; 6 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0  \end{array} \right)$"></a>$\left( \begin{array}{ccc} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 8 &amp; 0 \\ 0 &amp; 4 &amp; 6 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{array} \right)$</h4><p>在 average pooling 中，我们是把一个范围内的响应值取平均后，作为一个 pooling unit 的结果。可以认为是经过一个 <strong>average()</strong> 函数，即 $average(x)=\frac{1}{m}\sum_{k=1}^m x_k$ 在本例中，$m=4$。则对每个 $x_k$ 的导数均为：</p><h4 id="frac-partial-average-x-partial-x-k-frac-1-m"><a href="#frac-partial-average-x-partial-x-k-frac-1-m" class="headerlink" title="$\frac{\partial average(x)}{\partial x_k}=\frac{1}{m}$"></a>$\frac{\partial average(x)}{\partial x_k}=\frac{1}{m}$</h4><p>因此，对 average pooling 来说，其误差项为：</p><h4 id="begin-align-delta-l-1-amp-delta-l-frac-partial-average-partial-x-odot-sigma’-z-l-1-notag-amp-upsample-delta-l-odot-sigma’-z-l-1-amp-left-begin-array-ccc-0-5-amp-0-5-amp-2-amp-2-0-5-amp-0-5-amp-2-amp-2-1-amp-1-amp-1-5-amp-1-5-1-amp-1-amp-1-5-amp-1-5-end-array-right-odot-sigma’-z-l-1-end-align"><a href="#begin-align-delta-l-1-amp-delta-l-frac-partial-average-partial-x-odot-sigma’-z-l-1-notag-amp-upsample-delta-l-odot-sigma’-z-l-1-amp-left-begin-array-ccc-0-5-amp-0-5-amp-2-amp-2-0-5-amp-0-5-amp-2-amp-2-1-amp-1-amp-1-5-amp-1-5-1-amp-1-amp-1-5-amp-1-5-end-array-right-odot-sigma’-z-l-1-end-align" class="headerlink" title="$\begin{align} \delta^{l-1}=&amp;\delta^l \frac{\partial average}{\partial x} \odot \sigma’(z^{l-1}) \notag \\ =&amp;upsample(\delta^l) \odot \sigma’(z^{l-1})  \\  =&amp;\left( \begin{array}{ccc} 0.5&amp;0.5&amp;2&amp;2 \\ 0.5&amp;0.5&amp;2&amp;2 \\ 1&amp;1&amp;1.5&amp;1.5 \\ 1&amp;1&amp;1.5&amp;1.5 \end{array} \right)\odot \sigma’(z^{l-1})   \end{align}$"></a>$\begin{align} \delta^{l-1}=&amp;\delta^l \frac{\partial average}{\partial x} \odot \sigma’(z^{l-1}) \notag \\ =&amp;upsample(\delta^l) \odot \sigma’(z^{l-1}) \\ =&amp;\left( \begin{array}{ccc} 0.5&amp;0.5&amp;2&amp;2 \\ 0.5&amp;0.5&amp;2&amp;2 \\ 1&amp;1&amp;1.5&amp;1.5 \\ 1&amp;1&amp;1.5&amp;1.5 \end{array} \right)\odot \sigma’(z^{l-1}) \end{align}$</h4><p>在 max pooling 中，则是经过一个 <strong>max()</strong> 函数，对应的导数为：</p><h4 id="frac-partial-max-x-partial-x-k-begin-cases-1-amp-if-x-k-max-x-0-amp-otherwise-end-cases"><a href="#frac-partial-max-x-partial-x-k-begin-cases-1-amp-if-x-k-max-x-0-amp-otherwise-end-cases" class="headerlink" title="$\frac{\partial \max(x)}{\partial x_k}=\begin{cases} 1 &amp; if\ x_k=max(x) \ 0 &amp; otherwise \end{cases}$"></a>$\frac{\partial \max(x)}{\partial x_k}=\begin{cases} 1 &amp; if\ x_k=max(x) \ 0 &amp; otherwise \end{cases}$</h4><p>假设前向传播时记录的最大值位置分别是左上、右下、右上、左下，则误差项为：</p><h4 id="delta-l-1-left-begin-array-ccc-2-amp-0-amp-0-amp-0-0-amp-0-amp-0-amp-8-0-amp-4-amp-0-amp-0-0-amp-0-amp-6-amp-0-end-array-right-odot-sigma’-z-l-1"><a href="#delta-l-1-left-begin-array-ccc-2-amp-0-amp-0-amp-0-0-amp-0-amp-0-amp-8-0-amp-4-amp-0-amp-0-0-amp-0-amp-6-amp-0-end-array-right-odot-sigma’-z-l-1" class="headerlink" title="$\delta^{l-1}=\left( \begin{array}{ccc} 2&amp;0&amp;0&amp;0 \\ 0&amp;0&amp; 0&amp;8 \\ 0&amp;4&amp;0&amp;0 \\ 0&amp;0&amp;6&amp;0 \end{array} \right) \odot \sigma’(z^{l-1})  $"></a>$\delta^{l-1}=\left( \begin{array}{ccc} 2&amp;0&amp;0&amp;0 \\ 0&amp;0&amp; 0&amp;8 \\ 0&amp;4&amp;0&amp;0 \\ 0&amp;0&amp;6&amp;0 \end{array} \right) \odot \sigma’(z^{l-1}) $</h4><p>总结：在遇到池化层的时间就执行upsampling的反向操作。就这样实现了池化层的反向传播。</p><h4 id="这就是卷积神经网络的反向传播算法"><a href="#这就是卷积神经网络的反向传播算法" class="headerlink" title="这就是卷积神经网络的反向传播算法"></a>这就是卷积神经网络的反向传播算法</h4><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> 反向传播 </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络之卷积神经网络</title>
      <link href="/2018/08/21/2018-08-21-DL-CNN/"/>
      <url>/2018/08/21/2018-08-21-DL-CNN/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="卷积神经网络（Convolutional-Neural-Network-CNN）"><a href="#卷积神经网络（Convolutional-Neural-Network-CNN）" class="headerlink" title="卷积神经网络（Convolutional Neural Network, CNN）"></a>卷积神经网络（Convolutional Neural Network, CNN）</h3><p><code>卷积神经网络</code>是近年来广泛应用于模式识别、图像处理等领域的一种高效识别算法，它具有结构简单、训练参数少和适应性强等特点。</p><p><code>卷积神经网络</code>是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。</p><p><code>卷积神经网络</code>由一个或多个卷积计算层和顶端的全连接层（经典神经网络）组成，同时也包括关联权重和池化层。这一结构是的卷积神经网络能够利用输入数据的二维结构。与其他神经网络结构相比，卷积神经网络在图像和语音识别方面能够给出更优的结果。相比较其他深度、前馈神经网络，卷积神经网络需要估计的参数更少，从而使之成为一种颇具吸引力的深度学习框架。</p><p>我们前面所接触到的神经网络的结构是这样的：</p><p><img src="/images/dl/65.png" alt="images"></p><p>卷积神经网络依旧是层级网络，只是层的功能和形式做了变化，也可以是传统神经网络的一个改进，不同的层次有不同运算与功能，如下图中就多了很多传统神经网络中所没有的层次。</p><p><img src="/images/dl/66.png" alt="images"></p><h3 id="卷积神经网络的核心思想"><a href="#卷积神经网络的核心思想" class="headerlink" title="卷积神经网络的核心思想"></a>卷积神经网络的核心思想</h3><ul><li><code>局部感受野</code>：普通的多层感知器中，隐层节点会全连接到一个图像的每个像素点上；而在卷积神经网络中，每个隐层的节点只连接到图像某个足够小的像素点上，从而大大减少需要训练的权重参数。</li><li><code>权值共享</code>：在卷积神经网中，同一个卷积核内，所有的神经元的权值是相同的，从而大大减少需要训练的参数。</li><li><code>池化</code>：在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。</li></ul><h3 id="卷积神经网络的层级结构"><a href="#卷积神经网络的层级结构" class="headerlink" title="卷积神经网络的层级结构"></a>卷积神经网络的层级结构</h3><p><img src="/images/dl/71.png" alt="images"></p><ul><li><h4 id="数据输入层（Input-layer）"><a href="#数据输入层（Input-layer）" class="headerlink" title="数据输入层（Input layer）"></a><strong>数据输入层（Input layer）</strong></h4></li><li><h4 id="卷积计算层（CONV-layer）"><a href="#卷积计算层（CONV-layer）" class="headerlink" title="卷积计算层（CONV layer）"></a><strong>卷积计算层（CONV layer）</strong></h4></li><li><h4 id="ReLU激活层（ReLU-layer）"><a href="#ReLU激活层（ReLU-layer）" class="headerlink" title="ReLU激活层（ReLU layer）"></a><strong>ReLU激活层（ReLU layer）</strong></h4></li><li><h4 id="池化层（POOling-layer）"><a href="#池化层（POOling-layer）" class="headerlink" title="池化层（POOling layer）"></a><strong>池化层（POOling layer）</strong></h4></li><li><h4 id="全连接层（FC-layer）"><a href="#全连接层（FC-layer）" class="headerlink" title="全连接层（FC layer）"></a><strong>全连接层（FC layer）</strong></h4></li></ul><h3 id="数据输入层（Input-layer）-1"><a href="#数据输入层（Input-layer）-1" class="headerlink" title="数据输入层（Input layer）"></a><strong>数据输入层（Input layer）</strong></h3><h4 id="任务：主要是对原始图像进行预处理"><a href="#任务：主要是对原始图像进行预处理" class="headerlink" title="任务：主要是对原始图像进行预处理"></a><code>任务</code>：主要是对原始图像进行预处理</h4><ul><li><h4 id="去均值：把输入数据的各个维度都中心化到0。"><a href="#去均值：把输入数据的各个维度都中心化到0。" class="headerlink" title="去均值：把输入数据的各个维度都中心化到0。"></a><code>去均值</code>：把输入数据的各个维度都中心化到0。</h4></li><li><h4 id="归一化：把数据的幅度归一化到同样的范围。"><a href="#归一化：把数据的幅度归一化到同样的范围。" class="headerlink" title="归一化：把数据的幅度归一化到同样的范围。"></a><code>归一化</code>：把数据的幅度归一化到同样的范围。</h4></li><li><h4 id="PCA-白化：用PCA降维、白化是对数据的每个特征轴上的幅度归一化。"><a href="#PCA-白化：用PCA降维、白化是对数据的每个特征轴上的幅度归一化。" class="headerlink" title="PCA/白化：用PCA降维、白化是对数据的每个特征轴上的幅度归一化。"></a><code>PCA/白化</code>：用PCA降维、白化是对数据的每个特征轴上的幅度归一化。</h4></li></ul><h4 id="举个栗子："><a href="#举个栗子：" class="headerlink" title="举个栗子："></a>举个栗子：</h4><h4 id="去均值与归一化"><a href="#去均值与归一化" class="headerlink" title="去均值与归一化"></a><code>去均值与归一化</code></h4><p>去均值的目的在于把样本的中心拉回到坐标系原点上；归一化的目的就是减少各维度数据取值范围的差异而带来的干扰。比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。</p><p><img src="/images/dl/67.png" alt="images"></p><h4 id="去相关与白化"><a href="#去相关与白化" class="headerlink" title="去相关与白化"></a><code>去相关与白化</code></h4><p><img src="/images/dl/68.png" alt="images"></p><h3 id="卷积计算层（CONV-layer）-1"><a href="#卷积计算层（CONV-layer）-1" class="headerlink" title="卷积计算层（CONV layer）"></a><strong>卷积计算层（CONV layer）</strong></h3><h4 id="任务：对输入的图像进行特征提取"><a href="#任务：对输入的图像进行特征提取" class="headerlink" title="任务：对输入的图像进行特征提取"></a><code>任务</code>：对输入的图像进行特征提取</h4><p>卷积计算层是卷积神经网络中最重要的一个层次，也是“卷积神经网络”的名字来源。</p><p><img src="/images/dl/75.gif" alt="images"></p><h4 id="举个栗子：-1"><a href="#举个栗子：-1" class="headerlink" title="举个栗子："></a>举个栗子：</h4><p>假设一张图像有 5x5 个像素，1 代表白，0 代表黑，这幅图像被视为 5x5 的单色图像。现在用一个由随机地 0 和 1 组成的 3x3 矩阵去和图像中的子区域做Hadamard乘积，每次迭代移动一个像素，这样该乘法会得到一个新的 3x3 的矩阵。下面的动图展示了这个过程。</p><p><img src="/images/dl/69.gif" alt="images"></p><h4 id="直观上来理解："><a href="#直观上来理解：" class="headerlink" title="直观上来理解："></a><strong>直观上来理解</strong>：</h4><ul><li>用一个小的权重矩阵去覆盖输入数据，对应位置元素加权相乘，其和作为结果的一个像素点。</li><li>这个权重在输入数据上滑动，形成一张新的矩阵</li><li>这个权重矩阵就被称为<code>卷积核</code>（convolution kernel）</li><li>其覆盖的位置称为<code>感受野</code>（receptive fileld ）</li><li>生成的新矩阵叫做<code>特征图</code>（feature map）</li></ul><p>分解开来，就如下图所示：</p><p><img src="/images/dl/72.png" alt="images"></p><h4 id="其中："><a href="#其中：" class="headerlink" title="其中："></a>其中：</h4><ul><li><p>滑动的像素数量就叫做<code>步长</code>（stride），步长为1，表示跳过1个像素，步长为2，就表示跳过2个像素，以此类推</p><p><img src="/images/dl/76.gif" alt="images"></p></li><li><p>以卷积核的边还是中心点作为开始/结束的依据，决定了卷积的<code>补齐</code>（padding）方式。前面我们所举的栗子是<code>valid</code>方式，而<code>same</code>方式则会在图像的边缘用0补齐，如将前面的<code>valid</code>改为<code>same</code>方式，如图所示：</p></li></ul><p><img src="/images/dl/73.png" alt="images"></p><p>其采样方式对应变换为：</p><p><img src="/images/dl/77.gif" alt="images"></p><ul><li>我们前面所提到的输入图像都是灰色的，只有一个通道，但是我们一般会遇到输入通道不只有一个，那么卷积核是三阶的，也就是说所有的通道的结果做累加。</li></ul><p><img src="/images/dl/74.png" alt="images"></p><p><img src="/images/dl/78.gif" alt="images"></p><p><img src="/images/dl/79.gif" alt="images"></p><p>当然，最后，这里有一个术语：“<code>偏置</code>（bias）”，每个输出滤波器都有一个偏置项，偏置被添加到输出通道产生最终输出通道。</p><p><img src="/images/dl/80.gif" alt="images"></p><h4 id="再举个栗子："><a href="#再举个栗子：" class="headerlink" title="再举个栗子："></a>再举个栗子：</h4><p>下面蓝色矩阵周围有一圈灰色的框，那些就是上面所说到的填充值 (padding=same)的方式。这里的蓝色矩阵就是输入的图像，粉色矩阵就是卷积层的神经元，这里表示了有两个神经元（w0,w1）。绿色矩阵就是经过卷积运算后的输出矩阵，这里的步长设置为2。</p><p><img src="/images/dl/70.gif" alt="images"></p><h3 id="ReLU激活层（ReLU-layer）-1"><a href="#ReLU激活层（ReLU-layer）-1" class="headerlink" title="ReLU激活层（ReLU layer）"></a><strong>ReLU激活层（ReLU layer）</strong></h3><h4 id="任务：卷积后的结果压缩到某一个固定的范围做非线性映射，这样可以一直保持一层一层下去的数值范围是可控的。"><a href="#任务：卷积后的结果压缩到某一个固定的范围做非线性映射，这样可以一直保持一层一层下去的数值范围是可控的。" class="headerlink" title="任务：卷积后的结果压缩到某一个固定的范围做非线性映射，这样可以一直保持一层一层下去的数值范围是可控的。"></a><code>任务</code>：卷积后的结果压缩到某一个固定的范围做非线性映射，这样可以一直保持一层一层下去的数值范围是可控的。</h4><p><img src="/images/dl/81.png" alt="images"></p><h4 id="激活函数："><a href="#激活函数：" class="headerlink" title="激活函数："></a>激活函数：</h4><ul><li><code>Sigmoid</code></li><li><code>Tanh</code>（双曲正切）</li><li><code>ReLU</code></li><li><code>Leaky ReLU</code></li><li><code>ELU</code></li><li><code>Maxout</code></li></ul><p>卷积神经网络一般采用的激活函数是ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下：</p><p><img src="/images/dl/82.png" alt="images"></p><h4 id="激励层的实践经验："><a href="#激励层的实践经验：" class="headerlink" title="激励层的实践经验： 　　"></a>激励层的实践经验：</h4><ul><li>不要用sigmoid！不要用sigmoid！不要用sigmoid！</li><li>首先试RELU，因为快，但要小心点 　　、</li><li>如果2失效，请用Leaky ReLU或者Maxout</li><li>某些情况下tanh倒是有不错的结果，但是很少</li></ul><h3 id="池化层（POOling-layer）-1"><a href="#池化层（POOling-layer）-1" class="headerlink" title="池化层（POOling layer）"></a><strong>池化层（POOling layer）</strong></h3><h4 id="任务：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。"><a href="#任务：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。" class="headerlink" title="任务：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。"></a><code>任务</code>：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。</h4><p>通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，对卷积层进行池化/采样(Pooling)处理。池化/采样的方式通常有以下两种：</p><ol><li>最大池化（Max Pooling: 选择Pooling窗口中的最大值作为采样值；</li><li>均值池化（Mean Pooling）: 将Pooling窗口中的所有值相加取平均，以平均值作为采样值</li><li>高斯池化：借鉴高斯模糊的方法。不常用。</li><li>可训练池化：使用一个训练函数$y=f(x)$。不常用。</li></ol><p>主要使用不同的函数为输入降维。通常，最大池化层（max-pooling layer）出现在卷积层之后。池化层使用 2*2 的矩阵，以卷积层相同的方式处理图像，不过它是给图像本身降维。下面分别是使用「最大池化」和「平均池化」的示例。</p><p><img src="/images/dl/83.png" alt="images"></p><p>图像经过池化后，得到的是一系列的特征图，而多层感知器接受的输入是一个向量。因此需要将这些特征图中的像素依次取出，排列成一个向量（这个过程被称为光栅化）。</p><h3 id="全连接层（FC-layer）-1"><a href="#全连接层（FC-layer）-1" class="headerlink" title="全连接层（FC layer）"></a><strong>全连接层（FC layer）</strong></h3><h4 id="任务：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。主要是为了分类或回归，当然也可以没有。"><a href="#任务：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。主要是为了分类或回归，当然也可以没有。" class="headerlink" title="任务：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。主要是为了分类或回归，当然也可以没有。"></a><code>任务</code>：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。主要是为了分类或回归，当然也可以没有。</h4><p>其原理和我们前面所推导的DNN是一样的。</p><p><img src="/images/dl/85.png" alt="images"></p><h4 id="两层之间的所有神经元都有权重连接，通常全连接层在卷积神经网络的尾部。"><a href="#两层之间的所有神经元都有权重连接，通常全连接层在卷积神经网络的尾部。" class="headerlink" title="两层之间的所有神经元都有权重连接，通常全连接层在卷积神经网络的尾部。"></a>两层之间的所有神经元都有权重连接，通常全连接层在卷积神经网络的尾部。</h4><h3 id="Dropout层（Dropout-layer）"><a href="#Dropout层（Dropout-layer）" class="headerlink" title="Dropout层（Dropout layer）"></a><strong>Dropout层（Dropout layer）</strong></h3><h4 id="任务：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。"><a href="#任务：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。" class="headerlink" title="任务：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。"></a><code>任务</code>：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。</h4><p><img src="/images/dl/84.png" alt="images"></p><p>上图a是标准的一个全连接的神经网络，b是对a应用了dropout的结果，它会以一定的概率(dropout probability)随机的丢弃掉一些神经元。</p><h3 id="卷积神经网络之优缺点"><a href="#卷积神经网络之优缺点" class="headerlink" title="卷积神经网络之优缺点"></a><strong>卷积神经网络之优缺点</strong></h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点 　　"></a>优点</h4><ul><li>共享卷积核，对高维数据处理无压力</li><li>无需手动选取特征，训练好权重，即得特征分类效果好</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点 　　"></a>缺点</h4><ul><li>需要调参，需要大样本量，训练最好要GPU</li><li>物理含义不明确（也就说，我们并不知道每个卷积层到底提取到的是什么特征，而且神经网络本身就是一种难以解释的“黑箱模型”）</li></ul><h3 id="卷积神经网络之典型CNN"><a href="#卷积神经网络之典型CNN" class="headerlink" title="卷积神经网络之典型CNN"></a><strong>卷积神经网络之典型CNN</strong></h3><ul><li><code>LeNet</code>，这是最早用于数字识别的CNN</li><li><code>AlexNet</code>， 2012 ILSVRC比赛远超第2名的CNN，比 LeNet更深，用多层小卷积层叠加替换单大卷积层。</li><li><code>ZF Net</code>， 2013 ILSVRC比赛冠军</li><li><code>GoogLeNet</code>， 2014 ILSVRC比赛冠军</li><li><code>VGGNet</code>， 2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好</li></ul><h3 id="卷积神经网络的常用框架"><a href="#卷积神经网络的常用框架" class="headerlink" title="卷积神经网络的常用框架"></a><strong>卷积神经网络的常用框架</strong></h3><h4 id="Caffe"><a href="#Caffe" class="headerlink" title="Caffe"></a><strong>Caffe</strong></h4><ul><li><h4 id="源于Berkeley的主流CV工具包，支持C-python-matlab"><a href="#源于Berkeley的主流CV工具包，支持C-python-matlab" class="headerlink" title="源于Berkeley的主流CV工具包，支持C++,python,matlab"></a>源于Berkeley的主流CV工具包，支持C++,python,matlab</h4></li><li><h4 id="Model-Zoo中有大量预训练好的模型供使用"><a href="#Model-Zoo中有大量预训练好的模型供使用" class="headerlink" title="Model Zoo中有大量预训练好的模型供使用"></a>Model Zoo中有大量预训练好的模型供使用</h4></li></ul><h4 id="Torch"><a href="#Torch" class="headerlink" title="Torch"></a><strong>Torch</strong></h4><ul><li><h4 id="Facebook用的卷积神经网络工具包"><a href="#Facebook用的卷积神经网络工具包" class="headerlink" title="Facebook用的卷积神经网络工具包"></a>Facebook用的卷积神经网络工具包</h4></li><li><h4 id="通过时域卷积的本地接口，使用非常直观"><a href="#通过时域卷积的本地接口，使用非常直观" class="headerlink" title="通过时域卷积的本地接口，使用非常直观"></a>通过时域卷积的本地接口，使用非常直观</h4></li><li><h4 id="定义新网络层简单"><a href="#定义新网络层简单" class="headerlink" title="定义新网络层简单"></a>定义新网络层简单</h4></li></ul><h4 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a><strong>TensorFlow</strong></h4><ul><li><h4 id="Google的深度学习框架"><a href="#Google的深度学习框架" class="headerlink" title="Google的深度学习框架"></a>Google的深度学习框架</h4></li><li><h4 id="TensorBoard可视化很方便"><a href="#TensorBoard可视化很方便" class="headerlink" title="TensorBoard可视化很方便"></a>TensorBoard可视化很方便</h4></li><li><h4 id="数据和模型并行化好，速度快"><a href="#数据和模型并行化好，速度快" class="headerlink" title="数据和模型并行化好，速度快"></a>数据和模型并行化好，速度快</h4></li></ul><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow实现简单的深度神经网络-DNN</title>
      <link href="/2018/08/16/2018-08-16-DL-DNN-python/"/>
      <url>/2018/08/16/2018-08-16-DL-DNN-python/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="环境设定"><a href="#环境设定" class="headerlink" title="环境设定"></a>环境设定</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;] = &apos;2&apos;</span><br></pre></td></tr></table></figure><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 使用tensorflow自带的工具加载MNIST手写数字集合</span><br><span class="line">mnist = input_data.read_data_sets(&apos;data/mnist&apos;, one_hot=True)</span><br><span class="line"></span><br><span class="line"># 查看数据的维度和target的维度</span><br><span class="line">print(mnist.train.images.shape)</span><br><span class="line">print(mnist.train.labels.shape)</span><br></pre></td></tr></table></figure><h3 id="准备好placeholder"><a href="#准备好placeholder" class="headerlink" title="准备好placeholder"></a>准备好placeholder</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = tf.placeholder(tf.float32, [None, 784])</span><br><span class="line">Y = tf.placeholder(tf.float32, [None, 10])</span><br></pre></td></tr></table></figure><h3 id="准备权重参数"><a href="#准备权重参数" class="headerlink" title="准备权重参数"></a>准备权重参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 网格参数设置--三层网络结构</span><br><span class="line">n_input = 784  # MNIST 数据输入(28*28*1=784)</span><br><span class="line">n_hidden_1 = 512  # 第一个隐层</span><br><span class="line">n_hidden_2 = 256  # 第二个隐层</span><br><span class="line">n_hidden_3 = 128  # 第三个隐层</span><br><span class="line">n_classes = 10  # MNIST 总共10个手写数字类别</span><br><span class="line"></span><br><span class="line"># 权重参数</span><br><span class="line">weights = &#123;</span><br><span class="line">    &apos;h1&apos;: tf.Variable(tf.random_normal([n_input, n_hidden_1])),</span><br><span class="line">    &apos;h2&apos;: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),</span><br><span class="line">    &apos;h3&apos;: tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([n_hidden_3, n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">biases = &#123;</span><br><span class="line">    &apos;b1&apos;: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    &apos;b2&apos;: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">    &apos;b3&apos;: tf.Variable(tf.random_normal([n_hidden_3])),</span><br><span class="line">    &apos;out&apos;: tf.Variable(tf.random_normal([n_classes]))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="构建网络计算graph"><a href="#构建网络计算graph" class="headerlink" title="构建网络计算graph"></a>构建网络计算graph</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def multilayerPerceptron(x, weights, biases):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # 前向传播 y = wx + b</span><br><span class="line">    :param x: x</span><br><span class="line">    :param weights: w</span><br><span class="line">    :param biases: b</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # 计算第一个隐层，使用激活函数</span><br><span class="line">    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[&apos;h1&apos;]), biases[&apos;b1&apos;]))</span><br><span class="line">    # 计算第二个隐层，使用激活函数</span><br><span class="line">    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, weights[&apos;h2&apos;]), biases[&apos;b2&apos;]))</span><br><span class="line">    # 计算第三个隐层，使用激活函数</span><br><span class="line">    layer3 = tf.nn.sigmoid(tf.add(tf.matmul(layer2, weights[&apos;h3&apos;]), biases[&apos;b3&apos;]))</span><br><span class="line">    # 计算第输出层。</span><br><span class="line">    outLayer = tf.add(tf.matmul(layer3, weights[&apos;out&apos;]), biases[&apos;out&apos;])</span><br><span class="line"></span><br><span class="line">    return outLayer</span><br></pre></td></tr></table></figure><h3 id="获取预测值得score"><a href="#获取预测值得score" class="headerlink" title="获取预测值得score"></a>获取预测值得score</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictValue = multilayerPerceptron(X, weights, biases)</span><br></pre></td></tr></table></figure><h3 id="计算损失函数并初始化optimizer"><a href="#计算损失函数并初始化optimizer" class="headerlink" title="计算损失函数并初始化optimizer"></a>计算损失函数并初始化optimizer</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">learnRate = 0.01</span><br><span class="line">loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictValue, labels=Y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learnRate).minimize(loss)</span><br><span class="line"></span><br><span class="line"># 验证数据</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(predictValue, 1), tf.argmax(Y, 1))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, &apos;float&apos;))</span><br><span class="line">print(&quot;FUNCTIONS READY!!&quot;)</span><br></pre></td></tr></table></figure><h3 id="初始化变量"><a href="#初始化变量" class="headerlink" title="初始化变量"></a>初始化变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h3 id="在session中执行graph定义的运算"><a href="#在session中执行graph定义的运算" class="headerlink" title="在session中执行graph定义的运算"></a>在session中执行graph定义的运算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 训练的总轮数</span><br><span class="line">trainEpochs = 20</span><br><span class="line"># 每一批训练的数据大小</span><br><span class="line">batchSize = 128</span><br><span class="line"># 信息显示的频数</span><br><span class="line">displayStep = 5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    # 初始化变量</span><br><span class="line">    sess.run(init)</span><br><span class="line">    # 训练</span><br><span class="line">    for epoch in range(trainEpochs):</span><br><span class="line">        avg_loss = 0.</span><br><span class="line">        totalBatch = int(mnist.train.num_examples/batchSize)</span><br><span class="line">        # 遍历所有batch</span><br><span class="line">        for i in range(totalBatch):</span><br><span class="line">            batchX, batchY = mnist.train.next_batch(batchSize)</span><br><span class="line">            # 使用optimizer进行优化</span><br><span class="line">            _, loss_value = sess.run([optimizer, loss], feed_dict=&#123;X: batchX, Y: batchY&#125;)</span><br><span class="line">            # 求平均损失值</span><br><span class="line">            avg_loss += loss_value/totalBatch</span><br><span class="line"></span><br><span class="line">        # 显示信息</span><br><span class="line">        if (epoch+1) % displayStep == 0:</span><br><span class="line">            print(&quot;Epoch: %04d %04d Loss：%.9f&quot; % (epoch, trainEpochs, avg_loss))</span><br><span class="line">            train_acc = sess.run(accuracy, feed_dict=&#123;X: batchX, Y: batchY&#125;)</span><br><span class="line">            print(&quot;Train Accuracy: %.3f&quot; % train_acc)</span><br><span class="line">            test_acc = sess.run(accuracy, feed_dict=&#123;X: mnist.test.images, Y: mnist.test.labels&#125;)</span><br><span class="line">            print(&quot;Test Accuracy: %.3f&quot; % test_acc)</span><br><span class="line"></span><br><span class="line">    print(&quot;Optimization Finished&quot;)</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Extracting data/mnist\train-images-idx3-ubyte.gz</span><br><span class="line">Extracting data/mnist\train-labels-idx1-ubyte.gz</span><br><span class="line">Extracting data/mnist\t10k-images-idx3-ubyte.gz</span><br><span class="line">Extracting data/mnist\t10k-labels-idx1-ubyte.gz</span><br><span class="line">(55000, 784)</span><br><span class="line">(55000, 10)</span><br><span class="line">FUNCTIONS READY!!</span><br><span class="line">Epoch: 0004 0020 Loss：0.188512910</span><br><span class="line">Train Accuracy: 1.000</span><br><span class="line">Test Accuracy: 0.947</span><br><span class="line">Epoch: 0009 0020 Loss：0.142638438</span><br><span class="line">Train Accuracy: 0.969</span><br><span class="line">Test Accuracy: 0.954</span><br><span class="line">Epoch: 0014 0020 Loss：0.125044704</span><br><span class="line">Train Accuracy: 0.969</span><br><span class="line">Test Accuracy: 0.952</span><br><span class="line">Epoch: 0019 0020 Loss：0.114395266</span><br><span class="line">Train Accuracy: 0.984</span><br><span class="line">Test Accuracy: 0.957</span><br><span class="line">Optimization Finished</span><br></pre></td></tr></table></figure><h4 id="结论：简单写了个神经网络，没有进行参数调试。"><a href="#结论：简单写了个神经网络，没有进行参数调试。" class="headerlink" title="结论：简单写了个神经网络，没有进行参数调试。"></a>结论：简单写了个神经网络，没有进行参数调试。</h4><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> DNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络入门-反向传播</title>
      <link href="/2018/08/15/2018-08-15-DL-back-propagation/"/>
      <url>/2018/08/15/2018-08-15-DL-back-propagation/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="反向传播（back-propagation）算法推导"><a href="#反向传播（back-propagation）算法推导" class="headerlink" title="反向传播（back propagation）算法推导"></a>反向传播（back propagation）算法推导</h3><p><img src="/images/dl/60.png" alt="images"></p><h4 id="定义损失函数："><a href="#定义损失函数：" class="headerlink" title="定义损失函数："></a>定义损失函数：</h4><h4 id="E-total-frac12-y-outo-2"><a href="#E-total-frac12-y-outo-2" class="headerlink" title="$E_{total}=\frac12 (y-outo)^2$"></a>$E_{total}=\frac12 (y-outo)^2$</h4><h4 id="定义激活函数："><a href="#定义激活函数：" class="headerlink" title="定义激活函数："></a>定义激活函数：</h4><h4 id="sigma-x-sigmod-x"><a href="#sigma-x-sigmod-x" class="headerlink" title="$\sigma(x)=sigmod(x) $"></a>$\sigma(x)=sigmod(x) $</h4><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><h4 id="第一层-输入层-："><a href="#第一层-输入层-：" class="headerlink" title="第一层(输入层)："></a>第一层(输入层)：</h4><ul><li><h4 id="x-1-x-2-b-1"><a href="#x-1-x-2-b-1" class="headerlink" title="$x_1\ \ \ x_2\ \ \ b_1$"></a>$x_1\ \ \ x_2\ \ \ b_1$</h4></li></ul><h4 id="加权和："><a href="#加权和：" class="headerlink" title="加权和："></a>加权和：</h4><ul><li><h4 id="net-h-1-x-1w-1-x-2w-2-b-1"><a href="#net-h-1-x-1w-1-x-2w-2-b-1" class="headerlink" title="$net h_1=x_1w_1+x_2w_2+b_1$"></a>$net h_1=x_1w_1+x_2w_2+b_1$</h4></li></ul><h4 id="第二层-隐层-："><a href="#第二层-隐层-：" class="headerlink" title="第二层(隐层)："></a>第二层(隐层)：</h4><ul><li><h4 id="outh-1-sigmod-neth-1"><a href="#outh-1-sigmod-neth-1" class="headerlink" title="$outh_1=sigmod(neth_1)$"></a>$outh_1=sigmod(neth_1)$</h4></li></ul><h4 id="加权和：-1"><a href="#加权和：-1" class="headerlink" title="加权和："></a>加权和：</h4><ul><li><h4 id="neto-1-outh-1w-3-outh-2w-4-b-2"><a href="#neto-1-outh-1w-3-outh-2w-4-b-2" class="headerlink" title="$neto_1=outh_1w_3+outh_2w_4+b_2$"></a>$neto_1=outh_1w_3+outh_2w_4+b_2$</h4></li></ul><h4 id="第三层-输出层-："><a href="#第三层-输出层-：" class="headerlink" title="第三层(输出层)："></a>第三层(输出层)：</h4><ul><li><h4 id="outo-1-sigmod-neto-1"><a href="#outo-1-sigmod-neto-1" class="headerlink" title="$outo_1=sigmod(neto_1)$"></a>$outo_1=sigmod(neto_1)$</h4></li></ul><h4 id="计算误差值："><a href="#计算误差值：" class="headerlink" title="计算误差值："></a>计算误差值：</h4><ul><li><h4 id="Eo-1-frac12-y-1-outo-1-2"><a href="#Eo-1-frac12-y-1-outo-1-2" class="headerlink" title="$Eo_1 = \frac12 (y_1-outo_1)^2$"></a>$Eo_1 = \frac12 (y_1-outo_1)^2$</h4></li><li><h4 id="Eo-2-frac12-y-2-outo-2-2"><a href="#Eo-2-frac12-y-2-outo-2-2" class="headerlink" title="$Eo_2 = \frac12 (y_2-outo_2)^2$"></a>$Eo_2 = \frac12 (y_2-outo_2)^2$</h4></li><li><h4 id="E-total-Eo-1-Eo-2"><a href="#E-total-Eo-1-Eo-2" class="headerlink" title="$E_{total}=Eo_1+Eo_2$"></a>$E_{total}=Eo_1+Eo_2$</h4></li></ul><p><code>总结</code>：要是使误差值最小，就需要<code>误差反向传播算法</code>，更新得到最小误差的权重参数<code>w和b</code>。</p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p><code>须知</code>：我们需要反向传递回去更新每一层对应的权重参数<code>w和b</code>。我们使用<code>链式法则</code>来<code>反向模式求导</code>。</p><h4 id="更新第三层（输出层）的权重参数："><a href="#更新第三层（输出层）的权重参数：" class="headerlink" title="更新第三层（输出层）的权重参数："></a><strong>更新第三层（输出层）的权重参数：</strong></h4><h4 id="更新参数w："><a href="#更新参数w：" class="headerlink" title="更新参数w："></a>更新参数<code>w</code>：</h4><h4 id="frac-partial-E-total-partial-w-3-frac-partial-E-total-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-w-3"><a href="#frac-partial-E-total-partial-w-3-frac-partial-E-total-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-w-3" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_3}=\frac{\partial E_{total}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial w_3}$"></a>$\frac{\partial E_{total}}{\partial w_3}=\frac{\partial E_{total}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial w_3}$</h4><h4 id="frac-partial-frac12-y-1-outo-1-2-partial-outo-1-cdot-frac-partial-sigmod-neto-1-partial-neto-1-cdot-frac-partial-neto-1-partial-w-3"><a href="#frac-partial-frac12-y-1-outo-1-2-partial-outo-1-cdot-frac-partial-sigmod-neto-1-partial-neto-1-cdot-frac-partial-neto-1-partial-w-3" class="headerlink" title="$= \frac{\partial \frac12(y_1-outo_1)^2}{\partial outo_1} \cdot \frac{\partial sigmod(neto_1)}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial w_3}$"></a>$= \frac{\partial \frac12(y_1-outo_1)^2}{\partial outo_1} \cdot \frac{\partial sigmod(neto_1)}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial w_3}$</h4><h4 id="outo-1-y-1-cdot-outo-1-1-outo-1-cdot-outh-1"><a href="#outo-1-y-1-cdot-outo-1-1-outo-1-cdot-outh-1" class="headerlink" title="$=(outo_1-y_1)\cdot outo_1(1-outo_1)\cdot outh_1$"></a>$=(outo_1-y_1)\cdot outo_1(1-outo_1)\cdot outh_1$</h4><p>$w_{3new}=w_{3old}-\eta \frac{\partial E_{total}}{\partial w_3}$， $\eta$是学习率</p><h4 id="更新参数b："><a href="#更新参数b：" class="headerlink" title="更新参数b："></a>更新参数<code>b</code>：</h4><h4 id="frac-partial-E-total-partial-b-2-frac-partial-E-total-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-b-2"><a href="#frac-partial-E-total-partial-b-2-frac-partial-E-total-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-b-2" class="headerlink" title="$\frac{\partial E_{total}}{\partial b_2}=\frac{\partial E_{total}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial b_2}$"></a>$\frac{\partial E_{total}}{\partial b_2}=\frac{\partial E_{total}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial b_2}$</h4><h4 id="frac-partial-frac12-y-1-outo-1-2-partial-outo-1-cdot-frac-partial-sigmod-neto-1-partial-neto-1-cdot-frac-partial-neto-1-partial-b-2"><a href="#frac-partial-frac12-y-1-outo-1-2-partial-outo-1-cdot-frac-partial-sigmod-neto-1-partial-neto-1-cdot-frac-partial-neto-1-partial-b-2" class="headerlink" title="$= \frac{\partial \frac12(y_1-outo_1)^2}{\partial outo_1} \cdot \frac{\partial sigmod(neto_1)}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial b_2}$"></a>$= \frac{\partial \frac12(y_1-outo_1)^2}{\partial outo_1} \cdot \frac{\partial sigmod(neto_1)}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial b_2}$</h4><h4 id="outo-1-y-1-cdot-outo-1-1-outo-1"><a href="#outo-1-y-1-cdot-outo-1-1-outo-1" class="headerlink" title="$=(outo_1-y_1)\cdot outo_1(1-outo_1)$"></a>$=(outo_1-y_1)\cdot outo_1(1-outo_1)$</h4><h4 id="b-2new-b-2old-eta-frac-partial-E-total-partial-b-2-，-eta-是学习率"><a href="#b-2new-b-2old-eta-frac-partial-E-total-partial-b-2-，-eta-是学习率" class="headerlink" title="$b_{2new}=b_{2old}-\eta \frac{\partial E_{total}}{\partial b_2}$， $\eta$是学习率"></a>$b_{2new}=b_{2old}-\eta \frac{\partial E_{total}}{\partial b_2}$， $\eta$是学习率</h4><h4 id="同理可得：w4：也就是同一层的w都可以用这种方式更新。"><a href="#同理可得：w4：也就是同一层的w都可以用这种方式更新。" class="headerlink" title="同理可得：w4：也就是同一层的w都可以用这种方式更新。"></a>同理可得：<code>w4</code>：也就是同一层的<code>w</code>都可以用这种方式更新。</h4><h4 id="更新上一层-隐层-的权重参数："><a href="#更新上一层-隐层-的权重参数：" class="headerlink" title="更新上一层(隐层)的权重参数："></a><strong>更新上一层(隐层)的权重参数</strong>：</h4><h4 id="更新权重参数w和b："><a href="#更新权重参数w和b：" class="headerlink" title="更新权重参数w和b："></a>更新权重参数<code>w和b</code>：</h4><h4 id="frac-partial-E-total-partial-w-1-frac-partial-E-total-partial-outh-1-cdot-frac-partial-outh-1-partial-neth-1-cdot-frac-partial-neth-1-partial-w-1"><a href="#frac-partial-E-total-partial-w-1-frac-partial-E-total-partial-outh-1-cdot-frac-partial-outh-1-partial-neth-1-cdot-frac-partial-neth-1-partial-w-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_1}=\frac{\partial E_{total}}{\partial outh_1} \cdot \frac{\partial outh_1}{\partial neth_1} \cdot \frac{\partial neth_1}{\partial w_1}$"></a>$\frac{\partial E_{total}}{\partial w_1}=\frac{\partial E_{total}}{\partial outh_1} \cdot \frac{\partial outh_1}{\partial neth_1} \cdot \frac{\partial neth_1}{\partial w_1}$</h4><h4 id="frac-partial-E-total-partial-b-1-frac-partial-E-total-partial-outh-1-cdot-frac-partial-outh-1-partial-neth-1-cdot-frac-partial-neth-1-partial-b-1"><a href="#frac-partial-E-total-partial-b-1-frac-partial-E-total-partial-outh-1-cdot-frac-partial-outh-1-partial-neth-1-cdot-frac-partial-neth-1-partial-b-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial b_1}=\frac{\partial E_{total}}{\partial outh_1} \cdot \frac{\partial outh_1}{\partial neth_1} \cdot \frac{\partial neth_1}{\partial b_1}$"></a>$\frac{\partial E_{total}}{\partial b_1}=\frac{\partial E_{total}}{\partial outh_1} \cdot \frac{\partial outh_1}{\partial neth_1} \cdot \frac{\partial neth_1}{\partial b_1}$</h4><h4 id="其中："><a href="#其中：" class="headerlink" title="其中："></a>其中：</h4><ul><li><h4 id="frac-partial-E-total-partial-outh-1-frac-partial-Eo-1-partial-outh-1-frac-partial-Eo-2-partial-outh-1"><a href="#frac-partial-E-total-partial-outh-1-frac-partial-Eo-1-partial-outh-1-frac-partial-Eo-2-partial-outh-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial outh_1} = \frac{\partial Eo_1}{\partial outh_1}+ \frac{\partial Eo_2}{\partial outh_1}$"></a>$\frac{\partial E_{total}}{\partial outh_1} = \frac{\partial Eo_1}{\partial outh_1}+ \frac{\partial Eo_2}{\partial outh_1}$</h4></li><li><h4 id="frac-partial-Eo-1-partial-outh-1-frac-partial-Eo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-outh-1"><a href="#frac-partial-Eo-1-partial-outh-1-frac-partial-Eo-1-partial-neto-1-cdot-frac-partial-neto-1-partial-outh-1" class="headerlink" title="$\frac{\partial Eo_1}{\partial outh_1} = \frac{\partial Eo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial outh_1}$"></a>$\frac{\partial Eo_1}{\partial outh_1} = \frac{\partial Eo_1}{\partial neto_1} \cdot \frac{\partial neto_1}{\partial outh_1}$</h4></li><li><h4 id="frac-partial-Eo-1-partial-neto-1-frac-partial-E-o-1-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-outo-1-y-1-cdot-outo-1-1-outo-1"><a href="#frac-partial-Eo-1-partial-neto-1-frac-partial-E-o-1-partial-outo-1-cdot-frac-partial-outo-1-partial-neto-1-outo-1-y-1-cdot-outo-1-1-outo-1" class="headerlink" title="$ \frac{\partial Eo_1}{\partial neto_1} = \frac{\partial E_{o_1}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} = (outo_1-y_1)\cdot outo_1(1-outo_1)$"></a>$ \frac{\partial Eo_1}{\partial neto_1} = \frac{\partial E_{o_1}}{\partial outo_1} \cdot \frac{\partial outo_1}{\partial neto_1} = (outo_1-y_1)\cdot outo_1(1-outo_1)$</h4></li><li><h4 id="frac-partial-neto-1-partial-outh-1-frac-partial-outh-1w-3-outo-2w-4-b-2-partial-outh-1-w-3"><a href="#frac-partial-neto-1-partial-outh-1-frac-partial-outh-1w-3-outo-2w-4-b-2-partial-outh-1-w-3" class="headerlink" title="$ \frac{\partial neto_1}{\partial outh_1} = \frac{\partial (outh_1w_3+outo_2w_4+b_2)}{\partial outh_1} = w_3$"></a>$ \frac{\partial neto_1}{\partial outh_1} = \frac{\partial (outh_1w_3+outo_2w_4+b_2)}{\partial outh_1} = w_3$</h4></li><li><h4 id="同理可得："><a href="#同理可得：" class="headerlink" title="同理可得："></a>同理可得：</h4><ul><li><h4 id="frac-partial-Eo-2-partial-outh-1-frac-partial-Eo-2-partial-neto-2-cdot-frac-partial-neto-2-partial-outh-1"><a href="#frac-partial-Eo-2-partial-outh-1-frac-partial-Eo-2-partial-neto-2-cdot-frac-partial-neto-2-partial-outh-1" class="headerlink" title="$\frac{\partial Eo_2}{\partial outh_1} = \frac{\partial Eo_2}{\partial neto_2} \cdot \frac{\partial neto_2}{\partial outh_1}$"></a>$\frac{\partial Eo_2}{\partial outh_1} = \frac{\partial Eo_2}{\partial neto_2} \cdot \frac{\partial neto_2}{\partial outh_1}$</h4></li><li><h4 id="frac-partial-Eo-2-partial-neto-2-frac-partial-E-o-2-partial-outo-2-cdot-frac-partial-outo-2-partial-neto-2-outo-2-y-2-cdot-outo-2-1-outo-2"><a href="#frac-partial-Eo-2-partial-neto-2-frac-partial-E-o-2-partial-outo-2-cdot-frac-partial-outo-2-partial-neto-2-outo-2-y-2-cdot-outo-2-1-outo-2" class="headerlink" title="$ \frac{\partial Eo_2}{\partial neto_2} = \frac{\partial E_{o_2}}{\partial outo_2} \cdot \frac{\partial outo_2}{\partial neto_2} = (outo_2-y_2)\cdot outo_2(1-outo_2)$"></a>$ \frac{\partial Eo_2}{\partial neto_2} = \frac{\partial E_{o_2}}{\partial outo_2} \cdot \frac{\partial outo_2}{\partial neto_2} = (outo_2-y_2)\cdot outo_2(1-outo_2)$</h4></li><li><h4 id="frac-partial-neto-2-partial-outh-1-w-4"><a href="#frac-partial-neto-2-partial-outh-1-w-4" class="headerlink" title="$ \frac{\partial neto_2}{\partial outh_1} = w_4$"></a>$ \frac{\partial neto_2}{\partial outh_1} = w_4$</h4></li></ul></li></ul><h4 id="综合上式："><a href="#综合上式：" class="headerlink" title="综合上式："></a>综合上式：</h4><h4 id="frac-partial-E-total-partial-w-1-w-3-outo-1-y-1-cdot-outo-1-1-outo-1-w-4-outo-2-y-2-cdot-outo-2-1-outo-2-cdot-outh-1-1-outh-1-cdot-x-1"><a href="#frac-partial-E-total-partial-w-1-w-3-outo-1-y-1-cdot-outo-1-1-outo-1-w-4-outo-2-y-2-cdot-outo-2-1-outo-2-cdot-outh-1-1-outh-1-cdot-x-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_1}= [w_3 (outo_1-y_1)\cdot outo_1(1-outo_1) + w_4(outo_2-y_2)\cdot outo_2(1-outo_2)] \cdot outh_1(1-outh_1) \cdot x_1$"></a>$\frac{\partial E_{total}}{\partial w_1}= [w_3 (outo_1-y_1)\cdot outo_1(1-outo_1) + w_4(outo_2-y_2)\cdot outo_2(1-outo_2)] \cdot outh_1(1-outh_1) \cdot x_1$</h4><h4 id="frac-partial-E-total-partial-b-1-w-3-outo-1-y-1-cdot-outo-1-1-outo-1-w-4-outo-2-y-2-cdot-outo-2-1-outo-2-cdot-outh-1-1-outh-1"><a href="#frac-partial-E-total-partial-b-1-w-3-outo-1-y-1-cdot-outo-1-1-outo-1-w-4-outo-2-y-2-cdot-outo-2-1-outo-2-cdot-outh-1-1-outh-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial b_1}= [w_3 (outo_1-y_1)\cdot outo_1(1-outo_1) +w_4(outo_2-y_2)\cdot outo_2(1-outo_2)] \cdot outh_1(1-outh_1) $"></a>$\frac{\partial E_{total}}{\partial b_1}= [w_3 (outo_1-y_1)\cdot outo_1(1-outo_1) +w_4(outo_2-y_2)\cdot outo_2(1-outo_2)] \cdot outh_1(1-outh_1) $</h4><h4 id="更新："><a href="#更新：" class="headerlink" title="更新："></a>更新：</h4><h4 id="w-1new-w-1old-eta-frac-partial-E-total-partial-w-1"><a href="#w-1new-w-1old-eta-frac-partial-E-total-partial-w-1" class="headerlink" title="$w_{1new}=w_{1old}-\eta \frac{\partial E_{total}}{\partial w_1}$"></a>$w_{1new}=w_{1old}-\eta \frac{\partial E_{total}}{\partial w_1}$</h4><h4 id="b-1new-b-1old-eta-frac-partial-E-total-partial-b-1"><a href="#b-1new-b-1old-eta-frac-partial-E-total-partial-b-1" class="headerlink" title="$b_{1new}=b_{1old}-\eta \frac{\partial E_{total}}{\partial b_1}$"></a>$b_{1new}=b_{1old}-\eta \frac{\partial E_{total}}{\partial b_1}$</h4><h4 id="同理可得：w2：也就是同一层的w都可以用这种方式更新。"><a href="#同理可得：w2：也就是同一层的w都可以用这种方式更新。" class="headerlink" title="同理可得：w2：也就是同一层的w都可以用这种方式更新。"></a>同理可得：<code>w2</code>：也就是同一层的<code>w</code>都可以用这种方式更新。</h4><h4 id="至此，我们所有的参数都更新完毕了，然后我们可以再次用新的参数进行前向传播，得到的误差就会是最小的"><a href="#至此，我们所有的参数都更新完毕了，然后我们可以再次用新的参数进行前向传播，得到的误差就会是最小的" class="headerlink" title="至此，我们所有的参数都更新完毕了，然后我们可以再次用新的参数进行前向传播，得到的误差就会是最小的"></a>至此，我们所有的参数都更新完毕了，然后我们可以再次用新的参数进行前向传播，得到的误差就会是最小的</h4><h3 id="推广"><a href="#推广" class="headerlink" title="推广:"></a>推广:</h3><p><img src="/images/dl/62.png" alt="images"></p><h4 id="我们定义第L层的第i个神经元更新权重参数时-上标表示层数，下标表示神经元-："><a href="#我们定义第L层的第i个神经元更新权重参数时-上标表示层数，下标表示神经元-：" class="headerlink" title="我们定义第L层的第i个神经元更新权重参数时(上标表示层数，下标表示神经元)："></a>我们定义第<code>L</code>层的第<code>i</code>个神经元更新权重参数时(上标表示层数，下标表示神经元)：</h4><ul><li><h4 id="frac-partial-E-total-partial-net-i-L-delta-i-L"><a href="#frac-partial-E-total-partial-net-i-L-delta-i-L" class="headerlink" title="$\frac{\partial E_{total}}{\partial net_i^{(L)}} = \delta_i^{(L)}$"></a>$\frac{\partial E_{total}}{\partial net_i^{(L)}} = \delta_i^{(L)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l-，其中-w-ij-l-表示第-l-层的第-j-个神经元连接第-l-1-层的第-i-的神经元的相连的权重参数w。如下图所示："><a href="#frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l-，其中-w-ij-l-表示第-l-层的第-j-个神经元连接第-l-1-层的第-i-的神经元的相连的权重参数w。如下图所示：" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$ ，其中$w_{ij}^{(l)}$表示第$l$层的第$j$个神经元连接第$l-1$层的第$i$的神经元的相连的权重参数w。如下图所示："></a>$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$ ，其中$w_{ij}^{(l)}$表示第$l$层的第$j$个神经元连接第$l-1$层的第$i$的神经元的相连的权重参数<code>w</code>。如下图所示：</h4><p><img src="/images/dl/63.png" alt="images"></p></li></ul><h3 id="推广总结："><a href="#推广总结：" class="headerlink" title="推广总结："></a>推广总结：</h3><h4 id="根据前面我们所定义的："><a href="#根据前面我们所定义的：" class="headerlink" title="根据前面我们所定义的："></a>根据前面我们所定义的：</h4><p>$E_{total}=\frac12 (y-outo)^2$</p><h4 id="sigma-x-sigmod-x-1"><a href="#sigma-x-sigmod-x-1" class="headerlink" title="$\sigma(x)=sigmod(x) $"></a>$\sigma(x)=sigmod(x) $</h4><h4 id="frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l"><a href="#frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$</h4><h4 id="delta-i-L-frac-partial-E-total-partial-net-i-L"><a href="#delta-i-L-frac-partial-E-total-partial-net-i-L" class="headerlink" title="$\delta_i^{(L)}=\frac{\partial E_{total}}{\partial net_i^{(L)}}  $"></a>$\delta_i^{(L)}=\frac{\partial E_{total}}{\partial net_i^{(L)}} $</h4><h4 id="frac-partial-E-total-partial-outh-i-cdot-frac-partial-outh-i-partial-net-i-L"><a href="#frac-partial-E-total-partial-outh-i-cdot-frac-partial-outh-i-partial-net-i-L" class="headerlink" title="$= \frac{\partial E_{total}}{\partial outh_i} \cdot \frac{\partial outh_i}{\partial net_i^{(L)}}$"></a>$= \frac{\partial E_{total}}{\partial outh_i} \cdot \frac{\partial outh_i}{\partial net_i^{(L)}}$</h4><h4 id="bigtriangledown-out-E-total-times-sigma-prime-net-i-L"><a href="#bigtriangledown-out-E-total-times-sigma-prime-net-i-L" class="headerlink" title="$= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)})$"></a>$= \bigtriangledown_{out} E_{total} \times \sigma^{\prime}(net_i^{(L)})$</h4><h4 id="对于第-l-层："><a href="#对于第-l-层：" class="headerlink" title="对于第$l$层："></a>对于第$l$层：</h4><h4 id="delta-l-frac-partial-E-total-partial-net-l"><a href="#delta-l-frac-partial-E-total-partial-net-l" class="headerlink" title="$\delta^{(l)}=\frac{\partial E_{total}}{\partial net^{(l)}} $"></a>$\delta^{(l)}=\frac{\partial E_{total}}{\partial net^{(l)}} $</h4><h4 id="frac-partial-E-total-partial-net-l-1-cdot-frac-partial-net-l-1-partial-net-l"><a href="#frac-partial-E-total-partial-net-l-1-cdot-frac-partial-net-l-1-partial-net-l" class="headerlink" title="$= \frac{\partial E_{total}}{\partial net^{(l+1)}} \cdot \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$"></a>$= \frac{\partial E_{total}}{\partial net^{(l+1)}} \cdot \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-times-frac-partial-net-l-1-partial-net-l"><a href="#delta-l-1-times-frac-partial-net-l-1-partial-net-l" class="headerlink" title="$= \delta^{(l+1)} \times  \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$"></a>$= \delta^{(l+1)} \times \frac{\partial net^{(l+1)}}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-times-frac-partial-w-l-1-sigma-net-l-partial-net-l"><a href="#delta-l-1-times-frac-partial-w-l-1-sigma-net-l-partial-net-l" class="headerlink" title="$= \delta^{(l+1)} \times  \frac{\partial (w^{(l+1)}\sigma (net^{(l)}))}{\partial net^{(l)}}$"></a>$= \delta^{(l+1)} \times \frac{\partial (w^{(l+1)}\sigma (net^{(l)}))}{\partial net^{(l)}}$</h4><h4 id="delta-l-1-w-l-1-sigma-prime-net-L"><a href="#delta-l-1-w-l-1-sigma-prime-net-L" class="headerlink" title="$= \delta^{(l+1)} w^{(l+1)}  \sigma^{\prime}(net^{(L)})$"></a>$= \delta^{(l+1)} w^{(l+1)} \sigma^{\prime}(net^{(L)})$</h4><h4 id="对于偏置项bias："><a href="#对于偏置项bias：" class="headerlink" title="对于偏置项bias："></a>对于偏置项<code>bias</code>：</h4><h4 id="frac-partial-E-total-partial-bias-i-l-delta-i-l"><a href="#frac-partial-E-total-partial-bias-i-l-delta-i-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$</h4><h3 id="反向传播的四项基本原则："><a href="#反向传播的四项基本原则：" class="headerlink" title="反向传播的四项基本原则："></a>反向传播的四项基本原则：</h3><h4 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式:"></a>基本形式:</h4><ul><li><h4 id="delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L"><a href="#delta-i-L-bigtriangledown-out-E-total-times-sigma-prime-net-i-L" class="headerlink" title="$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times   \sigma^{\prime}(net_i^{(L)}) $"></a>$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \times \sigma^{\prime}(net_i^{(L)}) $</h4></li><li><h4 id="delta-l-sum-j-delta-j-l-1-w-ji-l-1-sigma-prime-net-i-l"><a href="#delta-l-sum-j-delta-j-l-1-w-ji-l-1-sigma-prime-net-i-l" class="headerlink" title="$\delta^{(l)} = \sum_j \delta_j^{(l+1)} w_{ji}^{(l+1)}  \sigma^{\prime}(net_i^{(l)})$"></a>$\delta^{(l)} = \sum_j \delta_j^{(l+1)} w_{ji}^{(l+1)} \sigma^{\prime}(net_i^{(l)})$</h4></li><li><h4 id="frac-partial-E-total-partial-bias-i-l-delta-i-l-1"><a href="#frac-partial-E-total-partial-bias-i-l-delta-i-l-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias_i^{(l)}}=\delta_i^{(l)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l-1"><a href="#frac-partial-E-total-partial-w-ij-l-outh-j-l-1-delta-i-l-1" class="headerlink" title="$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$"></a>$\frac{\partial E_{total}}{\partial w_{ij}^{(l)}}=outh_j^{(l-1)}\delta_i^{(l)}$</h4></li></ul><h4 id="矩阵形式："><a href="#矩阵形式：" class="headerlink" title="矩阵形式："></a>矩阵形式：</h4><ul><li><h4 id="delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L-，-bigodot-是Hadamard乘积（对应位置相乘）"><a href="#delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L-，-bigodot-是Hadamard乘积（对应位置相乘）" class="headerlink" title="$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot   \sigma^{\prime}(net_i^{(L)}) $ ， $\bigodot$是Hadamard乘积（对应位置相乘）"></a>$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot \sigma^{\prime}(net_i^{(L)}) $ ， $\bigodot$是Hadamard乘积（对应位置相乘）</h4></li><li><h4 id="delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l"><a href="#delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l" class="headerlink" title="$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$"></a>$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot \sigma^{\prime}(net^{(l)})$</h4></li><li><h4 id="frac-partial-E-total-partial-bias-l-delta-l"><a href="#frac-partial-E-total-partial-bias-l-delta-l" class="headerlink" title="$\frac{\partial E_{total}}{\partial bias^{(l)}}=\delta^{(l)}$"></a>$\frac{\partial E_{total}}{\partial bias^{(l)}}=\delta^{(l)}$</h4></li><li><h4 id="frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T"><a href="#frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T" class="headerlink" title="$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$"></a>$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$</h4></li></ul><p>当然如果你对具体推导不是很明白，你把这四项基本原则搞清楚，就可以直接使用了。</p><h3 id="举个栗子"><a href="#举个栗子" class="headerlink" title="举个栗子"></a>举个栗子</h3><p><img src="/images/dl/61.png" alt="images"></p><ul><li><h4 id="因为：-delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L"><a href="#因为：-delta-i-L-bigtriangledown-out-E-total-bigodot-sigma-prime-net-i-L" class="headerlink" title="因为：$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot   \sigma^{\prime}(net_i^{(L)}) $"></a>因为：$\delta_i^{(L)}= \bigtriangledown_{out} E_{total} \bigodot \sigma^{\prime}(net_i^{(L)}) $</h4><h4 id="所以："><a href="#所以：" class="headerlink" title="所以："></a>所以：</h4><h4 id="delta-2-out-y-bigodot-out-1-out"><a href="#delta-2-out-y-bigodot-out-1-out" class="headerlink" title="$\delta^{(2)}= (out-y)\bigodot out(1-out)$"></a>$\delta^{(2)}= (out-y)\bigodot out(1-out)$</h4><h4 id="begin-bmatrix-0-88134-0-89551-end-bmatrix-begin-bmatrix-1-0-end-bmatrix-bigodot-begin-bmatrix-0-88134-0-89551-end-bmatrix-bigodot-begin-bmatrix-1-1-end-bmatrix-begin-bmatrix-0-88134-0-89551-end-bmatrix"><a href="#begin-bmatrix-0-88134-0-89551-end-bmatrix-begin-bmatrix-1-0-end-bmatrix-bigodot-begin-bmatrix-0-88134-0-89551-end-bmatrix-bigodot-begin-bmatrix-1-1-end-bmatrix-begin-bmatrix-0-88134-0-89551-end-bmatrix" class="headerlink" title="$=(\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix} -\begin{bmatrix} 1 \\ 0 \end{bmatrix}) \bigodot (\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix} \bigodot (\begin{bmatrix} 1 \\ 1 \end{bmatrix}-\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix})) $"></a>$=(\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix} -\begin{bmatrix} 1 \\ 0 \end{bmatrix}) \bigodot (\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix} \bigodot (\begin{bmatrix} 1 \\ 1 \end{bmatrix}-\begin{bmatrix} 0.88134 \\ 0.89551 \end{bmatrix})) $</h4><h4 id="begin-bmatrix-0-01240932-0-08379177-end-bmatrix"><a href="#begin-bmatrix-0-01240932-0-08379177-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}$"></a>$=\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}$</h4></li><li><h4 id="因为：-delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l"><a href="#因为：-delta-l-w-l-1-T-delta-l-1-bigodot-sigma-prime-net-l" class="headerlink" title="因为：$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot  \sigma^{\prime}(net^{(l)})$"></a>因为：$\delta^{(l)} = (w^{(l+1)})^T \delta^{(l+1)} \bigodot \sigma^{\prime}(net^{(l)})$</h4><h4 id="所以：-1"><a href="#所以：-1" class="headerlink" title="所以："></a>所以：</h4><h4 id="delta-1-w-2-T-delta-2-bigodot-sigma-prime-net-1"><a href="#delta-1-w-2-T-delta-2-bigodot-sigma-prime-net-1" class="headerlink" title="$\delta^{(1)} = (w^{(2)})^T \delta^{(2)} \bigodot  \sigma^{\prime}(net^{(1)})$"></a>$\delta^{(1)} = (w^{(2)})^T \delta^{(2)} \bigodot \sigma^{\prime}(net^{(1)})$</h4><h4 id="begin-bmatrix-0-6-amp-0-8-0-7-amp-0-9-end-bmatrix-T-cdot-begin-bmatrix-0-01240932-0-08379177-end-bmatrix-bigodot-begin-bmatrix-0-20977282-0-19661193-end-bmatrix"><a href="#begin-bmatrix-0-6-amp-0-8-0-7-amp-0-9-end-bmatrix-T-cdot-begin-bmatrix-0-01240932-0-08379177-end-bmatrix-bigodot-begin-bmatrix-0-20977282-0-19661193-end-bmatrix" class="headerlink" title="$=(\begin{bmatrix} 0.6 &amp; 0.8 \\ 0.7 &amp; 0.9\end{bmatrix}^T \cdot \begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}) \bigodot \begin{bmatrix} 0.20977282 \\ 0.19661193\end{bmatrix}$"></a>$=(\begin{bmatrix} 0.6 &amp; 0.8 \\ 0.7 &amp; 0.9\end{bmatrix}^T \cdot \begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}) \bigodot \begin{bmatrix} 0.20977282 \\ 0.19661193\end{bmatrix}$</h4><h4 id="begin-bmatrix-0-01074218-0-01287516-end-bmatrix"><a href="#begin-bmatrix-0-01074218-0-01287516-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix}$"></a>$=\begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix}$</h4></li><li><h4 id="因为：-frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T"><a href="#因为：-frac-partial-E-total-partial-w-l-delta-l-outh-l-1-T" class="headerlink" title="因为：$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$"></a>因为：$\frac{\partial E_{total}}{\partial w^{(l)}}=\delta^{(l)}(outh^{(l-1)})^T$</h4><h4 id="所以：-2"><a href="#所以：-2" class="headerlink" title="所以："></a>所以：</h4><h4 id="Delta-w-2-delta-2-outh-1-T"><a href="#Delta-w-2-delta-2-outh-1-T" class="headerlink" title="$\Delta w^{(2)} = \delta^{(2)}(outh^{(1)})^T$"></a>$\Delta w^{(2)} = \delta^{(2)}(outh^{(1)})^T$</h4><h4 id="begin-bmatrix-0-01240932-0-08379177-end-bmatrix-cdot-begin-bmatrix-0-70056714-0-73105858-end-bmatrix-T"><a href="#begin-bmatrix-0-01240932-0-08379177-end-bmatrix-cdot-begin-bmatrix-0-70056714-0-73105858-end-bmatrix-T" class="headerlink" title="$=\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix} \cdot \begin{bmatrix} 0.70056714\\ 0.73105858 \end{bmatrix}^T$"></a>$=\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix} \cdot \begin{bmatrix} 0.70056714\\ 0.73105858 \end{bmatrix}^T$</h4><h4 id="begin-bmatrix-0-00869356-amp-0-00907194-0-5870176-amp-0-612567-end-bmatrix"><a href="#begin-bmatrix-0-00869356-amp-0-00907194-0-5870176-amp-0-612567-end-bmatrix" class="headerlink" title="$= \begin{bmatrix} -0.00869356 &amp; -0.00907194 \\ 0.5870176 &amp; 0.612567 \end{bmatrix}$"></a>$= \begin{bmatrix} -0.00869356 &amp; -0.00907194 \\ 0.5870176 &amp; 0.612567 \end{bmatrix}$</h4><h4 id="Delta-w-1-delta-1-x-T"><a href="#Delta-w-1-delta-1-x-T" class="headerlink" title="$\Delta w^{(1)} = \delta^{(1)}x^T$"></a>$\Delta w^{(1)} = \delta^{(1)}x^T$</h4><h4 id="begin-bmatrix-0-01074218-0-01287516-end-bmatrix-cdot-begin-bmatrix-0-5-1-end-bmatrix-T"><a href="#begin-bmatrix-0-01074218-0-01287516-end-bmatrix-cdot-begin-bmatrix-0-5-1-end-bmatrix-T" class="headerlink" title="$=\begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix} \cdot \begin{bmatrix} 0.5\\ 1\end{bmatrix}^T$"></a>$=\begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix} \cdot \begin{bmatrix} 0.5\\ 1\end{bmatrix}^T$</h4><h4 id="begin-bmatrix-0-00537109-amp-0-01074218-0-00643758-amp-0-01287516-end-bmatrix"><a href="#begin-bmatrix-0-00537109-amp-0-01074218-0-00643758-amp-0-01287516-end-bmatrix" class="headerlink" title="$= \begin{bmatrix} 0.00537109&amp; 0.01074218\\ 0.00643758 &amp; 0.01287516 \end{bmatrix}$"></a>$= \begin{bmatrix} 0.00537109&amp; 0.01074218\\ 0.00643758 &amp; 0.01287516 \end{bmatrix}$</h4></li><li><h4 id="权重更新"><a href="#权重更新" class="headerlink" title="权重更新"></a>权重更新</h4><h4 id="w-new-2-w-old-2-Delta-w-2"><a href="#w-new-2-w-old-2-Delta-w-2" class="headerlink" title="$w_{new}^2 = w_{old}^2-\Delta w^{(2)}$"></a>$w_{new}^2 = w_{old}^2-\Delta w^{(2)}$</h4><h4 id="begin-bmatrix-0-6-amp-0-8-0-7-amp-0-9-end-bmatrix-begin-bmatrix-0-00869356-amp-0-00907194-0-5870176-amp-0-612567-end-bmatrix"><a href="#begin-bmatrix-0-6-amp-0-8-0-7-amp-0-9-end-bmatrix-begin-bmatrix-0-00869356-amp-0-00907194-0-5870176-amp-0-612567-end-bmatrix" class="headerlink" title="$= {\begin{bmatrix} 0.6 &amp; 0.8 \\ 0.7 &amp; 0.9\end{bmatrix}}-\begin{bmatrix} -0.00869356 &amp; 0.00907194 \\ 0.5870176 &amp; 0.612567 \end{bmatrix}$"></a>$= {\begin{bmatrix} 0.6 &amp; 0.8 \\ 0.7 &amp; 0.9\end{bmatrix}}-\begin{bmatrix} -0.00869356 &amp; 0.00907194 \\ 0.5870176 &amp; 0.612567 \end{bmatrix}$</h4><h4 id="begin-bmatrix-0-60869356-amp-0-80907194-0-64129824-amp-0-8387433-end-bmatrix"><a href="#begin-bmatrix-0-60869356-amp-0-80907194-0-64129824-amp-0-8387433-end-bmatrix" class="headerlink" title="$= \begin{bmatrix} 0.60869356 &amp; 0.80907194 \\ 0.64129824&amp; 0.8387433 \end{bmatrix}$"></a>$= \begin{bmatrix} 0.60869356 &amp; 0.80907194 \\ 0.64129824&amp; 0.8387433 \end{bmatrix}$</h4><h4 id="b-new-2-b-old-2-Delta-b-2"><a href="#b-new-2-b-old-2-Delta-b-2" class="headerlink" title="$b_{new}^2=b_{old}^2-\Delta b^2$"></a>$b_{new}^2=b_{old}^2-\Delta b^2$</h4><h4 id="begin-bmatrix-1-1-end-bmatrix-begin-bmatrix-0-01240932-0-08379177-end-bmatrix"><a href="#begin-bmatrix-1-1-end-bmatrix-begin-bmatrix-0-01240932-0-08379177-end-bmatrix" class="headerlink" title="$= \begin{bmatrix} 1 \\ 1  \end{bmatrix}-\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}$"></a>$= \begin{bmatrix} 1 \\ 1 \end{bmatrix}-\begin{bmatrix} -0.01240932\\ 0.08379177\end{bmatrix}$</h4><h4 id="begin-bmatrix-1-01240932-0-91620823-end-bmatrix"><a href="#begin-bmatrix-1-01240932-0-91620823-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} 1.01240932\\ 0.91620823\end{bmatrix}$"></a>$=\begin{bmatrix} 1.01240932\\ 0.91620823\end{bmatrix}$</h4><h4 id="w-new-1-w-old-1-Delta-w-1"><a href="#w-new-1-w-old-1-Delta-w-1" class="headerlink" title="$w_{new}^1= w_{old}^1-\Delta w^{(1)}$"></a>$w_{new}^1= w_{old}^1-\Delta w^{(1)}$</h4><h4 id="begin-bmatrix-0-1-amp-0-3-0-2-amp-0-4-end-bmatrix-begin-bmatrix-0-00537109-amp-0-01074218-0-00643758-amp-0-01287516-end-bmatrix"><a href="#begin-bmatrix-0-1-amp-0-3-0-2-amp-0-4-end-bmatrix-begin-bmatrix-0-00537109-amp-0-01074218-0-00643758-amp-0-01287516-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} 0.1 &amp; 0.3 \\ 0.2 &amp; 0.4\end{bmatrix} -  \begin{bmatrix} 0.00537109&amp; 0.01074218\\ 0.00643758 &amp; 0.01287516 \end{bmatrix}$"></a>$=\begin{bmatrix} 0.1 &amp; 0.3 \\ 0.2 &amp; 0.4\end{bmatrix} - \begin{bmatrix} 0.00537109&amp; 0.01074218\\ 0.00643758 &amp; 0.01287516 \end{bmatrix}$</h4><h4 id="begin-bmatrix-0-09462891-amp-0-28925782-0-19356242-amp-0-38712484-end-bmatrix"><a href="#begin-bmatrix-0-09462891-amp-0-28925782-0-19356242-amp-0-38712484-end-bmatrix" class="headerlink" title="$= \begin{bmatrix} 0.09462891&amp; 0.28925782\\ 0.19356242&amp; 0.38712484\end{bmatrix}$"></a>$= \begin{bmatrix} 0.09462891&amp; 0.28925782\\ 0.19356242&amp; 0.38712484\end{bmatrix}$</h4><h4 id="b-new-1-b-old-1-Delta-b-1"><a href="#b-new-1-b-old-1-Delta-b-1" class="headerlink" title="$b_{new}^1=b_{old}^1-\Delta b^1$"></a>$b_{new}^1=b_{old}^1-\Delta b^1$</h4><h4 id="begin-bmatrix-0-5-0-5-end-bmatrix-begin-bmatrix-0-01074218-0-01287516-end-bmatrix"><a href="#begin-bmatrix-0-5-0-5-end-bmatrix-begin-bmatrix-0-01074218-0-01287516-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} 0.5 \\ 0.5  \end{bmatrix} - \begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix}$"></a>$=\begin{bmatrix} 0.5 \\ 0.5 \end{bmatrix} - \begin{bmatrix} 0.01074218\\ 0.01287516\end{bmatrix}$</h4><h4 id="begin-bmatrix-0-48925782-0-48712484-end-bmatrix"><a href="#begin-bmatrix-0-48925782-0-48712484-end-bmatrix" class="headerlink" title="$=\begin{bmatrix} 0.48925782\\ 0.48712484\end{bmatrix}$"></a>$=\begin{bmatrix} 0.48925782\\ 0.48712484\end{bmatrix}$</h4></li></ul><h4 id="就这样，我们就完成了一个简单神经网络的误差反向传播进行权重参数更新"><a href="#就这样，我们就完成了一个简单神经网络的误差反向传播进行权重参数更新" class="headerlink" title="就这样，我们就完成了一个简单神经网络的误差反向传播进行权重参数更新"></a>就这样，我们就完成了一个简单神经网络的误差反向传播进行权重参数更新</h4><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p><img src="/images/dl/64.png" alt="images"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> DNN </tag>
            
            <tag> 反向传播 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络入门-反向传播python实现</title>
      <link href="/2018/08/15/2018-08-15-DL-back-propagation-python/"/>
      <url>/2018/08/15/2018-08-15-DL-back-propagation-python/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="反向传播（back-propagation）算法python实现"><a href="#反向传播（back-propagation）算法python实现" class="headerlink" title="反向传播（back propagation）算法python实现"></a>反向传播（back propagation）算法<code>python</code>实现</h3><h4 id="根据上一章所推导的反向传播（back-propagation）算法所示例代码"><a href="#根据上一章所推导的反向传播（back-propagation）算法所示例代码" class="headerlink" title="根据上一章所推导的反向传播（back propagation）算法所示例代码"></a>根据上一章所推导的<a href="https://sevenold.github.io/2018/08/DL-back-propagation/" target="_blank" rel="noopener">反向传播（back propagation）算法</a>所示例代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"># @Time    : 2018/8/15 21:32</span><br><span class="line"># @Author  : Seven</span><br><span class="line"># @Site    : </span><br><span class="line"># @File    : bp.py</span><br><span class="line"># @Software: PyCharm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sigmod(x):</span><br><span class="line">    return 1 / (1 + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sigmodDerivative(x):</span><br><span class="line"></span><br><span class="line">    return np.multiply(x, np.subtract(1, x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def forward(weightsA, weightsB, bias):</span><br><span class="line">    # 前向传播</span><br><span class="line">    # 隐层</span><br><span class="line">    neth1 = inputX[0] * weightsA[0][0] + inputX[1] * weightsA[0][1] + bias[0]</span><br><span class="line">    outh1 = sigmod(neth1)</span><br><span class="line">    print(&quot;隐层第一个神经元&quot;, neth1, outh1)</span><br><span class="line"></span><br><span class="line">    neth2 = inputX[0] * weightsA[1][0] + inputX[1] * weightsA[1][1] + bias[1]</span><br><span class="line">    outh2 = sigmod(neth2)</span><br><span class="line">    print(&quot;隐层第二个神经元&quot;, neth2, outh2)</span><br><span class="line">    # 输出层</span><br><span class="line">    neto1 = outh1 * weightsB[0][0] + outh2 * weightsB[0][1] + bias[2]</span><br><span class="line">    outo1 = sigmod(neto1)</span><br><span class="line">    print(&quot;输出层第一个神经元&quot;, neto1, outo1)</span><br><span class="line"></span><br><span class="line">    neto2 = outh1 * weightsB[1][0] + outh2 * weightsB[1][1] + bias[3]</span><br><span class="line">    outo2 = sigmod(neto2)</span><br><span class="line">    print(&quot;输出层第二个神经元&quot;, neto2, outo2)</span><br><span class="line"></span><br><span class="line">    # 向量化</span><br><span class="line">    outA = np.array([outh1, outh2])</span><br><span class="line">    outB = np.array([outo1, outo2])</span><br><span class="line"></span><br><span class="line">    Etotal = 1 / 2 * np.subtract(y, outB) ** 2</span><br><span class="line">    print(&quot;误差值：&quot;, Etotal)</span><br><span class="line"></span><br><span class="line">    return outA, outB</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def backpagration(outA, outB):</span><br><span class="line">    # 反向传播</span><br><span class="line">    deltaB = np.multiply(np.subtract(outB, y), sigmodDerivative(outB))</span><br><span class="line">    print(&quot;deltaB：&quot;, deltaB)</span><br><span class="line"></span><br><span class="line">    deltaA = np.multiply(np.matmul(np.transpose(weightsB), deltaB), sigmodDerivative(outA))</span><br><span class="line">    print(&quot;deltaA：&quot;, deltaA)</span><br><span class="line"></span><br><span class="line">    deltaWB = np.matmul(deltaB.reshape(2, 1), outA.reshape(1, 2))</span><br><span class="line">    print(&quot;deltaWB：&quot;, deltaWB)</span><br><span class="line"></span><br><span class="line">    deltaWA = np.matmul(deltaA.reshape(2, 1), inputX.reshape(1, 2))</span><br><span class="line">    print(&quot;deltaWA&quot;, deltaWA)</span><br><span class="line"></span><br><span class="line">    # 权重参数更新</span><br><span class="line">    weightsB_new = np.subtract(weightsB, deltaWB)</span><br><span class="line">    print(&quot;weightsB_new&quot;, weightsB_new)</span><br><span class="line"></span><br><span class="line">    bias[3] = np.subtract(bias[3], deltaB[1])</span><br><span class="line">    print(&quot;biasB&quot;, bias[3])</span><br><span class="line">    bias[2] = np.subtract(bias[2], deltaB[0])</span><br><span class="line">    print(&quot;biasB&quot;, bias[2])</span><br><span class="line"></span><br><span class="line">    weightsA_new = np.subtract(weightsA, deltaWA)</span><br><span class="line">    print(&quot;weightsA_new&quot;, weightsA_new)</span><br><span class="line"></span><br><span class="line">    bias[1] = np.subtract(bias[1], deltaA[1])</span><br><span class="line">    print(&quot;biasA&quot;, bias[1])</span><br><span class="line">    bias[0] = np.subtract(bias[0], deltaA[0])</span><br><span class="line">    print(&quot;biasA&quot;, bias[0])</span><br><span class="line">    print(&quot;all bias&quot;, bias)</span><br><span class="line"></span><br><span class="line">    return weightsA_new, weightsB_new, bias</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    # 初始化数据</span><br><span class="line">    # 权重参数</span><br><span class="line">    bias = np.array([0.5, 0.5, 1.0, 1.0])</span><br><span class="line">    weightsA = np.array([[0.1, 0.3], [0.2, 0.4]])</span><br><span class="line">    weightsB = np.array([[0.6, 0.8], [0.7, 0.9]])</span><br><span class="line">    # 期望值</span><br><span class="line">    y = np.array([1, 0])</span><br><span class="line">    # 输入层</span><br><span class="line">    inputX = np.array([0.5, 1.0])</span><br><span class="line"></span><br><span class="line">    print(&quot;第一次前向传播&quot;)</span><br><span class="line">    outA, outB = forward(weightsA, weightsB, bias)</span><br><span class="line">    print(&quot;反向传播-参数更新&quot;)</span><br><span class="line">    weightsA_new, weightsB_new, bias = backpagration(outA, outB)</span><br><span class="line">    # 更新完毕</span><br><span class="line">    # 验证权重参数--第二次前向传播</span><br><span class="line">    print(&quot;第二次前向传播&quot;)</span><br><span class="line">    forward(weightsA_new, weightsB_new, bias)</span><br></pre></td></tr></table></figure><h4 id="程序执行结果："><a href="#程序执行结果：" class="headerlink" title="程序执行结果："></a>程序执行结果：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">第一次前向传播</span><br><span class="line">隐层第一个神经元 0.85 0.7005671424739729</span><br><span class="line">隐层第二个神经元 1.0 0.7310585786300049</span><br><span class="line">输出层第一个神经元 2.0051871483883876 0.8813406204337122</span><br><span class="line">输出层第二个神经元 2.1483497204987856 0.8955144637754225</span><br><span class="line">误差值： [0.00704002 0.40097308]</span><br><span class="line">反向传播-参数更新</span><br><span class="line">deltaB： [-0.01240932  0.08379177]</span><br><span class="line">deltaA： [0.01074218 0.01287516]</span><br><span class="line">deltaWB： [[-0.00869356 -0.00907194]</span><br><span class="line"> [ 0.05870176  0.0612567 ]]</span><br><span class="line">deltaWA [[0.00537109 0.01074218]</span><br><span class="line"> [0.00643758 0.01287516]]</span><br><span class="line">weightsB_new [[0.60869356 0.80907194]</span><br><span class="line"> [0.64129824 0.8387433 ]]</span><br><span class="line">biasB 0.9162082259892468</span><br><span class="line">biasB 1.0124093185565075</span><br><span class="line">weightsA_new [[0.09462891 0.28925782]</span><br><span class="line"> [0.19356242 0.38712484]]</span><br><span class="line">biasA 0.48712483967909465</span><br><span class="line">biasA 0.48925781687016445</span><br><span class="line">all bias [0.48925782 0.48712484 1.01240932 0.91620823]</span><br><span class="line">第二次前向传播</span><br><span class="line">隐层第一个神经元 0.8258300879578699 0.6954725026123048</span><br><span class="line">隐层第二个神经元 0.971030889277963 0.7253249281967498</span><br><span class="line">输出层第一个神经元 2.022578998544453 0.8831474198595102</span><br><span class="line">输出层第二个神经元 1.970574942645405 0.8776728542726611</span><br><span class="line">误差值： [0.00682726 0.38515482]</span><br></pre></td></tr></table></figure><h4 id="我们可以看出误差值明显下降了，说明我们的权重参数更新是正确的。"><a href="#我们可以看出误差值明显下降了，说明我们的权重参数更新是正确的。" class="headerlink" title="我们可以看出误差值明显下降了，说明我们的权重参数更新是正确的。"></a>我们可以看出误差值明显下降了，说明我们的权重参数更新是正确的。</h4><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> DNN </tag>
            
            <tag> 反向传播 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>神经网络入门-感知器、激活函数</title>
      <link href="/2018/08/14/2018-08-14-DL-method/"/>
      <url>/2018/08/14/2018-08-14-DL-method/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="感知器–神经网络的起源"><a href="#感知器–神经网络的起源" class="headerlink" title="感知器–神经网络的起源"></a>感知器–神经网络的起源</h3><h4 id="神经网络是由人工神经元组成的。我们先来看看人的神经元是什么构造："><a href="#神经网络是由人工神经元组成的。我们先来看看人的神经元是什么构造：" class="headerlink" title="神经网络是由人工神经元组成的。我们先来看看人的神经元是什么构造："></a>神经网络是由人工神经元组成的。我们先来看看人的神经元是什么构造：</h4><p><img src="/images/dl/47.png" alt="images"></p><h4 id="感知器是激活函数为阶跃函数的神经元。感知器的模型如下："><a href="#感知器是激活函数为阶跃函数的神经元。感知器的模型如下：" class="headerlink" title="感知器是激活函数为阶跃函数的神经元。感知器的模型如下："></a>感知器是激活函数为阶跃函数的神经元。感知器的模型如下：</h4><p><img src="/images/dl/48.png" alt="images"></p><p>是不是感觉很一样啊 ，神经元也叫做<strong>感知器</strong>。感知器算法在上个世纪50-70年代很流行，也成功解决了很多问题。并且，感知器算法也是非常简单的。</p><h3 id="感知器的定义"><a href="#感知器的定义" class="headerlink" title="感知器的定义"></a>感知器的定义</h3><p><img src="/images/dl/48.png" alt="images"></p><p>我们再来分析下上图这个感知器，可以看到，一个感知器有如下几个组成部分：</p><ul><li><strong>输入(inputs)</strong>：一个感知器可以接收多个输入$(x_1,x_2,…,x_n \vert x_i \in R)$</li><li><strong>权值(weights)</strong>：每一个输入上都有一个<code>权值</code>$w_i \in R$，此外还有一个<strong>偏置项</strong>$b \in R$，也就是上图的$w_0$。</li><li><strong>加权和(weighted sum)</strong>：就是<code>输入权值 x</code> x <code>权值 w</code> + <code>偏置项 b</code>的总和。</li><li><strong>激活函数(step function)</strong>：感知器的激活函数：$f(x)=\begin{cases} 0&amp; x&gt;0 \ 1&amp; x \le 0 \end{cases}$</li><li><strong>输出(output)</strong>：感知器的输出由<code>加权值</code>用<code>激活函数</code>做<code>非线性变换</code>。也就是这个公式：$y=f(w\cdot x +b )$</li></ul><h4 id="举个栗子："><a href="#举个栗子：" class="headerlink" title="举个栗子："></a>举个栗子：</h4><p>我们使用<code>unit激活函数</code>结合上图就有：</p><ul><li><h4 id="y-f-w-cdot-x-b-f-w-1x-1-w-2x-2-w-3x-3-bias"><a href="#y-f-w-cdot-x-b-f-w-1x-1-w-2x-2-w-3x-3-bias" class="headerlink" title="$y=f(w\cdot x +b )=f(w_1x_1+w_2x_2+w_3x_3+bias)$"></a>$y=f(w\cdot x +b )=f(w_1x_1+w_2x_2+w_3x_3+bias)$</h4></li><li><h5 id="其中-f-x-就是激活函数-f-x-begin-cases-1-amp-x-gt-0-0-amp-x-le-0-end-cases-，图像如下图所示。"><a href="#其中-f-x-就是激活函数-f-x-begin-cases-1-amp-x-gt-0-0-amp-x-le-0-end-cases-，图像如下图所示。" class="headerlink" title="其中$f(x)$就是激活函数 $f(x)=  \begin{cases} 1&amp;  x&gt;0 \ 0&amp; x  \le 0 \end{cases}$ ，图像如下图所示。"></a>其中$f(x)$就是激活函数 $f(x)= \begin{cases} 1&amp; x&gt;0 \ 0&amp; x \le 0 \end{cases}$ ，图像如下图所示。</h5></li></ul><p><img src="/images/dl/49.png" alt="images"></p><h3 id="感知器的前馈计算"><a href="#感知器的前馈计算" class="headerlink" title="感知器的前馈计算"></a>感知器的前馈计算</h3><p>再举个栗子：我们来计算下这个感知器：</p><p><img src="/images/dl/53.png" alt="images"></p><p>其中<code>激活函数f</code>:$f(x)= \begin{cases} 1&amp; x&gt;0 \ 0&amp; x \le 0 \end{cases}$</p><ul><li><p><code>加权和</code>：logits = 1.0 * (-0.2) + 0.5 * (-0.4) + (-1.4) * 1.3 + 2.0 * 3.0 = 1.98</p></li><li><p><code>输出值</code>：output = f(logits) = f(1.98) = 1</p></li></ul><p>如果数据很多呢，我们就要把数据向量化：</p><p>例如：</p><h4 id="x-1-1-0-3-0-2-0-x-2-2-0-1-0-5-0-x-3-2-0-0-0-3-0-x-4-4-0-1-0-6-0-w-4-0-3-0-5-0-b-2-0"><a href="#x-1-1-0-3-0-2-0-x-2-2-0-1-0-5-0-x-3-2-0-0-0-3-0-x-4-4-0-1-0-6-0-w-4-0-3-0-5-0-b-2-0" class="headerlink" title="$x_1=[-1.0, 3.0, 2.0] \\ x_2=[2.0, -1.0, 5.0] \\ x_3=[-2.0, 0.0, 3.0 ] \\ x_4=[4.0, 1.0, 6.0] \\ w=[4.0, -3.0, 5.0 ] \\ b=2.0$"></a>$x_1=[-1.0, 3.0, 2.0] \\ x_2=[2.0, -1.0, 5.0] \\ x_3=[-2.0, 0.0, 3.0 ] \\ x_4=[4.0, 1.0, 6.0] \\ w=[4.0, -3.0, 5.0 ] \\ b=2.0$</h4><h4 id="则：-X-begin-bmatrix-1-0-amp-3-0-amp-2-0-2-0-amp-1-0-amp-5-0-2-0-amp-0-0-amp-3-0-4-0-amp-1-0-amp-6-0-end-bmatrix"><a href="#则：-X-begin-bmatrix-1-0-amp-3-0-amp-2-0-2-0-amp-1-0-amp-5-0-2-0-amp-0-0-amp-3-0-4-0-amp-1-0-amp-6-0-end-bmatrix" class="headerlink" title="则：$X=\begin{bmatrix}  -1.0 &amp; 3.0 &amp; 2.0 \\ 2.0 &amp; -1.0&amp; 5.0 \\ -2.0&amp; 0.0&amp; 3.0 \\ 4.0&amp; 1.0 &amp; 6.0  \end{bmatrix}$"></a>则：$X=\begin{bmatrix} -1.0 &amp; 3.0 &amp; 2.0 \\ 2.0 &amp; -1.0&amp; 5.0 \\ -2.0&amp; 0.0&amp; 3.0 \\ 4.0&amp; 1.0 &amp; 6.0 \end{bmatrix}$</h4><h4 id="w-T-begin-bmatrix-4-0-3-0-5-0-end-bmatrix"><a href="#w-T-begin-bmatrix-4-0-3-0-5-0-end-bmatrix" class="headerlink" title="$w^T =\begin{bmatrix} 4.0 \\ -3.0 \\ 5.0 \end{bmatrix}$"></a>$w^T =\begin{bmatrix} 4.0 \\ -3.0 \\ 5.0 \end{bmatrix}$</h4><h4 id="所以：-logits-X-cdot-w-T-b-begin-bmatrix-1-0-amp-3-0-amp-2-0-2-0-amp-1-0-amp-5-0-2-0-amp-0-0-amp-3-0-4-0-amp-1-0-amp-6-0-end-bmatrix-cdot-begin-bmatrix-4-0-3-0-5-0-end-bmatrix-2-0-1-0-38-0-7-0-43-0"><a href="#所以：-logits-X-cdot-w-T-b-begin-bmatrix-1-0-amp-3-0-amp-2-0-2-0-amp-1-0-amp-5-0-2-0-amp-0-0-amp-3-0-4-0-amp-1-0-amp-6-0-end-bmatrix-cdot-begin-bmatrix-4-0-3-0-5-0-end-bmatrix-2-0-1-0-38-0-7-0-43-0" class="headerlink" title="所以：$logits =  X\cdot w^T + b= \begin{bmatrix}  -1.0 &amp; 3.0 &amp; 2.0 \\ 2.0 &amp; -1.0&amp; 5.0 \\ -2.0&amp; 0.0&amp; 3.0 \\ 4.0&amp; 1.0 &amp; 6.0  \end{bmatrix} \cdot \begin{bmatrix} 4.0 \\ -3.0 \\ 5.0 \end{bmatrix} + 2.0 \\ =[-1.0 \ \ \  38.0 \ \ \ 7.0 \ \ \ 43.0 ]$"></a>所以：$logits = X\cdot w^T + b= \begin{bmatrix} -1.0 &amp; 3.0 &amp; 2.0 \\ 2.0 &amp; -1.0&amp; 5.0 \\ -2.0&amp; 0.0&amp; 3.0 \\ 4.0&amp; 1.0 &amp; 6.0 \end{bmatrix} \cdot \begin{bmatrix} 4.0 \\ -3.0 \\ 5.0 \end{bmatrix} + 2.0 \\ =[-1.0 \ \ \ 38.0 \ \ \ 7.0 \ \ \ 43.0 ]$</h4><h4 id="最后带入激活函数："><a href="#最后带入激活函数：" class="headerlink" title="最后带入激活函数："></a>最后带入激活函数：</h4><h4 id="则：-output-f-x-0-1-1-1"><a href="#则：-output-f-x-0-1-1-1" class="headerlink" title="则：$output = f(x)=[0\ \ \ 1 \ \ \ 1 \ \ \ 1 ]$"></a>则：$output = f(x)=[0\ \ \ 1 \ \ \ 1 \ \ \ 1 ]$</h4><h3 id="感知器的运用"><a href="#感知器的运用" class="headerlink" title="感知器的运用"></a>感知器的运用</h3><p>使用感知器可以完成一些基础的逻辑操作</p><p>例如<code>逻辑与</code></p><p><img src="/images/dl/54.png" alt="images"></p><p>注意：激活函数是<code>unit激活函数</code>，再看看其他逻辑运算</p><p><img src="/images/dl/55.png" alt="images"></p><h3 id="感知器的局限性"><a href="#感知器的局限性" class="headerlink" title="感知器的局限性"></a>感知器的局限性</h3><ul><li>仅能做0-1输出</li><li>仅能处理线性分类的问题（无法处理XOR问题）</li></ul><h3 id="多层感知机–现代神经网络的原型"><a href="#多层感知机–现代神经网络的原型" class="headerlink" title="多层感知机–现代神经网络的原型"></a>多层感知机–现代神经网络的原型</h3><p>对比于感知器，引入了隐层，改变了激活函数，加入反向传播算法，优化算法，也就是后面要讲的神经网络。</p><h3 id="隐层的定义"><a href="#隐层的定义" class="headerlink" title="隐层的定义"></a>隐层的定义</h3><p><img src="/images/dl/56.png" alt="images"></p><p>从上图中可以看出：</p><ul><li>所有同一层的神经元都与上一层的每个输出相连</li><li>同一层的神经元之间不相互连接</li><li>各神经元的输出为数值</li></ul><h3 id="隐层的结构"><a href="#隐层的结构" class="headerlink" title="隐层的结构"></a>隐层的结构</h3><p><img src="/images/dl/57.png" alt="images"></p><h4 id="举个栗子：-1"><a href="#举个栗子：-1" class="headerlink" title="举个栗子："></a>举个栗子：</h4><p><img src="/images/dl/58.png" alt="images"></p><p>由上图可知：</p><h4 id="h-1-f-2x-1-2x-2-1-h-2-f-2x-1-2x-2-3-o-1-f-2h-1-2h-2-3"><a href="#h-1-f-2x-1-2x-2-1-h-2-f-2x-1-2x-2-3-o-1-f-2h-1-2h-2-3" class="headerlink" title="$h_1 = f(2x_1+2x_2-1) \\ h_2=f(-2x_1+-2x_2+3) \\ o_1 = f(2h_1+2h_2-3)$"></a>$h_1 = f(2x_1+2x_2-1) \\ h_2=f(-2x_1+-2x_2+3) \\ o_1 = f(2h_1+2h_2-3)$</h4><p>其中$f(x)$是激活函数</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>新的激活函数：</p><ul><li><h4 id="unit激活函数：-f-x-unit-x-begin-cases-0-amp-x-gt-0-1-amp-x-le-0-end-cases"><a href="#unit激活函数：-f-x-unit-x-begin-cases-0-amp-x-gt-0-1-amp-x-le-0-end-cases" class="headerlink" title="unit激活函数：$f(x)=unit(x)=  \begin{cases} 0&amp;  x&gt;0 \ 1&amp; x  \le 0 \end{cases} $"></a><strong><code>unit激活函数</code></strong>：$f(x)=unit(x)= \begin{cases} 0&amp; x&gt;0 \ 1&amp; x \le 0 \end{cases} $</h4><p><img src="/images/dl/49.png" alt="images"></p></li><li><h4 id="sigmod激活函数：-f-x-sigmod-x-frac-1-1-e-x"><a href="#sigmod激活函数：-f-x-sigmod-x-frac-1-1-e-x" class="headerlink" title="sigmod激活函数：$f(x)=sigmod(x)=\frac{1}{1+e^{-x}}$"></a><strong><code>sigmod激活函数</code></strong>：$f(x)=sigmod(x)=\frac{1}{1+e^{-x}}$</h4><p><img src="/images/dl/50.png" alt="images"></p></li><li><h5 id="tanh激活函数：-f-x-tanh-x-frac-e-x-e-x-e-x-e-x"><a href="#tanh激活函数：-f-x-tanh-x-frac-e-x-e-x-e-x-e-x" class="headerlink" title="tanh激活函数：$f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$"></a><strong><code>tanh激活函数</code></strong>：$f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$</h5><p>​ <img src="/images/dl/51.png" alt="images">`</p></li><li><h5 id="ReLU激活函数：-f-x-ReLU-x-begin-cases-x-amp-x-gt-0-0-amp-x-le-0-end-cases"><a href="#ReLU激活函数：-f-x-ReLU-x-begin-cases-x-amp-x-gt-0-0-amp-x-le-0-end-cases" class="headerlink" title="ReLU激活函数：$f(x)=ReLU(x)=\begin{cases} x&amp; x&gt;0\ 0&amp; x \le 0 \end{cases} $"></a><code>ReLU激活函数</code>：$f(x)=ReLU(x)=\begin{cases} x&amp; x&gt;0\ 0&amp; x \le 0 \end{cases} $</h5><p><img src="/images/dl/52.png" alt="images"></p></li></ul><h3 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h3><ul><li><p><strong>引入非线性因素。</strong></p><p>在我们面对线性可分的数据集的时候，简单的用线性分类器即可解决分类问题。但是现实生活中的数据往往不是线性可分的，面对这样的数据，一般有两个方法：引入非线性函数、线性变换。</p></li><li><h4 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h4><p>就是把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</p></li></ul><h3 id="激活函数的特点"><a href="#激活函数的特点" class="headerlink" title="激活函数的特点"></a>激活函数的特点</h3><ul><li><p><code>unit</code>：线性分界</p><p>– 几乎已经不用了</p></li><li><p><code>sigmoid</code>：非线性分界</p><p>– 两端软饱和，输出为 (0,1)区间</p><p>– 两端有梯度消失问题</p><p>– 因为输出恒正，可能有 zig现象</p></li><li><p><code>tanh</code>：非线性分界 ：非线性分界</p><p>– 两端软饱和，输出为 (-1, 1) 区间</p><p>– 仍然存在梯度消失问题</p><p>– 没有 zig，收敛更快 (LeCun 1989)</p></li><li><p><code>ReLU</code>：非线性分界<br>– 左侧硬饱和，右无输出为 [0,+∞)区间</p><p>– 左侧会出现梯度一直为 0的情况，导致神经元 不再更新（死亡）</p><p>– 改善了梯度弥散</p><p>– 同样存在 zig</p></li></ul><h3 id="一些新的激活函数"><a href="#一些新的激活函数" class="headerlink" title="一些新的激活函数"></a>一些新的激活函数</h3><p><img src="/images/dl/59.png" alt="images"><br>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> DNN </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow从入门到实践</title>
      <link href="/2018/08/13/2018-08-13-TensorFlow-course/"/>
      <url>/2018/08/13/2018-08-13-TensorFlow-course/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="TensorFlow-从入门到实践"><a href="#TensorFlow-从入门到实践" class="headerlink" title="TensorFlow 从入门到实践"></a>TensorFlow 从入门到实践</h3><p>导入TensorFlow模块，查看下当前模块的版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">print(tf.__version__)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.4.0</span><br></pre></td></tr></table></figure><p>tensorflow是一种图计算框架，所有的计算操作被声明为图（graph）中的节点（Node）</p><p>即使只是声明一个变量或者常量，也并不执行实际的操作，而是向图中增加节点</p><p>我们来看一上前面安装TensorFlow的时候的测试代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建名为hello_constant的TensorFlow对象</span></span><br><span class="line">hello_constant = tf.constant(<span class="string">'Hello World!'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在session中运行tf.constant 操作</span></span><br><span class="line">    output = sess.run(hello_constant)</span><br><span class="line">    </span><br><span class="line">    print(output)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b&apos;Hello World!&apos;</span><br></pre></td></tr></table></figure><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><h4 id="在-TensorFlow-中，数据不是以整数，浮点数或者字符串形式存在的。"><a href="#在-TensorFlow-中，数据不是以整数，浮点数或者字符串形式存在的。" class="headerlink" title="在 TensorFlow 中，数据不是以整数，浮点数或者字符串形式存在的。"></a>在 TensorFlow 中，数据不是以整数，浮点数或者字符串形式存在的。</h4><h4 id="这些值被封装在一个叫做-tensor-的对象中。"><a href="#这些值被封装在一个叫做-tensor-的对象中。" class="headerlink" title="这些值被封装在一个叫做 tensor 的对象中。"></a>这些值被封装在一个叫做 tensor 的对象中。</h4><p>在 hello_constant = tf.constant(‘Hello World!’) 代码中，<br>hello_constant是一个 0 维度的字符串 tensor。</p><h4 id="tensors-还有很多不同大小："><a href="#tensors-还有很多不同大小：" class="headerlink" title="tensors 还有很多不同大小："></a>tensors 还有很多不同大小：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A 是一个0维的 int32 tensor</span></span><br><span class="line">A = tf.constant(<span class="number">1234</span>) </span><br><span class="line"><span class="comment"># B 是一个1维的  int32 tensor</span></span><br><span class="line">B = tf.constant([<span class="number">123</span>,<span class="number">456</span>,<span class="number">789</span>]) </span><br><span class="line"> <span class="comment"># C 是一个2维的  int32 tensor</span></span><br><span class="line">C = tf.constant([ [<span class="number">123</span>,<span class="number">456</span>,<span class="number">789</span>], [<span class="number">222</span>,<span class="number">333</span>,<span class="number">444</span>] ])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    out_A = sess.run(A)</span><br><span class="line">    out_B = sess.run(B)</span><br><span class="line">    out_C = sess.run(C)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'\n'</span>, out_A, <span class="string">'\n'</span>, <span class="string">'----------'</span>, <span class="string">'\n'</span>, out_B, <span class="string">'\n'</span>, <span class="string">'----------'</span>, <span class="string">'\n'</span>, out_C)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1234 </span><br><span class="line">---------- </span><br><span class="line">[123 456 789] </span><br><span class="line">---------- </span><br><span class="line">[[123 456 789]</span><br><span class="line">[222 333 444]]</span><br></pre></td></tr></table></figure><h3 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h3><p>TensorFlow 的 api 构建在 computational graph（计算图) 的概念上，它是一种对数学运算过程进行可视化的一种方法。让我们把你刚才运行的 TensorFlow 的代码变成一个图：</p><p><img src="/images/dl/44.png" alt="44.png"></p><p>如上图所示，一个 “TensorFlow Session” 是用来运行图的环境。这个 session 负责分配 GPU(s) 和／或 CPU(s) 包括远程计算机的运算。让我们看看怎么使用它：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    output = sess.run(hello_constant)</span><br></pre></td></tr></table></figure><p>这段代码已经从上一行创建了一个 tensor hello_constant。下一行是在session里对 tensor 求值。</p><p>这段代码用 tf.Session 创建了一个sess的 session 实例。 sess.run() 函数对 tensor 求值，并返回结果。</p><h3 id="constant运算"><a href="#constant运算" class="headerlink" title="constant运算"></a>constant运算</h3><p>我们先计算一个线性函数，y=wx+b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">w = tf.Variable([[<span class="number">1.</span>, <span class="number">2.</span>]])</span><br><span class="line">x = tf.Variable([[<span class="number">3.</span>], [<span class="number">4.</span>]])</span><br><span class="line">b = tf.constant(<span class="number">.9</span>)</span><br><span class="line"></span><br><span class="line">y = tf.add(tf.matmul(w, x), b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 变量必须要初始化后才能使用</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(y.eval())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[11.9]]</span><br></pre></td></tr></table></figure><p>计算图中所有的数据均以tensor来存储和表达。<br>tensor是一个高阶张量，二阶张量为矩阵，一阶张量为向量，0阶张量为一个数（标量）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="number">0</span>, name=<span class="string">'B'</span>)</span><br><span class="line">b = tf.constant(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;B:0&quot;, shape=(), dtype=int32)</span><br><span class="line">Tensor(&quot;Const_5:0&quot;, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure><p>其他计算操作也同样如此。 tensorflow中的大部分操作都需要通过tf.xxxxx的方式进行调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = tf.add(a,b)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;Add_1:0&quot;, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure><p>从加法开始， tf.add() 完成的工作与你期望的一样。它把两个数字，两个 tensor，返回他们的和。减法和乘法同样也是直接调用对应的接口函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.subtract(<span class="number">10</span>, <span class="number">4</span>) <span class="comment"># 6</span></span><br><span class="line">y = tf.multiply(<span class="number">2</span>, <span class="number">5</span>)  <span class="comment"># 10</span></span><br></pre></td></tr></table></figure><h3 id="还有很多关于数学的api-你可以自己去查阅文档"><a href="#还有很多关于数学的api-你可以自己去查阅文档" class="headerlink" title="还有很多关于数学的api,你可以自己去查阅文档"></a>还有很多关于数学的api,你可以自己去查阅<a href="https://www.tensorflow.org/api_guides/python/math_ops" target="_blank" rel="noopener">文档</a></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mat_a = tf.constant([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]])</span><br><span class="line">mat_b = tf.constant([[<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>],[<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>]], name=<span class="string">'mat_b'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mul_a_b = mat_a * mat_b</span><br><span class="line">tf_mul_a_b = tf.multiply(mat_a, mat_b)</span><br><span class="line">tf_matmul_a_b = tf.matmul(mat_a, tf.transpose(mat_b), name=<span class="string">'matmul_with_name'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(mul_a_b)</span><br><span class="line">print(tf_mul_a_b)</span><br><span class="line">print(tf_matmul_a_b)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;mul:0&quot;, shape=(2, 3), dtype=int32)</span><br><span class="line">Tensor(&quot;Mul_1:0&quot;, shape=(2, 3), dtype=int32)</span><br><span class="line">Tensor(&quot;matmul_with_name:0&quot;, shape=(2, 2), dtype=int32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    mul_value, tf_mul_value, tf_matmul_value = sess.run([mul_a_b, tf_mul_a_b, tf_matmul_a_b])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(mul_value)</span><br><span class="line">print(tf_mul_value)</span><br><span class="line">print(tf_matmul_value)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[[ 2  2  2]</span><br><span class="line"> [15 15 15]]</span><br><span class="line">[[ 2  2  2]</span><br><span class="line"> [15 15 15]]</span><br><span class="line">[[ 6 15]</span><br><span class="line"> [18 45]]</span><br></pre></td></tr></table></figure><p>我们再举几个具体的实例来看看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br><span class="line">x = tf.add(a, b)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">'./graphs/const_add'</span>, sess.graph)</span><br><span class="line">print(sess.run(x))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([<span class="number">2</span>, <span class="number">2</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>]], name=<span class="string">'b'</span>)</span><br><span class="line">x = tf.multiply(a, b, name=<span class="string">'dot_product'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">'./graphs/const_mul'</span>, sess.graph) </span><br><span class="line">print(sess.run(x))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[0 2]</span><br><span class="line"> [4 6]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">2</span>, <span class="number">2</span>],[<span class="number">1</span>, <span class="number">4</span>]], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>]], name=<span class="string">'b'</span>)</span><br><span class="line">x = tf.multiply(a, b, name=<span class="string">'dot_product'</span>)</span><br><span class="line">y = tf.matmul(a, b, name=<span class="string">'mat_mul'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">'./graphs/const_mul_2'</span>, sess.graph) </span><br><span class="line">print(sess.run(x))</span><br><span class="line">print(sess.run(y))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[ 0  2]</span><br><span class="line"> [ 2 12]]</span><br><span class="line">[[ 4  8]</span><br><span class="line"> [ 8 13]]</span><br></pre></td></tr></table></figure><h3 id="各式各样的常量"><a href="#各式各样的常量" class="headerlink" title="各式各样的常量"></a>各式各样的常量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.zeros([<span class="number">2</span>, <span class="number">3</span>], tf.int32)</span><br><span class="line">y = tf.zeros_like(x, optimize=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;zeros_like:0&quot;, shape=(2, 3), dtype=int32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[0 0 0]</span><br><span class="line"> [0 0 0]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t_0 = <span class="number">19</span> </span><br><span class="line">x = tf.zeros_like(t_0) <span class="comment"># ==&gt; 0</span></span><br><span class="line">y = tf.ones_like(t_0) <span class="comment"># ==&gt; 1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run([x]))</span><br><span class="line">    print(sess.run([y]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[0]</span><br><span class="line">[1]</span><br></pre></td></tr></table></figure><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'meh'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    a = tf.get_variable(<span class="string">'a'</span>, [<span class="number">10</span>])</span><br><span class="line">    b = tf.get_variable(<span class="string">'b'</span>, [<span class="number">100</span>])</span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">'./graphs/test'</span>, tf.get_default_graph())</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>例如：求导也是一个运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">2.0</span>)</span><br><span class="line">y = <span class="number">2.0</span> * (x ** <span class="number">3</span>)</span><br><span class="line">z = <span class="number">3.0</span> + y ** <span class="number">2</span></span><br><span class="line">grad_z = tf.gradients(z, y)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(x.initializer)</span><br><span class="line">    print(sess.run(grad_z))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[32.0]</span><br></pre></td></tr></table></figure><h4 id="一次性初始化所有变量"><a href="#一次性初始化所有变量" class="headerlink" title="一次性初始化所有变量"></a>一次性初始化所有变量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">my_var = tf.Variable(<span class="number">2</span>, name=<span class="string">"my_var"</span>) </span><br><span class="line"></span><br><span class="line">my_var_times_two = my_var.assign(<span class="number">2</span> * my_var)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(my_var_times_two))</span><br><span class="line">    print(sess.run(my_var_times_two))</span><br><span class="line">    print(sess.run(my_var_times_two))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">8</span><br><span class="line">16</span><br></pre></td></tr></table></figure><hr><p>在前面我们都是采用，先赋值再计算的方法，但是在我们的工作过程中，我会遇到先定义变量，后赋值的情况。<br>如果你想用一个非常量 non-constant 该怎么办？这就是 tf.placeholder() 和 feed_dict 派上用场的时候了。我们来看看如何向 TensorFlow 传输数据的基本知识。</p><h3 id="tf-placeholder"><a href="#tf-placeholder" class="headerlink" title="tf.placeholder()"></a>tf.placeholder()</h3><p>当你你不能把数据赋值到 x 在把它传给 TensorFlow。因为后面你需要你的 TensorFlow 模型对不同的数据集采取不同的参数。这时你需要 tf.placeholder()！</p><p>数据经过 tf.session.run() 函数得到的值，由 tf.placeholder() 返回成一个 tensor，这样你可以在 session 开始跑之前，设置输入。</p><h4 id="Session’s-feed-dict-功能"><a href="#Session’s-feed-dict-功能" class="headerlink" title="Session’s feed_dict 功能"></a>Session’s feed_dict 功能</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.string)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    out = sess.run(x, feed_dict=&#123;x: <span class="string">'Hello World'</span>&#125;)</span><br><span class="line">    print(out)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello World</span><br></pre></td></tr></table></figure><p>用 tf.session.run() 里 feed_dict 参数设置占位 tensor。上面的例子显示 tensor x 被设置成字符串 “Hello, world”。当然你也可以用 feed_dict 设置多个 tensor。</p><p>placeholder是一个占位符，它通常代表着从外界输入的值。 其中None代表着尚不确定的维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line">output = tf.multiply(input1, input2)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run([output], feed_dict=&#123;input1:[<span class="number">7.</span>], input2:[<span class="number">2.</span>]&#125;))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[array([14.], dtype=float32)]</span><br></pre></td></tr></table></figure><h3 id="把numpy转换成Tensor"><a href="#把numpy转换成Tensor" class="headerlink" title="把numpy转换成Tensor"></a>把numpy转换成Tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">print(a)</span><br><span class="line">print(<span class="string">'----------------'</span>)</span><br><span class="line">ta = tf.convert_to_tensor(a)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">     print(sess.run(ta))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[0. 0. 0.]</span><br><span class="line"> [0. 0. 0.]</span><br><span class="line"> [0. 0. 0.]]</span><br><span class="line">----------------</span><br><span class="line">[[0. 0. 0.]</span><br><span class="line"> [0. 0. 0.]</span><br><span class="line"> [0. 0. 0.]]</span><br></pre></td></tr></table></figure><h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><p>为了让特定运算能运行，有时会对类型进行转换。例如，你尝试下列代码，会报错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.subtract(tf.constant(<span class="number">2.0</span>),tf.constant(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: Input &apos;y&apos; of &apos;Sub&apos; Op has type int32 that does not match type float32 of argument &apos;x&apos;.</span><br></pre></td></tr></table></figure><p>只是因为常量 1 是整数，但是常量 2.0 是浮点数 subtract 需要他们能相符。</p><p>在这种情况下，你可以让数据都是同一类型，或者强制转换一个值到另一个类型。这里，我们可以把 2.0 转换成整数再相减，这样就能得出正确的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">number = tf.subtract(tf.cast(tf.constant(<span class="number">2.0</span>), tf.int32), tf.constant(<span class="number">1</span>))   </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(number))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><h3 id="tensorboard-了解图结构-可视化利器"><a href="#tensorboard-了解图结构-可视化利器" class="headerlink" title="tensorboard 了解图结构/可视化利器"></a>tensorboard 了解图结构/可视化利器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant(<span class="number">2</span>, name=<span class="string">'A'</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>, name=<span class="string">'B'</span>)</span><br><span class="line">x = tf.add(a,b, name=<span class="string">'addAB'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(x))</span><br><span class="line">    <span class="comment">#写到日志文件里</span></span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">'./graphs/add_test'</span>, sess.graph)</span><br><span class="line">    <span class="comment">#关闭writer</span></span><br><span class="line">    writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5</span><br></pre></td></tr></table></figure><h3 id="启动-TensorBoard"><a href="#启动-TensorBoard" class="headerlink" title="启动 TensorBoard"></a>启动 TensorBoard</h3><p>在命令端运行：tensorboard –logdir=”./graphs” –port 7007</p><p>然后打开Google浏览器访问：<a href="http://localhost:7007/" target="_blank" rel="noopener">http://localhost:7007/</a></p><h3 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h3><h4 id="TensorFlow实现线性回归"><a href="#TensorFlow实现线性回归" class="headerlink" title="TensorFlow实现线性回归"></a>TensorFlow实现线性回归</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>设置生成的图像尺寸和去除警告</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>] = <span class="string">'2'</span></span><br><span class="line">plt.rcParams[<span class="string">"figure.figsize"</span>] = (<span class="number">14</span>, <span class="number">8</span>)  <span class="comment"># 生成的图像尺寸</span></span><br></pre></td></tr></table></figure><p>随机生成一个线性的数据，当然你可以换成读取对应的数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n_observations = <span class="number">100</span></span><br><span class="line">xs = np.linspace(<span class="number">-3</span>, <span class="number">3</span>, n_observations)</span><br><span class="line">ys = <span class="number">0.8</span>*xs + <span class="number">0.1</span> + np.random.uniform(<span class="number">-0.5</span>, <span class="number">0.5</span>, n_observations)</span><br><span class="line">plt.scatter(xs, ys)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/dl/45.png" alt="images"></p><h3 id="准备好placeholder"><a href="#准备好placeholder" class="headerlink" title="准备好placeholder"></a>准备好placeholder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = tf.placeholder(tf.float32, name=<span class="string">'X'</span>)</span><br><span class="line">Y = tf.placeholder(tf.float32, name=<span class="string">'Y'</span>)</span><br></pre></td></tr></table></figure><h3 id="初始化参数-权重"><a href="#初始化参数-权重" class="headerlink" title="初始化参数/权重"></a>初始化参数/权重</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(tf.random_normal([<span class="number">1</span>]), name=<span class="string">'weight'</span>)</span><br><span class="line">b = tf.Variable(tf.random_normal([<span class="number">1</span>]), name=<span class="string">'bias'</span>)</span><br></pre></td></tr></table></figure><h3 id="计算预测结果"><a href="#计算预测结果" class="headerlink" title="计算预测结果"></a>计算预测结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y_pred = tf.add(tf.multiply(X, W), b)</span><br></pre></td></tr></table></figure><h3 id="计算损失值函数"><a href="#计算损失值函数" class="headerlink" title="计算损失值函数"></a>计算损失值函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.square(Y - Y_pred, name=<span class="string">'loss'</span>)</span><br></pre></td></tr></table></figure><h3 id="初始化optimizer"><a href="#初始化optimizer" class="headerlink" title="初始化optimizer"></a>初始化optimizer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span><br></pre></td></tr></table></figure><h3 id="指定迭代次数，并在session里执行graph"><a href="#指定迭代次数，并在session里执行graph" class="headerlink" title="指定迭代次数，并在session里执行graph"></a>指定迭代次数，并在session里执行graph</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">n_samples = xs.shape[<span class="number">0</span>]</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 记得初始化所有变量</span></span><br><span class="line">    sess.run(init)</span><br><span class="line"></span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">'./graphs/linear_regression'</span>, sess.graph)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(xs, ys):</span><br><span class="line">            <span class="comment"># 通过feed_dic把数据灌进去</span></span><br><span class="line">            _, loss_value = sess.run([optimizer, loss], feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line">            total_loss += loss_value</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch &#123;0&#125;: &#123;1&#125;'</span>.format(i, total_loss / n_samples))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关闭writer</span></span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取出w和b的值</span></span><br><span class="line">    W, b = sess.run([W, b])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0: [0.7908481]</span><br><span class="line">Epoch 5: [0.07929672]</span><br><span class="line">Epoch 10: [0.07929661]</span><br><span class="line">Epoch 15: [0.07929661]</span><br><span class="line">Epoch 20: [0.07929661]</span><br><span class="line">Epoch 25: [0.07929661]</span><br><span class="line">Epoch 30: [0.07929661]</span><br><span class="line">Epoch 35: [0.07929661]</span><br><span class="line">Epoch 40: [0.07929661]</span><br><span class="line">Epoch 45: [0.07929661]</span><br></pre></td></tr></table></figure><h3 id="打印最后更新的w、b的值"><a href="#打印最后更新的w、b的值" class="headerlink" title="打印最后更新的w、b的值"></a>打印最后更新的w、b的值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(W, b)</span><br><span class="line">print(<span class="string">"W:"</span>+str(W[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">"b:"</span>+str(b[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[0.82705235] [0.16835527]</span><br><span class="line">W:0.82705235</span><br><span class="line">b:0.16835527</span><br></pre></td></tr></table></figure><h3 id="画出线性回归线"><a href="#画出线性回归线" class="headerlink" title="画出线性回归线"></a>画出线性回归线</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(xs, ys, <span class="string">'bo'</span>, label=<span class="string">'Real data'</span>)</span><br><span class="line">plt.plot(xs, xs * W + b, <span class="string">'r'</span>, label=<span class="string">'Predicted data'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/dl/46.png" alt="44.png"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>使用 Virtualenv 进行安装TensorFlow</title>
      <link href="/2018/08/11/2018-08-11-TensorFlow-Virtualenv/"/>
      <url>/2018/08/11/2018-08-11-TensorFlow-Virtualenv/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="使用-Virtualenv-进行安装TensorFlow"><a href="#使用-Virtualenv-进行安装TensorFlow" class="headerlink" title="使用 Virtualenv 进行安装TensorFlow"></a>使用 Virtualenv 进行安装TensorFlow</h3><p>环境：<code>ubuntu 16.04</code></p><ul><li><p>安装 pip 和 Virtualenv</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$  sudo apt-get install python-pip python-dev python-virtualenv # for Python 2.7</span><br><span class="line">$  sudo apt-get install python3-pip python3-dev python-virtualenv # for Python 3.n</span><br></pre></td></tr></table></figure></li><li><p>创建 Virtualenv 环境</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$  virtualenv --system-site-packages targetDirectory # for Python 2.7</span><br><span class="line">$  virtualenv --system-site-packages -p python3 targetDirectory # for Python 3.n</span><br></pre></td></tr></table></figure><p><code>targetDirectory</code> 用于指定 Virtualenv 目录。我们假定创建的虚拟环境为<code>tensorflow</code>，即<code>targetDirectory</code>为<code>tensorflow</code>。</p></li><li><p>激活 Virtualenv 环境</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$  source ~/tensorflow/bin/activate</span><br></pre></td></tr></table></figure><p>执行上述 <code>source</code> 命令后，您的提示符应该会变成如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow)$</span><br></pre></td></tr></table></figure></li><li><p>安装或更新<code>pip</code></p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow)$ easy_install -U pip</span><br></pre></td></tr></table></figure><p>安装 <code>TensorFlow</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow)$ pip install --upgrade tensorflow      # for Python 2.7</span><br><span class="line">(tensorflow)$ pip3 install --upgrade tensorflow     # for Python 3.n</span><br><span class="line">(tensorflow)$ pip install --upgrade tensorflow-gpu  # for Python 2.7 and GPU</span><br><span class="line">(tensorflow)$ pip3 install --upgrade tensorflow-gpu # for Python 3.n and GPU</span><br></pre></td></tr></table></figure></li><li><p>备注：如果安装失败或者网速慢，可参考<a href="https://sevenold.github.io/2018/08/TensorFlow-ubuntu/" target="_blank" rel="noopener">TensorFlow-CPU/GPU安装ubuntu16.04版</a> ,离线安装<code>tensorflow</code>。</p></li><li><p>退出 Virtualenv 环境</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensorflow)$ deactivate</span><br></pre></td></tr></table></figure></li></ul><h3 id="验证环境"><a href="#验证环境" class="headerlink" title="验证环境"></a>验证环境</h3><ul><li><p>激活 Virtualenv 环境</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ source ~/tensorflow/bin/activate</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  在 Virtualenv 环境激活后，您就可以从这个 shell 运行 TensorFlow 程序。您的提示符将变成如下所示，这表示您的 Tensorflow 环境已处于活动状态： </span><br><span class="line"></span><br><span class="line">-</span><br></pre></td></tr></table></figure><p>(tensorflow)$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">输入代码验证</span><br></pre></td></tr></table></figure><p>import tensorflow as tf</p><p>hello = tf.constant(‘Hello, TensorFlow!’)</p><p>sess = tf.Session()</p><p>print(sess.run(hello))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 看到如下输出，表示安装正确</span><br><span class="line"></span><br><span class="line">-</span><br></pre></td></tr></table></figure><p>Hello, TensorFlow!</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">用完 TensorFlow 后，可以通过发出以下命令调用 `deactivate` 函数来停用环境：</span><br></pre></td></tr></table></figure><p>(tensorflow)$ deactivate</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 提示符将恢复为您的默认提示符 ，就表示退出了。</span><br><span class="line"></span><br><span class="line">-</span><br></pre></td></tr></table></figure><p>$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 卸载 Virtualenv 环境 `targetDirectory`</span><br><span class="line"></span><br><span class="line">要卸载你刚刚新建的Virtualenv 环境 `TensorFlow`的话，只需要删除该文件夹就可以了。</span><br></pre></td></tr></table></figure></li></ul><p>$ rm -r targetDirectory # 在这里 targetDirectory改为你创建的环境名字<br><code>`</code></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow-CPU/GPU安装ubuntu16.04版</title>
      <link href="/2018/08/10/2018-08-09-TensorFlow-ubuntu/"/>
      <url>/2018/08/10/2018-08-09-TensorFlow-ubuntu/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p><code>系统：Ubuntu 16.04。显卡：GTX 1050，独显无集成显卡。</code></p><h3 id="Ubuntu-TensorFlow-CPU安装"><a href="#Ubuntu-TensorFlow-CPU安装" class="headerlink" title="Ubuntu TensorFlow-CPU安装"></a>Ubuntu TensorFlow-CPU安装</h3><p>最简单的方法使用 pip 来安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Python 2.7</span><br><span class="line">pip install --upgrade tensorflow</span><br><span class="line"># Python 3.x</span><br><span class="line">pip3 install --upgrade tensorflow</span><br></pre></td></tr></table></figure><p><img src="/images/dl/26.png" alt="image"></p><p>安装出错，或者下载速度慢，可以采用离线安装的方式</p><p>离线安装包下载地址：<a href="https://pypi.org/project/tensorflow/" target="_blank" rel="noopener">https://pypi.org/project/tensorflow/</a></p><p>然后进入安装包路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Python 2.7</span><br><span class="line">pip install tensorflow-1.10.0-cp27-cp27mu-manylinux1_x86_64.whl </span><br><span class="line"># Python 3.x</span><br><span class="line">pip3 install tensorflow-1.10.0-cp35-cp35m-manylinux1_x86_64.whl</span><br></pre></td></tr></table></figure><p><img src="/images/dl/27.png" alt="image"></p><p>然后等待，安装成功。</p><p><img src="/images/dl/28.png" alt="image"></p><h3 id="验证安装"><a href="#验证安装" class="headerlink" title="验证安装"></a>验证安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">hello = tf.constant(&apos;Hello, TensorFlow!&apos;)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">print(sess.run(hello))</span><br></pre></td></tr></table></figure><p>看到如下的输出，表示安装正确。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello, TensorFlow!</span><br></pre></td></tr></table></figure><p><img src="/images/dl/29.png" alt="image"></p><h3 id="Ubuntu-TensorFlow-GPU安装"><a href="#Ubuntu-TensorFlow-GPU安装" class="headerlink" title="Ubuntu TensorFlow-GPU安装"></a>Ubuntu TensorFlow-GPU安装</h3><p>如果你的电脑的GPU支持CUDA，那么你就可以使用GPU加速了</p><h3 id="检查自己的GPU是否是CUDA-capable"><a href="#检查自己的GPU是否是CUDA-capable" class="headerlink" title="检查自己的GPU是否是CUDA-capable"></a>检查自己的GPU是否是CUDA-capable</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i nvidia</span><br></pre></td></tr></table></figure><p><img src="/images/dl/24.png" alt="image"></p><p>查看你的电脑GPU是否支持CUDA:<a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-gpus</a></p><p><img src="/images/dl/15.png" alt="image"></p><h3 id="安装NVIDIA驱动"><a href="#安装NVIDIA驱动" class="headerlink" title="安装NVIDIA驱动"></a>安装NVIDIA驱动</h3><ul><li><p><code>System Setting</code>–&gt;<code>software&amp;Updates</code>–&gt;<code>Additional Drives</code>，然后选择<code>NVIDIA</code>驱动</p><p><img src="/images/dl/41.png" alt="image"></p></li><li><p>安装成功–重启电脑</p></li></ul><h4 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p><img src="/images/dl/30.png" alt="image"></p><p>出现这个页面就表示安装成功了。</p><h3 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h3><p>查询电脑的版本号：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p><img src="/images/dl/31.png" alt="image"></p><p>对应版本号去下载对应的CUDA安装包</p><p><img src="/images/dl/17.png" alt="image"></p><p>由上图可以看出，我这台演示电脑的</p><ul><li>版本驱动号：<code>384.130</code></li><li>对应版本：<code>CUDA：8.0</code></li></ul><p>所以我们对应的CUDA的下载版本就是8.0，注意我下载的是<code>runfile</code>，下载网站：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></p><p><img src="/images/dl/32.png" alt="image"></p><h3 id="安装说明"><a href="#安装说明" class="headerlink" title="安装说明"></a>安装说明</h3><p>进入安装包的路径执行以下命令（注意版本号）</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sh cuda_8.0.61_375.26_linux.run</span><br></pre></td></tr></table></figure></li></ul><p>按照命令行提示 在安装过程中会询问是否安装显卡驱动，由于我们在第一步中已经安装，所以我们选择否（不安装）</p><p><img src="/images/dl/38.png" alt="image"></p><p>然后等待，安装完成。</p><p>安装完成后可能会有警告，提示samplees缺少必要的包：</p><p><img src="/images/dl/43.png" alt="image"></p><p>原因是缺少相关的依赖库,安装相应库就解决了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev</span><br></pre></td></tr></table></figure><p><img src="/images/dl/39.png" alt="image"></p><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>打开shell运行： <code>gedit ~/.bashrc</code></p><p>加入如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda-8.0/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><p>立即生效，运行<code>source ~/.bashrc</code></p><h3 id="测试是否安装成功"><a href="#测试是否安装成功" class="headerlink" title="测试是否安装成功"></a>测试是否安装成功</h3><ul><li><p>查看CUDA版本:<code>nvcc -V</code></p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seven@seven:~$ nvcc -V</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2016 NVIDIA Corporation</span><br><span class="line">Built on Tue_Jan_10_13:22:03_CST_2017</span><br><span class="line">Cuda compilation tools, release 8.0, V8.0.61</span><br></pre></td></tr></table></figure><ol><li>编译 CUDA Samples<br>进入samples的安装目录<br>我们选择其中一个进行编译验证下如：</li></ol></li></ul><p><img src="/images/dl/40.png" alt="image"></p><p>如果没有报错，则安装完成</p><h3 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h3><p>同样，根据我们的驱动程序版本号：我们下载的对应版本：</p><ul><li>版本驱动号：<code>384.130</code></li><li>对应版本：<code>CUDA：8.0</code></li><li>对应版本：<code>cuDNN: v6.0</code></li></ul><p>下载网址：<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a></p><p><img src="/images/dl/34.png" alt="image"></p><h3 id="安装说明-1"><a href="#安装说明-1" class="headerlink" title="安装说明"></a>安装说明</h3><ul><li><p>解压下载好的安装包以后会出现cuda的目录，进入该目录 执行以下命令</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br><span class="line">tar -zxf cudnn-8.0-linux-x64-v5.1.tgz</span><br><span class="line">cd cuda</span><br><span class="line">sudo cp lib64/* /usr/local/cuda/lib64/</span><br><span class="line">sudo cp include/* /usr/local/cuda/include/</span><br></pre></td></tr></table></figure></li></ul><h3 id="安装完成"><a href="#安装完成" class="headerlink" title="安装完成"></a>安装完成</h3><p>至此，CUDA与cuDNN已经安装完成</p><h3 id="安装-TensorFlow-GPU"><a href="#安装-TensorFlow-GPU" class="headerlink" title="安装 TensorFlow-GPU"></a>安装 TensorFlow-GPU</h3><p>备注：我用的是cuda 8.0和cudnn6.0 所以TensorFlow的版本应该是1.4。</p><p>最简单的方式是使用pip安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Python 2.7</span><br><span class="line">pip install --upgrade tensorflow-gpu==1.4</span><br><span class="line"># Python 3.x</span><br><span class="line">pip3 install --upgrade tensorflow-gpu==1.4</span><br></pre></td></tr></table></figure><p>如果你网速很慢的话，你可以选择离线安装。</p><p>下载所需的离线包：<a href="https://github.com/tensorflow/tensorflow/tags" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/tags</a></p><p>打开终端，进入你保存文件的目录，使用命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install tensorflow_gpu-1.4.0-cp35-cp35m-manylinux1_x86_64.whl</span><br></pre></td></tr></table></figure><p>然后等待，直至安装成功。</p><p><img src="/images/dl/37.png" alt="image"></p><h3 id="验证安装-1"><a href="#验证安装-1" class="headerlink" title="验证安装"></a>验证安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">hello = tf.constant(&apos;Hello, TensorFlow!&apos;)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">print(sess.run(hello))</span><br></pre></td></tr></table></figure><p>看到如下的输出，表示安装正确。</p><p><img src="/images/dl/42.png" alt="image"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello, TensorFlow!</span><br></pre></td></tr></table></figure><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>TensorFlow-CPU/GPU安装windows版</title>
      <link href="/2018/08/09/2018-08-09-TensorFlow-windows/"/>
      <url>/2018/08/09/2018-08-09-TensorFlow-windows/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="Windows-TensorFlow-CPU安装"><a href="#Windows-TensorFlow-CPU安装" class="headerlink" title="Windows TensorFlow-CPU安装"></a>Windows TensorFlow-CPU安装</h3><p>备注：CPU和GPU是不能同时安装的，如果需要的话可以再配置一个环境或者建个虚拟环境。</p><p>环境：window7或以上</p><p>python版本要求：<code>3.5.x</code>以上</p><p>打开<strong>window cmd</strong>，直接使用CPU-only命令安装，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install --upgrade tensorflow</span><br></pre></td></tr></table></figure><p><img src="/images/dl/11.png" alt="image"></p><p>然后等待，直至安装成功。</p><blockquote><blockquote><blockquote></blockquote></blockquote></blockquote><p>如果你网速很慢的话，你可以选择离线安装。</p><p>下载所需的离线包：<a href="https://pypi.org/project/tensorflow/#files" target="_blank" rel="noopener">https://pypi.org/project/tensorflow/#files</a></p><p>打开<strong>window cmd</strong>，进入你保存文件的目录，使用命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow-1.10.0-cp36-cp36m-win_amd64.whl</span><br></pre></td></tr></table></figure><p>然后等待，直至安装成功。</p><p><img src="/images/dl/14.png" alt="image"></p><h3 id="验证安装"><a href="#验证安装" class="headerlink" title="验证安装"></a>验证安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">hello = tf.constant(&apos;Hello, TensorFlow!&apos;)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">print(sess.run(hello))</span><br></pre></td></tr></table></figure><p>看到如下的输出，表示安装正确。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello, TensorFlow!</span><br></pre></td></tr></table></figure><h3 id="Windows-TensorFlow-GPU安装"><a href="#Windows-TensorFlow-GPU安装" class="headerlink" title="Windows TensorFlow-GPU安装"></a>Windows TensorFlow-GPU安装</h3><p>如果你的电脑的GPU支持CUDA，那么你就可以使用GPU加速了</p><p>查看你的电脑GPU是否支持CUDA:<a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-gpus</a></p><p><img src="/images/dl/15.png" alt="image"></p><h3 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h3><p>当然你的先安装显卡的驱动，这个就自己查询了。我们默认你已经装好了。</p><p>查看驱动程序版本号</p><p><img src="/images/dl/16.png" alt="image"></p><p>对应版本号去下载对应的CUDA安装包</p><p><img src="/images/dl/17.png" alt="image"></p><p>由上图可以看出，我这台演示电脑的</p><ul><li>版本驱动号：391.24</li><li>对应版本：CUDA：9.0</li></ul><p>所以我们对应的CUDA的下载版本就是9.0，下载网站：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></p><p><img src="/images/dl/18.png" alt="image"></p><h3 id="安装说明"><a href="#安装说明" class="headerlink" title="安装说明"></a>安装说明</h3><ul><li>双击cuda_9.0.176_windows.exe</li><li>按照屏幕上的提示操作</li><li>如果出现下面这个界面–说明你还需要安装<code>[ Visual Studio 2012或以上版本]</code><br><img src="/images/dl/21.png" alt="image"></li></ul><h3 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h3><p>同样，根据我们的驱动程序版本号：我们下载的对应版本：</p><ul><li>版本驱动号：391.24</li><li>CUDA：9.0</li><li>cuDNN：v7.1.4</li></ul><p>下载网址：<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a></p><p><img src="/images/dl/19.png" alt="image"></p><p><img src="/images/dl/20.png" alt="image"></p><h3 id="安装说明-1"><a href="#安装说明-1" class="headerlink" title="安装说明"></a>安装说明</h3><p>说明：把cuDNN的文件复制到CUDA Toolkit 安装目录</p><p>解压cudnn5.0</p><ul><li>生成cuda/include、cuda/lib、cuda/bin三个目录；</li><li>分别将<code>cuda/include、cuda/lib、cuda/bin</code>三个目录中的内容拷贝到<code>C:\Program Files\NVIDIAGPU Computing Toolkit\CUDA\v8.0</code>对应的<code>include、lib、bin</code>目录下即可</li><li>如果你是自定义的安装路径，需要自己搜索一下NVIDIA GPU Computing Toolkit的目录</li></ul><h3 id="安装-TensorFlow-GPU"><a href="#安装-TensorFlow-GPU" class="headerlink" title="安装 TensorFlow-GPU"></a>安装 TensorFlow-GPU</h3><p>打开<strong>window cmd</strong>，直接使用命令安装，如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade tensorflow-gpu</span><br></pre></td></tr></table></figure><p>然后等待，直至安装成功。</p><blockquote><blockquote><blockquote></blockquote></blockquote></blockquote><p>如果你网速很慢的话，你可以选择离线安装。</p><p>下载所需的离线包：<a href="https://pypi.org/project/tensorflow-gpu/#files" target="_blank" rel="noopener">https://pypi.org/project/tensorflow-gpu/#files</a></p><p>打开<strong>window cmd</strong>，进入你保存文件的目录，使用命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install tensorflow_gpu-1.10.0-cp36-cp36m-win_amd64.whl</span><br></pre></td></tr></table></figure><p>然后等待，直至安装成功。</p><p><img src="/images/dl/22.png" alt="image"></p><h3 id="验证安装-1"><a href="#验证安装-1" class="headerlink" title="验证安装"></a>验证安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">hello = tf.constant(&apos;Hello, TensorFlow!&apos;)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">print(sess.run(hello))</span><br></pre></td></tr></table></figure><p>看到如下的输出，表示安装正确。</p><p><img src="/images/dl/23.png" alt="image"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>深度学习简介</title>
      <link href="/2018/08/08/2018-08-08-DL-introduction/"/>
      <url>/2018/08/08/2018-08-08-DL-introduction/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="人工智能、机器学习、神经网络、深度学习关系"><a href="#人工智能、机器学习、神经网络、深度学习关系" class="headerlink" title="人工智能、机器学习、神经网络、深度学习关系"></a><strong>人工智能、机器学习、神经网络、深度学习关系</strong></h3><p><img src="/images/dl/1.png" alt="image"></p><h3 id="理解深度学习和传统算法区别"><a href="#理解深度学习和传统算法区别" class="headerlink" title="理解深度学习和传统算法区别"></a><strong>理解深度学习和传统算法区别</strong></h3><p><code>机器学习与人类思维</code></p><p><img src="/images/dl/2.png" alt="image"></p><h3 id="深度学习与应用"><a href="#深度学习与应用" class="headerlink" title="深度学习与应用"></a>深度学习与应用</h3><p><code>图像识别</code><br>图像识别，是指利用计算机对图像进行处理、分析和理解，以识别各种不同模式的目标和对像的技术。</p><p><img src="/images/dl/3.png" alt="image"></p><p><code>目标识别</code><br>目标识别是指一个特殊目标（或一种类型的目标）从其它目标（或其它类型的目标）中被区分出来的过程。它既包括两个非常相似目标的识别，也包括一种类型的目标同其他类型目标的识别。</p><p><img src="/images/dl/4.png" alt="image"></p><p><code>人脸识别</code><br>人脸识别，是基于人的脸部特征信息进行身份识别的一种生物识别技术。用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行脸部识别的一系列相关技术，通常也叫做人像识别、面部识别。</p><p><img src="/images/dl/5.png" alt="image"></p><p><code>图片描述</code><br>根据识别出的内容，组织成一段内容，用于描述图片信息。</p><p><img src="/images/dl/6.png" alt="image"></p><p><code>图像风格变换</code><br>把一张图片，变换成不同的风格的图片。</p><p><img src="/images/dl/7.png" alt="image"></p><p><code>语音识别</code><br>把语音进行处理，识别出语音内容，感情等等。</p><p><img src="/images/dl/8.png" alt="image"></p><p><code>文本分类</code><br>文本分类用电脑对文本集(或其他实体或物件)按照一定的分类体系或标准进行自动分类标记</p><p><img src="/images/dl/9.png" alt="image"></p><p><code>图像生成</code><br>根据用户所描述的特征来生成对应的图像。</p><p><img src="/images/dl/10.png" alt="image"></p><h3 id="神经网络的分类"><a href="#神经网络的分类" class="headerlink" title="神经网络的分类"></a>神经网络的分类</h3><h4 id="按数据流向"><a href="#按数据流向" class="headerlink" title="按数据流向"></a><strong>按数据流向</strong></h4><ul><li>前馈神经网络</li><li>递归神经网络</li><li>反馈神经网络</li></ul><h4 id="按网络中神经元组织形式"><a href="#按网络中神经元组织形式" class="headerlink" title="按网络中神经元组织形式"></a><strong>按网络中神经元组织形式</strong></h4><ul><li>全连接神经网络</li><li>部分连接神经网络</li></ul><h4 id="按网络中神经元的行为和连接方式"><a href="#按网络中神经元的行为和连接方式" class="headerlink" title="按网络中神经元的行为和连接方式"></a><strong>按网络中神经元的行为和连接方式</strong></h4><ul><li>全连接神经网络</li><li>卷积神经网络</li><li>循环神经网络</li></ul><h4 id="按训练方法"><a href="#按训练方法" class="headerlink" title="按训练方法"></a><strong>按训练方法</strong></h4><ul><li>监督学习</li><li>无监督学习</li><li>强化学习</li></ul><h3 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h3><ul><li>全连接神经网络<ul><li>用作数据分析</li><li>可作为其他网络的组成部分</li></ul></li><li>卷积神经网络<ul><li>计算机视觉、图像处理</li><li>具有局部相关性的数据</li></ul></li><li>循环神经网络<ul><li>自然语言处理</li><li>语音识别</li><li>具有顺序及前后相关性的数据</li></ul></li></ul><h3 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h3><p>图像生成<br><a href="https://make.girls.moe/#/" target="_blank" rel="noopener">用AI生成二次元角色</a></p><p>神经网络演示<br><a href="http://scs.ryerson.ca/~aharley/vis/conv/" target="_blank" rel="noopener">卷积神经网络的三维可视化</a><br>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-贝叶斯网络</title>
      <link href="/2018/08/02/2018-08-02-ml-NB-network/"/>
      <url>/2018/08/02/2018-08-02-ml-NB-network/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="贝叶斯网络的概念"><a href="#贝叶斯网络的概念" class="headerlink" title="贝叶斯网络的概念"></a>贝叶斯网络的概念</h3><p>把某个研究系统中涉及的<strong>随机变量</strong>，根据是否条件独立绘制在一个<strong>有向图</strong>中，就形成了<strong><code>贝叶斯网络</code></strong>。</p><p>贝叶斯网络(Bayesian network)，又称信念网络(Belief Network)，或<strong>有向无环图模型</strong>。是一种概率图模型，根据概率图的拓扑结构，考察一组随机变量${ X_1,X_2…X_n}$及其n组<strong>条件概率分布</strong>的性质。也就是说它用网络结构代表领域的基本因果知识。</p><h3 id="贝叶斯网络的形式化定义"><a href="#贝叶斯网络的形式化定义" class="headerlink" title="贝叶斯网络的形式化定义"></a>贝叶斯网络的形式化定义</h3><ul><li><p>$BN(G,Θ)$: 贝叶斯网络(Bayesian Network)</p><ul><li><p>$G$:有向无环图 (Directed Acyclic Graphical model, DAG)</p></li><li><p>$G$的结点:随机变量$X_1,X_2…X_n$</p></li><li><p>$G$的边:结点间的有向依赖</p></li><li><p>$Θ$:所有条件概率分布的参数集合</p></li><li><p>结点X的条件概率: $P(X |parent(X))$</p><p>$P(S,C,B,X,D)=P(S)P(C \mid S)P(B \mid S)P(X \mid C,S)P(D \mid C,B)$</p></li></ul></li></ul><p>每个结点所需参数的个数:</p><p>若结点的$parent$数目是$M$,结点和$parent$的可取值数目都是$K:K^M∗(K−1)$</p><h3 id="一个简单的贝叶斯网络"><a href="#一个简单的贝叶斯网络" class="headerlink" title="一个简单的贝叶斯网络"></a>一个简单的贝叶斯网络</h3><p><img src="/images/ml/44.png" alt="image"></p><h4 id="P-a-b-c-P-c-mid-a-b-P-a-b-P-c-mid-a-b-P-b-mid-a-P-a"><a href="#P-a-b-c-P-c-mid-a-b-P-a-b-P-c-mid-a-b-P-b-mid-a-P-a" class="headerlink" title="$P(a,b,c)=P(c \mid a,b)P(a,b)=P(c \mid a,b)P(b \mid a)P(a)$"></a>$P(a,b,c)=P(c \mid a,b)P(a,b)=P(c \mid a,b)P(b \mid a)P(a)$</h4><h3 id="全连接贝叶斯网络"><a href="#全连接贝叶斯网络" class="headerlink" title="全连接贝叶斯网络"></a>全连接贝叶斯网络</h3><p><strong>每一对结点之间都有边连接</strong></p><h4 id="p-x-1-…x-n-p-x-K-mid-x-1-…-x-K-1-…p-x-2-mid-x-1-p-x-1"><a href="#p-x-1-…x-n-p-x-K-mid-x-1-…-x-K-1-…p-x-2-mid-x-1-p-x-1" class="headerlink" title="$p(x_1,…x_n)=p(x_K \mid x_1,…,x_{K-1})…p(x_2 \mid x_1)p(x_1)$"></a>$p(x_1,…x_n)=p(x_K \mid x_1,…,x_{K-1})…p(x_2 \mid x_1)p(x_1)$</h4><h4 id="P-X-1-x-1-…-X-n-x-n-prod-i-1-nP-X-i-x-i-mid-X-i-1-…-X-n-x-n"><a href="#P-X-1-x-1-…-X-n-x-n-prod-i-1-nP-X-i-x-i-mid-X-i-1-…-X-n-x-n" class="headerlink" title="$P(X_1=x_1,…,X_n=x_n)=\prod_{i=1}^nP(X_i=x_i \mid X_{i+1},…,X_n=x_n)$"></a>$P(X_1=x_1,…,X_n=x_n)=\prod_{i=1}^nP(X_i=x_i \mid X_{i+1},…,X_n=x_n)$</h4><h3 id="一个”正常“的贝叶斯网络"><a href="#一个”正常“的贝叶斯网络" class="headerlink" title="一个”正常“的贝叶斯网络"></a>一个”正常“的贝叶斯网络</h3><p><img src="/images/ml/45.png" alt="image"></p><p>从图中我们可以看出：</p><ul><li><p>有些边是缺失的</p></li><li><p>直观上来看：$x_1,x_2$是相互独立的</p></li><li><p>直观上来看：$x_6,x_7$在$x_4$给定的条件下独立</p></li><li><p>$x_1,x_2,…x_7$的联合分布：</p><h4 id="P-x-1-P-x-2-P-x-3-P-x-4-mid-x-1-x-2-x-3-P-x-5-mid-x-1-x-3-P-x-6-mid-x-4-P-x-7-mid-x-4-x-5"><a href="#P-x-1-P-x-2-P-x-3-P-x-4-mid-x-1-x-2-x-3-P-x-5-mid-x-1-x-3-P-x-6-mid-x-4-P-x-7-mid-x-4-x-5" class="headerlink" title="$P(x_1)P(x_2)P(x_3)P(x_4\mid x_1, x_2, x_3)P(x_5\mid x_1, x_3)P(x_6\mid x_4)P(x_7\mid x_4, x_5)$"></a>$P(x_1)P(x_2)P(x_3)P(x_4\mid x_1, x_2, x_3)P(x_5\mid x_1, x_3)P(x_6\mid x_4)P(x_7\mid x_4, x_5)$</h4></li></ul><h3 id="贝叶斯网络的条件独立判定"><a href="#贝叶斯网络的条件独立判定" class="headerlink" title="贝叶斯网络的条件独立判定"></a>贝叶斯网络的条件独立判定</h3><p>我们来看一下贝叶斯网络的条件是如何判定的：</p><ol><li><h4 id="条件独立：tail-to-tail"><a href="#条件独立：tail-to-tail" class="headerlink" title="条件独立：tail-to-tail"></a>条件独立：<strong>tail-to-tail</strong></h4><p><img src="/images/ml/46.png" alt="image"></p><p>根据图模型，得：$P(a,b,c)=P(c)P(a \mid c)P(b \mid c)$</p><p>从而：$P(a,b,c)/P(c)=P(a \mid c)P(b \mid c)$</p><p>因为$P(a,b \mid c)=P(a,b,c)/P(c)$</p><p>得：$P(a,b\mid c)=P(a\mid c)P(b\mid c)$</p><p>解释：在<code>c</code>给定的条件下，因为<code>a,b</code>被阻断（blocked），因此是独立的：$P(a,b\mid c)=P(a\mid c)P(b\mid c)$</p></li><li><h4 id="条件独立：head-to-tail"><a href="#条件独立：head-to-tail" class="headerlink" title="条件独立：head-to-tail"></a>条件独立：head-to-tail</h4><p><img src="/images/ml/47.png" alt="image"></p><p>根据图模型，得：$P(a,b,c)=P(a)P(c \mid a)P(b \mid c)$</p><p>$P(a,b \mid c) \\ =P(a,b,c)/P(c) \\ =P(a)P(c \mid a)P(b \mid c) / P(c) \\ =P(a,c)P(b \mid c) / P(c) \\ = P(a\mid c)P(b\mid c)$</p><p>解释：在<code>c</code>给定的条件下，因为<code>a,b</code>被阻断（blocked），因此是独立的：$P(a,b\mid c)=P(a\mid c)P(b\mid c)$</p></li><li><h4 id="条件独立：head-to-head"><a href="#条件独立：head-to-head" class="headerlink" title="条件独立：head-to-head"></a>条件独立：head-to-head</h4><p><img src="/images/ml/48.png" alt="image"></p><p>根据图模型，得：$P(a,b,c)=P(a)P(b)P(c \mid a,b) $</p><p>由：$\sum_c P(a,b,c)= \sum_n P(a)P(b)P(c \mid a,b)$</p><p>得：$P(a,b)=P(a)P(b)$</p><p>解释：在<code>c</code>给定的条件下，因为<code>a,b</code>被阻断（blocked），因此是独立的：$P(a,b)=P(a)P(b)$</p></li></ol><h3 id="有向分离"><a href="#有向分离" class="headerlink" title="有向分离"></a>有向分离</h3><p>对于任意的结点集,<code>有向分离</code>(D-separation): 对于任意的结点集<code>A,B,C</code>,考察所有通过A中任意结点到B中任意结点的路径,若要求<code>A,B</code>条件独立,则需要所有的路径都被阻断(blocked),即满足下列两个前提之一:</p><ol><li>A和B的<code>head-to-tail型</code>和<code>tail-to-tail型</code>路径都通过C;</li><li>A和B的<code>head-to-head型</code>路径不通过C以及C的子孙结点;</li></ol><p><img src="/images/ml/49.png" alt="image"></p><p>图(a), 在<code>tail-to-tail</code>中, <code>f</code>没有阻断; 在<code>head-to-head</code>中, <code>e</code>阻断, 然而它的子结点<code>c</code>没有阻断, 即<code>e</code>所在的结点集没有阻断; 因此, 结点<code>a, b</code>关于<code>c</code>不独立.</p><p>图(b), 在<code>tail-to-tail</code>中, <code>f</code>阻断; 因此, 结点<code>a,b</code>关于<code>f</code> 独立. 在<code>head-to-head</code>中, <code>e</code>和它的子孙结点<code>c</code>都阻断; 因此, 结点<code>a,b</code>关于<code>e</code>独立.</p><h3 id="特殊的贝叶斯网络"><a href="#特殊的贝叶斯网络" class="headerlink" title="特殊的贝叶斯网络"></a>特殊的贝叶斯网络</h3><ol><li><h4 id="马尔科夫模型"><a href="#马尔科夫模型" class="headerlink" title="马尔科夫模型"></a>马尔科夫模型</h4><p><img src="/images/ml/52.png" alt="image"></p><p>结点形成一条链式网络，这种按顺次演变的随机过程模型就称作<strong>马尔科夫模型</strong></p><p>$A_{i+1}$只与$A_i$有关，与$A_1,…,A_{i-1}$无关。</p></li><li><h4 id="隐马尔科夫模型"><a href="#隐马尔科夫模型" class="headerlink" title="隐马尔科夫模型"></a><strong>隐马尔科夫模型</strong></h4><h4 id="Hidden-Markov-Model"><a href="#Hidden-Markov-Model" class="headerlink" title="Hidden Markov Model"></a><code>Hidden Markov Model</code></h4><p><img src="/images/ml/50.png" alt="image"></p><ul><li>隐马尔科夫模型（HMM）可用标注问题，在语音识别、NLP、生物信息、模式识别等领域别实践证明的有效算法。</li><li>HMM是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。</li><li>HMM随机生成的状态的序列，成为<code>状态序列</code>，每个状态生成一个观测，由此产生的观测随机序列，称为<code>观测序列</code><ul><li>序列的每一个位置可看做是一个时刻。</li><li>空间序列也可以使用该模型.</li></ul></li></ul></li><li><h4 id="马尔科夫毯"><a href="#马尔科夫毯" class="headerlink" title="马尔科夫毯"></a><strong>马尔科夫毯</strong></h4><p>一个结点的<strong><code>Markov Blanket</code> </strong>是一个集合，在这个集合中的结点都给定条件下，该结点条件独立于其他结点。</p><p><strong><code>Markov Blanket</code> </strong>: 一个结点的<code>Markov Blanket</code>是它的<code>parents,children</code>以及<code>spouses</code></p><p><img src="/images/ml/51.png" alt="image"></p></li></ol><pre><code>深色的结点集合，就是“马尔科夫毯”（**`Markov Blanket` **）</code></pre><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-朴素贝叶斯</title>
      <link href="/2018/08/01/2018-08-01-ml-NB/"/>
      <url>/2018/08/01/2018-08-01-ml-NB/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="朴素贝叶斯的概念"><a href="#朴素贝叶斯的概念" class="headerlink" title="朴素贝叶斯的概念"></a>朴素贝叶斯的概念</h3><p><strong>朴素贝叶斯分类（Naive Bayes Classifier）</strong>是一种简单而容易理解的分类方法，看起来很Naive，但用起来却很有效。其原理就是<strong>贝叶斯定理</strong>，从数据中得到新的信息，然后对<strong>先验概率</strong>进行更新，从而得到<strong>后验概率</strong>。</p><p>就好比说我们判断一个人的品质好坏，对于陌生人我们对他的判断是五五开，如果说他做了一件好事，那么这个新的信息使我们判断他是好人的概率增加了。<strong>朴素贝叶斯分类的优势</strong>在于不怕噪声和无关变量，其Naive之处在于它假设各特征属性是无关的。而<strong>贝叶斯网络（Bayesian Network）</strong>则放宽了变量无关的假设，将贝叶斯原理和图论相结合，建立起一种基于概率推理的数学模型,对于解决复杂的不确定性和关联性问题有很强的优势。</p><h3 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h3><p>条件概率（Conditional Probability）是指在事件B发生的情况下，事件A发生的概率，用$P(A |B)$表示，读作在B条件下的A的概率。</p><p><img src="/images/ml/41.png" alt="image"></p><p>在上方的文氏图中，描述了两个事件A和B，与它们的交集<code>A ∩ B</code>，代入条件概率公式，可推出事件A发生的概率为</p><h4 id="P-A-B-frac-P-A⋂B-P-B-。"><a href="#P-A-B-frac-P-A⋂B-P-B-。" class="headerlink" title="$P(A |B)=\frac{P(A⋂B)}{P(B)}$。"></a>$P(A |B)=\frac{P(A⋂B)}{P(B)}$。</h4><p>对该公式做一下变换可推得$P(A⋂B)=P(A | B)P(B)与P(A⋂B)=P(B | A)P(A)$,（<code>P(B|A)</code>为在A条件下的B的概率）。</p><p>同理可得$P(A |B)P(B)=P(B |A)P(A)$。</p><h3 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h3><p>全概率公式是将边缘概率与条件概率关联起来的基本规则，它表示了一个结果的总概率，可以通过几个不同的事件来实现。</p><p>全概率公式将对一复杂事件的概率求解问题转化为了在不同情况下发生的简单事件的概率的求和问题，公式为</p><h4 id="P-B-sum-i-1-n-P-A-i-P-B-A-i"><a href="#P-B-sum-i-1-n-P-A-i-P-B-A-i" class="headerlink" title="$P(B) = {\sum_{i=1}^n}P(A_i)P(B |A_i)$"></a>$P(B) = {\sum_{i=1}^n}P(A_i)P(B |A_i)$</h4><p>假定一个样本空间S，它是两个事件A与C之和，同时事件B与它们两个都有交集，如下图所示：</p><p><img src="/images/ml/42.png" alt="image"></p><p>那么事件B的概率可以表示为$P(B)=P(B⋂A)+P(B⋂C)$</p><p>通过条件概率，可以推断出$P(B⋂A)=P(B |A)P(A)$，所以$P(B)=P(B |A)P(A)+P(B |C)P(C)$</p><p>这就是全概率公式，即事件B的概率等于事件A与事件C的概率分别乘以B对这两个事件的条件概率之和。</p><h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><h4 id="贝叶斯公式："><a href="#贝叶斯公式：" class="headerlink" title="贝叶斯公式："></a><code>贝叶斯公式</code>：</h4><h4 id="P-A-B-frac-P-B-A-P-A-P-B"><a href="#P-A-B-frac-P-B-A-P-A-P-B" class="headerlink" title="$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$"></a>$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$</h4><ul><li>$P(A |B)$：在B条件下的事件A的概率，在贝叶斯定理中，条件概率也被称为<code>后验概率</code>，即在事件B发生之后，我们对事件A概率的重新评估。</li><li>$P(B |A)$：在A条件下的事件B的概率，其实就是和上一条是一样的意思。</li><li>$P(A)$与$P(B)$被称为<code>先验概率</code>（也被称为边缘概率），即在事件B发生之前，我们对事件A概率的一个推断（不考虑任何事件B方面的因素），后面同理。</li><li>$P(B |A)P(B)$被称为<code>标准相似度</code>，它是一个调整因子，主要是为了保证预测概率更接近真实概率。</li><li>根据这些术语，贝叶斯定理表述为： <strong>后验概率 = 标准相似度 * 先验概率</strong>。</li></ul><h3 id="朴素贝叶斯分类的原理"><a href="#朴素贝叶斯分类的原理" class="headerlink" title="朴素贝叶斯分类的原理"></a>朴素贝叶斯分类的原理</h3><p>朴素贝叶斯分类是一种十分简单的分类算法，叫它朴素贝叶斯分类是因为这种方法的思想真的很朴素，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。</p><p>我们设一个待分类项$X=f_1,f_2,⋯,f_n$，其中每个<code>f</code>为<code>X</code>的一个特征属性，然后设一个类别集合$C_1,C_2,⋯,C_m$。</p><p>然后需要计算$P(C_1 |X),P(C_2 |X),⋯,P(C_m |X)$，然后我们就可以根据一个训练样本集合（已知分类的待分类项集合），然后统计得到在各类别下各个特征属性的条件概率：</p><p>$P(f_1 |C_1),P(f_2 |C_1),⋯,P(f_n |C_1),\\, P(f_1 |C_2),P(f_2 |C_2),⋯ P(f_n |C_2), \\ P(f_1 |C_m),P(f_2 |C_m),⋯,P(f_n |C_m)$</p><p>如果$P(C_k |X)=MAX(P(C_1 |X),P(C_2 |X),⋯,P(C_m |X))$，则$X∈C_k$（贝叶斯分类其实就是取<strong>概率最大</strong>的那一个）。</p><p>朴素贝叶斯会假设每个特征都是独立的，根据贝叶斯定理可推得：$P(C_i |X)=P(X |C_i)P(C_i)P(X)$，由于分母对于所有类别为常数，因此只需要将分子最大化即可，又因为各特征是互相独立的，所以最终推得：</p><h4 id="P-X-C-i-P-C-i-P-f-1-C-i-P-f-2-C-i-…-P-f-n-C-i-P-C-i-P-C-i-prod-i-1-nP-f-i-C-i"><a href="#P-X-C-i-P-C-i-P-f-1-C-i-P-f-2-C-i-…-P-f-n-C-i-P-C-i-P-C-i-prod-i-1-nP-f-i-C-i" class="headerlink" title="$P(X | C_i)P(C_i)=P(f_1 |C_i)P(f_2 |C_i)…,P(f_n | C_i)P(C_i) \\ =P(C_i)\prod_{i=1}^nP(f_i |C_i)$"></a>$P(X | C_i)P(C_i)=P(f_1 |C_i)P(f_2 |C_i)…,P(f_n | C_i)P(C_i) \\ =P(C_i)\prod_{i=1}^nP(f_i |C_i)$</h4><p>根据上述的公式推导，朴素贝叶斯的流程可如下图所示：</p><p><img src="/images/ml/43.png" alt="image"></p><h3 id="朴素贝叶斯的算法模型"><a href="#朴素贝叶斯的算法模型" class="headerlink" title="朴素贝叶斯的算法模型"></a>朴素贝叶斯的算法模型</h3><p>在朴素贝叶斯中含有以下三种算法模型：</p><ul><li><p><strong><code>Gaussian Naive Bayes</code></strong>：适合在特征变量具有连续性的时候使用，同时它还假设特征遵从于高斯分布（正态分布）。</p><p>假设我们有一组人体特征的统计资料，该数据中的特征：身高、体重和脚掌长度等都为连续变量，很明显我们不能采用离散变量的方法来计算概率，由于样本太少，也无法分成区间计算，那么要怎么办呢？解决方法是假设特征项都是正态分布，然后通过样本计算出均值与标准差，这样就得到了正态分布的密度函数，有了密度函数，就可以代入值，进而算出某一点的密度函数的值。</p></li><li><p><strong><code>MultiNomial Naive Bayes</code></strong>：与Gaussian Naive Bayes相反，多项式模型更适合处理特征是离散变量的情况，该模型会在计算先验概率$P(C_m)$和条件概率$P(F_n |C_m)$时会做一些平滑处理。具体公式为</p><h4 id="P-C-m-frac-T-cm-a-T-ma"><a href="#P-C-m-frac-T-cm-a-T-ma" class="headerlink" title="$P(C_m)=\frac{T_{cm}+a}{T+ma}$"></a>$P(C_m)=\frac{T_{cm}+a}{T+ma}$</h4><p>其中T为总的样本数，m为总类别数，$T*{cm}即类别为即类别为C_m$的样本个数，<code>a</code>是一个平滑值。条件概率的公式为</p><h4 id="P-F-n-C-m-frac-T-cm-f-n-a-T-cm-an"><a href="#P-F-n-C-m-frac-T-cm-f-n-a-T-cm-an" class="headerlink" title="$P(F_n |C_m) = \frac{T_{cm}f_n+a}{T_{cm}+an}$"></a>$P(F_n |C_m) = \frac{T_{cm}f_n+a}{T_{cm}+an}$</h4><p><code>n</code>为特征的个数，$$T_{cm}f_n$$为类别为$$C_m$$特征为$$F_n$$的样本个数。</p><p>当平滑值<code>a = 1</code>，被称作为<code>Laplace</code>平滑，</p><p>当平滑值<code>a &lt; 1</code>，被称为<code>Lidstone</code>平滑。</p><p>它的思想其实就是对每类别下所有划分的计数加1，这样如果训练样本数量足够大时，就不会对结果产生影响，并且解决了$P(F |C)$的频率为0的现象（某个类别下的某个特征划分没有出现，这会严重影响分类器的质量）。</p></li><li><p><strong><code>Bernoulli Naive Bayes</code></strong>：Bernoulli适用于在特征属性为二进制的场景下，它对每个特征的取值是基于布尔值的，一个典型例子就是判断单词有没有在文本中出现。</p></li></ul><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 朴素贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-无监督学习-EM算法</title>
      <link href="/2018/07/31/2018-07-31-ml-em/"/>
      <url>/2018/07/31/2018-07-31-ml-em/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="EM算法简介"><a href="#EM算法简介" class="headerlink" title="EM算法简介"></a>EM算法简介</h3><p><strong>最大期望演算法</strong>（<strong>Expectation-maximization algorithm</strong>，又译<strong>期望最大化算法</strong>）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。</p><p>在<a href="https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1" target="_blank" rel="noopener">统计</a><a href="https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97" target="_blank" rel="noopener">计算</a>中，<strong>最大期望（EM）算法</strong>是在<a href="https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener">概率模型</a>中寻找<a href="https://zh.wikipedia.org/wiki/%E5%8F%82%E6%95%B0" target="_blank" rel="noopener">参数</a><a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="noopener">最大似然估计</a>或者<a href="https://zh.wikipedia.org/w/index.php?title=%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">最大后验估计</a>的<a href="https://zh.wikipedia.org/wiki/%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">算法</a>，其中概率模型依赖于无法观测的<a href="https://zh.wikipedia.org/w/index.php?title=%E9%9A%90%E6%80%A7%E5%8F%98%E9%87%8F&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">隐性变量</a>。最大期望算法经常用在<a href="https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" target="_blank" rel="noopener">机器学习</a>和<a href="https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89" target="_blank" rel="noopener">计算机视觉</a>的<a href="https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB" target="_blank" rel="noopener">数据聚类</a>（Data Clustering）领域。最大期望算法经过两个步骤交替进行计算，第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；第二步是最大化（M），最大化在E步上求得的<a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" target="_blank" rel="noopener">最大似然值</a>来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。</p><h3 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h3><p><strong>注意</strong>：EM算法主要用于从不完整数据中计算最大似然估计，本身可以看成是特殊情况下计算极大似然的一种方法。</p><p><strong>极大似然估计</strong>所要解决的问题是：给定一组数据和一个参数待定的模型，如何确定模型的参数。使得这个确定的参数后的模型在所有模型中产生已知数据的概率最大。（<strong>模型已定，参数未知</strong>）。</p><h4 id="简单举个例子："><a href="#简单举个例子：" class="headerlink" title="简单举个例子："></a><strong>简单举个例子</strong>：</h4><ul><li>10次抛硬币的结果是：正正反正正正反反正正</li><li>假设P是每次抛硬币的结果为正的概率。</li><li>得到这样实验结果的概率我们就可以算出来：<ul><li>$P=pp(1-p)ppp(1-p)(1-p)pp \\ = p^7(1-p)^3$</li></ul></li><li>然后很愉快的最优解就算出来了：$p=0.7$</li></ul><p>我们引申到<strong>二项分布的最大似然估计</strong></p><ul><li>在抛硬币的试验中，进行N次独立试验，n次朝上，N-n次朝下。</li><li>然后假定朝上的概率为p，使用对数似然函数作为目标函数：<ul><li>$f(n | p)=log(p^n(1-p^{N-n}))$</li><li>$\frac{\partial(f(n | p))}{\partial p}=\frac nP-\frac{N-n}{1-p}=0$</li><li>然后就可以得到概率：$p=\frac nN$</li></ul></li></ul><p>升级一下，<strong>进一步考察</strong></p><ul><li><p>若给定有一组样本$x_1,x_2,…x_n$，已知它们来自于高斯分布$N(u,\sigma)$，试估计参数$u, \sigma$.</p></li><li><p>高斯分布的概率密度函数：</p><ul><li><h4 id="f-x-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2"><a href="#f-x-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2" class="headerlink" title="$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}}$"></a>$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}}$</h4></li></ul></li><li><p>将$X_i$的样本值$x_i$带入，得：</p><ul><li><h4 id="L-x-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2"><a href="#L-x-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2" class="headerlink" title="$L(x)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}}$"></a>$L(x)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}}$</h4></li></ul></li><li><p>化简对数似然函数：</p><ul><li><h4 id="l-x-log-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2-sum-ilog-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2-sum-ilog-frac-1-sqrt-2-pi-sigma-sum-i-frac-x-i-u-2-2-sigma-2-frac-n2log-2-pi-sigma-2-frac-1-2-sigma-2-sum-i-x-i-u-2"><a href="#l-x-log-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2-sum-ilog-frac-1-sqrt-2-pi-sigma-e-frac-x-i-u-2-2-sigma-2-sum-ilog-frac-1-sqrt-2-pi-sigma-sum-i-frac-x-i-u-2-2-sigma-2-frac-n2log-2-pi-sigma-2-frac-1-2-sigma-2-sum-i-x-i-u-2" class="headerlink" title="$l(x)=log\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}} \\ = \sum_ilog\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}} \\ =\sum_ilog\frac{1}{\sqrt{2\pi}\sigma} + \sum_i {-\frac{(x_i-u)^2}{2\sigma^2}} \\ =-\frac n2log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x_i-u)^2 $"></a>$l(x)=log\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}} \\ = \sum_ilog\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-u)^2}{2\sigma^2}} \\ =\sum_ilog\frac{1}{\sqrt{2\pi}\sigma} + \sum_i {-\frac{(x_i-u)^2}{2\sigma^2}} \\ =-\frac n2log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x_i-u)^2 $</h4></li></ul></li><li><p>参数估计</p><ul><li><p>目标函数：</p><ul><li><h4 id="l-x-frac-n2log-2-pi-sigma-2-frac-1-2-sigma-2-sum-i-x-i-u-2"><a href="#l-x-frac-n2log-2-pi-sigma-2-frac-1-2-sigma-2-sum-i-x-i-u-2" class="headerlink" title="$l(x)=-\frac n2log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x_i-u)^2 $"></a>$l(x)=-\frac n2log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x_i-u)^2 $</h4></li></ul></li><li><p>将目标函数对参数$u,、\sigma$分别<strong>求偏导</strong>，然后<strong>令偏导等于0</strong>，就可以得到$u,\sigma$的式子：</p><ul><li><h4 id="u-frac-1n-sum-ix-i"><a href="#u-frac-1n-sum-ix-i" class="headerlink" title="$u=\frac 1n \sum_ix_i$"></a>$u=\frac 1n \sum_ix_i$</h4></li><li><h4 id="sigma-2-frac-1n-sum-i-x-i-u-2"><a href="#sigma-2-frac-1n-sum-i-x-i-u-2" class="headerlink" title="$\sigma^2=\frac 1n \sum_i(x_i-u)^2$"></a>$\sigma^2=\frac 1n \sum_i(x_i-u)^2$</h4></li></ul></li></ul></li><li><p>到现在，我们就可以很直观的看出，这个结果是和矩估计的结果是一致的，也就是说样本的均值就是高斯分布的均值，样本的伪方差就是高斯分布的方差。</p></li></ul><p>这就是我们所熟知的极大似然估计。</p><h3 id="Jensen-不等式"><a href="#Jensen-不等式" class="headerlink" title="Jensen 不等式"></a>Jensen 不等式</h3><p>设$f$是定义域为实数的函数，如果对于所有的实数$x$。如果对于所有的实数$x$，$f(x)$的二次导数大于等于0，那么$f$是凸函数。当$x$是向量时，如果其$hessian$矩阵$H$是半正定的，那么$f$是凸函数。如果只大于0，不等于0，那么称f是严格凸函数。<br>Jensen不等式表述如下：<br>如果$f$是凸函数，X是随机变量，那么：$E[f(X)]&gt;=f(E[X])$<br>特别地，如果$f$是严格凸函数，当且仅当X是常量时，上式取等号。<br>如果用图表示会很清晰：</p><p><img src="/images/ml/38.png" alt="images"></p><p>图中，实线$f$是凸函数，$X$是随机变量，有0.5的概率是a，有0.5的概率是b。（就像掷硬币一样）。X的期望值就是a和b的中值了，图中可以看到$E[f(X)]&gt;=f(E[X])$成立。<br>当f是（严格）凹函数当且仅当-f是（严格）凸函数。<br>Jensen不等式应用于凹函数时，不等号方向反向。</p><h3 id="如何理解EM算法？"><a href="#如何理解EM算法？" class="headerlink" title="如何理解EM算法？"></a>如何理解EM算法？</h3><p>在前面我们所遇到的，一堆由已知分布得到的数据，如果模型中的变量都是可以观测到了，为了求其中的参数，这时就可以直接使用极大似然估计。</p><p>但是，有这么一种情况，就是当模型中含有<strong>隐变量</strong>（数据观测不到了）的时候，再使用极大似然估计的时候，计算过程就会显得极为复杂甚至不可解。当我们正一筹莫展的时候，<strong>EM算法</strong>就出现了。</p><h3 id="EM算法的思路"><a href="#EM算法的思路" class="headerlink" title="EM算法的思路"></a>EM算法的思路</h3><p><strong>输入</strong>：观测变量数据X，隐变量数据Z，联合分布$P(X,Z | \theta)$，条件分布$P(Z | X,\theta)$</p><p><strong>输出</strong>：模型参数$\theta$</p><p><img src="/images/ml/40.png" alt="images"></p><p>未完待续</p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/ml-em/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-无监督学习-聚类</title>
      <link href="/2018/07/30/2018-07-29-ml-clustering/"/>
      <url>/2018/07/30/2018-07-29-ml-clustering/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>无监督学习是一种机器学习方法，用于<strong>发现数据中的模式</strong>。输入无监督算法的数据都没有标签，也就是只为算法提供了输入变量$(X)$而没有对应的输出变量。在无监督学习中，算法需要<strong>自行寻找数据中的结构</strong>。</p><p>无监督学习问题可以有以下三种类型：</p><ul><li><strong>关联</strong>：发现目录中项目共现的概率。其广泛应用于“购物车分析”。例如，如果一个顾客购买了火腿，他会有80% 的概率也购买鸡蛋。</li><li><strong>聚类</strong>：将样本分组，这样，同一聚类中的物体与来自另一聚类的物体相比，相互之间会更加类似。</li><li><strong>降维</strong>：<strong>降维指减少一个数据集的变量数量，同时保证还能传达重要信息</strong>。降维可以通过特征抽取方法和特征选择方法完成。特征选择方法会选择初始变量的子集。特征抽取方法执行从高维度空间到低维度空间的数据转换。例如，PCA算法就是一种特征抽取方式。</li></ul><h3 id="聚类的概念"><a href="#聚类的概念" class="headerlink" title="聚类的概念"></a>聚类的概念</h3><p>聚类是一种无监督机器学习方法，它基于数据的内部结构寻找观察样本的自然族群（即集群），常用于新闻分类、推荐系统等。聚类的特点是训练数据没有标注，通常使用数据可视化评价结果。</p><p>聚类分析仅根据在数据中发现的描述对象及其关系的信息，将数据对象分组。其目标是，组内的对象相互之间是相似的（相关的），而不同组中的对象是不同的（不相关的）。组内的相似性（同质性）越大，组间差别越大，聚类就越好。</p><h3 id="聚类的方法"><a href="#聚类的方法" class="headerlink" title="聚类的方法"></a>聚类的方法</h3><p>聚类的常用方法包括</p><ul><li><strong>划分聚类法</strong>，<strong>K均值</strong>:是基于原型的、划分的聚类技术。它试图发现用户指定个数K的簇（由质心代表）。</li><li><strong>层次聚类。凝聚的层次聚类：</strong>开始，每个点作为一个单点簇；然后，重复地合并两个最靠近的簇，直到产生单个的、包含所有点的簇。</li><li><strong>基于密度的聚类，</strong> <strong>DBSCAN</strong>是一种产生划分聚类的基于密度的聚类算法，簇的个数由算法自动地确定。低密度区域中的点被视为噪声而忽略，因此DBSCAN不产生完全聚类。</li></ul><h3 id="常用的聚类数据集"><a href="#常用的聚类数据集" class="headerlink" title="常用的聚类数据集"></a>常用的聚类数据集</h3><p>常用的聚类数据集包括</p><ul><li>scikit-learn blob: 简单聚类</li><li>scikit-learn circle: 非线性可分数据集</li><li>scikit-learn moon: 更复杂的数据集</li></ul><p><img src="/images/ml/31.png" alt="image"></p><h3 id="聚类的性能度量"><a href="#聚类的性能度量" class="headerlink" title="聚类的性能度量"></a>聚类的性能度量</h3><p>我们希望聚类结果的<strong>“簇内相似度”高且“簇间相似度”低</strong>。</p><p>其性能度量大致有两类：</p><h4 id="一类是将聚类结果与某个“参考模型”进行比较。称为“外部指标”。"><a href="#一类是将聚类结果与某个“参考模型”进行比较。称为“外部指标”。" class="headerlink" title="一类是将聚类结果与某个“参考模型”进行比较。称为“外部指标”。"></a>一类是将聚类结果与某个“参考模型”进行比较。称为“外部指标”。</h4><h4 id="一类是直接考查聚类结果而不利于任何参考模型。称为“内部指标”。"><a href="#一类是直接考查聚类结果而不利于任何参考模型。称为“内部指标”。" class="headerlink" title="一类是直接考查聚类结果而不利于任何参考模型。称为“内部指标”。"></a>一类是直接考查聚类结果而不利于任何参考模型。称为“内部指标”。</h4><h3 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h3><p>对数据集$D={x_1,x_2,…,x_m}$,假定通过聚类给出额簇划分为$C=C_1,C_2,…,C_k$,参考模型给出的簇划分为$C’=C_1^T,C_2^T,…,C_s^T$。相应的，令λ与$λ^T$分别表示与C和$C^T$对应的簇标记向量。注意的是，参考模型给出的划分类别数量不一定等于通过聚类得到的数量。</p><p>样本两两配对：</p><ol><li>$a=\mid SS \mid ,SS={(x_i,x_j)\mid \lambda_i = \lambda_j,\lambda_i^T=\lambda_j^T,i&lt;j}$</li><li>$b=\mid SS \mid ,SD={(x_i,x_j)\mid \lambda_i = \lambda_j,\lambda_i^T\neq \lambda_j^T,i&lt;j}$</li><li>$c=\mid SS \mid ,DS={(x_i,x_j)\mid \lambda_i \neq \lambda_j,\lambda_i^T=\lambda_j^T,i&lt;j}$</li><li>$d=\mid SS \mid ,DD={(x_i,x_j)\mid \lambda_i \neq \lambda_j,\lambda_i^T \neq \lambda_j^T,i&lt;j}$</li></ol><p>集合SS包含了C中隶属于相同簇且在$C’$中也隶属于相同簇的样本对，集合SD包含了在C中隶属于相同簇但在$C^T$中隶属于不同簇的样本对 .</p><ol><li>Jaccard系数：$JC=\frac{a}{a+b+c}$</li><li>FM指数： $FMI=\sqrt{\frac{a}{a+b}\frac{a}{a+c}}$</li><li>Rand指数： $RI=\frac{2(a+d)}{m(m-1)}$</li></ol><p>上述性能度量的结果值均在[0,1]区间，值越大越好。</p><h3 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h3><p>考虑聚类结果的簇划分$C=C_1,C_2,…,C_k$，定义</p><ol><li>$avg(C)=\frac{2}{\mid C \mid(\mid C \mid -1)}\sum_{1 \leq i &lt; j \leq \mid C \mid}dist(x_i,x_j)$</li><li>$diam(C)=\max_{1 \leq i &lt;j \leq \mid C \mid}dist(x_i,x_j)$</li><li>$d_\min(C_i,C_j)=\min_{x_i \in C_i , x_j \in C_j} dist(x_i,x_j)$</li><li>$d_cen(C_i,C_j)=dist(\mu_i,\mu_j)$</li></ol><p>我们在上面的式子中，dist是计算两个样本之间的距离，$u$代表簇的中心点$\mu=\frac{\sum_{1 \leq i \leq \mid C \mid x_i}}{\mid C \mid}$ ，avg(C)与簇内样本间的平均距离，diam(C)对应与簇C内样本间的最远距离，$d_min(C_i,C<em>j)对应与簇i和簇j最近样本间的距离；对应与簇i和簇j最近样本间的距离；d</em>{cen}(C_i,C_j)$对应与簇i和j中心点间的距离。</p><p>基于上面的指标，推出下面几个内部指标：</p><ol><li>$DBI=\frac{1}{k}\sum\limits_{i=1}^k\max\limits_{j \neq i}(\frac{avg(C_i)+avg(C_j)}{d_{cen}(\mu_i,\mu_j)})$</li><li>$DI=\min\limits_{1 \leq i \leq k}{ \min\limits_{j \neq i}(\frac{d_{min}(C_i,C_j)}{\max_{1\leq l \leq k diam(C_l)}}) }$</li></ol><p>显然，DBI的值越小越好，DI值越大越好</p><h3 id="相似度-距离度量"><a href="#相似度-距离度量" class="headerlink" title="相似度/距离度量"></a>相似度/距离度量</h3><p>“距离度量”需满足一些基本性质，如<strong>非负性、同一性、对称性和直递性</strong>。最常用的是闵可夫斯基距离、欧氏距离和曼哈顿距离（后两者其实都是闵可夫斯基距离的特例）。</p><p>闵可夫斯基距离只可用于有序属性，对无序属性可采用VDM（Value Difference Metric）。</p><h4 id="计算方法总结："><a href="#计算方法总结：" class="headerlink" title="计算方法总结："></a>计算方法总结：</h4><ul><li><h5 id="闵可夫斯基距离Minkowski-欧式距离：-dist-X-Y-sum-i-1-n-x-i-y-i-p-frac1p"><a href="#闵可夫斯基距离Minkowski-欧式距离：-dist-X-Y-sum-i-1-n-x-i-y-i-p-frac1p" class="headerlink" title="闵可夫斯基距离Minkowski/欧式距离：$dist(X,Y)=(\sum_{i=1}^n|x_i-y_i|^p)^\frac1p$"></a>闵可夫斯基距离Minkowski/欧式距离：$dist(X,Y)=(\sum_{i=1}^n|x_i-y_i|^p)^\frac1p$</h5></li><li><h5 id="杰卡德相似系数-Jaccard-J-A-B-frac-A-cap-B-A-cup-B"><a href="#杰卡德相似系数-Jaccard-J-A-B-frac-A-cap-B-A-cup-B" class="headerlink" title="杰卡德相似系数(Jaccard): $J(A,B)=\frac{|A\cap B|}{|A\cup B|}$"></a>杰卡德相似系数(Jaccard): $J(A,B)=\frac{|A\cap B|}{|A\cup B|}$</h5></li><li><h5 id="余弦相似度-cosine-similarity-cos-theta-frac-a-Tb-a-cdot-b"><a href="#余弦相似度-cosine-similarity-cos-theta-frac-a-Tb-a-cdot-b" class="headerlink" title="余弦相似度(cosine similarity):$cos(\theta)=\frac{a^Tb}{|a|\cdot |b|}$"></a>余弦相似度(cosine similarity):$cos(\theta)=\frac{a^Tb}{|a|\cdot |b|}$</h5></li><li><h5 id="Pearson相似系数：-rho-xy-frac-cov-X-Y-sigma-X-sigma-Y-frac-E-X-u-X-Y-u-Y-sigma-X-sigma-Y-frac-sum-i-1-n-X-i-u-X-Y-i-u-Y-sqrt-sum-i-1-n-X-i-u-X-2-sqrt-sum-i-1-n-Y-i-u-Y-2"><a href="#Pearson相似系数：-rho-xy-frac-cov-X-Y-sigma-X-sigma-Y-frac-E-X-u-X-Y-u-Y-sigma-X-sigma-Y-frac-sum-i-1-n-X-i-u-X-Y-i-u-Y-sqrt-sum-i-1-n-X-i-u-X-2-sqrt-sum-i-1-n-Y-i-u-Y-2" class="headerlink" title="Pearson相似系数：$\rho_{xy}=\frac{cov(X,Y)}{\sigma_X\sigma_Y}=\frac{E[(X-u_X)(Y-u_Y)]}{\sigma_X\sigma_Y}=\frac{\sum_{i=1}^n (X_i-u_X)(Y_i-u_Y) }{\sqrt{\sum_{i=1}^n(X_i-u_X)^2} \sqrt{\sum_{i=1}^n(Y_i-u_Y)^2}}$"></a>Pearson相似系数：$\rho_{xy}=\frac{cov(X,Y)}{\sigma_X\sigma_Y}=\frac{E[(X-u_X)(Y-u_Y)]}{\sigma_X\sigma_Y}=\frac{\sum_{i=1}^n (X_i-u_X)(Y_i-u_Y) }{\sqrt{\sum_{i=1}^n(X_i-u_X)^2} \sqrt{\sum_{i=1}^n(Y_i-u_Y)^2}}$</h5></li><li><h5 id="相对熵（K-L距离）：-D-P-q-sum-xp-x-log-frac-p-x-q-x-E-p-x-log-frac-p-x-q-x"><a href="#相对熵（K-L距离）：-D-P-q-sum-xp-x-log-frac-p-x-q-x-E-p-x-log-frac-p-x-q-x" class="headerlink" title="相对熵（K-L距离）：$D(P||q)=\sum_xp(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}$"></a>相对熵（K-L距离）：$D(P||q)=\sum_xp(x)log\frac{p(x)}{q(x)}=E_{p(x)}log\frac{p(x)}{q(x)}$</h5></li><li><h5 id="Hellinger距离：-D-a-p-q-frac-2-1-a-2-1-int-p-x-frac-1-a-2-cdot-q-x-frac-1-a-2-dx"><a href="#Hellinger距离：-D-a-p-q-frac-2-1-a-2-1-int-p-x-frac-1-a-2-cdot-q-x-frac-1-a-2-dx" class="headerlink" title="Hellinger距离：$D_a(p||q)=\frac{2}{1-a^2}(1-\int (p(x)^\frac{1+a}{2}\cdot q(x)^\frac{1-a}{2}dx$"></a>Hellinger距离：$D_a(p||q)=\frac{2}{1-a^2}(1-\int (p(x)^\frac{1+a}{2}\cdot q(x)^\frac{1-a}{2}dx$</h5></li></ul><h3 id="聚类的基本思想"><a href="#聚类的基本思想" class="headerlink" title="聚类的基本思想"></a>聚类的基本思想</h3><p>给定一个有N个对象的数据集，构造数据的K个簇，$k\le n$,满足下列条件：</p><ul><li>每一个簇至少包含一个对象</li><li>每一个对象属于且仅属于一个簇</li><li>将满足上述条件的K个簇称作一个合理的划分</li></ul><p><strong>基本思想</strong>：对于给定的类别数目K，首先给出初始划分，通过迭代改变样本和簇的隶属关系，使得每一次改进之后的划分方案都比前一次好。</p><h3 id="原型聚类"><a href="#原型聚类" class="headerlink" title="原型聚类"></a>原型聚类</h3><p>原型聚类亦称“基于原型的聚类”，假设聚类结构能通过一组原型（原型是指样本空间中具有代表性的点）刻画。</p><p>常用的方法包括</p><ul><li>k均值算法</li><li>学习向量化</li><li>高斯混合聚类（基于概率模型）</li></ul><h3 id="K-Means-聚类"><a href="#K-Means-聚类" class="headerlink" title="K-Means 聚类"></a>K-Means 聚类</h3><p><strong>K-Means</strong>算法的基本思想是初始随机给定K个簇中心，按照最邻近原则把待分类样本点分到各个簇。然后按平均法重新计算各个簇的质心(这个点可以不是样本点)，从而确定新的簇心。一直迭代，直到簇心的移动距离小于某个给定的值。</p><h3 id="K-Means聚类算法步骤："><a href="#K-Means聚类算法步骤：" class="headerlink" title="K-Means聚类算法步骤："></a><strong>K-Means聚类算法步骤</strong>：</h3><p>(1)选择K个初始质心，其中K是用户指定的参数，即所期望的簇的个数。</p><p>(2)每个点指派到最近的质心，而指派到一个质心的点集为一个簇。</p><p>(3)根据指派到簇的点，更新每个簇的质心。</p><p>(4)重复指派和更新步骤，直到簇不发生变化，或等价地，直到质心不发生变化。</p><p><img src="/images/ml/33.png" alt="image"></p><h3 id="K均值算法"><a href="#K均值算法" class="headerlink" title="K均值算法"></a>K均值算法</h3><p><img src="/images/ml/32.png" alt="image"></p><h3 id="k均值常用的邻近度，质心和目标函数的选择："><a href="#k均值常用的邻近度，质心和目标函数的选择：" class="headerlink" title="k均值常用的邻近度，质心和目标函数的选择："></a><strong>k均值常用的邻近度，质心和目标函数</strong>的选择：</h3><p>邻近度函数：曼哈顿距离。质心：中位数。目标函数：最小化对象到其簇质心的距离和。</p><p>邻近度函数：平方欧几里德距离。质心：均值。目标函数：最小化对象到其簇质心的距离的平方和。</p><p>邻近度函数：余弦。质心：均值。最大化对象与其质心的余弦相似度和。</p><p>邻近度函数：Bregman散度。质心：均值。目标函数：最小化对象到其簇质心的Bregman散度和。</p><p>由于基本K均值算法采取随机地选取初始质心的办法，导致最后形成的簇的质量常常很糟糕。在此基础上引出了基本K均值算法的扩充：<strong>二分K均值算法</strong>。二分K均值算法不太受初始化问题的影响。</p><h3 id="二分K均值算法"><a href="#二分K均值算法" class="headerlink" title="二分K均值算法"></a>二分K均值算法</h3><ol><li><p>把所有数据作为一个cluster加入cluster list</p></li><li><p>repeat</p></li><li><p>​ 从cluster list中挑选出一个SSE最大的cluster来进行划分</p></li><li>​ for i=1 to预设的循环次数</li><li>​ 用基本K均值算法把挑选出来的cluster划分成两个子cluster</li><li>​ 计算两个子cluster的SSE和。</li><li>​ end for</li><li>​ 把for循环中SSE和最小的那两个子cluster加入cluster list</li><li><strong>until</strong> cluster list拥有K个cluster</li></ol><p>除此以外，每次划分不止执行一次基本K均值算法，而是预先设置一个ITER值，然后对这个cluster进行ITER次执行基本K均值运算。因为基本K均值每次一开始都是随机选K个质心来执行，所以i一般来说ITER次执行基本K均值，每次都会得到不同的两个cluster。那么应该选哪对cluster来作为划分以后的cluster呢？答案就是在每次循环中，每次都计算当次基本K均值划分出来的两个cluster的SSE和，最后就选SSE和最小的那对cluster作为划分以后的cluster。</p><h3 id="学习向量化"><a href="#学习向量化" class="headerlink" title="学习向量化"></a>学习向量化</h3><p>与k均值算法类似，“学习向量量化”（Learning Vector Quantization，简称LVQ）也是试图找到一组原型向量来刻画聚类结构，但与一般的聚类算法不同的是，LVQ假设数据样本带有类别标记，学习过程用样本的这些监督信息来辅助聚类。</p><h3 id="学习向量化算法"><a href="#学习向量化算法" class="headerlink" title="学习向量化算法"></a>学习向量化算法</h3><p><img src="/images/ml/34.png" alt="image"></p><h3 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h3><p>高斯混合聚类使用了一个很流行的算法：GMM(Gaussian Mixture Model)。高斯混合聚类与k均值聚类类似，但是采用了<strong>概率模型</strong>来表达聚类原型。每个高斯模型（Gaussian Model）就代表了一个簇（类）。GMM是单一高斯概率密度函数的延伸，能够平滑地近似任意形状的密度分布。在高斯混合聚类中，每个GMM会由k个高斯模型分布组成，每个高斯模型被称为一个component，这些component线性加乘在一起就组成了GMM的。</p><p>简单地来说，k-Means的结果是每个数据点没分配到其中某一个cluster,而GMM则给出的是这个数据点被分配到每个cluster的概率，又称为soft assignment。</p><h3 id="高斯混合模型聚类算法"><a href="#高斯混合模型聚类算法" class="headerlink" title="高斯混合模型聚类算法"></a>高斯混合模型聚类算法</h3><p><img src="/images/ml/35.png" alt="image"></p><h3 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h3><p>层次聚类（hierarchical clustering）试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集的划分可采用“自底向上”的聚合策略，也可采用“自顶向下”的分拆策略。 典型的<strong>AGNES</strong>是一种采用自底向上聚合策略的层次聚类算法。它先将数据集中的每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数。</p><h4 id="有两种产生层次聚类的基本方法："><a href="#有两种产生层次聚类的基本方法：" class="headerlink" title="有两种产生层次聚类的基本方法："></a>有两种产生层次聚类的基本方法：</h4><ol><li>凝聚的。从点作为个体簇开始，每一步合并两个最接近的簇。这需要定义簇的临近性概念。凝聚层次聚类技术最常见。</li><li>分裂的。从包含所有点的某个簇开始，每一步分裂一个簇，直到仅剩下单点簇。在这种情况下，我们需要确定每一步分裂哪个簇，以及如何分裂。</li><li></li></ol><h3 id="层次聚类算法"><a href="#层次聚类算法" class="headerlink" title="层次聚类算法"></a>层次聚类算法</h3><p><img src="/images/ml/37.png" alt="image"></p><h4 id="基本凝聚层次聚类算法："><a href="#基本凝聚层次聚类算法：" class="headerlink" title="基本凝聚层次聚类算法："></a>基本凝聚层次聚类算法：</h4><ol><li>如果需要，计算临近度矩阵</li><li>repeat</li><li>​ 合并最接近的两个簇</li><li>​ 更新临近度矩阵，以反映新的簇与原来的簇之间的临近性。</li><li>until 仅剩下一个簇</li></ol><h4 id="簇之间的临近性有3种定义方式："><a href="#簇之间的临近性有3种定义方式：" class="headerlink" title="簇之间的临近性有3种定义方式："></a>簇之间的临近性有3种定义方式：</h4><ol><li>MIN（单链）。不同簇中的两个最近的点之间的距离作为临近度。</li><li>MAX（全链）。不同簇中的两个最远的点之间的距离作为临近度。</li><li>GROUP（组平均）。取自不同簇的所有点对距离的平均值作为临近度。</li></ol><h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><ol><li>簇与簇合并的原则永远是dist最小。</li><li>但在计算dist值的时候，可以采用MIN, MAX, GROUP AVG 3中方式得出dist的值。</li></ol><h3 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h3><p>密度聚类亦称“基于密度的聚类”，假设聚类结构能通过样本分布的紧密程度确定。典型的代表算法为<strong>DBSCAN算法</strong>，它基于一组“领域”（neighborhood）参数来刻画样本分布的紧密程度。</p><p>DBSCAN将“簇”定义为：由密度可达关系导出的最大的密度相连样本集合。</p><h3 id="DBSCAN算法"><a href="#DBSCAN算法" class="headerlink" title="DBSCAN算法"></a>DBSCAN算法</h3><p>基于密度的聚类寻找被低密度区域分离的高密度区域。DBSCAN是一种简单、有效的基于密度的聚类算法。</p><h4 id="DBSCAN算法："><a href="#DBSCAN算法：" class="headerlink" title="DBSCAN算法："></a>DBSCAN算法：</h4><p><img src="/images/ml/36.png" alt="image"></p><ol><li>将所有点标记为核心点、边界点或噪声点。</li><li>删除噪声点。</li><li>为距离在Eps之内的所有核心点之间连线。</li><li>每组连通的核心点形成一个簇。</li><li>将每个边界点指派到一个与之关联的核心点的簇中。</li></ol><h4 id="DBSCAN算法阐释："><a href="#DBSCAN算法阐释：" class="headerlink" title="DBSCAN算法阐释："></a>DBSCAN算法阐释：</h4><ol><li>算法需要用户输入2个参数： 半径Eps; 最小（少）点值MinPts。</li><li>确定Eps和MinPts需要用到K-距离的概念。K-距离就是“到第K近的点的距离”，按经验一般取值为4。并且，一般取K的值为MinPts参数的值。</li><li>首先计算每个点到所有其余点的欧式距离，升序排序后，选出每个点的“K距离”。</li><li>所有点的K距离形成一个集合D。对D进行升序排序，依此可以形成一个样本数据的K距离图。</li><li>图中急剧变化处的值，即为Eps。</li><li>根据Eps和MinPts，计算出所有的核心点。</li><li>给核心点到小于Eps的另一个核心点赋予一个连线，到核心点的距离等于Eps的点被识别为边界点。最后，核心点、边界点之外的点都是噪声点。</li><li>将能够连线的点和与之关联的边界点都放到一起，形成了一个簇。</li></ol><h3 id="几种聚类的优缺点"><a href="#几种聚类的优缺点" class="headerlink" title="几种聚类的优缺点"></a>几种聚类的优缺点</h3><h4 id="层次聚类的优缺点："><a href="#层次聚类的优缺点：" class="headerlink" title="层次聚类的优缺点："></a><strong>层次聚类的优缺点</strong>：</h4><p>优点：</p><ol><li>距离和规则的相似度容易定义，限制少；</li><li>不需要预先指定聚类数；</li><li>可以发现类的层次关系；</li><li>可以聚类成其他形状。</li></ol><p>缺点：</p><ol><li>计算复杂度太高；</li><li>奇异值也能产生很大影响；</li><li>算法很可能聚类成链状。</li></ol><h4 id="DBSCAN的优缺点："><a href="#DBSCAN的优缺点：" class="headerlink" title="DBSCAN的优缺点："></a><strong>DBSCAN的优缺点</strong>：</h4><p>优点：</p><ol><li>不需要事先知道要形成的簇的数量。</li><li>可以发现任意形状的簇类。</li><li>对噪声点不敏感。</li><li>对样本点的顺序不敏感。</li></ol><p>缺点：</p><ol><li>簇的密度变化太大时，DBSCAN会有麻烦。</li><li>对于高维数据，密度定义困难，DBSCAN也有问题。</li></ol><p>注意：</p><ol><li>K均值对于圆形区域聚类的效果很好，DBSCAN基于密度，对于集中区域效果很好。</li><li>对于不规则形状，K均值完全无法使用。DBSCAN可以起到很好的效果。</li></ol><h4 id="K均值的优缺点："><a href="#K均值的优缺点：" class="headerlink" title="K均值的优缺点："></a><strong>K均值的优缺点</strong>：</h4><p>优点：</p><ol><li>简单，易于理解和实现。</li><li>时间复杂度低。</li></ol><p>缺点：</p><ol><li>要手工输入K值，对初始值的设置很敏感。</li><li>对噪声和离群点很敏感。</li><li>只用于数值型数据，不适用于categorical类型的数据。</li><li>不能解决非凸数据。</li><li>主要发现圆形或者球形簇，不能识别非球形的簇。</li></ol><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/ml-clustering/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 聚类 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-SVM-软间隔与正则化</title>
      <link href="/2018/07/26/2018-07-26-ml-svm-L1/"/>
      <url>/2018/07/26/2018-07-26-ml-svm-L1/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="软间隔和正则化"><a href="#软间隔和正则化" class="headerlink" title="软间隔和正则化"></a>软间隔和正则化</h3><p>我们在最开始讨论支持向量机的时候，我们就假定数据在样本空间是线性可分的，也就是我们可以找到一个可行的超平面将数据完全分开。后来为了处理非线性数据，我们又推出使用 Kernel 方法对原来的线性 SVM 进行了推广，使得非线性的的情况也能处理。虽然通过映射$\Phi(x)$将原始数据映射到高维空间之后，使得数据集在特征空间中线性可分，但是也很难断定这个貌似线性可分的结果是不是由于<strong>过拟合</strong>造成的。</p><h3 id="软间隔的概念"><a href="#软间隔的概念" class="headerlink" title="软间隔的概念"></a>软间隔的概念</h3><p>为了缓解该问题，我们允许支持向量机在一些样本上出错。如图所示：</p><p><img src="/images/ml/27.png" alt="image"></p><p>前面我们所介绍的支持向量机的形式是每个样本必须分正确，这是<strong>“硬间隔”</strong>，而<strong>软间隔</strong>就是允许一些样本不满足约束.</p><h3 id="软间隔分类器"><a href="#软间隔分类器" class="headerlink" title="软间隔分类器"></a>软间隔分类器</h3><h5 id="软间隔分类器-soft-margin-classifier-可以解决两种情况"><a href="#软间隔分类器-soft-margin-classifier-可以解决两种情况" class="headerlink" title="软间隔分类器(soft margin classifier)可以解决两种情况."></a>软间隔分类器(soft margin classifier)可以解决两种情况.</h5><p>前面我们都假定数据是线性可分的, 但实际上数据即使映射到了高维也不一定是线性可分. 这个时候就要对超平面进行一个调整, 即这里所说的软间隔.</p><p>另一种情况是即使数据是线性可分的, 但数据中可能存在噪点. 而如果按照前面那种常规处理的话, 这些噪点会对我们的结果造成很大的影响.这个时候也是需要使用软间隔来尽可能减少噪点对我们的影响.</p><p>如下图所示, 如果数据是线性可分并且不存在噪点的话, 我们可以找到一个完美的分类超平面:</p><p><img src="/images/ml/28.png" alt="image"></p><p>但是, 如果数据中出现了一个噪点并且仍然是线性可分, 如果我们还是按照之前的办法处理, 那么我们就会得到如下的分类超平面, 这明显是不合理的，这也就出现了过拟合的状况.</p><p><img src="/images/ml/29.png" alt="image"></p><h3 id="软间隔支持向量机："><a href="#软间隔支持向量机：" class="headerlink" title="软间隔支持向量机："></a>软间隔支持向量机：</h3><p>在硬间隔最大化的目标函数中添加<strong>松弛变量</strong>和<strong>惩罚参数</strong>，在约束条件中添加<strong>松弛变量</strong>，即</p><h4 id="min-w-b-frac-1-2-w-2-C-sum-i-1-n-xi-i"><a href="#min-w-b-frac-1-2-w-2-C-sum-i-1-n-xi-i" class="headerlink" title="$min_{w,b}\frac{1}{2}{||w||}^2+C\sum_{i=1}^n \xi_i$"></a>$min_{w,b}\frac{1}{2}{||w||}^2+C\sum_{i=1}^n \xi_i$</h4><h4 id="s-t-y-i-w-Tx-b-ge1-xi-i-xi-i-ge0-i-1-2-cdot-cdot-cdot-m"><a href="#s-t-y-i-w-Tx-b-ge1-xi-i-xi-i-ge0-i-1-2-cdot-cdot-cdot-m" class="headerlink" title="$s.t.y_i(w^Tx+b)\ge1-\xi_i,\xi_i\ge0, i=1,2,\cdot \cdot \cdot m$"></a>$s.t.y_i(w^Tx+b)\ge1-\xi_i,\xi_i\ge0, i=1,2,\cdot \cdot \cdot m$</h4><h3 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h3><ul><li><h4 id="要使间隔尽量大，那-frac-1-2-w-2-尽量小"><a href="#要使间隔尽量大，那-frac-1-2-w-2-尽量小" class="headerlink" title="要使间隔尽量大，那$\frac{1}{2}{||w||}^2$尽量小"></a>要使间隔尽量大，那$\frac{1}{2}{||w||}^2$尽量小</h4></li><li><h4 id="误分类点的个数尽量小，即松弛变量-xi-i-尽量小，-C-为调和两者的系数"><a href="#误分类点的个数尽量小，即松弛变量-xi-i-尽量小，-C-为调和两者的系数" class="headerlink" title="误分类点的个数尽量小，即松弛变量$\xi_i$尽量小，$C$为调和两者的系数"></a>误分类点的个数尽量小，即松弛变量$\xi_i$尽量小，$C$为调和两者的系数</h4></li></ul><h3 id="构建拉格朗日函数"><a href="#构建拉格朗日函数" class="headerlink" title="构建拉格朗日函数"></a>构建拉格朗日函数</h3><p>在<strong>软间隔支持向量机</strong>中每个样本都有一个对应的松弛变量，用来表示该样本不满足约束条件:$y_i(w^Tx+b)\ge1$</p><p>但是这个函数同样是一个二次规划问题，我们就可以通过拉格朗日乘子法就可以得到拉格朗日函数。</p><h4 id="L-w-b-a-xi-u-frac-1-2-w-2-C-sum-i-1-n-xi-i-sum-i-1-na-i-1-xi-i-y-i-w-Tx-b-sum-i-1-nu-i-xi-i"><a href="#L-w-b-a-xi-u-frac-1-2-w-2-C-sum-i-1-n-xi-i-sum-i-1-na-i-1-xi-i-y-i-w-Tx-b-sum-i-1-nu-i-xi-i" class="headerlink" title="$L(w,b,a,\xi,u)=\frac{1}{2}{||w||}^2+C\sum_{i=1}^n \xi_i+\sum_{i=1}^na_i(1-\xi_i- y_i(w^Tx+b))-\sum_{i=1}^nu_i\xi_i$"></a>$L(w,b,a,\xi,u)=\frac{1}{2}{||w||}^2+C\sum_{i=1}^n \xi_i+\sum_{i=1}^na_i(1-\xi_i- y_i(w^Tx+b))-\sum_{i=1}^nu_i\xi_i$</h4><p>其中$a_i\ge0, u_i\ge0$是拉格朗日的乘子。</p><p>令$L(w,b,a,\xi,u)$对$w,b,\xi_i$的偏导为零可得：</p><p>求导方法就按照<a href="https://sevenold.github.io/2018/07/ml-svm/" target="_blank" rel="noopener">支持向量机的算法推导</a>来进行：</p><h4 id="w-sum-i-1-na-ix-iy-i"><a href="#w-sum-i-1-na-ix-iy-i" class="headerlink" title="$w=\sum_{i=1}^na_ix_iy_i$"></a>$w=\sum_{i=1}^na_ix_iy_i$</h4><h4 id="sum-i-1-na-iy-i-0"><a href="#sum-i-1-na-iy-i-0" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><h4 id="C-a-i-u-i"><a href="#C-a-i-u-i" class="headerlink" title="$C=a_i+u_i$"></a>$C=a_i+u_i$</h4><h3 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h3><p>类似线性可分支持向量机中做法，引入拉格朗日乘子并构建拉格朗日函数，利用拉格朗日对偶性，问题转化为求解对偶问题</p><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h4><h4 id="s-t-0-le-a-i-le-C-i-1-cdot-cdot-cdot-n"><a href="#s-t-0-le-a-i-le-C-i-1-cdot-cdot-cdot-n" class="headerlink" title="$s.t. ,0\le a_i\le C, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,0\le a_i\le C, i=1,\cdot \cdot \cdot n$</h4><h4 id="sum-i-1-na-iy-i-0-1"><a href="#sum-i-1-na-iy-i-0-1" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><p>与前面所推导的<strong>硬间隔</strong>下的对偶问题，两者的差别就在于对偶变量的约束不同，前者是$0\le a_i$，后者是$0\le a_i\le C$。后续同样可以引入核函数得到对应的支持向量展开式。</p><h3 id="软间隔支持向量机KKT条件"><a href="#软间隔支持向量机KKT条件" class="headerlink" title="软间隔支持向量机KKT条件"></a>软间隔支持向量机KKT条件</h3><h4 id="left-lbrace-begin-aligned-a-i-ge0-u-i-ge0-y-i-w-Tx-i-b-1-xi-i-ge-0-a-i-y-i-w-Tx-i-b-1-xi-i-0-xi-i-ge0-u-i-xi-i-0-end-aligned-right"><a href="#left-lbrace-begin-aligned-a-i-ge0-u-i-ge0-y-i-w-Tx-i-b-1-xi-i-ge-0-a-i-y-i-w-Tx-i-b-1-xi-i-0-xi-i-ge0-u-i-xi-i-0-end-aligned-right" class="headerlink" title="$ \left\lbrace\ \begin{aligned} a_i \ge0 , u_i\ge0\\ y_i(w^Tx_i+b) -1+\xi_i\ge 0 \\ a_i(y_i(w^Tx_i+b)-1+\xi_i)=0 \\ \xi_i\ge0, u_i\xi_i=0 \end{aligned} \right. $"></a>$ \left\lbrace\ \begin{aligned} a_i \ge0 , u_i\ge0\\ y_i(w^Tx_i+b) -1+\xi_i\ge 0 \\ a_i(y_i(w^Tx_i+b)-1+\xi_i)=0 \\ \xi_i\ge0, u_i\xi_i=0 \end{aligned} \right. $</h4><h3 id="合页损失函数"><a href="#合页损失函数" class="headerlink" title="合页损失函数"></a>合页损失函数</h3><p>间隔/线性支持向量机的原始问题可以等价于添加了<strong>正则化项的合页损失函数</strong>，即最小化以下目标函数</p><h4 id="min-w-b-sum-i-1-n-1-y-i-w-Tx-b-lambda-w-2"><a href="#min-w-b-sum-i-1-n-1-y-i-w-Tx-b-lambda-w-2" class="headerlink" title="$min_{w,b}\sum_{i=1}^n[1- y_i(w^Tx+b)]_++\lambda||w||^2$"></a>$min_{w,b}\sum_{i=1}^n[1- y_i(w^Tx+b)]_++\lambda||w||^2$</h4><ul><li>第一项为合页损失函数 $L(y(w^Tx+b))=[1- y_i(w^Tx+b)]<em>+$，我们再结合前面我在进行数据分割的约束条件，一般对于函数$[z]</em>+$就有：</li></ul><h4 id="z-z-如果z-gt-0"><a href="#z-z-如果z-gt-0" class="headerlink" title="$[z]_+=z, 如果z&gt;0$"></a>$[z]_+=z, 如果z&gt;0$</h4><h4 id="z-0-如果z-lt-0"><a href="#z-0-如果z-lt-0" class="headerlink" title="$[z]_+=0, 如果z&lt;0$"></a>$[z]_+=0, 如果z&lt;0$</h4><p>​ 这样原式就能表明当样本点$x_i,y_i$被正确分类且<strong>函数间隔</strong>$y_i(w^Tx+b)&gt;1$时损失为0，</p><p>​ 否则损失就为$1-y_i(w^Tx+b)$</p><ul><li>第二项为正则化项，是系数为λ的w的L2范数</li></ul><h4 id="线性支持向量机的原始最优化问题与上述目标函数的最优化问题是等价的"><a href="#线性支持向量机的原始最优化问题与上述目标函数的最优化问题是等价的" class="headerlink" title="线性支持向量机的原始最优化问题与上述目标函数的最优化问题是等价的"></a><strong>线性支持向量机的原始最优化问题与上述目标函数的最优化问题是等价的</strong></h4><h4 id="我们把一些损失函数做一个对比："><a href="#我们把一些损失函数做一个对比：" class="headerlink" title="我们把一些损失函数做一个对比："></a>我们把一些损失函数做一个对比：</h4><p><img src="/images/ml/30.png" alt="image"></p><p>如图所示为常用的一些损失函数，可以看到，各个图中损失函数的曲线基本位于0-1损失函数的上方，所以可以作为0-1损失函数的上界；</p><ul><li><p>由于0-1损失函数不是连续可导的，直接优化由其构成的目标损失函数比较困难，所以对于svm而言，可以认为是在优化由0-1损失函数的上界(合页损失函数)构成的目标函数，又称为<strong>代理损失函数</strong></p></li><li><p>合页损失函数对学习有更高的要求</p></li></ul><h3 id="常用替代损失函数"><a href="#常用替代损失函数" class="headerlink" title="常用替代损失函数"></a>常用替代损失函数</h3><p>通常具有较好的数学性质，比如凸的连续函数且是0/1损失函数的上界</p><ul><li><h4 id="hinge损失-：-l-hinge-z-max-0-1-z"><a href="#hinge损失-：-l-hinge-z-max-0-1-z" class="headerlink" title="hinge损失  ： $l_{hinge}(z)=max(0,1-z)$"></a>hinge损失 ： $l_{hinge}(z)=max(0,1-z)$</h4></li><li><h4 id="指数损失：-l-exp-z-exp-z"><a href="#指数损失：-l-exp-z-exp-z" class="headerlink" title="指数损失： $l_{exp}(z)=exp(-z)$"></a>指数损失： $l_{exp}(z)=exp(-z)$</h4></li><li><h4 id="对率损失：-l-log-z-log-1-exp-z"><a href="#对率损失：-l-log-z-log-1-exp-z" class="headerlink" title="对率损失： $l_{log}(z)=log(1+exp(-z))$"></a>对率损失： $l_{log}(z)=log(1+exp(-z))$</h4></li></ul><h3 id="SVM和Logistic-Regression对比"><a href="#SVM和Logistic-Regression对比" class="headerlink" title="SVM和Logistic Regression对比"></a>SVM和Logistic Regression对比</h3><p><strong>相同点</strong>：优化目标相近，通常情形下性能也相当</p><p><strong>不同点</strong>：</p><ul><li>LR的优势在于其输出具有自然的概率意义，给出预测标记的同时也给出了概率，而SVM要想得到概率输出需特殊处理</li><li>LR能直接用于多分类任务，而SVM则需要进行推广</li><li>SVM的解具有稀疏性（仅依赖于支持向量），LR对应的对率损失则是光滑的单调递减函数，因此LR的解依赖于更多的训练样本，其预测开销更大</li></ul><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/ml-svm-L1/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> svm </tag>
            
            <tag> 软间隔和正则化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-SVM-核函数</title>
      <link href="/2018/07/26/2018-07-26-ml-svm-kernel/"/>
      <url>/2018/07/26/2018-07-26-ml-svm-kernel/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="SVM回顾"><a href="#SVM回顾" class="headerlink" title="SVM回顾"></a>SVM回顾</h3><p>上文<a href="https://sevenold.github.io/2018/07/ml-svm/" target="_blank" rel="noopener">支持向量机SVM</a> ，简单总结了对于<strong>线性可分</strong>数据的SVM的算法原理，现在我们对于<strong>非线性可分</strong>以及有噪声存在的时候我们需要对基本SVM算法的改进进行下总结其中包括:</p><ul><li>核函数在SVN算法中的使用</li><li><p>引入松弛变量和惩罚函数的软间隔分类器</p><p>我们再回顾一下我们上次推导最终的对偶优化问题，我们后面的改进和优化都是在对偶问题形式上展开的。</p></li></ul><h3 id="SVM标准形式"><a href="#SVM标准形式" class="headerlink" title="SVM标准形式"></a>SVM标准形式</h3><h4 id="min-w-b-frac-1-2-w-2"><a href="#min-w-b-frac-1-2-w-2" class="headerlink" title="$min_{w,b}\frac{1}{2}{||w||}^2$"></a>$min_{w,b}\frac{1}{2}{||w||}^2$</h4><h4 id="s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m"><a href="#s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m" class="headerlink" title="$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h4><h3 id="对偶形式"><a href="#对偶形式" class="headerlink" title="对偶形式"></a>对偶形式</h3><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h4><h4 id="s-t-a-i-ge0-i-1-cdot-cdot-cdot-n"><a href="#s-t-a-i-ge0-i-1-cdot-cdot-cdot-n" class="headerlink" title="$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$</h4><h4 id="sum-i-1-na-iy-i-0"><a href="#sum-i-1-na-iy-i-0" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><h3 id="SVM预测模型"><a href="#SVM预测模型" class="headerlink" title="SVM预测模型"></a>SVM预测模型</h3><p>SVM通过分割超平面$w^Tx+b$来获取未知数据的类型，将上述的$w和b$替换就可以得到：</p><h4 id="h-w-b-x-g-w-Tx-b-g-sum-i-1-n-a-iy-i-x-i-Tx-b"><a href="#h-w-b-x-g-w-Tx-b-g-sum-i-1-n-a-iy-i-x-i-Tx-b" class="headerlink" title="$h_{w,b}(x)=g(w^Tx+b)=g(\sum_{i=1}^n a_iy_i{x_i}^Tx+b)$"></a>$h_{w,b}(x)=g(w^Tx+b)=g(\sum_{i=1}^n a_iy_i{x_i}^Tx+b)$</h4><p>通过$g(x)来输出+1还是-1$来获取未知数据的类型</p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>前面我在推导SVM算法的时候解决的一般是线性可分的数据，而对于非线性可分的数据的时候（如图），我们就需要引出核函数</p><p><img src="/images/ml/23.png" alt="image"></p><p>所以对于这类问题，SVM的处理方法就是选择一个核函数，其通过将数据映射到更高维的特征空间（非线性映射），使得样本在这个特征空间内线性可分，从而解决原始空间中线性不可分的问题。</p><p><img src="/images/ml/24.png" alt="image"></p><p>从数据上来看就是把数据映射到多维：例如从一维映射到四维：</p><p><img src="/images/ml/25.png" alt="image"></p><p>这里是通过$\Phi(x)$把数据映射到高维空间，所以对应的对偶问题就改写为：</p><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-Phi-x-i-T-Phi-x-j）y-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-Phi-x-i-T-Phi-x-j）y-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{\Phi(x_i)}^T\Phi(x_j）y_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{\Phi(x_i)}^T\Phi(x_j）y_iy_j$</h4><h4 id="s-t-a-i-ge0-i-1-cdot-cdot-cdot-n-1"><a href="#s-t-a-i-ge0-i-1-cdot-cdot-cdot-n-1" class="headerlink" title="$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$</h4><h4 id="sum-i-1-na-iy-i-0-1"><a href="#sum-i-1-na-iy-i-0-1" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><p>${\Phi(x_i)}^T\Phi(x_j$,是我们把样本$x_i和 x_j$映射到特征空间之后的内积，但是由于特征空间的维数可能很高，甚至是无穷维，所以直接计算${\Phi(x_i)}^T\Phi(x_j）$是非常困难的，但是我们有又必须要计算他，所以为了避开这个障碍，我们就设一个函数：</p><h4 id="k-x-i-x-j-Phi-x-i-T-Phi-x-j）"><a href="#k-x-i-x-j-Phi-x-i-T-Phi-x-j）" class="headerlink" title="$k(x_i,x_j)={\Phi(x_i)}^T\Phi(x_j）$"></a>$k(x_i,x_j)={\Phi(x_i)}^T\Phi(x_j）$</h4><p>我们就直接通过函数$h(\cdot, \cdot)$计算获得${\Phi(x_i)}^T\Phi(x_j）$的结果，就不必直接去计算高维甚至无穷维的特征空间中的内积。于是对偶问题和预测模型就改写为：</p><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-jk-x-i-x-j-y-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-jk-x-i-x-j-y-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_jk(x_i,x_j)y_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_jk(x_i,x_j)y_iy_j$</h4><h4 id="s-t-a-i-ge0-i-1-cdot-cdot-cdot-n-2"><a href="#s-t-a-i-ge0-i-1-cdot-cdot-cdot-n-2" class="headerlink" title="$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$</h4><h4 id="sum-i-1-na-iy-i-0-2"><a href="#sum-i-1-na-iy-i-0-2" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><h4 id="f-x-sum-i-1-n-a-iy-i-x-i-Tx-b-sum-i-1-n-a-iy-ik-x-x-i-b"><a href="#f-x-sum-i-1-n-a-iy-i-x-i-Tx-b-sum-i-1-n-a-iy-ik-x-x-i-b" class="headerlink" title="$f(x)=\sum_{i=1}^n a_iy_i{x_i}^Tx+b=\sum_{i=1}^n a_iy_ik(x,x_i)+b$"></a>$f(x)=\sum_{i=1}^n a_iy_i{x_i}^Tx+b=\sum_{i=1}^n a_iy_ik(x,x_i)+b$</h4><p>而这里的$k(\cdot, \cdot )$就是<strong>核函数</strong>（kernel function）.上述的$f(x)$显示出模型最优解可通过训练样本的核函数展开，这也就是<strong>支持向量展式</strong>。</p><h3 id="核函数定理"><a href="#核函数定理" class="headerlink" title="核函数定理"></a>核函数定理</h3><p>令$X$为输入空间，$k(\cdot , \cdot )$是定义在$X \times X$上的对称函数，则$k$是核函数当且仅当对于数据$D={x_1,x_2 ,\cdot \cdot \cdot x_n}$，“核矩阵”K就是总是一个半正定矩阵：</p><p><img src="/images/ml/26.png" alt="image"></p><p>通俗来讲：只要一个对称函数所对应的核矩阵是半正定矩阵，它就能作为核函数。</p><h3 id="常用的核函数"><a href="#常用的核函数" class="headerlink" title="常用的核函数"></a>常用的核函数</h3><ul><li><h4 id="线性核：-k-x-i-x-j-x-i-Tx-j"><a href="#线性核：-k-x-i-x-j-x-i-Tx-j" class="headerlink" title="线性核：$k(x_i,x_j)={x_i}^Tx_j$"></a>线性核：$k(x_i,x_j)={x_i}^Tx_j$</h4></li><li><h4 id="多项式核：-k-x-i-x-j-x-i-Tx-j-n-n-ge1-为多项式的次数"><a href="#多项式核：-k-x-i-x-j-x-i-Tx-j-n-n-ge1-为多项式的次数" class="headerlink" title="多项式核：$k(x_i,x_j)=({x_i}^Tx_j)^n, n\ge1$为多项式的次数"></a>多项式核：$k(x_i,x_j)=({x_i}^Tx_j)^n, n\ge1$为多项式的次数</h4></li><li><h4 id="高斯核：-k-x-i-x-j-e-frac-x-i-x-j-2-2-sigma-2-sigma-gt-0-为高斯核的带宽（width）"><a href="#高斯核：-k-x-i-x-j-e-frac-x-i-x-j-2-2-sigma-2-sigma-gt-0-为高斯核的带宽（width）" class="headerlink" title="高斯核：$k(x_i,x_j)=e^{-\frac{||x_i-x_j||^2}{2\sigma^2}}, \sigma&gt;0$为高斯核的带宽（width）"></a>高斯核：$k(x_i,x_j)=e^{-\frac{||x_i-x_j||^2}{2\sigma^2}}, \sigma&gt;0$为高斯核的带宽（width）</h4></li><li><h4 id="拉普拉斯核：-k-x-i-x-j-e-frac-x-i-x-j-2-2-sigma-sigma-gt-0"><a href="#拉普拉斯核：-k-x-i-x-j-e-frac-x-i-x-j-2-2-sigma-sigma-gt-0" class="headerlink" title="拉普拉斯核：$k(x_i,x_j)=e^{-\frac{||x_i-x_j||^2}{2\sigma}}, \sigma&gt;0$"></a>拉普拉斯核：$k(x_i,x_j)=e^{-\frac{||x_i-x_j||^2}{2\sigma}}, \sigma&gt;0$</h4></li><li><h4 id="Sigmoid核：-k-x-i-x-j-tanh-beta-x-i-Tx-j-theta-，-tanh为双曲正切函数，-beta-gt-0-theta-gt-0"><a href="#Sigmoid核：-k-x-i-x-j-tanh-beta-x-i-Tx-j-theta-，-tanh为双曲正切函数，-beta-gt-0-theta-gt-0" class="headerlink" title="Sigmoid核：$k(x_i,x_j)=tanh(\beta{x_i}^Tx_j+\theta)  $，$tanh为双曲正切函数，$\beta&gt;0, \theta&gt;0$"></a>Sigmoid核：$k(x_i,x_j)=tanh(\beta{x_i}^Tx_j+\theta) $，$tanh为双曲正切函数，$\beta&gt;0, \theta&gt;0$</h4></li></ul><h4 id="通过函数组合："><a href="#通过函数组合：" class="headerlink" title="通过函数组合："></a>通过函数组合：</h4><ul><li><h4 id="若-k-1-和-k-2-为核函数，则对任意正数-lambda-1-lambda-2-，其线性组合：-lambda-1k-1-lambda-2k-2"><a href="#若-k-1-和-k-2-为核函数，则对任意正数-lambda-1-lambda-2-，其线性组合：-lambda-1k-1-lambda-2k-2" class="headerlink" title="若$k_1$和$k_2$为核函数，则对任意正数$\lambda_1, \lambda_2$，其线性组合：$\lambda_1k_1+\lambda_2k_2$"></a>若$k_1$和$k_2$为核函数，则对任意正数$\lambda_1, \lambda_2$，其线性组合：$\lambda_1k_1+\lambda_2k_2$</h4></li><li><h4 id="若-k-1-和-k-2-为核函数，则核函数的直积：-k-1-otimes-k-2-x-z-k-1-x-z-k-2-x-z"><a href="#若-k-1-和-k-2-为核函数，则核函数的直积：-k-1-otimes-k-2-x-z-k-1-x-z-k-2-x-z" class="headerlink" title="若$k_1$和$k_2$为核函数，则核函数的直积：$k_1 \otimes k_2 (x,z)=k_1(x,z)k_2(x,z)$"></a>若$k_1$和$k_2$为核函数，则核函数的直积：$k_1 \otimes k_2 (x,z)=k_1(x,z)k_2(x,z)$</h4></li><li><h4 id="若-k-1-为核函数，则对于任意函数-g-x-k-x-z-g-x-k-1-x-z-g-z"><a href="#若-k-1-为核函数，则对于任意函数-g-x-k-x-z-g-x-k-1-x-z-g-z" class="headerlink" title="若$k_1$为核函数，则对于任意函数$g(x)$ :$k(x,z)=g(x)k_1(x,z)g(z)$"></a>若$k_1$为核函数，则对于任意函数$g(x)$ :$k(x,z)=g(x)k_1(x,z)g(z)$</h4></li></ul><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/2018-07-25-ml-svm-kernel/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> svm </tag>
            
            <tag> 核函数 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-支持向量机SVM</title>
      <link href="/2018/07/25/2018-07-25-ml-svm/"/>
      <url>/2018/07/25/2018-07-25-ml-svm/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h2 id="支持向量机SVM的概念及起源"><a href="#支持向量机SVM的概念及起源" class="headerlink" title="支持向量机SVM的概念及起源"></a>支持向量机SVM的概念及起源</h2><h3 id="什么是支持向量机SVM"><a href="#什么是支持向量机SVM" class="headerlink" title="什么是支持向量机SVM"></a>什么是支持向量机SVM</h3><p>支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。</p><h3 id="分类标准的起源：Logistic回归"><a href="#分类标准的起源：Logistic回归" class="headerlink" title="分类标准的起源：Logistic回归"></a>分类标准的起源：Logistic回归</h3><h4 id="我们先看看什么是线性分类器"><a href="#我们先看看什么是线性分类器" class="headerlink" title="我们先看看什么是线性分类器"></a>我们先看看什么是线性分类器</h4><p>给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用$x$表示数据点，用$y$表示类别（$y=1$或者$y=-1$，分别代表两个不同的类），一个线性分类器的学习目标便是要在$n$维的数据空间中找到一个超平面（hyper plane），这个超平面的方程可以表示为（$w^T$中的T代表转置）：</p><h2 id="w-Tx-b-0"><a href="#w-Tx-b-0" class="headerlink" title="$w^Tx+b=0$"></a>$w^Tx+b=0$</h2><p><strong>Logistic回归</strong>目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于$y=1$的概率。</p><p>根据我们前面<a href="https://sevenold.github.io/2018/07/ml-logisticRegression/" target="_blank" rel="noopener">Logistic回归的推导</a> ，假设函数：</p><h2 id="h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx"><a href="#h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx" class="headerlink" title="$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$"></a>$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$</h2><p>其中x是n维特征向量，函数g就是Logistic函数。</p><h4 id="同样-g-z-frac-1-1-e-z-的图像："><a href="#同样-g-z-frac-1-1-e-z-的图像：" class="headerlink" title="同样$g(z)=\frac{1}{1+e^{-z}}$的图像："></a>同样$g(z)=\frac{1}{1+e^{-z}}$的图像：</h4><p><img src="/images/ml/18.png" alt="images"></p><p>从图像中，我们就可以看出将无穷映射到了（0,1）。</p><p>而假设函数就是特征属于y=1的概率。</p><h2 id="P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x"><a href="#P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x" class="headerlink" title="$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$"></a>$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$</h2><p>所以，我们在判别一个新的特征属于哪个类别的时候，就只需要求$h_\theta(x)$就可以了，由上面的图中可以看出如果$h_\theta(x)$ 大于0.5就是y=1的类，否则就是属于y=1的类。</p><h4 id="然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y-0和y-1替换为-y-1-y-1-然后把-theta-0-替换成b，把-w-替换-theta-i-所以就变换为："><a href="#然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y-0和y-1替换为-y-1-y-1-然后把-theta-0-替换成b，把-w-替换-theta-i-所以就变换为：" class="headerlink" title="然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y=0和y=1替换为$y=-1,y=1 $,然后把$\theta_0$替换成b，把$w$替换$\theta_i$,所以就变换为："></a>然后我们尝试给Logistic回归做一个变形。首先，将使用的结果标签y=0和y=1替换为$y=-1,y=1 $,然后把$\theta_0$替换成b，把$w$替换$\theta_i$,所以就变换为：</h4><h2 id="h-theta-x-g-w-Tx-b"><a href="#h-theta-x-g-w-Tx-b" class="headerlink" title="$h_\theta(x)=g(w^Tx+b)$"></a>$h_\theta(x)=g(w^Tx+b)$</h2><h4 id="所以我们就可以假设函数：-h-w-b-x-g-w-Tx-b-中的g-z-做一个简化，将其简单映射到y-1和y-1上。映射关系如下："><a href="#所以我们就可以假设函数：-h-w-b-x-g-w-Tx-b-中的g-z-做一个简化，将其简单映射到y-1和y-1上。映射关系如下：" class="headerlink" title="所以我们就可以假设函数：$h_{w,b}(x)=g(w^Tx+b)$,中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下："></a>所以我们就可以假设函数：$h_{w,b}(x)=g(w^Tx+b)$,中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下：</h4><h2 id="g-z-left-lbrace-begin-aligned-1-amp-amp-z-ge0-1-amp-amp-z-lt-0-end-aligned-right"><a href="#g-z-left-lbrace-begin-aligned-1-amp-amp-z-ge0-1-amp-amp-z-lt-0-end-aligned-right" class="headerlink" title="$ g(z)=\left\lbrace  \begin{aligned} 1 &amp;&amp; z \ge0 \\ -1 &amp;&amp; z&lt;0 \end{aligned} \right. $"></a>$ g(z)=\left\lbrace \begin{aligned} 1 &amp;&amp; z \ge0 \\ -1 &amp;&amp; z&lt;0 \end{aligned} \right. $</h2><h4 id="然后我们举个线性分类的例子来看看"><a href="#然后我们举个线性分类的例子来看看" class="headerlink" title="然后我们举个线性分类的例子来看看"></a>然后我们举个线性分类的例子来看看</h4><p>如下图所示，现在有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示。由于这些数据是线性可分的，所以可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是-1，另一边所对应的y全是1。</p><p><img src="/images/ml/19.png" alt="images"></p><p>这个超平面可以用分类函数$f(x)=w^Tx+b$来表示，当f(x)等于0时候，x便是位于超平面上的点，而f(x)大于0的点对应y=1的数据点，f(x)小于0的点对应y=-1的点，如下图所示：</p><p><img src="/images/ml/20.png" alt="images"></p><p>接下来的问题是，如何确定这个超平面呢？从直观上而言，这个超平面应该是最适合分开两类数据的直线。而判定“最适合”的标准就是这条直线离直线两边的数据的间隔最大。所以，得寻找有着最大间隔的超平面。</p><h3 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h3><p>SVM支持向量机（英文全称：support vector machine）是一个分类算法， 通过找到一个分类平面， 将数据分隔在平面两侧， 从而达到分类的目的。如下图所示， 直线表示的是训练出的一个分类平面， 将数据有效的分隔开。</p><p><img src="/images/ml/16.png" alt="images"></p><p>根据上面的逻辑，我们在做数据分隔的时候，有很多个分类平面，这时我们就需要找出“最优”的那一个平面模型，根据【超平面】【数据点】【分开】这几个词，我们可以想到最优的模型必然是最大程度地将数据点划分开的模型，不能靠近负样本也不能靠近正样本，要不偏不倚，并且与所有Support Vector的距离尽量大才可以。</p><p><img src="/images/ml/17.png" alt="images"></p><h4 id="上图中-x-0-是-x-在超平面上的投影，-ω-是超平面的法向量"><a href="#上图中-x-0-是-x-在超平面上的投影，-ω-是超平面的法向量" class="headerlink" title="上图中 $x_0$ 是 $x $在超平面上的投影，$ω$是超平面的法向量."></a>上图中 $x_0$ 是 $x $在超平面上的投影，$ω$是超平面的法向量.</h4><h4 id="超平面用线性方程来描述："><a href="#超平面用线性方程来描述：" class="headerlink" title="超平面用线性方程来描述："></a>超平面用线性方程来描述：</h4><h2 id="w-T-b-0"><a href="#w-T-b-0" class="headerlink" title="$w^T+b=0$"></a>$w^T+b=0$</h2><h3 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h3><p>在超平面$w ^T x + b = 0$确定的情况下，$|w^Tx+b|$表示点距离超平面的距离，而超平面作为二分类器，如果$w^Tx+b&gt;0$， 判断类别y为1, 否则判定为-1。从而引出函数间隔的定义：</p><h2 id="r-y-w-Tx-b-yf-x"><a href="#r-y-w-Tx-b-yf-x" class="headerlink" title="$r=y(w^Tx+b)=yf(x)$"></a>$r=y(w^Tx+b)=yf(x)$</h2><p>其中y是训练数据的类标记值， 如果$y(w^T x + b) &gt;0$说明，预测的值和标记的值相同， 分类正确，而且值越大，说明点离平面越远，分类的可靠程度更高。这是对单个样本的函数定义， 对整个样本集来说，要找到所有样本中间隔值最小的作为整个集合的函数间隔：</p><h2 id="r-min-r-i-i-1-2-cdot-cdot-cdot-n"><a href="#r-min-r-i-i-1-2-cdot-cdot-cdot-n" class="headerlink" title="$r=min \   r_i  , i=1,2 \cdot \cdot \cdot n$"></a>$r=min \ r_i , i=1,2 \cdot \cdot \cdot n$</h2><p>即w和b同时缩小或放大M倍后，超平面并没有变化，但是函数间隔跟着w和b变化。所以，需要加入约束条件使得函数间隔固定, 也就是几何间隔。</p><h3 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h3><h4 id="样本空间-x-到超平面-x-0-的距离："><a href="#样本空间-x-到超平面-x-0-的距离：" class="headerlink" title="样本空间$x$到超平面$x_0$的距离："></a>样本空间$x$到超平面$x_0$的距离：</h4><h2 id="r-frac-w-Tx-b-w"><a href="#r-frac-w-Tx-b-w" class="headerlink" title="$r=\frac{|w^Tx+b|}{||w||}$"></a>$r=\frac{|w^Tx+b|}{||w||}$</h2><h4 id="如果超平面将样本成功分类，若-y-i-1-则有-w-Tx-b-gt-0-若-y-i-1-则有-w-Tx-b-lt-0-则下式成立"><a href="#如果超平面将样本成功分类，若-y-i-1-则有-w-Tx-b-gt-0-若-y-i-1-则有-w-Tx-b-lt-0-则下式成立" class="headerlink" title="如果超平面将样本成功分类，若$y_i=+1$,则有$w^Tx+b&gt;0$;若$y_i=-1$,则有$w^Tx+b&lt;0$;则下式成立"></a>如果超平面将样本成功分类，若$y_i=+1$,则有$w^Tx+b&gt;0$;若$y_i=-1$,则有$w^Tx+b&lt;0$;则下式成立</h4><h2 id="left-lbrace-begin-aligned-w-Tx-b-ge-1-amp-amp-y-i-1-w-Tx-b-le-1-amp-amp-y-i-1-end-aligned-right"><a href="#left-lbrace-begin-aligned-w-Tx-b-ge-1-amp-amp-y-i-1-w-Tx-b-le-1-amp-amp-y-i-1-end-aligned-right" class="headerlink" title="$ \left \lbrace \begin{aligned} w^Tx+b \ge+1&amp;&amp; y_i=+1 \\ w^Tx+b\le-1 &amp;&amp; y_i=-1 \end{aligned} \right.$"></a>$ \left \lbrace \begin{aligned} w^Tx+b \ge+1&amp;&amp; y_i=+1 \\ w^Tx+b\le-1 &amp;&amp; y_i=-1 \end{aligned} \right.$</h2><p><img src="/images/ml/21.png" alt="images"></p><h4 id="从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support-vector）"><a href="#从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support-vector）" class="headerlink" title="从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为“支持向量”（support vector）"></a>从上图中可以看出，距离超平面最近的这几个训练样本点使上述公式的等号成立，这几个训练样本就称为<strong>“支持向量”</strong>（support vector）</h4><h4 id="两个异类支持向量到超平面的距离之和（被称为间隔）："><a href="#两个异类支持向量到超平面的距离之和（被称为间隔）：" class="headerlink" title="两个异类支持向量到超平面的距离之和（被称为间隔）："></a>两个异类支持向量到超平面的距离之和（被称为间隔）：</h4><h2 id="r-frac-2-w"><a href="#r-frac-2-w" class="headerlink" title="$r=\frac{2}{||w||}$"></a>$r=\frac{2}{||w||}$</h2><h3 id="”最大间隔“的超平面"><a href="#”最大间隔“的超平面" class="headerlink" title="”最大间隔“的超平面"></a>”最大间隔“的超平面</h3><h4 id="我们要找的”最大间隔“的超平面，即："><a href="#我们要找的”最大间隔“的超平面，即：" class="headerlink" title="我们要找的”最大间隔“的超平面，即："></a>我们要找的”最大间隔“的超平面，即：</h4><h2 id="max-w-b-frac-2-w"><a href="#max-w-b-frac-2-w" class="headerlink" title="$max_{w,b}\frac{2}{||w||}$"></a>$max_{w,b}\frac{2}{||w||}$</h2><h2 id="s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m"><a href="#s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m" class="headerlink" title="$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h2><h3 id="SVM的二次凸函数和约束条件"><a href="#SVM的二次凸函数和约束条件" class="headerlink" title="SVM的二次凸函数和约束条件"></a>SVM的二次凸函数和约束条件</h3><p>最大间隔分类器的求解， 可以转换为上面的一个最优化问题， 即在满足约束条件：</p><h2 id="y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m"><a href="#y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m" class="headerlink" title="$y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h2><h4 id="求出就最大的-frac1-w-。"><a href="#求出就最大的-frac1-w-。" class="headerlink" title="求出就最大的$\frac1{||w||}$。"></a>求出就最大的$\frac1{||w||}$。</h4><h4 id="为更好的利用现有的理论和计算方法，-可以将求解-frac1-w-最大值，-转换为一个二次凸函数优化问题：求解-min-w-b-frac-1-2-w-2-，-两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。"><a href="#为更好的利用现有的理论和计算方法，-可以将求解-frac1-w-最大值，-转换为一个二次凸函数优化问题：求解-min-w-b-frac-1-2-w-2-，-两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。" class="headerlink" title="为更好的利用现有的理论和计算方法， 可以将求解$\frac1{||w||}$最大值， 转换为一个二次凸函数优化问题：求解 $min_{w,b}\frac{1}{2}{||w||}^2$， 两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。"></a>为更好的利用现有的理论和计算方法， 可以将求解$\frac1{||w||}$最大值， 转换为一个二次凸函数优化问题：求解 $min_{w,b}\frac{1}{2}{||w||}^2$， 两者问题是等价的。原来的问题转换为二次凸函数优化问题。在一定的约束条件下，目标最优，损失最小。</h4><h3 id="SVM的基本型"><a href="#SVM的基本型" class="headerlink" title="SVM的基本型"></a>SVM的基本型</h3><h4 id="我们前面已经知道，最大化-w-1-等价于最小化-w-2-所以："><a href="#我们前面已经知道，最大化-w-1-等价于最小化-w-2-所以：" class="headerlink" title="我们前面已经知道，最大化$||w||^{-1}$等价于最小化$||w||^2$,所以："></a>我们前面已经知道，最大化$||w||^{-1}$等价于最小化$||w||^2$,所以：</h4><h2 id="min-w-b-frac-1-2-w-2"><a href="#min-w-b-frac-1-2-w-2" class="headerlink" title="$min_{w,b}\frac{1}{2}{||w||}^2$"></a>$min_{w,b}\frac{1}{2}{||w||}^2$</h2><h2 id="s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m-1"><a href="#s-t-y-i-w-Tx-b-ge1-i-1-2-cdot-cdot-cdot-m-1" class="headerlink" title="$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$"></a>$s.t.y_i(w^Tx+b)\ge1, i=1,2,\cdot \cdot \cdot m$</h2><h3 id="拉格朗日构建方程"><a href="#拉格朗日构建方程" class="headerlink" title="拉格朗日构建方程"></a>拉格朗日构建方程</h3><p>由于这个问题的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality）变换到对偶变量(dual variable)的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法.</p><h4 id="这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，"><a href="#这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，" class="headerlink" title="这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，"></a>这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，</h4><p>进而推广到非线性分类问题。 具体来说就是对svm基本型的每条约束添加拉格朗日乘子$a_i\ge0$,则该问题的拉格朗日函数可写为：</p><h4 id="L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b-a-i-a-1-a-2-cdot-cdot-cdot-a-m"><a href="#L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b-a-i-a-1-a-2-cdot-cdot-cdot-a-m" class="headerlink" title="$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$                   $a_i=(a_1;a_2; \cdot \cdot \cdot a_m)$."></a>$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$ $a_i=(a_1;a_2; \cdot \cdot \cdot a_m)$.</h4><p>我们的目标是让拉格朗如函数$ L(ω,b,α)$ 针对 $α$ 达到最大值。为什么能够这么写呢，我们可以这样想，哪怕有一个 $y_i(ω^Tx_i+b)⩾1$不满足，只要让对应的 $α_i$ 是正无穷就好了。所以，如果$L(ω,b,α)$有有限的最大值，那么那些不等式条件是自然满足的。 之后，我们再让 $L(ω,b,α)$ 针对 $ω,b$ 达到最小值，就可以了。 从而，我们的目标函数变成：</p><h4 id="原问题是极小极大的问题"><a href="#原问题是极小极大的问题" class="headerlink" title="原问题是极小极大的问题"></a>原问题是极小极大的问题</h4><h2 id="min-w-b-max-aL-w-b-a-p"><a href="#min-w-b-max-aL-w-b-a-p" class="headerlink" title="$min_{w,b}max_aL(w,b,a)=p^*$"></a>$min_{w,b}max_aL(w,b,a)=p^*$</h2><h4 id="原始问题的对偶问题，是极大极小问题"><a href="#原始问题的对偶问题，是极大极小问题" class="headerlink" title="原始问题的对偶问题，是极大极小问题"></a>原始问题的对偶问题，是极大极小问题</h4><h2 id="max-amin-w-b-L-w-b-a-b"><a href="#max-amin-w-b-L-w-b-a-b" class="headerlink" title="$max_amin_{w,b}L(w,b,a)=b^*$"></a>$max_amin_{w,b}L(w,b,a)=b^*$</h2><p>交换以后的新问题是原始问题的对偶问题，这个新问题的最优值用$d^*$来表示。而且有$d^∗⩽p^∗$，在满足某些条件的情况下，这两者相等，这个时候就可以通过求解对偶问题来间接地求解原始问题。</p><p>这所谓的“满足某些条件”就是要满足KKT条件。</p><h2 id="left-lbrace-begin-aligned-a-i-ge0-y-i-w-Tx-i-b-1-ge-0-a-i-y-i-w-Tx-i-b-1-0-end-aligned-right"><a href="#left-lbrace-begin-aligned-a-i-ge0-y-i-w-Tx-i-b-1-ge-0-a-i-y-i-w-Tx-i-b-1-0-end-aligned-right" class="headerlink" title="$ \left\lbrace\ \begin{aligned} a_i \ge0 \\ y_i(w^Tx_i+b) -1\ge 0 \\ a_i(y_i(w^Tx_i+b)-1)=0 \end{aligned} \right. $"></a>$ \left\lbrace\ \begin{aligned} a_i \ge0 \\ y_i(w^Tx_i+b) -1\ge 0 \\ a_i(y_i(w^Tx_i+b)-1)=0 \end{aligned} \right. $</h2><h3 id="KKT条件的意义"><a href="#KKT条件的意义" class="headerlink" title="KKT条件的意义"></a>KKT条件的意义</h3><h5 id="一般地，一个最优化数学模型能够表示成下列标准形式："><a href="#一般地，一个最优化数学模型能够表示成下列标准形式：" class="headerlink" title="一般地，一个最优化数学模型能够表示成下列标准形式："></a>一般地，一个最优化数学模型能够表示成下列标准形式：</h5><h2 id="min-f-x"><a href="#min-f-x" class="headerlink" title="$min.f(x)$"></a>$min.f(x)$</h2><h2 id="s-t-h-j-x-0-j-1-cdot-cdot-cdot-n"><a href="#s-t-h-j-x-0-j-1-cdot-cdot-cdot-n" class="headerlink" title="$s.t.  h_j(x)=0,j=1,\cdot \cdot \cdot n$"></a>$s.t. h_j(x)=0,j=1,\cdot \cdot \cdot n$</h2><h2 id="g-k-x-le0-k-1-cdot-cdot-cdot-m"><a href="#g-k-x-le0-k-1-cdot-cdot-cdot-m" class="headerlink" title="$g_k(x)\le0,k=1,\cdot \cdot \cdot m$"></a>$g_k(x)\le0,k=1,\cdot \cdot \cdot m$</h2><h2 id="x-in-X-subset-R-n"><a href="#x-in-X-subset-R-n" class="headerlink" title="$x\in X \subset R^n$"></a>$x\in X \subset R^n$</h2><h5 id="其中，f-x-是需要最小化的函数，h-x-是等式约束，g-x-是不等式约束，p和q分别为等式约束和不等式约束的数量。"><a href="#其中，f-x-是需要最小化的函数，h-x-是等式约束，g-x-是不等式约束，p和q分别为等式约束和不等式约束的数量。" class="headerlink" title="其中，f(x)是需要最小化的函数，h(x)是等式约束，g(x)是不等式约束，p和q分别为等式约束和不等式约束的数量。"></a>其中，f(x)是需要最小化的函数，h(x)是等式约束，g(x)是不等式约束，p和q分别为等式约束和不等式约束的数量。</h5><ul><li><h4 id="凸优化的概念：-x-subset-R-n-为一凸集，-f-x-to-R-为一凸函数，凸优化就是要找出一点-x-in-X-使得每一-x-in-X-满足-f-x-le-f-x"><a href="#凸优化的概念：-x-subset-R-n-为一凸集，-f-x-to-R-为一凸函数，凸优化就是要找出一点-x-in-X-使得每一-x-in-X-满足-f-x-le-f-x" class="headerlink" title="凸优化的概念：$x\subset R^n$为一凸集，$f:x\to R$为一凸函数，凸优化就是要找出一点$x^\in X$,使得每一$x\in X$满足$f(x^)\le f(x)$"></a>凸优化的概念：$x\subset R^n$为一凸集，$f:x\to R$为一凸函数，凸优化就是要找出一点$x^<em>\in X$,使得每一$x\in X$满足$f(x^</em>)\le f(x)$</h4></li><li><h4 id="KKT条件的意义：它是一个非线性规划（Nonlinear-Programming）问题能有最优化解法的必要和充分条件。"><a href="#KKT条件的意义：它是一个非线性规划（Nonlinear-Programming）问题能有最优化解法的必要和充分条件。" class="headerlink" title="KKT条件的意义：它是一个非线性规划（Nonlinear Programming）问题能有最优化解法的必要和充分条件。"></a>KKT条件的意义：它是一个非线性规划（Nonlinear Programming）问题能有最优化解法的必要和充分条件。</h4></li></ul><p>而KKT条件就是指上面最优化数学模型的标准形式中的最小点$ x*$ 必须满足下面的条件：</p><p><img src="/images/ml/22.png" alt="images"></p><p>经过论证，我们这里的问题是满足KKT条件的（首先已经满足Slater condition，再者$f(x)$和$g(x)$也都是可微的，即$L$对$w$和$b$都可导），因此现在我们便转化为求解第二个问题。</p><p>也就是说，原始问题通过满足KKT条件，已经转化成了对偶问题。而求解这个对偶学习问题，分为3个步骤：首先要让$L(w，b，a)$关于$w$和$b$最小化，然后求对$a$的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。</p><h3 id="对偶问题求解"><a href="#对偶问题求解" class="headerlink" title="对偶问题求解"></a>对偶问题求解</h3><h4 id="首先固定-a-先求出-min-w-b-L-w-b-a-所以分别对-w-b-进行求偏导并令其等于0。"><a href="#首先固定-a-先求出-min-w-b-L-w-b-a-所以分别对-w-b-进行求偏导并令其等于0。" class="headerlink" title="首先固定$a$,先求出$min_{w,b}L(w,b,a)$,所以分别对$w,b$进行求偏导并令其等于0。"></a>首先固定$a$,先求出$min_{w,b}L(w,b,a)$,所以分别对$w,b$进行求偏导并令其等于0。</h4><h2 id="frac-partial-L-w-b-a-partial-w-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-w-0"><a href="#frac-partial-L-w-b-a-partial-w-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-w-0" class="headerlink" title="$\frac{\partial L(w,b,a)}{\partial w}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial w}  =0$"></a>$\frac{\partial L(w,b,a)}{\partial w}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial w} =0$</h2><h2 id="frac-partial-L-w-b-a-partial-b-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-b-0"><a href="#frac-partial-L-w-b-a-partial-b-frac-partial-frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx-partial-b-0" class="headerlink" title="$\frac{\partial L(w,b,a)}{\partial b}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial b}  =0$"></a>$\frac{\partial L(w,b,a)}{\partial b}=\frac{\partial(\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx)}{\partial b} =0$</h2><h2 id="w-sum-i-1-na-ix-iy-i"><a href="#w-sum-i-1-na-ix-iy-i" class="headerlink" title="$w=\sum_{i=1}^na_ix_iy_i$"></a>$w=\sum_{i=1}^na_ix_iy_i$</h2><h2 id="sum-i-1-na-iy-i-0"><a href="#sum-i-1-na-iy-i-0" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h2><p>然后我们将以上结果带入原式<strong>$L(w,b,a)$</strong>:</p><h2 id="L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b"><a href="#L-w-b-a-frac12-w-2-sum-i-1-na-i-1-y-i-w-Tx-b" class="headerlink" title="$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$"></a>$L(w,b,a)= \frac12 ||w||^2+\sum_{i=1}^na_i(1-y_i(w^Tx+b))$</h2><h2 id="frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx"><a href="#frac12w-Tw-sum-i-1-na-i-sum-i-1-n-a-iby-i-sum-i-1-n-a-iy-iw-Tx" class="headerlink" title="$=\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx$"></a>$=\frac12w^Tw+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-\sum_{i=1}^n a_iy_iw^Tx$</h2><h4 id="导入-w-sum-i-1-na-ix-iy-i"><a href="#导入-w-sum-i-1-na-ix-iy-i" class="headerlink" title="导入$w=\sum_{i=1}^na_ix_iy_i$:"></a><strong>导入$w=\sum_{i=1}^na_ix_iy_i$:</strong></h4><h2 id="frac12w-T-sum-i-1-na-ix-iy-i-sum-i-1-na-i-sum-i-1-n-a-iby-i-w-T-sum-i-1-n-a-iy-ix"><a href="#frac12w-T-sum-i-1-na-ix-iy-i-sum-i-1-na-i-sum-i-1-n-a-iby-i-w-T-sum-i-1-n-a-iy-ix" class="headerlink" title="$=\frac12w^T(\sum_{i=1}^na_ix_iy_i)+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-w^T(\sum_{i=1}^n a_iy_ix)$"></a>$=\frac12w^T(\sum_{i=1}^na_ix_iy_i)+\sum_{i=1}^na_i-\sum_{i=1}^n a_iby_i-w^T(\sum_{i=1}^n a_iy_ix)$</h2><h4 id="导入-sum-i-1-na-iy-i-0"><a href="#导入-sum-i-1-na-iy-i-0" class="headerlink" title="导入$\sum_{i=1}^na_iy_i=0$:"></a><strong>导入$\sum_{i=1}^na_iy_i=0$:</strong></h4><h2 id="sum-i-1-na-i-frac12w-T-sum-i-1-na-ix-iy-i"><a href="#sum-i-1-na-i-frac12w-T-sum-i-1-na-ix-iy-i" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12w^T(\sum_{i=1}^na_ix_iy_i)$"></a>$=\sum_{i=1}^na_i-\frac12w^T(\sum_{i=1}^na_ix_iy_i)$</h2><h2 id="sum-i-1-na-i-frac12-sum-i-1-na-ix-iy-i-T-sum-i-1-na-ix-iy-i"><a href="#sum-i-1-na-i-frac12-sum-i-1-na-ix-iy-i-T-sum-i-1-na-ix-iy-i" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12{(\sum_{i=1}^na_ix_iy_i)}^T(\sum_{i=1}^na_ix_iy_i)$"></a>$=\sum_{i=1}^na_i-\frac12{(\sum_{i=1}^na_ix_iy_i)}^T(\sum_{i=1}^na_ix_iy_i)$</h2><h2 id="sum-i-1-na-i-frac12-sum-i-1-na-i-x-i-Ty-i-sum-i-1-na-ix-iy-i"><a href="#sum-i-1-na-i-frac12-sum-i-1-na-i-x-i-Ty-i-sum-i-1-na-ix-iy-i" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12(\sum_{i=1}^na_i{x_i}^Ty_i)(\sum_{i=1}^na_ix_iy_i)$"></a>$=\sum_{i=1}^na_i-\frac12(\sum_{i=1}^na_i{x_i}^Ty_i)(\sum_{i=1}^na_ix_iy_i)$</h2><h2 id="sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><a href="#sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j" class="headerlink" title="$=\sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$=\sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h2><p>从上面的最后一个式子，我们可以看出，此时的拉格朗日函数只包含了一个变量，那就是$a_i$(求出了$a_i$便能求出$w,b$,然后我们的分类函数$f(x)=w^Tx+b$就非常容易的求出来了)。</p><h4 id="然后求对-a-的极大值："><a href="#然后求对-a-的极大值：" class="headerlink" title="然后求对$a$的极大值："></a>然后求对$a$的极大值：</h4><p>即是关于对偶问题的最优化问题。经过上面第一个步骤的求w和b，得到的拉格朗日函数式子已经没有了变量w，b，只有从上面的式子得到：</p><h2 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h2><h2 id="s-t-a-i-ge0-i-1-cdot-cdot-cdot-n"><a href="#s-t-a-i-ge0-i-1-cdot-cdot-cdot-n" class="headerlink" title="$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$"></a>$s.t. ,a_i\ge0, i=1,\cdot \cdot \cdot n$</h2><h2 id="sum-i-1-na-iy-i-0-1"><a href="#sum-i-1-na-iy-i-0-1" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h2><p>我们一般用SMO算法来求解$a$</p><h3 id="SMO优化算法"><a href="#SMO优化算法" class="headerlink" title="SMO优化算法"></a>SMO优化算法</h3><p>SMO算法由Microsoft Research的John C. Platt在1998年提出，并成为最快的二次规划优化算法，特别针对线性SVM和数据稀疏时性能更优。</p><p>SMO的基本思路是先固定$a_i$之外的所有参数，然后求$a_i$上的极值。由于存在约束$\sum_{i=1}^na_iy_i=0$，若固定$a_i$之外的其他变量，则$a_i$可由其他变量导出。于是，SMO每次选择两个变量$a_i和a_j$,并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛：</p><ul><li><h4 id="选取一对需更新的变量-a-i和-a-j"><a href="#选取一对需更新的变量-a-i和-a-j" class="headerlink" title="选取一对需更新的变量$a_i和 a_j$."></a>选取一对需更新的变量$a_i和 a_j$.</h4></li><li><h4 id="固定-a-i和-a-j-以外的参数，求解-max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-获得更新后的-a-i和-a-j"><a href="#固定-a-i和-a-j-以外的参数，求解-max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-获得更新后的-a-i和-a-j" class="headerlink" title="固定$a_i和 a_j$以外的参数，求解$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$,获得更新后的$a_i和 a_j$"></a>固定$a_i和 a_j$以外的参数，求解$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$,获得更新后的$a_i和 a_j$</h4></li></ul><h4 id="那如何做才能做到不断收敛呢？"><a href="#那如何做才能做到不断收敛呢？" class="headerlink" title="那如何做才能做到不断收敛呢？"></a>那如何做才能做到不断收敛呢？</h4><p>注意只需选取的$a_i和 a_j$ 中有一个不满足KKT的条件，目标函数就会在不断迭代后减小。直观来说<strong>KKT条件的违背的程度越大，则变量更新后可能导致的目标函数值减幅越大</strong>，</p><h4 id="那如何选取变量呢？"><a href="#那如何选取变量呢？" class="headerlink" title="那如何选取变量呢？"></a>那如何选取变量呢？</h4><p>第一个变量SMO 先选取违背KKT条件程度最大的变量。</p><p>第二个变量应该选择一个使目标函数值减小最快的变量。</p><p><strong>但是</strong>由于比较各变量所对应的目标函数值减幅的复杂度过高，因此SMO就采用了一个启发式：<strong>使选取的变量所对应的样本之间的间隔最大</strong>。</p><p><strong>总结</strong>：这样选取的两个变量有很大的差别，与对两个相似的变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。SMO之所以高效，就是在于固定其他参数后，只优化两个参数的过程能做到非常高效。</p><h4 id="所以：只考虑-a-i和-a-j-时，约束条件就改变为："><a href="#所以：只考虑-a-i和-a-j-时，约束条件就改变为：" class="headerlink" title="所以：只考虑$a_i和 a_j$时，约束条件就改变为："></a>所以：只考虑$a_i和 a_j$时，约束条件就改变为：</h4><h4 id="max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-1"><a href="#max-a-sum-i-1-na-i-frac12-sum-i-1-j-1-na-ia-j-x-i-Tx-jy-iy-j-1" class="headerlink" title="$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$"></a>$max_a \sum_{i=1}^na_i-\frac12\sum_{i=1,j=1}^na_ia_j{x_i}^Tx_jy_iy_j$</h4><h4 id="s-t-a-iy-i-a-jy-j-c-a-i-ge-0-a-j-ge-0"><a href="#s-t-a-iy-i-a-jy-j-c-a-i-ge-0-a-j-ge-0" class="headerlink" title="$s.t. ,a_iy_i+a_jy_j=c, a_i \ge 0, a_j \ge 0$"></a>$s.t. ,a_iy_i+a_jy_j=c, a_i \ge 0, a_j \ge 0$</h4><h4 id="sum-i-1-na-iy-i-0-2"><a href="#sum-i-1-na-iy-i-0-2" class="headerlink" title="$\sum_{i=1}^na_iy_i=0$"></a>$\sum_{i=1}^na_iy_i=0$</h4><h4 id="其中：-c-sum-k-ne-i-j-a-ky-k-是使-sum-i-1-na-iy-i-0-成立的常数。"><a href="#其中：-c-sum-k-ne-i-j-a-ky-k-是使-sum-i-1-na-iy-i-0-成立的常数。" class="headerlink" title="其中：$c= -\sum_{k \ne i,j}a_ky_k$是使$\sum_{i=1}^na_iy_i=0$ 成立的常数。"></a>其中：$c= -\sum_{k \ne i,j}a_ky_k$是使$\sum_{i=1}^na_iy_i=0$ 成立的常数。</h4><h4 id="然后用-a-iy-i-a-jy-j-c-消去变量-a-j-，就可以得到一个关于-a-j-的单变量的二次规划问题，通过约束条件-a-i-ge0-就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的-a-i-和-a-j-。"><a href="#然后用-a-iy-i-a-jy-j-c-消去变量-a-j-，就可以得到一个关于-a-j-的单变量的二次规划问题，通过约束条件-a-i-ge0-就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的-a-i-和-a-j-。" class="headerlink" title="然后用$a_iy_i+a_jy_j=c$ 消去变量$a_j$ ，就可以得到一个关于$a_j$ 的单变量的二次规划问题，通过约束条件$a_i \ge0$,就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的$a_i 和 a_j$。"></a>然后用$a_iy_i+a_jy_j=c$ 消去变量$a_j$ ，就可以得到一个关于$a_j$ 的单变量的二次规划问题，通过约束条件$a_i \ge0$,就可以解出这个二次规划问题的闭式解，于是不必调用数值优化算法就可以很快的算出更新后的$a_i 和 a_j$。</h4><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><h4 id="用SMO-解出-a-后，我们前面以前算出-w-的值，所以我们还需要在前面的基础上算出b的值"><a href="#用SMO-解出-a-后，我们前面以前算出-w-的值，所以我们还需要在前面的基础上算出b的值" class="headerlink" title="用SMO 解出$a$后，我们前面以前算出$w$的值，所以我们还需要在前面的基础上算出b的值"></a>用SMO 解出$a$后，我们前面以前算出$w$的值，所以我们还需要在前面的基础上算出b的值</h4><h2 id="w-sum-i-1-na-ix-iy-i-1"><a href="#w-sum-i-1-na-ix-iy-i-1" class="headerlink" title="$w=\sum_{i=1}^na_ix_iy_i$"></a>$w=\sum_{i=1}^na_ix_iy_i$</h2><h2 id="b-y-j-sum-i-1-n-a-iy-i-x-i-Tx-j"><a href="#b-y-j-sum-i-1-n-a-iy-i-x-i-Tx-j" class="headerlink" title="$b=y_j-\sum_{i=1}^n a_iy_i{x_i}^Tx_j$"></a>$b=y_j-\sum_{i=1}^n a_iy_i{x_i}^Tx_j$</h2><h4 id="所以就可以得到模型"><a href="#所以就可以得到模型" class="headerlink" title="所以就可以得到模型"></a>所以就可以得到模型</h4><h2 id="f-x-w-Tx-b-sum-i-1-n-a-iy-i-x-i-Tx-b"><a href="#f-x-w-Tx-b-sum-i-1-n-a-iy-i-x-i-Tx-b" class="headerlink" title="$f(x)=w^Tx+b=\sum_{i=1}^n a_iy_i{x_i}^Tx+b$"></a>$f(x)=w^Tx+b=\sum_{i=1}^n a_iy_i{x_i}^Tx+b$</h2><h3 id="实战一下"><a href="#实战一下" class="headerlink" title="实战一下"></a>实战一下</h3><p><img src="/images/ml/58.png" alt="image"></p><p><img src="/images/ml/53.png" alt="image"></p><p><img src="/images/ml/54.png" alt="image"></p><p><img src="/images/ml/55.png" alt="image"></p><p><img src="/images/ml/56.png" alt="image"></p><p><img src="/images/ml/57.png" alt="image"></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/ml-svm/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> svm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>优化算法-梯度下降</title>
      <link href="/2018/07/24/2018-07-24-arithmetic-gradientDescent/"/>
      <url>/2018/07/24/2018-07-24-arithmetic-gradientDescent/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="优化与机器学习"><a href="#优化与机器学习" class="headerlink" title="优化与机器学习"></a>优化与机器学习</h3><h4 id="机器学习的主要任务之一就是通过训练，学习获得一组最优的参数，我们经常以成本函数来作为参数估计的函数。所以机器学习的任务也就是最小成本函数。"><a href="#机器学习的主要任务之一就是通过训练，学习获得一组最优的参数，我们经常以成本函数来作为参数估计的函数。所以机器学习的任务也就是最小成本函数。" class="headerlink" title="机器学习的主要任务之一就是通过训练，学习获得一组最优的参数，我们经常以成本函数来作为参数估计的函数。所以机器学习的任务也就是最小成本函数。"></a>机器学习的主要任务之一就是通过训练，学习获得一组最优的参数，我们经常以<strong>成本函数</strong>来作为参数估计的函数。所以机器学习的任务也就是最小成本函数。</h4><h4 id="优化也是机器学习算法的非常重要的组成部分，基本上每一个机器学习算法都有一个优化算法"><a href="#优化也是机器学习算法的非常重要的组成部分，基本上每一个机器学习算法都有一个优化算法" class="headerlink" title="优化也是机器学习算法的非常重要的组成部分，基本上每一个机器学习算法都有一个优化算法"></a>优化也是机器学习算法的非常重要的组成部分，基本上每一个机器学习算法都有一个优化算法</h4><h3 id="梯度下降方法"><a href="#梯度下降方法" class="headerlink" title="梯度下降方法"></a>梯度下降方法</h3><h4 id="用负梯度作搜索方向，即令-bigtriangleup-x-bigtriangledown-f-x-是一种自然的选择。相应的方法就称梯度方法或者梯度下降方法。"><a href="#用负梯度作搜索方向，即令-bigtriangleup-x-bigtriangledown-f-x-是一种自然的选择。相应的方法就称梯度方法或者梯度下降方法。" class="headerlink" title="用负梯度作搜索方向，即令$\bigtriangleup x=-\bigtriangledown f(x)$, 是一种自然的选择。相应的方法就称梯度方法或者梯度下降方法。"></a>用负梯度作搜索方向，即令<strong>$\bigtriangleup x=-\bigtriangledown f(x)$</strong>, 是一种自然的选择。相应的方法就称梯度方法或者梯度下降方法。</h4><h3 id="梯度下降算法的概念"><a href="#梯度下降算法的概念" class="headerlink" title="梯度下降算法的概念"></a>梯度下降算法的概念</h3><p><strong>梯度下降算法</strong>就是一个被广泛使用的优化算法, 它可以用于寻找<strong>最小化成本函数</strong>的参数值. 也就是说: <em>当函数 $$J(\theta)$$</em> 取得最小值时, 求所对应的自变量<strong>$\theta$</strong>的过程， 此处<strong>$\theta$</strong>就是机器要学习的参数，$$J(\theta)$$ 就是用于参数估计的成本函数, 是关于$$\theta$$ 的函数.</p><h3 id="梯度下降的基本步骤"><a href="#梯度下降的基本步骤" class="headerlink" title="梯度下降的基本步骤"></a>梯度下降的基本步骤</h3><p>梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）</p><hr><h5 id="给定-初始点-x-in-dom-f"><a href="#给定-初始点-x-in-dom-f" class="headerlink" title="给定 初始点 $x \in dom f $"></a>给定 初始点 $x \in dom f $</h5><h5 id="重复进行："><a href="#重复进行：" class="headerlink" title="重复进行："></a>重复进行：</h5><ol><li><h5 id="bigtriangleup-x-bigtriangledown-f-x"><a href="#bigtriangleup-x-bigtriangledown-f-x" class="headerlink" title="$\bigtriangleup x :=-\bigtriangledown f(x)$"></a>$\bigtriangleup x :=-\bigtriangledown f(x)$</h5></li><li><h5 id="直线搜索。通过精确或回溯直线搜索方法确实步长-t"><a href="#直线搜索。通过精确或回溯直线搜索方法确实步长-t" class="headerlink" title="直线搜索。通过精确或回溯直线搜索方法确实步长$t$."></a>直线搜索。通过精确或回溯直线搜索方法确实步长$t$.</h5></li><li><h5 id="修改-x-x-t-bigtriangleup-x"><a href="#修改-x-x-t-bigtriangleup-x" class="headerlink" title="修改 :$x :=x+t\bigtriangleup x$."></a>修改 :$x :=x+t\bigtriangleup x$.</h5><h4 id="直到：满足停止准则。"><a href="#直到：满足停止准则。" class="headerlink" title="直到：满足停止准则。"></a>直到：满足停止准则。</h4></li></ol><hr><h4 id="换种方式："><a href="#换种方式：" class="headerlink" title="换种方式："></a>换种方式：</h4><ol><li>对成本函数进行微分, 得到其在给定点的梯度. 梯度的正负指示了成本函数值的上升或下降:$ Δ(\theta)=\frac{∂J(\theta)}{∂\theta}$</li><li>选择使成本函数值减小的方向, 即梯度负方向, 乘以学习率为 α 计算得参数的更新量, 并更新参数:<strong>$\theta=\theta−αΔ(\theta) $</strong></li><li>重复以上步骤, 直到取得最小的成本</li></ol><hr><h3 id="批量梯度下降法（Batch-Gradient-Descent）"><a href="#批量梯度下降法（Batch-Gradient-Descent）" class="headerlink" title="批量梯度下降法（Batch Gradient Descent）"></a>批量梯度下降法（Batch Gradient Descent）</h3><p>批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新，这个方法对应于<a href="https://sevenold.github.io/2018/07/ml-linearRegression/" target="_blank" rel="noopener">线性回归的梯度下降算法</a>，也就是说线性回归的梯度下降算法就是批量梯度下降法。</p><h4 id="具体实现过程："><a href="#具体实现过程：" class="headerlink" title="具体实现过程："></a>具体实现过程：</h4><hr><ol><li><h5 id="假设函数：-h-theta-sum-i-1-n-theta-ix-i"><a href="#假设函数：-h-theta-sum-i-1-n-theta-ix-i" class="headerlink" title="假设函数：$h_\theta = \sum_{i=1}^n\theta_ix_i$"></a>假设函数：$h_\theta = \sum_{i=1}^n\theta_ix_i$</h5></li><li><h5 id="成本函数：-J-theta-frac-1-2m-sum-i-1-n-h-theta-x-i-y-i-2"><a href="#成本函数：-J-theta-frac-1-2m-sum-i-1-n-h-theta-x-i-y-i-2" class="headerlink" title="成本函数：$J(\theta)=\frac{1}{2m} \sum_{i=1}^n(h_\theta(x_i)-y_i)^2$"></a>成本函数：$J(\theta)=\frac{1}{2m} \sum_{i=1}^n(h_\theta(x_i)-y_i)^2$</h5></li><li><h5 id="对成本函数进行求偏导：对每一个参数-theta-j-进行分别求偏导，得出各自的梯度。"><a href="#对成本函数进行求偏导：对每一个参数-theta-j-进行分别求偏导，得出各自的梯度。" class="headerlink" title="对成本函数进行求偏导：对每一个参数$\theta_j$进行分别求偏导，得出各自的梯度。"></a>对成本函数进行求偏导：对每一个参数$\theta_j$进行分别求偏导，得出各自的梯度。</h5><h4 id="frac-partial-J-theta-partial-theta-frac1-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i"><a href="#frac-partial-J-theta-partial-theta-frac1-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i" class="headerlink" title="$\frac{\partial J(\theta)}{\partial  \theta}=-\frac1 m  \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$"></a>$\frac{\partial J(\theta)}{\partial \theta}=-\frac1 m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$</h4></li><li><h5 id="每个参数都按照梯度的负方向进行更新："><a href="#每个参数都按照梯度的负方向进行更新：" class="headerlink" title="每个参数都按照梯度的负方向进行更新："></a>每个参数都按照梯度的负方向进行更新：</h5><h4 id="theta-j-theta-j-frac-a-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i"><a href="#theta-j-theta-j-frac-a-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i" class="headerlink" title="$\theta_j=\theta_j+\frac a m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$"></a>$\theta_j=\theta_j+\frac a m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$</h4></li></ol><hr><h4 id="BGD伪代码："><a href="#BGD伪代码：" class="headerlink" title="BGD伪代码："></a>BGD伪代码：</h4><hr><h4 id="repeat"><a href="#repeat" class="headerlink" title="repeat{"></a>repeat{</h4><h4 id="theta-j-theta-j-frac-a-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i-1"><a href="#theta-j-theta-j-frac-a-m-sum-i-1-n-y-i-h-theta-x-i-x-j-i-1" class="headerlink" title="$\theta_j=\theta_j+\frac a m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$"></a>$\theta_j=\theta_j+\frac a m \sum_{i=1}^n(y_i-h_\theta(x_i))x_j^i$</h4><h4 id="for-every-j-0-1-n"><a href="#for-every-j-0-1-n" class="headerlink" title="(for every j = 0, 1, .. n)"></a>(for every j = 0, 1, .. n)</h4><h4><a href="#" class="headerlink" title="}"></a>}</h4><hr><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><p>优点：BGD 得到的是全局最优解, 因为它总是以整个训练集来计算梯度,</p><p>缺点：因此带来了巨大的计算量, 计算迭代速度很很慢.</p><h3 id="随机梯度下降法（Stochastic-Gradient-Descent）"><a href="#随机梯度下降法（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降法（Stochastic Gradient Descent）"></a>随机梯度下降法（Stochastic Gradient Descent）</h3><p>随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。</p><h4 id="具体实现过程：-1"><a href="#具体实现过程：-1" class="headerlink" title="具体实现过程："></a>具体实现过程：</h4><p>SGD 每次以一个样本, 而不是整个数据集来计算梯度. 因此, SGD 从成本函数开始, 就不必再求和了, 针对单个样例的成本函数可以写成:</p><h4 id="J-theta-frac-1-2-h-theta-x-i-y-i-2"><a href="#J-theta-frac-1-2-h-theta-x-i-y-i-2" class="headerlink" title="$J(\theta)=\frac{1}{2} (h_\theta(x_i)-y_i)^2$"></a>$J(\theta)=\frac{1}{2} (h_\theta(x_i)-y_i)^2$</h4><p>于是, SGD 的参数更新规则就可以写成 ：</p><h4 id="theta-j-theta-j-a-y-i-h-theta-x-i-x-j-i"><a href="#theta-j-theta-j-a-y-i-h-theta-x-i-x-j-i" class="headerlink" title="$\theta_j=\theta_j+a (y_i-h_\theta(x_i))x_j^i$"></a>$\theta_j=\theta_j+a (y_i-h_\theta(x_i))x_j^i$</h4><h4 id="SGD伪代码："><a href="#SGD伪代码：" class="headerlink" title="SGD伪代码："></a>SGD伪代码：</h4><hr><h4 id="repeat-1"><a href="#repeat-1" class="headerlink" title="repeat {     "></a>repeat {</h4><h4 id="for-i-1-m"><a href="#for-i-1-m" class="headerlink" title="for i = 1, .., m{    "></a>for i = 1, .., m{</h4><h4 id="theta-j-theta-j-a-y-i-h-theta-x-i-x-j-i-1"><a href="#theta-j-theta-j-a-y-i-h-theta-x-i-x-j-i-1" class="headerlink" title="               $\theta_j=\theta_j+a (y_i-h_\theta(x_i))x_j^i$"></a>      $\theta_j=\theta_j+a (y_i-h_\theta(x_i))x_j^i$</h4><h4 id="for-every-j-0-1-n-1"><a href="#for-every-j-0-1-n-1" class="headerlink" title="                     (for every j = 0, 1, .. n)    "></a>        (for every j = 0, 1, .. n)</h4><h4 id="-1"><a href="#-1" class="headerlink" title="}"></a>}</h4><h4 id="-2"><a href="#-2" class="headerlink" title="}"></a>}</h4><hr><h4 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h4><p>SGD 的关键点在于以随机顺序选取样本. 因为 SGD 存在局部最优困境, 若每次都以相同的顺序选取样本, 其有很大的可能会在相同的地方陷入局部最优解困境, 或者收敛减缓. 因此, 欲使 SGD 发挥更好的效果, 应充分利用<strong>随机化</strong> 带来的优势: 可以<strong>在每次迭代之前 (伪代码中最外围循环), 对训练集进行随机排列.</strong></p><p>缺点：因为每次只取一个样本来进行梯度下降, SGD 的训练<strong>速度很快</strong>, 但会引入噪声, 使准确度下降</p><p>优点：. 可以使用<strong>在线学习</strong>. 也就是说, 在模型训练好之后, 只要有新的数据到来, 模型都可以利用新的数据进行再学习, 更新参数,以适应新的变化.</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p><strong>随机梯度下降法和批量梯度下降法</strong>是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。</p><h4 id="MBGD就综合了这两种方法的优点。"><a href="#MBGD就综合了这两种方法的优点。" class="headerlink" title="MBGD就综合了这两种方法的优点。"></a>MBGD就综合了这两种方法的优点。</h4><h3 id="小批量梯度下降法（Mini-batch-Gradient-Descent）"><a href="#小批量梯度下降法（Mini-batch-Gradient-Descent）" class="headerlink" title="小批量梯度下降法（Mini-batch Gradient Descent）"></a>小批量梯度下降法（Mini-batch Gradient Descent）</h3><p>MBGD 是为解决 BGD 与 SGD 各自缺点而发明的折中算法, 或者说它利用了 BGD 和 SGD 各自优点. 其基本思想是: <em>每次更新参数时, 使用 n 个样本, 既不是全部, 也不是 1.</em> (SGD 可以看成是 n=1 的 MBGD 的一个特例)</p><h4 id="MBGD-的成本函数或其求导公式或参数更新规则公式基本同-BGD-。"><a href="#MBGD-的成本函数或其求导公式或参数更新规则公式基本同-BGD-。" class="headerlink" title="MBGD 的成本函数或其求导公式或参数更新规则公式基本同 BGD 。"></a>MBGD 的成本函数或其求导公式或参数更新规则公式基本同 BGD 。</h4><h4 id="MBGD-的伪代码："><a href="#MBGD-的伪代码：" class="headerlink" title="MBGD 的伪代码："></a>MBGD 的伪代码：</h4><hr><h4 id="say-b-10-m-1000"><a href="#say-b-10-m-1000" class="headerlink" title="say b=10, m=1000,"></a>say b=10, m=1000,</h4><h4 id="repeat-2"><a href="#repeat-2" class="headerlink" title="repeat {     "></a>repeat {</h4><h4 id="for-i-1-11-21-991"><a href="#for-i-1-11-21-991" class="headerlink" title="for i = 1, 11, 21, .., 991 {"></a>for i = 1, 11, 21, .., 991 {</h4><h4 id="theta-j-theta-j-frac-a-10-sum-i-1-i-9-y-i-h-theta-x-i-x-j-i"><a href="#theta-j-theta-j-frac-a-10-sum-i-1-i-9-y-i-h-theta-x-i-x-j-i" class="headerlink" title="$\theta_j=\theta_j+\frac a {10} \sum_{i=1}^{i+9}(y_i-h_\theta(x_i))x_j^i$"></a>$\theta_j=\theta_j+\frac a {10} \sum_{i=1}^{i+9}(y_i-h_\theta(x_i))x_j^i$</h4><h4 id="for-every-j-0-1-n-2"><a href="#for-every-j-0-1-n-2" class="headerlink" title=" (for every j = 0, 1, .. n)    "></a> (for every j = 0, 1, .. n)</h4><h4 id="-3"><a href="#-3" class="headerlink" title=" }"></a> }</h4><h4 id="-4"><a href="#-4" class="headerlink" title="}"></a>}</h4><hr><h3 id="梯度下降算法总结"><a href="#梯度下降算法总结" class="headerlink" title="梯度下降算法总结"></a>梯度下降算法总结</h3><table><thead><tr><th>梯度下降算法</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>BGD</td><td>全局最优解</td><td>计算量大, 迭代速度慢, 训练速度慢</td></tr><tr><td>SGD</td><td>1.训练速度快 ,对于很大的数据集，也能以较快的速度收敛 2. 支持在线学习</td><td>准确度下降, 有噪声, 非全局最优解</td></tr><tr><td>MBGD</td><td>1. 训练速度较快, 取决于小批量的数目 2. 支持在线学习</td><td>准确度不如 BGD, 速度比SGD慢，仍然有噪声, 非全局最优解</td></tr></tbody></table><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/arithmetic-gradientDescent/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 优化算法 </tag>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-决策树和随机森林</title>
      <link href="/2018/07/23/2018-07-23-ml-DecisionTree/"/>
      <url>/2018/07/23/2018-07-23-ml-DecisionTree/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p><strong>决策树（decision tree）</strong>是一种<strong>分类</strong>与<strong>回归</strong>方法，本文主要讨论用于<strong>分类</strong>的决策树，决策树的结构呈<strong>树形</strong>结构，在分类问题中，其代表基于特征对数据进行分类的过程，通常可以认为是if-then规则的集合，也可以认为是定义在<strong>特征空间与类空间上的条件概率分布</strong>。其主要优点是模型可读性好并且分类速度快。训练的时候，利用训练数据根据损失函数最小化的原则建立决策树模型。</p><p>预测时对于新的数据，利用决策树进行分类。决策树的学习通常包括三个步骤：<strong>特征选择，生成决策树，对决策树进行剪枝</strong>。这些决策树的思想主要来自Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及Breiman等人在1984年提出的CART算法。</p><p>用于分类的决策树是一种对数据进行分类的树形结构。决策树主要由节点（node）和有向边（directed edge）组成。节点有两种类型：内部节点（internal node）以及叶节点（leaf node）。内部节点表示一个特征或者属性，叶节点表示一个类。其结构如图所示：</p><p><img src="/images/ml/14.png" alt="image"></p><p>决策树学习采用的是<strong>自顶向下</strong>的递归方法, 其基本思想是以<strong>信息熵</strong>为度量构造一棵<strong>熵值 下降最快</strong>的树,到叶子节点处的熵值为零, 此时每个叶节点中的实例都属于同一类。</p><ul><li>最大优点: 可以自学习。在学习的过程中,不需要使用者了解过多背景知识,只需要对训练实例进行较好的标注,就能够进行学习。</li><li>显然,属于<strong>有监督学习</strong>。</li></ul><h3 id="决策树三种生成算法"><a href="#决策树三种生成算法" class="headerlink" title="决策树三种生成算法"></a>决策树三种生成算法</h3><ol><li><h4 id="ID3-—-信息增益-最大的准则"><a href="#ID3-—-信息增益-最大的准则" class="headerlink" title="ID3 — 信息增益 最大的准则"></a>ID3 — <strong>信息增益</strong> <strong>最大</strong>的准则</h4><p>​ ID3算法的核心是在决策树各个节点上使用<strong>信息增益</strong>作为特征选择的依据，递归的构建决策树。 从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归的调用以上方法，构建决策树；知道所有特征的信息增益均很小或没有特征可以选择为止，得到最后一个决策树。ID3相当于用最大似然法进行概率模型的选择。</p></li><li><h4 id="C4-5-—-信息增益比-最大的准则"><a href="#C4-5-—-信息增益比-最大的准则" class="headerlink" title="C4.5 — 信息增益比 最大的准则"></a>C4.5 — <strong>信息增益比</strong> 最大的准则</h4><p>​ C4.5算法使用<strong>信息增益率</strong>作为特征选择的依据，算法的流程与ID3相似。</p></li><li><p>CART</p><ul><li>回归树: <strong>平方误差</strong> <strong>最小</strong> 的准则</li><li>分类树: <strong>基尼系数</strong> <strong>最小</strong>的准则</li></ul><p>CART树的名字其实非常有意思，Classification And Regression Tree（分类回归树），它使用基尼系数(Gini Index)作为特征的划分依据。顾名思义，CART既可以用来做分类也可以用来做回归。它是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。 CART树假设我么的决策树是二叉树，内部节点特征的取值为是或否。</p><p><strong>CART算法</strong>为：<br>​ 1. 决策树的生成：基于训练数据集生成决策树，生成的决策树要尽量大；</p><p>​ 2.决策树剪枝：用验证数据集对已经生成的巨额额数进行剪枝并选择最优子树。</p></li></ol><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><h4 id="信息熵（information-entropy）-是度量样本集合纯度的最常用的一种指标。"><a href="#信息熵（information-entropy）-是度量样本集合纯度的最常用的一种指标。" class="headerlink" title="信息熵（information entropy） 是度量样本集合纯度的最常用的一种指标。"></a><strong>信息熵（information entropy）</strong> 是度量样本集合纯度的最常用的一种指标。</h4><ul><li><h4 id="熵：-H-X-sum-x-in-X-p-x-y-logp-x"><a href="#熵：-H-X-sum-x-in-X-p-x-y-logp-x" class="headerlink" title="熵：$H(X)=-\sum_{x \in X} p(x,y)logp(x)$"></a>熵：$H(X)=-\sum_{x \in X} p(x,y)logp(x)$</h4></li><li><h4 id="联合熵：-H-X，Y-sum-x-in-X-y-in-Y-p-x-y-logp-x-y"><a href="#联合熵：-H-X，Y-sum-x-in-X-y-in-Y-p-x-y-logp-x-y" class="headerlink" title="联合熵：$H(X，Y)=-\sum_{x \in X,y \in Y} p(x,y)logp(x,y)$"></a>联合熵：$H(X，Y)=-\sum_{x \in X,y \in Y} p(x,y)logp(x,y)$</h4></li><li><h4 id="条件熵：-H-X-Y-sum-x-in-X-y-in-Y-p-x-y-logp-x-y"><a href="#条件熵：-H-X-Y-sum-x-in-X-y-in-Y-p-x-y-logp-x-y" class="headerlink" title="条件熵：$H(X|Y)=-\sum_{x \in X,y \in Y} p(x,y)logp(x|y)$"></a>条件熵：$H(X|Y)=-\sum_{x \in X,y \in Y} p(x,y)logp(x|y)$</h4></li><li><h4 id="相对熵：-D-p-q-sum-x-p-x-log-frac-p-x-q-x"><a href="#相对熵：-D-p-q-sum-x-p-x-log-frac-p-x-q-x" class="headerlink" title="相对熵：$D(p||q)=\sum_x p(x)log\frac{p(x)}{q(x)}$"></a>相对熵：$D(p||q)=\sum_x p(x)log\frac{p(x)}{q(x)}$</h4></li><li><h4 id="互信息：-I-x-y-sum-x-in-X-y-in-Y-p-x-y-log-frac-p-x-y-p-x-p-y"><a href="#互信息：-I-x-y-sum-x-in-X-y-in-Y-p-x-y-log-frac-p-x-y-p-x-p-y" class="headerlink" title="互信息：$I(x,y)=\sum_{x\in X, y \in Y} p(x,y )log\frac{p(x,y)}{p(x)p(y)}$"></a>互信息：$I(x,y)=\sum_{x\in X, y \in Y} p(x,y )log\frac{p(x,y)}{p(x)p(y)}$</h4></li></ul><h3 id="决策树学习基本算法"><a href="#决策树学习基本算法" class="headerlink" title="决策树学习基本算法"></a>决策树学习基本算法</h3><p><img src="/images/ml/15.png" alt="image"></p><h3 id="信息增益—-g-D-A"><a href="#信息增益—-g-D-A" class="headerlink" title="信息增益—$g(D,A)$"></a>信息增益—$g(D,A)$</h3><p><strong>信息增益(Information Gain)</strong>：表示得知特征A的信息而使得类X的信息的不确定性减少的程度。<br>定义：特征A对训练数据集D的<strong>信息增益</strong>g(D, A)，定义为集合D的<strong>经验熵</strong>H(D)与<strong>经验条件熵</strong>H(D|A)的差值。</p><h2 id="g-D-A-H-D-−H-D-A"><a href="#g-D-A-H-D-−H-D-A" class="headerlink" title="$g(D,A)=H(D)−H(D|A)$"></a>$g(D,A)=H(D)−H(D|A)$</h2><p>而这又是互信息的定义。所以<strong>决策树中的信息增益等价于训练集中类与特征的互信息。</strong></p><h4 id="总结：一般而言，信息增益g-D-A-越大，就意味着使用特征A来进行划分所获得的“纯度提升”就越大。著名的ID3决策树学习算法就是以信息增益为准则来学习划分属性的。"><a href="#总结：一般而言，信息增益g-D-A-越大，就意味着使用特征A来进行划分所获得的“纯度提升”就越大。著名的ID3决策树学习算法就是以信息增益为准则来学习划分属性的。" class="headerlink" title="总结：一般而言，信息增益g(D, A)越大，就意味着使用特征A来进行划分所获得的“纯度提升”就越大。著名的ID3决策树学习算法就是以信息增益为准则来学习划分属性的。"></a>总结：一般而言，<strong>信息增益</strong>g(D, A)越大，就意味着使用特征A来进行划分所获得的“纯度提升”就越大。著名的ID3决策树学习算法就是以信息增益为准则来学习划分属性的。</h4><h3 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h3><p><strong>信息率(Information Gain Ratio)</strong>： 用信息增益作为划分特征的依据时，会存在一些问题。例如，如果使用数据的ID作为特征，这样，每个数据点相当于均匀分布，所以得到的信息增益一定是最大的，但是我们都知道ID是不能作为特征的。这样如果单纯的使用信息增益作为划分特征的依据的话，会导致我们生成的树比较矮胖，容易<strong>过拟合</strong>。</p><p>定义：特征A对训练数据集D的<strong>信息增益率</strong>$$gR(D,A)$$定义为其<strong>信息增益</strong>$$g(D,A)$$与训练数据集D关于特征A的值得<strong>信息熵</strong>$$IV(D)$$之比：</p><h2 id="gR-D-A-frac-g-D-A-IV-D"><a href="#gR-D-A-frac-g-D-A-IV-D" class="headerlink" title="$ gR(D,A)=\frac {g(D,A)}{IV(D)} $"></a>$ gR(D,A)=\frac {g(D,A)}{IV(D)} $</h2><h4 id="总结：信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4-5决策树算法就不直接使用信息增益而是使用“增益率”-。"><a href="#总结：信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4-5决策树算法就不直接使用信息增益而是使用“增益率”-。" class="headerlink" title="总结：信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法就不直接使用信息增益而是使用“增益率” 。"></a>总结：信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法就不直接使用信息增益而是使用“增益率” 。</h4><h3 id="Gini系数："><a href="#Gini系数：" class="headerlink" title="Gini系数："></a>Gini系数：</h3><h4 id="数据集P的纯度可以用Gini值来度量。"><a href="#数据集P的纯度可以用Gini值来度量。" class="headerlink" title="数据集P的纯度可以用Gini值来度量。"></a>数据集P的纯度可以用Gini值来度量。</h4><h2 id="Gini-P-sum-k-1-K-p-k-1-p-k-1-sum-k-1-Kp-k-2"><a href="#Gini-P-sum-k-1-K-p-k-1-p-k-1-sum-k-1-Kp-k-2" class="headerlink" title="$Gini(P)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$"></a>$Gini(P)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$</h2><h4 id="总结：CART决策树使用Gini指数来选择划分属性。-Gini-P-反映了从数据集D中随机抽取了两个样本，其类别标记不一致的概率，因此，-Gini（D）-越小，则数据集P的纯度就越高。"><a href="#总结：CART决策树使用Gini指数来选择划分属性。-Gini-P-反映了从数据集D中随机抽取了两个样本，其类别标记不一致的概率，因此，-Gini（D）-越小，则数据集P的纯度就越高。" class="headerlink" title="总结：CART决策树使用Gini指数来选择划分属性。$Gini(P)$反映了从数据集D中随机抽取了两个样本，其类别标记不一致的概率，因此，$Gini（D）$越小，则数据集P的纯度就越高。"></a>总结：CART决策树使用Gini指数来选择划分属性。$Gini(P)$反映了从数据集D中随机抽取了两个样本，其类别标记不一致的概率，因此，$Gini（D）$越小，则数据集P的纯度就越高。</h4><h3 id="决策树的评价-——-loss-function："><a href="#决策树的评价-——-loss-function：" class="headerlink" title="决策树的评价 —— loss function："></a>决策树的评价 —— loss function：</h3><p>假定样本的总类别数为$K$个；树T的叶节点个数为$\mid T \mid$，t是树T的叶节点，叶节点有$N_t$个样本点，其中$k$类的样本点有$N_{ik}$个，$H_t(T)$为叶节点t上的<strong>经验熵</strong>，则决策树的loss function可以定义为：</p><h2 id="C-a-T-sum-t-in-Leaf-N-tH-t-T-a-T"><a href="#C-a-T-sum-t-in-Leaf-N-tH-t-T-a-T" class="headerlink" title="$C_a(T)=\sum_{t\in Leaf}N_tH_t(T)+a|T|$"></a>$C_a(T)=\sum_{t\in Leaf}N_tH_t(T)+a|T|$</h2><h3 id="决策树种避免过拟合的方法——剪枝"><a href="#决策树种避免过拟合的方法——剪枝" class="headerlink" title="决策树种避免过拟合的方法——剪枝"></a>决策树种避免过拟合的方法——剪枝</h3><ul><li><strong>预剪枝</strong>：在决策树生成过程中，对每个结点在划分前先进行估计，如果当前的结点划分不能带来决策树泛化性能提升，则停止划分并将当前的结点标记为叶节点。</li><li><strong>后剪枝</strong>：先从训练集生成一个完整的决策树，然后自底向上的对非叶节点进行考察，若将该结点对应的子树替换为叶节点能带来决策树泛化能力性能提升，则就把该子树替换为叶结点。</li></ul><h4 id="在上面决策树的评价指标loss-function-：-C-a-T-sum-t-in-Leaf-N-tH-t-T-a-T-中，"><a href="#在上面决策树的评价指标loss-function-：-C-a-T-sum-t-in-Leaf-N-tH-t-T-a-T-中，" class="headerlink" title="在上面决策树的评价指标loss function ：$C_a(T)=\sum_{t\in Leaf}N_tH_t(T)+a|T|$中，"></a>在上面决策树的评价指标loss function ：$C_a(T)=\sum_{t\in Leaf}N_tH_t(T)+a|T|$中，</h4><ul><li><p>C(T)表示模型对训练数据集的预测误差，即模型与训练数据的拟合程度，</p></li><li><p>$\mid T \mid$表示模型复杂度，由参数α控制两者之间的影响。</p></li></ul><h4 id="当α确定时："><a href="#当α确定时：" class="headerlink" title="当α确定时："></a>当α确定时：</h4><ul><li>子树越大，与训练集的拟合越好，但模型的复杂度就越高；</li><li>子树越小，模型简单，但与训练集的拟合度不好。</li></ul><h4 id="决策树生成学习局部的模型，而剪枝学习整体的模型"><a href="#决策树生成学习局部的模型，而剪枝学习整体的模型" class="headerlink" title="决策树生成学习局部的模型，而剪枝学习整体的模型!"></a>决策树生成学习局部的模型，而剪枝学习整体的模型!</h4><h4 id="剪枝的过程为："><a href="#剪枝的过程为：" class="headerlink" title="剪枝的过程为："></a>剪枝的过程为：</h4><ul><li>计算每个节点的经验熵；</li><li>递归的从树的叶节点向上回缩，设一组叶节点回缩到其父节点之前和之后的整体树分别为$T_B$和$T_A$，对应的损失函数值分别为$C_α(T_B)$和$C_α(T_A),$如果$C_α(T_A)&lt;=C_α(T_B)$，则进行剪枝。</li></ul><h3 id="决策树的优缺点"><a href="#决策树的优缺点" class="headerlink" title="决策树的优缺点"></a>决策树的优缺点</h3><ul><li>优点: 决策树对训练属于有很好的分类能力</li><li>缺点: 但对未知的测试数据未必有好的分类能力,泛化 能力弱,即可能发生过拟合现象。</li></ul><h3 id="Bagging（套袋法）"><a href="#Bagging（套袋法）" class="headerlink" title="Bagging（套袋法）"></a>Bagging（套袋法）</h3><p>bagging的算法过程如下：</p><ol><li>从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复）</li><li>对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等）</li><li>对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同）</li></ol><h3 id="Boosting（提升法）"><a href="#Boosting（提升法）" class="headerlink" title="Boosting（提升法）"></a>Boosting（提升法）</h3><p>boosting的算法过程如下：</p><ol><li>对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。</li><li>进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大）</li></ol><h3 id="Bagging，Boosting的主要区别"><a href="#Bagging，Boosting的主要区别" class="headerlink" title="Bagging，Boosting的主要区别"></a>Bagging，Boosting的主要区别</h3><ol><li><p>样本选择上：</p><p>Bagging采用的是Bootstrap随机有放回抽样；</p><p>Boosting每一轮的训练集是不变的，改变的只是每一个样本的权重。</p></li><li><p>样本权重：</p><p>Bagging使用的是均匀取样，每个样本权重相等；</p><p>Boosting根据错误率调整样本权重，错误率越大的样本权重越大。</p></li><li><p>预测函数：</p><p>Bagging所有的预测函数的权重相等；</p><p>Boosting中误差越小的预测函数其权重越大。</p></li><li><p>并行计算：</p><p>Bagging各个预测函数可以并行生成；</p><p>Boosting各个预测函数必须按顺序迭代生成。</p></li></ol><h3 id="决策树与这些算法框架进行结合所得到的新的算法："><a href="#决策树与这些算法框架进行结合所得到的新的算法：" class="headerlink" title="决策树与这些算法框架进行结合所得到的新的算法："></a>决策树与这些算法框架进行结合所得到的新的算法：</h3><p>1）Bagging + 决策树 = 随机森林</p><p>2）AdaBoost + 决策树 = 提升树</p><p>3）Gradient Boosting + 决策树 = GBDT</p><h3 id="随机森林（Random-Forests）"><a href="#随机森林（Random-Forests）" class="headerlink" title="随机森林（Random Forests）"></a>随机森林（Random Forests）</h3><p>随机森林是一种重要的基于Bagging的集成学习方法，可以用来做分类、回归等问题。</p><h3 id="随机森林有许多优点："><a href="#随机森林有许多优点：" class="headerlink" title="随机森林有许多优点："></a>随机森林有许多优点：</h3><ul><li>具有极高的准确率</li><li>随机性的引入，使得随机森林不容易过拟合</li><li>随机性的引入，使得随机森林有很好的抗噪声能力</li><li>能处理很高维度的数据，并且不用做特征选择</li><li>既能处理离散型数据，也能处理连续型数据，数据集无需规范化</li><li>训练速度快，可以得到变量重要性排序</li><li>容易实现并行化</li></ul><h3 id="随机森林的缺点："><a href="#随机森林的缺点：" class="headerlink" title="随机森林的缺点："></a>随机森林的缺点：</h3><ul><li>当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大</li><li>随机森林模型还有许多不好解释的地方，有点算个黑盒模型</li></ul><h3 id="随机森林构建过程"><a href="#随机森林构建过程" class="headerlink" title="随机森林构建过程"></a>随机森林构建过程</h3><h4 id="与上面介绍的Bagging过程相似，随机森林的构建过程大致如下："><a href="#与上面介绍的Bagging过程相似，随机森林的构建过程大致如下：" class="headerlink" title="与上面介绍的Bagging过程相似，随机森林的构建过程大致如下："></a>与上面介绍的Bagging过程相似，随机森林的构建过程大致如下：</h4><ol><li>从原始训练集中使用Bootstraping方法随机有放回采样选出m个样本，共进行n_tree次采样，生成n_tree个训练集</li><li>对于n_tree个训练集，我们分别训练n_tree个决策树模型</li><li>对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂</li><li>每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝</li><li>将生成的多棵决策树组成随机森林。对于分类问题，按多棵树分类器投票决定最终分类结果；对于回归问题，由多棵树预测值的均值决定最终预测结果</li></ol><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/ml-DecisionTree/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> 决策树 </tag>
            
            <tag> 随机森林 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-逻辑回归推导</title>
      <link href="/2018/07/23/2018-07-23-ml-logisticRegression/"/>
      <url>/2018/07/23/2018-07-23-ml-logisticRegression/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="逻辑回归的概念"><a href="#逻辑回归的概念" class="headerlink" title="逻辑回归的概念"></a>逻辑回归的概念</h3><p><strong>Logistic Regression</strong> 在《机器学习》-周志华一书中又叫<strong>对数几率回归</strong>。逻辑回归和多重线性回归实际上有很多的相同之处，除了它们的因变量（函数）不同外，其他的基本差不多，所以逻辑回归和线性回归又统属于<strong>广义线性模型</strong>（generalizedlinear model）。</p><p>广义线性模型的形式其实都差不多，不同的就是因变量（函数）的不同。</p><ul><li>如果是<strong>连续</strong>的，就是<strong>多重线性回归</strong></li><li>如果是<strong>二项分布</strong>，就是<strong>Logistic回归</strong></li><li>如果是<strong>Poisson分布</strong>，就是<strong>Poisson分布</strong></li><li>如果是<strong>负二项分布</strong>，就是<strong>负二项回归</strong></li></ul><p>Logistic回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的Logistic回归。</p><h3 id="线性回归-Logistic回归"><a href="#线性回归-Logistic回归" class="headerlink" title="线性回归-Logistic回归"></a>线性回归-Logistic回归</h3><p><img src="/images/ml/7.png" alt="image"></p><h3 id="Logistic回归的主要用途："><a href="#Logistic回归的主要用途：" class="headerlink" title="Logistic回归的主要用途："></a>Logistic回归的<strong>主要用途</strong>：</h3><ul><li>寻找危险因素：寻找某一疾病的危险因素等；</li><li>预测：根据模型，预测在不同的自变量情况下，发生某病或某种情况的概率有多大；</li><li>判别：实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。</li></ul><p>Logistic回归主要在<strong>流行病学</strong>中应用较多，比较常用的情形是探索某疾病的危险因素，根据危险因素预测某疾病发生的概率，等等。例如，想探讨胃癌发生的危险因素，可以选择两组人群，一组是胃癌组，一组是非胃癌组，两组人群肯定有不同的体征和生活方式等。这里的因变量就是是否胃癌，即“是”或“否”，自变量就可以包括很多了，例如年龄、性别、饮食习惯、幽门螺杆菌感染等。自变量既可以是连续的，也可以是分类的。</p><h3 id="常规步骤"><a href="#常规步骤" class="headerlink" title="常规步骤"></a><strong>常规步骤</strong></h3><p>Regression问题的常规步骤为：</p><ol><li>寻找<strong>h</strong>函数（即hypothesis）；</li><li>构造<strong>J</strong>函数（损失函数）；</li><li>想办法使得J<strong>函数最小</strong>并求得回归参数（θ）</li></ol><h3 id="构造预测函数（hypothesis）"><a href="#构造预测函数（hypothesis）" class="headerlink" title="构造预测函数（hypothesis）"></a>构造预测函数（hypothesis）</h3><p>Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别），所以利用了Logistic函数（或称为<strong>Sigmoid函数</strong>），函数形式为：</p><h2 id="g-z-frac-1-1-e-z"><a href="#g-z-frac-1-1-e-z" class="headerlink" title="$g(z)=\frac{1}{1+e^{-z}}$"></a>$g(z)=\frac{1}{1+e^{-z}}$</h2><p>为了方便后面使用我们求出$g(z)$的导数：</p><h2 id="g-‘-z-frac1-1-e-x-cdot-1-frac1-1-e-x-g-z-1-g-z"><a href="#g-‘-z-frac1-1-e-x-cdot-1-frac1-1-e-x-g-z-1-g-z" class="headerlink" title="$g^{‘}(z)=\frac1{1-e^{-x} } \cdot (1- \frac1{1-e^{-x} })=g(z)(1-g(z))$"></a>$g^{‘}(z)=\frac1{1-e^{-x} } \cdot (1- \frac1{1-e^{-x} })=g(z)(1-g(z))$</h2><p>Sigmoid 函数在有个很漂亮的“S”形，如下图所示</p><p><img src="/images/ml/6.png" alt="image"></p><h3 id="构建预测函数"><a href="#构建预测函数" class="headerlink" title="构建预测函数"></a>构建预测函数</h3><p><strong>决策边界</strong>又分为<strong>线性的决策边界</strong>和<strong>非线性的决策边界</strong></p><p>线性的决策边界：</p><p><img src="/images/ml/8.png" alt="1"></p><p>非线性的决策边界：</p><p><img src="/images/ml/9.png" alt="2"></p><p>对于线性边界的情况，边界形式如下：</p><h2 id="theta-0-theta-1x-1-cdot-cdot-cdot-theta-nx-n-sum-i-1-n-theta-ix-i-theta-Tx"><a href="#theta-0-theta-1x-1-cdot-cdot-cdot-theta-nx-n-sum-i-1-n-theta-ix-i-theta-Tx" class="headerlink" title="$\theta_0+\theta_1x_1+\cdot \cdot \cdot + \theta_nx_n=\sum_{i=1}^n \theta_ix_i = \theta^Tx$"></a>$\theta_0+\theta_1x_1+\cdot \cdot \cdot + \theta_nx_n=\sum_{i=1}^n \theta_ix_i = \theta^Tx$</h2><p>构建的预测函数为：</p><h2 id="h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx"><a href="#h-theta-x-g-theta-Tx-frac1-1-e-theta-Tx" class="headerlink" title="$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$"></a>$h_\theta(x)=g(\theta^Tx)=\frac1{1+e^{-\theta^Tx}}$</h2><h3 id="构建损失函数"><a href="#构建损失函数" class="headerlink" title="构建损失函数"></a>构建损失函数</h3><p>由于是二项分布，函数<strong>$h_ \theta(x)$</strong>的值就有特殊的含义，它所表示的是结果取1的概率，因此对于输入x的分类结果就判别为类别1和类别0的概率分别为：</p><h2 id="P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x"><a href="#P-y-1-x-theta-h-theta-x-P-y-0-x-theta-1-h-theta-x" class="headerlink" title="$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$"></a>$P(y=1|x;\theta)=h_\theta(x) \\ P(y=0|x;\theta)=1-h_\theta(x)$</h2><p>所以：</p><h2 id="P-y-x-theta-h-theta-x-y-1-h-theta-x-1-y"><a href="#P-y-x-theta-h-theta-x-y-1-h-theta-x-1-y" class="headerlink" title="$P(y|x;\theta) = {h_\theta(x) }^y{(1-h_\theta(x)) }^{1-y}$"></a>$P(y|x;\theta) = {h_\theta(x) }^y{(1-h_\theta(x)) }^{1-y}$</h2><h3 id="构建似然函数"><a href="#构建似然函数" class="headerlink" title="构建似然函数"></a>构建似然函数</h3><h2 id="L-theta-prod-i-1-n-P-y-i-x-i-theta-prod-i-1-n-h-theta-x-i-y-i-1-h-theta-x-i-1-y-i"><a href="#L-theta-prod-i-1-n-P-y-i-x-i-theta-prod-i-1-n-h-theta-x-i-y-i-1-h-theta-x-i-1-y-i" class="headerlink" title="$L(\theta)=\prod_{i=1}^n P(y_i|x_i;\theta) =\prod_{i=1}^n {h_\theta(x_i) }^{y_i}{(1-h_\theta(x_i)) }^{1-y_i}$"></a>$L(\theta)=\prod_{i=1}^n P(y_i|x_i;\theta) =\prod_{i=1}^n {h_\theta(x_i) }^{y_i}{(1-h_\theta(x_i)) }^{1-y_i}$</h2><h3 id="对数似然函数"><a href="#对数似然函数" class="headerlink" title="对数似然函数"></a>对数似然函数</h3><h2 id="l-theta-logL-theta-sum-i-1-n-y-ilogh-theta-x-i-1-y-i-log-1-h-theta-x-i"><a href="#l-theta-logL-theta-sum-i-1-n-y-ilogh-theta-x-i-1-y-i-log-1-h-theta-x-i" class="headerlink" title="$l(\theta)=logL(\theta)=\sum_{i=1}^n(y_ilogh_\theta(x_i)+(1-y_i)log(1-h_\theta(x_i)))$"></a>$l(\theta)=logL(\theta)=\sum_{i=1}^n(y_ilogh_\theta(x_i)+(1-y_i)log(1-h_\theta(x_i)))$</h2><p>最大似然估计就是求使<strong>$l(\theta)$</strong>取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。</p><h3 id="梯度下降法求的最小值"><a href="#梯度下降法求的最小值" class="headerlink" title="梯度下降法求的最小值"></a><strong>梯度下降法求的最小值</strong></h3><h4 id="θ更新过程："><a href="#θ更新过程：" class="headerlink" title="θ更新过程："></a>θ更新过程：</h4><h2 id="theta-j-theta-j-a-frac-partial-l-theta-partial-theta-j"><a href="#theta-j-theta-j-a-frac-partial-l-theta-partial-theta-j" class="headerlink" title="$\theta_j :=\theta_j-a(\frac{\partial l(\theta)}{\partial \theta_j})$"></a><strong><em>$\theta_j :=\theta_j-a(\frac{\partial l(\theta)}{\partial \theta_j})$</em></strong></h2><h4 id="对-theta-求偏导"><a href="#对-theta-求偏导" class="headerlink" title="对$\theta$求偏导"></a>对$\theta$求偏导</h4><h2 id="frac-partial-l-theta-partial-theta-j-frac-partial-g-theta-Tx-partial-theta-j-frac-y-g-theta-Tx-frac-1-y-g-theta-Tx"><a href="#frac-partial-l-theta-partial-theta-j-frac-partial-g-theta-Tx-partial-theta-j-frac-y-g-theta-Tx-frac-1-y-g-theta-Tx" class="headerlink" title="$\frac{\partial l(\theta)}{\partial \theta_j}=\frac{\partial g(\theta^Tx)}{\partial \theta_j}(\frac{y}{g(\theta^Tx)}-\frac{1-y}{g(\theta^Tx)})$"></a>$\frac{\partial l(\theta)}{\partial \theta_j}=\frac{\partial g(\theta^Tx)}{\partial \theta_j}(\frac{y}{g(\theta^Tx)}-\frac{1-y}{g(\theta^Tx)})$</h2><h2 id="g-theta-Tx-1-g-theta-Tx-frac-partial-theta-Tx-partial-theta-j-frac-y-g-theta-Tx-frac-1-y-g-theta-Tx"><a href="#g-theta-Tx-1-g-theta-Tx-frac-partial-theta-Tx-partial-theta-j-frac-y-g-theta-Tx-frac-1-y-g-theta-Tx" class="headerlink" title="$=g(\theta^Tx)(1-g(\theta^Tx)) \frac{\partial\theta^Tx}{\partial \theta_j}(\frac{y}{g(\theta^Tx)}-\frac{1-y}{g(\theta^Tx)})$"></a>$=g(\theta^Tx)(1-g(\theta^Tx)) \frac{\partial\theta^Tx}{\partial \theta_j}(\frac{y}{g(\theta^Tx)}-\frac{1-y}{g(\theta^Tx)})$</h2><h2 id="y-1-g-theta-Tx-1-y-g-theta-Tx-x-j"><a href="#y-1-g-theta-Tx-1-y-g-theta-Tx-x-j" class="headerlink" title="$=(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j$"></a>$=(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j$</h2><h2 id="y-h-theta-x-x-j"><a href="#y-h-theta-x-x-j" class="headerlink" title="$=(y-h_\theta(x))x_j$"></a>$=(y-h_\theta(x))x_j$</h2><h4 id="θ更新过程就可以写为："><a href="#θ更新过程就可以写为：" class="headerlink" title="θ更新过程就可以写为："></a>θ更新过程就可以写为：</h4><h2 id="theta-j-theta-j-a-sum-i-1-n-y-i-h-theta-x-i-x-i-j"><a href="#theta-j-theta-j-a-sum-i-1-n-y-i-h-theta-x-i-x-i-j" class="headerlink" title="$\theta_j :=\theta_j-a\sum_{i=1}^n (y_i-h_\theta(x_i))x_i^j$"></a>$\theta_j :=\theta_j-a\sum_{i=1}^n (y_i-h_\theta(x_i))x_i^j$</h2><h4 id="但是在在Andrew-Ng的课程中将-J-theta-取为下式，即："><a href="#但是在在Andrew-Ng的课程中将-J-theta-取为下式，即：" class="headerlink" title="但是在在Andrew Ng的课程中将 $J(\theta)$取为下式，即："></a>但是在在Andrew Ng的课程中将 $J(\theta)$取为下式，即：</h4><h2 id="J-theta-frac-1-m-l-theta"><a href="#J-theta-frac-1-m-l-theta" class="headerlink" title="$J(\theta)=-\frac{1}{m}l(\theta)$"></a>$J(\theta)=-\frac{1}{m}l(\theta)$</h2><h4 id="因为乘了一个负的系数-1-m，所以取-J-theta-最小值时的θ为要求的最佳参数。"><a href="#因为乘了一个负的系数-1-m，所以取-J-theta-最小值时的θ为要求的最佳参数。" class="headerlink" title="因为乘了一个负的系数-1/m，所以取 $J(\theta)$最小值时的θ为要求的最佳参数。"></a>因为乘了一个负的系数-1/m，所以取 $J(\theta)$最小值时的θ为要求的最佳参数。</h4><h2 id="frac-partial-l-theta-partial-theta-j-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j"><a href="#frac-partial-l-theta-partial-theta-j-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j" class="headerlink" title="$\frac{\partial l(\theta)}{\partial \theta_j}=\frac 1 m \sum_{i=1}^n(h_\theta(x_i)-y_i)x_i^j$"></a>$\frac{\partial l(\theta)}{\partial \theta_j}=\frac 1 m \sum_{i=1}^n(h_\theta(x_i)-y_i)x_i^j$</h2><h4 id="相应的-theta"><a href="#相应的-theta" class="headerlink" title="相应的$\theta$:"></a>相应的$\theta$:</h4><h2 id="theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j"><a href="#theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j" class="headerlink" title="$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j$"></a>$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j$</h2><h3 id="向量化（Vectorization-）"><a href="#向量化（Vectorization-）" class="headerlink" title="向量化（Vectorization ）"></a>向量化（<strong>Vectorization</strong> ）</h3><p>Vectorization是使用矩阵计算来代替for循环，以简化计算过程，提高效率。</p><p>如上式，Σ(…)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization。</p><h4 id="下面介绍向量化的过程："><a href="#下面介绍向量化的过程：" class="headerlink" title="下面介绍向量化的过程："></a>下面介绍向量化的过程：</h4><p>约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值：</p><p><img src="/images/ml/10.png" alt="image"></p><h5 id="g-A-的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知-h-theta-x-y-可以由-g-A-y-一次求得"><a href="#g-A-的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知-h-theta-x-y-可以由-g-A-y-一次求得" class="headerlink" title="g(A)的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知 $h_\theta(x)-y$可以由$g(A)-y$一次求得"></a>g(A)的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知 $h_\theta(x)-y$可以由$g(A)-y$一次求得</h5><h4 id="θ更新过程可以改为"><a href="#θ更新过程可以改为" class="headerlink" title="θ更新过程可以改为"></a>θ更新过程可以改为</h4><h2 id="theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j-theta-j-a-frac-1-m-sum-i-1-n-e-ix-i-j-theta-j-a-frac1-m-x-TE"><a href="#theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j-theta-j-a-frac-1-m-sum-i-1-n-e-ix-i-j-theta-j-a-frac1-m-x-TE" class="headerlink" title="$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j=\theta_j-a \frac 1 m \sum_{i=1}^n e_ix_i^j=\theta_j-a \frac1 m x^TE$"></a>$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j=\theta_j-a \frac 1 m \sum_{i=1}^n e_ix_i^j=\theta_j-a \frac1 m x^TE$</h2><h4 id="综上所述，Vectorization后θ更新的步骤如下："><a href="#综上所述，Vectorization后θ更新的步骤如下：" class="headerlink" title="综上所述，Vectorization后θ更新的步骤如下："></a>综上所述，Vectorization后θ更新的步骤如下：</h4><hr><ol><li><h4 id="求-A-x-cdot-theta"><a href="#求-A-x-cdot-theta" class="headerlink" title="求:$A=x \cdot \theta$"></a>求:$A=x \cdot \theta$</h4></li><li><h4 id="求-E-g-A-y"><a href="#求-E-g-A-y" class="headerlink" title="求$E=g(A)-y$"></a>求$E=g(A)-y$</h4></li><li><h4 id="求-theta-theta-ax-TE"><a href="#求-theta-theta-ax-TE" class="headerlink" title="求$\theta:=\theta-ax^TE$"></a>求$\theta:=\theta-ax^TE$</h4></li></ol><hr><h3 id="正则化Regularization"><a href="#正则化Regularization" class="headerlink" title="正则化Regularization"></a><strong>正则化Regularization</strong></h3><p>同样逻辑回归也有<strong>欠拟合、适合拟合、过拟合问题</strong></p><p>对于线性回归或逻辑回归的损失函数构成的模型，可能会有些权重很大，有些权重很小，导致过拟合（就是过分拟合了训练数据），使得模型的复杂度提高，泛化能力较差（对未知数据的预测能力）。</p><p>下面左图即为欠拟合，中图为合适的拟合，右图为过拟合。</p><p>​ <img src="/images/ml/11.png" alt="image"></p><p>过拟合问题往往源自过多的特征。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a><strong>解决方法</strong></h4><p>1）减少特征数量（减少特征会失去一些信息，即使特征选的很好）</p><ul><li>可用人工选择要保留的特征；</li><li>模型选择算法；</li></ul><p>2）正则化（特征较多时比较有效）</p><ul><li>保留所有特征，但减少θ的大小</li></ul><h4 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a><strong>正则化方法</strong></h4><p>正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化项就越大。</p><p>在<a href="https://sevenold.github.io/2018/07/ml-linearRegressionL1L2/" target="_blank" rel="noopener">线性回归算法的正则化问题</a>,正则项可以取不同的形式，在回归问题中取平方损失，就是参数的L2范数，也可以取L1范数。取平方损失时，模型的损失函数变为：</p><h2 id="J-theta-frac1-2m-sum-i-1-n-h-theta-x-i-y-i-2-lambda-sum-j-1-n-theta-j-2"><a href="#J-theta-frac1-2m-sum-i-1-n-h-theta-x-i-y-i-2-lambda-sum-j-1-n-theta-j-2" class="headerlink" title="$J(\theta)=\frac1{2m}\sum_{i=1}^{n}(h_\theta(x_i)-y_i)^2+\lambda\sum_{j=1}^n \theta_j^2$"></a>$J(\theta)=\frac1{2m}\sum_{i=1}^{n}(h_\theta(x_i)-y_i)^2+\lambda\sum_{j=1}^n \theta_j^2$</h2><h4 id="lambda是正则项系数："><a href="#lambda是正则项系数：" class="headerlink" title="lambda是正则项系数："></a>lambda是正则项系数：</h4><ul><li>如果它的值很大，说明对模型的复杂度惩罚大，对拟合数据的损失惩罚小，这样它就不会过分拟合数据，在训练数据上的偏差较大，在未知数据上的方差较小，但是可能出现欠拟合的现象；</li><li>如果它的值很小，说明比较注重对训练数据的拟合，在训练数据上的偏差会小，但是可能会导致过拟合。</li></ul><h4 id="正则化后的梯度下降算法θ的更新变为："><a href="#正则化后的梯度下降算法θ的更新变为：" class="headerlink" title="正则化后的梯度下降算法θ的更新变为："></a>正则化后的梯度下降算法θ的更新变为：</h4><h2 id="theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j-frac-lambda-m-theta-j"><a href="#theta-j-theta-j-a-frac-1-m-sum-i-1-n-h-theta-x-i-y-i-x-i-j-frac-lambda-m-theta-j" class="headerlink" title="$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j - \frac \lambda m \theta_j$"></a>$\theta_j :=\theta_j-a \frac 1 m \sum_{i=1}^n (h_\theta(x_i)-y_i)x_i^j - \frac \lambda m \theta_j$</h2><h3 id="其他优化算法"><a href="#其他优化算法" class="headerlink" title="其他优化算法"></a><strong>其他优化算法</strong></h3><ul><li>Conjugate gradient method(共轭梯度法)</li><li>Quasi-Newton method(拟牛顿法)</li><li>BFGS method(局部优化法)</li><li>L-BFGS(Limited-memory BFGS)（有限内存局部优化法）</li></ul><p>后二者由拟牛顿法引申出来，与梯度下降算法相比，这些算法的优点是：</p><ul><li>第一，不需要手动的选择步长；</li><li>第二，通常比梯度下降算法快；</li></ul><p>但是缺点是更复杂。</p><h3 id="多类分类问题"><a href="#多类分类问题" class="headerlink" title="多类分类问题"></a>多类分类问题</h3><p>多类分类问题中,我们的训练集中有多个类(&gt;2),我们无法仅仅用一个二元变量(0或1)来做判断依据。例如我们要预测天气情况分四种类型:晴天、多云、下雨或下雪。下面是一个多类分类问题可能的情况:</p><p><img src="/images/ml/12.png" alt="image"></p><p>一种解决这类问题的途径是采用一对多(One-vs-All)方法（可以将其看做成二类分类问题：保留其中的一类，剩下的作为另一类 ）。在一对多方法中,我们将多类分类问题转化成二元分类问题。为了能实现这样的转变,我们将多个类中的一个类标记为正向类(y=1),然后将其他所有类都标记为负向类,这个模型记作：</p><h2 id="h-theta-1-x"><a href="#h-theta-1-x" class="headerlink" title="$h_\theta^{(1)}(x)$"></a>$h_\theta^{(1)}(x)$</h2><p>接着,类似地第我们选择另一个类标记为正向类(y=2),再将其它类都标记为负向类,将这个模型记作,</p><h2 id="h-theta-2-x"><a href="#h-theta-2-x" class="headerlink" title="$h_\theta^{(2)}(x)$"></a>$h_\theta^{(2)}(x)$</h2><p>依此类推。最后我们得到一系列的模型简记为:</p><h2 id="h-theta-i-x-p-y-i-x-theta"><a href="#h-theta-i-x-p-y-i-x-theta" class="headerlink" title="$h_\theta^{(i)}(x)=p(y=i|x;\theta)$"></a>$h_\theta^{(i)}(x)=p(y=i|x;\theta)$</h2><p>其中 i = 1,2,3,…,k步骤可以记作下图：</p><p><img src="/images/ml/13.png" alt="image"></p><p>最后,在我们需要做预测时,我们将所有的分类机都运行一遍,然后对每一个输入变量,都选择最高可能性的输出变量。</p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/ml-logisticRegression/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习-线性回归推导总结</title>
      <link href="/2018/07/21/2018-07-21-ml-linearRegressionL1L2/"/>
      <url>/2018/07/21/2018-07-21-ml-linearRegressionL1L2/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>上一节<a href="https://sevenold.github.io/2018/07/ml-linearRegression/" target="_blank" rel="noopener">线性回归-算法推导</a> , 我们已经大致的知道了，线性回归的算法推导过程，但是往往我们在使用线性回归算法的过程中模型会出现<strong>过拟合的现象</strong>， 我们现从例子来看看什么是<strong>过拟合</strong>。</p><p>还是以房价预测为例，来看几张张图片：</p><h3 id="1-欠拟合（Underfitting）"><a href="#1-欠拟合（Underfitting）" class="headerlink" title="1.欠拟合（Underfitting）"></a><strong>1.欠拟合（Underfitting）</strong></h3><p><img src="/images/ml/1.png" alt="image"></p><p>上图中，我们用$h_\theta(x)=\theta_0+\theta_1x $来拟合训练集中的数据，但是我们可以很明显的从图中看出，房价是不会随着面积成比例的增长的，这种情况，我们就称之为<strong>欠拟合</strong>。</p><h3 id="2-过拟合（Overfitting）"><a href="#2-过拟合（Overfitting）" class="headerlink" title="2.过拟合（Overfitting）"></a><strong>2.过拟合（Overfitting）</strong></h3><p><img src="/images/ml/2.png" alt="image"></p><p>如上图所示，我们用一条高次的曲线 $h_θ(x)=θ_0+θ_1x+θ_2x^2+θ_3x^3+θ_4x^4$ 来拟合训练集中的数据，因为参数过多，对训练集的匹配度太高、太准确，以至于在后面的预测过程中可能会导致预测值非常偏离合适的值，预测非常不准确，也就是说能力太强了，导致震荡的非常强烈。这就是<strong>过拟合</strong>。</p><h3 id="3-合适的拟合（Properfitting）"><a href="#3-合适的拟合（Properfitting）" class="headerlink" title="3.合适的拟合（Properfitting）"></a><strong>3.合适的拟合（Properfitting）</strong></h3><p>​ <img src="/images/ml/3.png" alt="image"></p><p>如上图，如何参数选择的恰当，选用一个合适的曲线，比如说是$h_θ(x)=θ_0+θ_1x+θ_2x^2$来拟合上面的数据集就非常适合，这样这就是一个比较恰当的假设参数（hypothesis function）.</p><h3 id="简单总结一下"><a href="#简单总结一下" class="headerlink" title="简单总结一下"></a><strong>简单总结一下</strong></h3><p>一般在实际的应用中是不会遇到<strong>欠拟合</strong>的情况的，但是<strong>过拟合</strong>是经常出现的，一般情况下，<strong>过拟合</strong>（Overfitting）就是：如果我在训练一个数据集的时候，用了太多的特征（features）来训练一个假设函数，就会造成匹配度非常高（误差几乎就为0， 也就是我上一节得出的损失函数：$ J(θ)=∑_N^{i=1}(y_i−θ^Tx_i)^2$),但是不能推广到其他的未知数据上，也就是对于其他的训练集是没有任何用的，不能做出正确的预测。</p><p>所以为了避免这种<strong>过拟合现象</strong>的发生，我们也有对应得惩罚，让他的能力不要那么强，所以就有L1(LASSO)、岭回归L2(Ridge)。我们来直观的了解下这两种正则。</p><ol><li><p>最小均方函数导数不为0时，L2导数加上最小均方函数导数肯定不为0。但是L1的正则项是绝对值函数，导数为0只要在x从左边趋向于0和从右边趋向于0时导数异号就行，所以更容易得到稀疏解。</p></li><li><p>目标函数最小均方差解空间为同心圆，L2解空间也为同心圆，L1解空间为菱形，两个解空间相交处为最优值。如图所示。</p><p>​ <img src="/images/ml/4.png" alt="image"></p></li></ol><h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><p>总结上节的知识点，我们就有三种方式解出线性回归算法的表达式或解析解：</p><ol><li>最小二乘法的解析解可以用高斯分布（Gaussian）以及最大似然估计法求得</li><li>岭回归Ridge（L2正则）的解析解可以用高斯分布（Gaussian）以及最大后验概率解释</li><li>LASSO（L1正则）的解释解可以用拉普拉斯（Laplace）分布以及最大后验概率解释</li></ol><h4 id="在推导之前："><a href="#在推导之前：" class="headerlink" title="在推导之前："></a>在推导之前：</h4><ol><li>假设你已经懂得：高斯分布，拉普拉斯分布，最大似然估计，最大后验估计</li><li>机器学习的三要素：<strong>模型、策略、算法</strong>（李航《统计学习方法》）。就是说，一种模型可以有多种求解策略，每一种求解策略可能又有多种计算方法。所以先把模型策略搞懂，然后算法。</li></ol><h3 id="线性回归模型总结"><a href="#线性回归模型总结" class="headerlink" title="线性回归模型总结"></a>线性回归模型总结</h3><p>首先我们先假设线性回归模型：</p><h4 id="f-x-sum-i-1-nx-iw-i-b-w-TX-b"><a href="#f-x-sum-i-1-nx-iw-i-b-w-TX-b" class="headerlink" title="$f(x)= \sum_{i=1}^nx_iw_i+b=w^TX+b$"></a>$f(x)= \sum_{i=1}^nx_iw_i+b=w^TX+b$</h4><h4 id="其中-x-in-R-1-times-n-w-in-R-1-times-n-当前已知：X-x-1-cdot-cdot-cdot-x-m-in-R-m-times-n-y-in-R-n-times-1-b-in-R-求出-w"><a href="#其中-x-in-R-1-times-n-w-in-R-1-times-n-当前已知：X-x-1-cdot-cdot-cdot-x-m-in-R-m-times-n-y-in-R-n-times-1-b-in-R-求出-w" class="headerlink" title="其中$x \in R^{1\times n}, w\in R^{1\times n},当前已知：X=(x_1 \cdot \cdot \cdot x_m) \in R^{m\times n}, y \in R^{n\times 1}, b \in R$,求出$w$"></a>其中$x \in R^{1\times n}, w\in R^{1\times n},当前已知：X=(x_1 \cdot \cdot \cdot x_m) \in R^{m\times n}, y \in R^{n\times 1}, b \in R$,求出$w$</h4><h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>如果$b \sim N(u, \sigma^2)$, 其中$u=0$, 也就是说$y_i \sim N(w^TX,\sigma^2)$</p><h4 id="采用最大似然估计法："><a href="#采用最大似然估计法：" class="headerlink" title="采用最大似然估计法："></a>采用最大似然估计法：</h4><h4 id="L-w-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2"><a href="#L-w-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2" class="headerlink" title="$L(w)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})}$"></a>$L(w)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})}$</h4><h4 id="对数似然函数："><a href="#对数似然函数：" class="headerlink" title="对数似然函数："></a>对数似然函数：</h4><h4 id="l-w-nlog-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2"><a href="#l-w-nlog-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2" class="headerlink" title="$l(w)=-nlog\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 $"></a>$l(w)=-nlog\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 $</h4><p>因为我们要求的是似然函数的最大值：</p><h4 id="arg-max-w-L-w-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2"><a href="#arg-max-w-L-w-prod-i-1-N-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2" class="headerlink" title="$arg max_w \ L(w)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})}$"></a>$arg max_w \ L(w)=\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})}$</h4><p>通过对数似然进行变换后，因为$-nlog\sigma\sqrt{2\pi}$是定值，所以最终解析解：</p><h4 id="arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-y-w-TX-2-2"><a href="#arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-y-w-TX-2-2" class="headerlink" title="$arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 =||y-w^TX||_2^2$"></a>$arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 =||y-w^TX||_2^2$</h4><h3 id="岭回归Redge-L2正则"><a href="#岭回归Redge-L2正则" class="headerlink" title="岭回归Redge(L2正则)"></a>岭回归Redge(L2正则)</h3><p>如果$b \sim N(u, \sigma^2), w_i \sim N(u, \tau^2)$,其中$u=0$;</p><p>所以使用最大后验估计推导：</p><p>构建似然函数：</p><h4 id="L-w-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2-cdot-prod-j-1-d-frac-1-sqrt-2-pi-tau-e-frac-w-j-2-2-tau-2"><a href="#L-w-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2-cdot-prod-j-1-d-frac-1-sqrt-2-pi-tau-e-frac-w-j-2-2-tau-2" class="headerlink" title="$L(w)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})} \cdot \prod_{j=1}^{d}\frac{1}{\sqrt{2\pi}\tau}e^{-(\frac{(w_j)^2}{2\tau^2})}$"></a>$L(w)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})} \cdot \prod_{j=1}^{d}\frac{1}{\sqrt{2\pi}\tau}e^{-(\frac{(w_j)^2}{2\tau^2})}$</h4><p>对数似然函数</p><h4 id="l-w-nln-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-dln-tau-sqrt-2-pi-frac-1-2-tau-2-sum-j-1-d-w-j-2"><a href="#l-w-nln-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-dln-tau-sqrt-2-pi-frac-1-2-tau-2-sum-j-1-d-w-j-2" class="headerlink" title="$l(w)=-nln\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 -dln \tau \sqrt{2\pi}-\frac{1}{2\tau^2}\sum_{j=1}^{d}(w_j)^2 $"></a>$l(w)=-nln\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 -dln \tau \sqrt{2\pi}-\frac{1}{2\tau^2}\sum_{j=1}^{d}(w_j)^2 $</h4><p>因为$-nln\sigma\sqrt{2\pi}-dln \tau \sqrt{2\pi}$是定值，最后的解析解：</p><h4 id="arg-max-w-L-w-arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-lambda-sum-j-1-d-w-j-2-y-w-TX-2-2-lambda-w-2-2"><a href="#arg-max-w-L-w-arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-lambda-sum-j-1-d-w-j-2-y-w-TX-2-2-lambda-w-2-2" class="headerlink" title="$arg max_w \ L(w) = arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2+ \lambda\sum_{j=1}^{d}(w_j)^2 \\  =||y-w^TX||_2^2+\lambda||w||_2^2 $"></a>$arg max_w \ L(w) = arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2+ \lambda\sum_{j=1}^{d}(w_j)^2 \\ =||y-w^TX||_2^2+\lambda||w||_2^2 $</h4><h3 id="LASSO-L1正则"><a href="#LASSO-L1正则" class="headerlink" title="LASSO(L1正则)"></a>LASSO(L1正则)</h3><p>如果$b \sim N(u, \sigma^2), w_i \sim Laplace(u,b)$,其中$u=0$;</p><p>所以使用最大后验估计推导：</p><p>构建似然函数：</p><h4 id="L-w-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2-cdot-prod-j-1-d-frac-1-2b-e-frac-w-i-b"><a href="#L-w-prod-i-1-n-frac-1-sqrt-2-pi-sigma-e-frac-y-i-w-Tx-i-2-2-sigma-2-cdot-prod-j-1-d-frac-1-2b-e-frac-w-i-b" class="headerlink" title="$L(w)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})} \cdot \prod_{j=1}^{d}\frac{1}{2b}e^{-(\frac{|w_i|}{b})}$"></a>$L(w)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-(\frac{(y_i-w^Tx_i)^2}{2\sigma^2})} \cdot \prod_{j=1}^{d}\frac{1}{2b}e^{-(\frac{|w_i|}{b})}$</h4><p>对数似然：</p><h4 id="l-w-nln-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-dln-2b-frac-1-b-sum-j-1-d-w-j"><a href="#l-w-nln-sigma-sqrt-2-pi-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-dln-2b-frac-1-b-sum-j-1-d-w-j" class="headerlink" title="$l(w)=-nln\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 -dln 2b -\frac{1}{b}\sum_{j=1}^{d}|w_j| $"></a>$l(w)=-nln\sigma\sqrt{2\pi}-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2 -dln 2b -\frac{1}{b}\sum_{j=1}^{d}|w_j| $</h4><p>因为$-nln\sigma\sqrt{2\pi}-dln2b$是定值，最后的解析解：</p><h4 id="arg-max-w-L-w-arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-lambda-sum-j-1-d-w-j-y-w-TX-2-2-lambda-w-1"><a href="#arg-max-w-L-w-arg-min-w-f-w-frac-1-2-sigma-2-sum-i-1-n-y-i-w-Tx-i-2-lambda-sum-j-1-d-w-j-y-w-TX-2-2-lambda-w-1" class="headerlink" title="$arg max_w \ L(w) = arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2+ \lambda\sum_{j=1}^{d}|w_j| \\  =||y-w^TX||_2^2+\lambda||w||_1$"></a>$arg max_w \ L(w) = arg min_w f(w)=\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-w^Tx_i)^2+ \lambda\sum_{j=1}^{d}|w_j| \\ =||y-w^TX||_2^2+\lambda||w||_1$</h4><h3 id="线性回归正则化总结"><a href="#线性回归正则化总结" class="headerlink" title="线性回归正则化总结"></a>线性回归正则化总结</h3><p>L1正则化和L2正则化可以看做是损失函数的惩罚项，所谓『惩罚』是指对损失函数中的某些参数做一些限制 ，都能防止过拟合，一般L2的效果更好一些，L1能够产生稀疏模型，能够帮助我们去除某些特征，因此可以用于特征选择。</p><hr><ul><li>L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合</li><li>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择</li></ul><hr><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2018/07/ml-linearRegressionL1L2/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> 线性回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>机器学习简介</title>
      <link href="/2018/07/05/2016-07-05-MachineLearning_introduce/"/>
      <url>/2018/07/05/2016-07-05-MachineLearning_introduce/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="通用名词"><a href="#通用名词" class="headerlink" title="通用名词"></a>通用名词</h3><h4 id="ML"><a href="#ML" class="headerlink" title="ML"></a>ML</h4><p><strong>名词解释：</strong> 机器学习(Machine Learning)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。<br>它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。</p><h4 id="DL"><a href="#DL" class="headerlink" title="DL"></a>DL</h4><p><strong>名词解释：</strong> 深度学习（Deep Learning）是机器学习拉出的分支，它试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。<br>深度学习是机器学习中表征学习方法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的矢量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是将用非监督式或半监督式的特征学习和分层特征提取的高效算法来替代手工获取特征。</p><h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><p><strong>名词解释：</strong> 卷积神经网络（Convolutional neural networks，简称CNNs）是一种深度的监督学习下的机器学习模型</p><h3 id="算法名词"><a href="#算法名词" class="headerlink" title="算法名词"></a>算法名词</h3><h4 id="KNN"><a href="#KNN" class="headerlink" title="KNN:"></a>KNN:</h4><p><strong>名词解释：</strong> 邻近算法，或者说K最近邻(kNN，k-NearestNeighbor)分类算法。<br>邻近算法是数据挖掘分类技术中最简单的方法之一。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。<br>kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 kNN方法在类别决策时，只与极少量的相邻样本有关。由于kNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，kNN方法较其他方法更为适合。</p><h4 id="SVM"><a href="#SVM" class="headerlink" title="SVM:"></a>SVM:</h4><p><strong>名词解释：</strong> 支持向量机（Support Vector Machine）。<br>在机器学习领域，支持向量机SVM(Support Vector Machine)是一个有监督的学习模型，通常用来进行模式识别、分类、以及回归分析。</p><p><br></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2016/07/MachineLearning_introduce/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Markdown工具集</title>
      <link href="/2016/11/20/2016-11-20-markdownTool/"/>
      <url>/2016/11/20/2016-11-20-markdownTool/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="什么是-Markdown"><a href="#什么是-Markdown" class="headerlink" title="什么是 Markdown"></a>什么是 Markdown</h3><p>Markdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：如您正在阅读的这篇文章。它使用简单的符号标记不同的标题，分割不同的段落，<strong>粗体</strong> 或者 <em>斜体</em> 某些文字.</p><p>很多产品的文档也是用markdown编写的，并且以“README.MD”的文件名保存在软件的目录下面。</p><h3 id="一些基本语法"><a href="#一些基本语法" class="headerlink" title="一些基本语法"></a>一些基本语法</h3><p>标题<br>H1 :# Header 1<br>H2 :## Header 2<br>H3 :### Header 3<br>H4 :#### Header 4<br>H5 :##### Header 5<br>H6 :###### Header 6<br>链接 :<a href="URL">Title</a><br>加粗 :<strong>Bold</strong><br>斜体字 :<em>Italics</em><br><em>删除线 :<del>text</del><br>段落 : 段落之间空一行<br>换行符 : 一行结束时输入两个空格<br>列表 :</em> 添加星号成为一个新的列表项。<br>引用 :&gt; 引用内容<br>内嵌代码 : <code>alert(&#39;Hello World&#39;);</code><br>画水平线 (HR) :——–<br>​</p><p>css 的大部分语法同样可以在 markdown 上使用，但不同的渲染器渲染出来的 markdown 内容样式也不一样，下面这些链接里面有 markdown 基本语法，你也可以在下面几个平台上尝试着写一些。</p><h3 id="一些好用的-Markdown-编辑器"><a href="#一些好用的-Markdown-编辑器" class="headerlink" title="一些好用的 Markdown 编辑器"></a>一些好用的 Markdown 编辑器</h3><p><br></p><p><a href="http://mahua.jser.me/?utm_source=mindstore.io" target="_blank" rel="noopener">MaHua</a> 在线 Markdown 编辑器 ,无须测试。</p><p><br></p><p><img src="/images/posts/markdown/image1.png" alt=""></p><p><br></p><p><a href="http://mdp.tylingsoft.com/" target="_blank" rel="noopener">Markdown Plus</a> 一款 Markdown 编辑器，可以支持添加任务列表、emoji、流程图等。</p><p><br></p><p><img src="/images/posts/markdown/image2.png" alt=""></p><p><br></p><p><a href="https://www.zybuluo.com/cmd/?utm_source=mindstore.io" target="_blank" rel="noopener">Cmd Markdown</a> 作业部落在线 Markdown 编辑器推出桌面版客户端啦，全平台支援。</p><p><img src="/images/posts/markdown/image3.png" alt=""></p><p><a href="https://github.com/MacDownApp/macdown" target="_blank" rel="noopener">Macdown</a> Github 上开源的 Mac 平台上的 Markdown 编辑器</p><p><a href="https://www.gitbook.com/editor?utm_source=mindstore.io" target="_blank" rel="noopener">GitBook Editor</a> 一款团队在线编辑文档工具。可以轻松书写笔记，支持团队协同编辑。同时支持 Markdown 语法，还保持了印象笔记的风格并可在线预览。</p><p><a href="http://www.glamdevelopment.com/outlinely?utm_source=mindstore.io" target="_blank" rel="noopener">Outlinely</a> 界面简洁大方的大纲类 Mac 软件，使用起来很简单，而且支持输出 Markdown 格式。</p><p><a href="http://classeur.io/?utm_source=mindstore.io" target="_blank" rel="noopener">Classeur</a> 实用简洁的 Markdown 写作工具，快速上手。</p><p><a href="https://github.com/geekcompany/DeerResume?utm_source=mindstore.io" target="_blank" rel="noopener">DeerResume</a> 程序员专用 MarkDown 简历制作在线工具。</p><p><br></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2016/11/markdownTool/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>使用 TensorFlow 实现神经网络</title>
      <link href="/2016/11/20/2016-11-20-neural_networks_using_TensorFlow/"/>
      <url>/2016/11/20/2016-11-20-neural_networks_using_TensorFlow/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>一直关注 <code>数据科学</code> 、 <code>机器学习</code> 的同学，一定会经常看到或听到关于 <code>深度学习</code> 和 <code>神经网络</code> 相关信息。如果你对 <code>深度学习</code> 感兴趣，但却还没有实际动手操作过，你可以从这里得到实践。</p><p>在本文中，我将介绍 <code>TensorFlow</code> , 帮你了解 <code>神经网络</code> 的实际作用，并使用 <code>TensorFlow</code> 来解决现实生活中的问题。 读这篇文章前，需要知道 <code>神经网络</code> 的基础知识和一些熟悉编程理念，文章中的代码是使用 <code>Pyhton</code> 编写的，所以还需要了解一些 <code>Python</code> 的基本语法，才能更有利对于文章的理解。<br>​</p><div align="center"><br><img src="/images/posts/tfimg/logo.jpg" height="300" width="500"><br></div><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ul><li><a href="#When-to-apply-neural-net">什么时候应用神经网络？</a></li><li><a href="#solve-problems">通常神经网络能解决的问题</a></li><li><a href="#popular-libraries">了解图像数据和主流的库来解决问题</a></li><li><a href="#What-is-TensorFlow">什么是 TensorFlow？</a></li><li><a href="#A-typical-flow">TensorFlow 一个 典型 的 “ 流 ”</a></li><li><a href="#MLP">在 TensorFlow 中实现 MLP</a></li><li><a href="#Limitations-of-TensorFlow">TensorFlow 的限制</a></li><li><a href="#vs-libraries">TensorFlow 与其他库</a></li><li><a href="#Where-to-go-from-here">从这里去哪里？</a></li></ul><h3 id="什么时候用神经网络？"><a href="#什么时候用神经网络？" class="headerlink" title="什么时候用神经网络？"></a><a name="When-to-apply-neural-net"></a>什么时候用神经网络？</h3><p><code>神经网络</code> 已经在相当一段时间成为机器学习中的焦点。 对于 <code>神经网络</code> 和 <code>深度学习</code> 上这里有更详细的解释 <a href="https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/" target="_blank" rel="noopener">点击阅读</a> 。 其 “更深” 的功能在许多领域都有取得巨大的突破，如图像识别，语音和自然语言处理等。</p><p>主要的问题在于如何用好 <code>神经网络</code> ？现在，每天都会有许多新发现，这个领域就像一个金矿，为了成为这个 “淘金热” 的一部分，必须记住几件事：</p><ul><li><p><strong>首先，<code>神经网络</code> 需要有明确和翔实的数据（主要是大数据）训练</strong>， 试着想象 <code>神经网络</code> 作为一个孩子，它一开始会观察它父母走路，然后它试图自己走，每一步就像学习执行一个特定的任务。 它可能会失败几次，但经过几次失败的尝试，它将会如何走路。所以需要为孩子提供更多的机会，如果不让它走，它可能永远不会学习如何走路。</p></li><li><p><strong>一些人会利用 <code>神经网络</code> 解决复杂的问题，如图像处理，</strong> <code>神经网络</code> 属于一类代表学习的算法，这些算法可以把复杂的问题分解为简单的形式，使他们成为可以理解的（或 “可表示”），就像吞咽食物之前的咀嚼，让我们更容易吸收和消化。这个分解的过程如果使用传统的算法来实现也可以，但是实现过程将会很困难。</p></li><li><p><strong>选择适当类型的 <code>神经网络</code> ，来解决问题，</strong> 每个问题的复杂情况都不一样，所以数据决定你解决问题的方式。 例如，如果问题是序列生成的问题，<code>递归神经网络</code> 更合适。如果它是图像相关的问题，想更好地解决可以采取 <code>卷积神经网络</code>。</p></li><li><p><strong>最后最重要的就是 <code>硬件</code> 要求了，硬件是运行 <code>神经网络</code> 模型的关键。</strong> 神经网被 “发现” 很久以前，他们在近年来得到推崇的主要的原因就是计算资源更好，能更大发挥它的光芒，如果你想使用 <code>神经网络</code> 解决这些现实生活中的问题，那么你得准备购买一些高端的硬件了😆！</p></li></ul><h3 id="通常神经网络解决的问题"><a href="#通常神经网络解决的问题" class="headerlink" title="通常神经网络解决的问题"></a><a name="solve-problems"></a>通常神经网络解决的问题</h3><p>神经网络是一种特殊类型的 机器学习（ML）算法。 因此，作为每个 ML 算法都遵循 数据预处理 、模型建立 和 模型评估 的工作流流程。为了简明起见，下面列出了如何处理 <code>神经网络</code> 问题的 TODO 列表。</p><ul><li>检查它是否为 神经网络 ，把它看成一个传统的算法问题</li><li>做一个调查，哪个 神经网络 框架最适合解决这个问题</li><li>定义 神经网络 框架，通过它选择对应的 编程语言 和 库</li><li>将数据转换为正确的格式并分批分割</li><li>根据您的需要预处理数据</li><li>增强数据以增加大小并制作更好的训练模型</li><li>批次供给到 神经网络</li><li>训练和监测，培训和验证数据集的变化</li><li>测试你的模型，并保存以备将来使用</li></ul><p>本文将专注于图像数据，我们从 TensorFlow 入手。</p><h3 id="了解图像数据和主流的库来解决问题"><a href="#了解图像数据和主流的库来解决问题" class="headerlink" title="了解图像数据和主流的库来解决问题"></a><a name="popular-libraries"></a>了解图像数据和主流的库来解决问题</h3><p>图像大多排列为 3-D 阵列，具体指 高度、宽度 和 颜色通道。例如，如果你使用电脑截屏，它将首先转换成一个 3-D 数组，然后压缩它为 ‘.jpeg’ 或 ‘.png’ 文件格式。</p><p>虽然这些图像对于人类来说很容易理解，但计算机很难理解它们。 这种现象称为“语义空隙”。我们的大脑可以看看图像，并在几秒钟内读懂完整的图片。但计算机会将图像看作一个数字数组，问题来了，它想知道这是一张什么样的图像，我们应该怎么样把图像解释给机器它才能读懂？</p><p>在早期，人们试图将图像分解为机器的 “可理解” 格式，如“模板”。例如，面部总是具有在每个人中有所保留的特定结构，例如眼睛，鼻子或我们的脸的形状。 但是这种方法将是有缺陷的，因为当要识别的对象的数量将增加到一定量级时，“模板” 将不成立。</p><p>2012年一个深层神经网络架构赢得了 ImageNet 的挑战，从自然场景中识别对象，它在即将到来的 ImageNet 挑战中继续统治其主权，从而证明了解决图像问题的有用性。<br>人们通常使用哪些 库 / 语言 来解决图像识别问题？<a href="https://www.analyticsvidhya.com/blog/2016/08/deep-learning-path/" target="_blank" rel="noopener">最近的一次调查</a>中，最流行的深度学习库，支持的最友好的语言有 Python ，其次是 Lua ，对 Java 和 Matlab 支持的也有。最流行的库举几个例子：</p><ul><li><a href="http://caffe.berkeleyvision.org/" target="_blank" rel="noopener">Caffe</a></li><li><a href="http://deeplearning4j.org/" target="_blank" rel="noopener">DeepLearning4j</a></li><li><a href="https://www.tensorflow.org/" target="_blank" rel="noopener">TensorFlow</a></li><li><a href="http://www.deeplearning.net/software/theano" target="_blank" rel="noopener">Theano</a></li><li><a href="http://torch.ch/" target="_blank" rel="noopener">Torch</a></li></ul><p>现在，我们了解了图像的存储方式以及使用的常用库，让我们看看 <code>TensorFlow</code> 提供的功能。</p><h3 id="什么是-TensorFlow-？"><a href="#什么是-TensorFlow-？" class="headerlink" title="什么是 TensorFlow ？"></a><a name="What-is-TensorFlow"></a>什么是 TensorFlow ？</h3><p>让我们从官方定义开始.</p><p>“<code>TensorFlow</code> 是一个开源软件库，用于使用数据流图进行数值计算。图中的节点表示数学运算，而图边表示在它们之间传递的多维数据阵列（也称为张量）。 灵活的架构允许您使用单一 API 将计算部署到桌面、服务器或移动设备中的一个或多个的 CPU 或 GPU 中。</p><p><img src="http://www.tensorfly.cn/images/tensors_flowing.gif" alt=""></p><p>如果感觉这听起来太高大上，不要担心。这里有我简单的定义，<code>TensorFlow</code> 看起来没什么，只是 numpy 有些难以理解。如果你以前使用过 numpy ，理解 TensorFlow 将是手到擒来！ numpy 和 TensorFlow 之间的主要区别是 TensorFlow 遵循惰性编程范例。 TensorFlow 的操作基本上都是对 session 的操作，它首先构建一个所有操作的图形，当我们调用 session 时 TensorFlow 就开始工作了。它通过将内部数据表示转换为张量（Tensor，也称为多维数组）来构建为可扩展的。 构建计算图可以被认为是 TensorFlow 的主要成分。想更多地了解一个计算图形的数学结构，可以阅读 <a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank" rel="noopener">这篇文章</a> 。</p><p>通过上面的介绍，很容易将 TensorFlow 分类为神经网络库，但它不仅仅是如此。它被设计成一个强大的神经网络库， 但它有能力做更多的事情。可以构建它为其他机器学习算法，如 决策树 或 k-最近邻，你可以从字面上理解，你可以做一切你在 numpy 上能做的事情！我们暂且称它为 “全能的 numpy” 。</p><p><strong>使用 TensorFlow 的优点是：</strong></p><ul><li><strong>它有一个直观的结构</strong> ，顾名思义它有 “张量流”，你可以轻松地可视每个图中的每一个部分。</li><li><strong>轻松地在 cpu / gpu 上进行分布式计算</strong></li><li><strong>平台的灵活性</strong> 。可以随时随地运行模型，无论是在移动端、服务器还是 PC 上。</li></ul><h3 id="TensorFlow-的典型-“流”"><a href="#TensorFlow-的典型-“流”" class="headerlink" title="TensorFlow 的典型 “流”"></a><a name="A-typical-flow"></a>TensorFlow 的典型 “流”</h3><p>每个库都有自己的“实现细节”，即一种写其遵循其编码范例的方式。 例如，当实现 scikit-learn 时，首先创建所需算法的对象，然后在训练和测试集上构建一个模型获得预测，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># define hyperparamters of ML algorithm</span></span><br><span class="line">clf = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100.</span>)</span><br><span class="line"><span class="comment"># train </span></span><br><span class="line">clf.fit(X, y)</span><br><span class="line"><span class="comment"># test </span></span><br><span class="line">clf.predict(X_test)</span><br></pre></td></tr></table></figure><p>正如我前面所说，TensorFlow 遵循一种懒惰的方法。 在 TensorFlow 中运行程序的通常工作流程如下：</p><ul><li><strong>建立一个计算图</strong>， 任何的数学运算可以使用 TensorFlow 支撑。</li><li><strong>初始化变量</strong>， 编译预先定义的变量</li><li><strong>创建 session</strong>， 这是神奇的开始的地方 ！</li><li><strong>在 session 中运行图</strong>， 编译图形被传递到 session ，它开始执行它。</li><li><strong>关闭 session</strong>， 结束这次使用。</li></ul><p>TensoFlow 中使用的术语很少</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">placeholder：将数据输入图形的一种方法</span><br><span class="line">feed_dict：将数值传递到计算图的字典</span><br></pre></td></tr></table></figure><p>让我们写一个小程序来添加两个数字！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># import tensorflow</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line"># build computational graph</span><br><span class="line">a = tf.placeholder(tf.int16)</span><br><span class="line">b = tf.placeholder(tf.int16)</span><br><span class="line"></span><br><span class="line">addition = tf.add(a, b)</span><br><span class="line"></span><br><span class="line"># initialize variables</span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"># create session and run the graph</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print &quot;Addition: %i&quot; % sess.run(addition, feed_dict=&#123;a: 2, b: 3&#125;)</span><br><span class="line"></span><br><span class="line"># close session</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><h3 id="在-TensorFlow-中实现神经网络"><a href="#在-TensorFlow-中实现神经网络" class="headerlink" title="在 TensorFlow 中实现神经网络"></a><a name="MLP"></a>在 TensorFlow 中实现神经网络</h3><p><em>注意：我们可以使用不同的神经网络体系结构来解决这个问题，但是为了简单起见，我们在深入实施中讨论 <code>前馈多层感知器</code>。</em></p><p>让我们记住对神经网络的了解。</p><p>神经网络的典型实现如下：</p><ul><li>定义要编译的神经网络体系结构</li><li>将数据传输到模型</li><li>整个运行中，数据首先被分成批次，以便它可以被摄取。首先对批次进行预处理，扩增，然后送入神经网络进行训练</li><li>然后，模型被逐步地训练</li><li>显示特定数量的时间步长的精度</li><li>训练后保存模型供将来使用</li><li>在新数据上测试模型并检查其运行方式</li></ul><p>在这里，我们解决了我们深刻的学习实践中的问题 - [识别数字]，让再我们花一点时间看看问题陈述。</p><p>我们的问题是图像识别，以识别来自给定的 28×28 图像的数字。 我们有一个图像子集用于训练，其余的用于测试我们的模型。首先下载训练和测试文件。数据集包含数据集中所有图像的压缩文件， train.csv 和 test.csv 都有相应的训练和测试图像的名称。数据集中不提供任何其他功能，只是原始图像以 “.png” 格式提供。</p><p>如之前说的，我们将使用 TensorFlow 来创建一个神经网络模型。 所以首先在你的系统中安装 TensorFlow 。 请参考 <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md" target="_blank" rel="noopener">官方的安装指南</a> 进行安装，按您的系统规格。</p><p>我们将按照上述模板</p><ul><li>让我们来 导入所有需要的模块</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">%pylab inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> scipy.misc <span class="keyword">import</span> imread</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><ul><li>让我们来 设置一个种子值，这样我们就可以控制我们的模型随机性</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># To stop potential randomness</span></span><br><span class="line">seed = <span class="number">128</span></span><br><span class="line">rng = np.random.RandomState(seed)</span><br></pre></td></tr></table></figure><ul><li>第一步是设置目录路径，以便保管！</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">root_dir = os.path.abspath(<span class="string">'../..'</span>)</span><br><span class="line">data_dir = os.path.join(root_dir, <span class="string">'data'</span>)</span><br><span class="line">sub_dir = os.path.join(root_dir, <span class="string">'sub'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check for existence</span></span><br><span class="line">os.path.exists(root_dir)</span><br><span class="line">os.path.exists(data_dir)</span><br><span class="line">os.path.exists(sub_dir)</span><br></pre></td></tr></table></figure><ul><li>现在让我们读取我们的数据集，这些是 .csv 格式，并有一个文件名以及相应的标签</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">train = pd.read_csv(os.path.join(data_dir，<span class="string">'Train'</span>，<span class="string">'train.csv'</span>))</span><br><span class="line">test = pd.read_csv(os.path.join（data_dir，<span class="string">'Test.csv'</span>))</span><br><span class="line">sample_submission = pd.read_csv(os.path.join(data_dir，<span class="string">'Sample_Submission.csv'</span>))</span><br><span class="line">train.head()</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th style="text-align:center">文件名</th><th style="text-align:right">标签</th></tr></thead><tbody><tr><td>0</td><td style="text-align:center">0.png</td><td style="text-align:right">4</td></tr><tr><td>1</td><td style="text-align:center">1.png</td><td style="text-align:right">9</td></tr><tr><td>2</td><td style="text-align:center">2.png</td><td style="text-align:right">1</td></tr><tr><td>3</td><td style="text-align:center">3.png</td><td style="text-align:right">7</td></tr><tr><td>4</td><td style="text-align:center">4.png</td><td style="text-align:right">3</td></tr></tbody></table><ul><li>让我们看看我们的数据是什么样子！我们读取我们的形象并显示出来。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">img_name = rng.choice(train.filename)</span><br><span class="line">filepath = os.path.join(data_dir, <span class="string">'Train'</span>, <span class="string">'Images'</span>, <span class="string">'train'</span>, img_name)</span><br><span class="line"></span><br><span class="line">img = imread(filepath, flatten=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">pylab.imshow(img, cmap=<span class="string">'gray'</span>)</span><br><span class="line">pylab.axis(<span class="string">'off'</span>)</span><br><span class="line">pylab.show()</span><br></pre></td></tr></table></figure><p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2016/10/3.png" alt=""></p><p>上面的图像表示为 numpy 数组，如下所示</p><p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2016/10/one.png" alt=""></p><ul><li>为了方便数据操作，让我们 的存储作为 numpy 的阵列的所有图片</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">temp = []</span><br><span class="line"><span class="keyword">for</span> img_name <span class="keyword">in</span> train.filename:</span><br><span class="line">    image_path = os.path.join(data_dir, <span class="string">'Train'</span>, <span class="string">'Images'</span>, <span class="string">'train'</span>, img_name)</span><br><span class="line">    img = imread(image_path, flatten=<span class="keyword">True</span>)</span><br><span class="line">    img = img.astype(<span class="string">'float32'</span>)</span><br><span class="line">    temp.append(img)</span><br><span class="line">    </span><br><span class="line">train_x = np.stack(temp)</span><br><span class="line"></span><br><span class="line">temp = []</span><br><span class="line"><span class="keyword">for</span> img_name <span class="keyword">in</span> test.filename:</span><br><span class="line">    image_path = os.path.join(data_dir, <span class="string">'Train'</span>, <span class="string">'Images'</span>, <span class="string">'test'</span>, img_name)</span><br><span class="line">    img = imread(image_path, flatten=<span class="keyword">True</span>)</span><br><span class="line">    img = img.astype(<span class="string">'float32'</span>)</span><br><span class="line">    temp.append(img)</span><br><span class="line">    </span><br><span class="line">test_x = np.stack(temp)</span><br></pre></td></tr></table></figure><ul><li>由于这是典型的 ML 问题，为了测试我们的模型的正确功能，我们创建一个验证集，让我们以 70:30 的分割训练集 和 验证集</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">split_size = int(train_x.shape[<span class="number">0</span>]*<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line">train_x, val_x = train_x[:split_size], train_x[split_size:]</span><br><span class="line">train_y, val_y = train.label.values[:split_size], train.label.values[split_size:]</span><br></pre></td></tr></table></figure><ul><li>我们定义一些辅助函数，我们稍后在我们的程序中使用</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dense_to_one_hot</span><span class="params">(labels_dense, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Convert class labels from scalars to one-hot vectors"""</span></span><br><span class="line">    num_labels = labels_dense.shape[<span class="number">0</span>]</span><br><span class="line">    index_offset = np.arange(num_labels) * num_classes</span><br><span class="line">    labels_one_hot = np.zeros((num_labels, num_classes))</span><br><span class="line">    labels_one_hot.flat[index_offset + labels_dense.ravel()] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> labels_one_hot</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preproc</span><span class="params">(unclean_batch_x)</span>:</span></span><br><span class="line">    <span class="string">"""Convert values to range 0-1"""</span></span><br><span class="line">    temp_batch = unclean_batch_x / unclean_batch_x.max()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> temp_batch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_creator</span><span class="params">(batch_size, dataset_length, dataset_name)</span>:</span></span><br><span class="line">    <span class="string">"""Create batch with random samples and return appropriate format"""</span></span><br><span class="line">    batch_mask = rng.choice(dataset_length, batch_size)</span><br><span class="line">    </span><br><span class="line">    batch_x = eval(dataset_name + <span class="string">'_x'</span>)[[batch_mask]].reshape(<span class="number">-1</span>, <span class="number">784</span>)</span><br><span class="line">    batch_x = preproc(batch_x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> dataset_name == <span class="string">'train'</span>:</span><br><span class="line">        batch_y = eval(dataset_name).ix[batch_mask, <span class="string">'label'</span>].values</span><br><span class="line">        batch_y = dense_to_one_hot(batch_y)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> batch_x, batch_y</span><br></pre></td></tr></table></figure><ul><li>主要部分！ 让我们定义我们的神经网络架构。 我们定义一个神经网络具有 3 层，输入、隐藏 和 输出， 输入和输出中的神经元数目是固定的，因为输入是我们的 28×28 图像，并且输出是表示类的 10×1 向量。 我们在隐藏层中取 500 神经元。这个数字可以根据你的需要变化。我们把值 赋给 其余变量。 可以阅读 <a href="https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/" target="_blank" rel="noopener">神经网络的基础知识的文章</a> ，以更深的了解它是如何工作。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### set all variables</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># number of neurons in each layer</span></span><br><span class="line"></span><br><span class="line">input_num_units = <span class="number">28</span>*<span class="number">28</span></span><br><span class="line"></span><br><span class="line">hidden_num_units = <span class="number">500</span></span><br><span class="line"></span><br><span class="line">output_num_units = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># define placeholders</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, input_num_units])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, output_num_units])</span><br><span class="line"></span><br><span class="line"><span class="comment"># set remaining variables</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### define weights and biases of the neural network (refer this article if you don't understand the terminologies)</span></span><br><span class="line"></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'hidden'</span>: tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),</span><br><span class="line">    <span class="string">'output'</span>: tf.Variable(tf.random_normal([hidden_num_units, output_num_units], seed=seed))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'hidden'</span>: tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),</span><br><span class="line">    <span class="string">'output'</span>: tf.Variable(tf.random_normal([output_num_units], seed=seed))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>现在创建我们的神经网络计算图</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">hidden_layer = tf.add(tf.matmul(x, weights[<span class="string">'hidden'</span>]), biases[<span class="string">'hidden'</span>])</span><br><span class="line">hidden_layer = tf.nn.relu(hidden_layer)</span><br><span class="line"></span><br><span class="line">output_layer = tf.matmul(hidden_layer, weights[<span class="string">'output'</span>]) + biases[<span class="string">'output'</span>]</span><br></pre></td></tr></table></figure><ul><li>此外，我们需要定义神经网络的成本</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_layer, y))</span><br></pre></td></tr></table></figure><ul><li>设置优化器，即我们的反向传播算法。 这里我们使用 <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam</a> ，这是梯度下降算法的高效变体。 有在 tensorflow 可用许多其它优化（参照 <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/train.html#optimizers" target="_blank" rel="noopener">此处</a> ）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br></pre></td></tr></table></figure><ul><li>定义我们的神经网络结构后，让我们来 初始化所有的变量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br></pre></td></tr></table></figure><ul><li>现在让我们创建一个 Session ，并在 Session 中运行我们的神经网络。我们还验证我们创建的验证集的模型准确性</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># create initialized variables</span></span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### for each epoch, do:</span></span><br><span class="line">    <span class="comment">###   for each batch, do:</span></span><br><span class="line">    <span class="comment">###     create pre-processed batch</span></span><br><span class="line">    <span class="comment">###     run optimizer by feeding batch</span></span><br><span class="line">    <span class="comment">###     find cost and reiterate to minimize</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        avg_cost = <span class="number">0</span></span><br><span class="line">        total_batch = int(train.shape[<span class="number">0</span>]/batch_size)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</span><br><span class="line">            batch_x, batch_y = batch_creator(batch_size, train_x.shape[<span class="number">0</span>], <span class="string">'train'</span>)</span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict = &#123;x: batch_x, y: batch_y&#125;)</span><br><span class="line">            </span><br><span class="line">            avg_cost += c / total_batch</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Epoch:"</span>, (epoch+<span class="number">1</span>), <span class="string">"cost ="</span>, <span class="string">"&#123;:.5f&#125;"</span>.format(avg_cost)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\nTraining complete!"</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># find predictions on val set</span></span><br><span class="line">    pred_temp = tf.equal(tf.argmax(output_layer, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(pred_temp, <span class="string">"float"</span>))</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Validation Accuracy:"</span>, accuracy.eval(&#123;x: val_x.reshape(<span class="number">-1</span>, <span class="number">784</span>), y: dense_to_one_hot(val_y.values)&#125;)</span><br><span class="line">    </span><br><span class="line">    predict = tf.argmax(output_layer, <span class="number">1</span>)</span><br><span class="line">    pred = predict.eval(&#123;x: test_x.reshape(<span class="number">-1</span>, <span class="number">784</span>)&#125;)</span><br></pre></td></tr></table></figure><p>这将是上面代码的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Epoch: <span class="number">1</span> cost = <span class="number">8.93566</span></span><br><span class="line">Epoch: <span class="number">2</span> cost = <span class="number">1.82103</span></span><br><span class="line">Epoch: <span class="number">3</span> cost = <span class="number">0.98648</span></span><br><span class="line">Epoch: <span class="number">4</span> cost = <span class="number">0.57141</span></span><br><span class="line">Epoch: <span class="number">5</span> cost = <span class="number">0.44550</span></span><br><span class="line"></span><br><span class="line">Training complete!</span><br><span class="line">Validation Accuracy: <span class="number">0.952823</span></span><br></pre></td></tr></table></figure><ul><li>验证我们自己的眼睛，让我们来 想象它的预言</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">img_name = rng.choice(test.filename)</span><br><span class="line">filepath = os.path.join(data_dir, <span class="string">'Train'</span>, <span class="string">'Images'</span>, <span class="string">'test'</span>, img_name)</span><br><span class="line"></span><br><span class="line">img = imread(filepath, flatten=<span class="keyword">True</span>)</span><br><span class="line"> </span><br><span class="line">test_index = int(img_name.split(<span class="string">'.'</span>)[<span class="number">0</span>]) - <span class="number">49000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Prediction is: "</span>, pred[test_index]</span><br><span class="line"></span><br><span class="line">pylab.imshow(img, cmap=<span class="string">'gray'</span>)</span><br><span class="line">pylab.axis(<span class="string">'off'</span>)</span><br><span class="line">pylab.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Prediction <span class="keyword">is</span>:  <span class="number">8</span></span><br></pre></td></tr></table></figure><p><img src="https://www.analyticsvidhya.com/wp-content/uploads/2016/10/8.png" alt=""></p><ul><li>我们看到的模型性能是相当不错！ 现在让我们 创建一个提交</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">sample_submission.filename = test.filename</span><br><span class="line"> </span><br><span class="line">sample_submission.label = pred</span><br><span class="line"></span><br><span class="line">sample_submission.to_csv(os.path.join(sub_dir, <span class="string">'sub01.csv'</span>), index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>终于完成了！ 我们刚刚创建了自己的训练神经网络！</p><h3 id="TensorFlow-的限制"><a href="#TensorFlow-的限制" class="headerlink" title="TensorFlow 的限制"></a><a name="Limitations-of-TensorFlow"></a>TensorFlow 的限制</h3><ul><li>尽管 TensorFlow 是强大的，它仍然是一个低水平库，例如，它可以被认为是机器级语言，但对于大多数功能，您需要自己去模块化和高级接口，如 keras</li><li>它仍然在继续开发和维护，这是多么👍啊！</li><li>它取决于你的硬件规格，配置越高越好</li><li>不是所有变成语言能使用它的 API 。</li><li>TensorFlow 中仍然有很多库需要手动导入，比如 OpenCL 支持。</li></ul><p>上面提到的大多数是在 TensorFlow 开发人员的愿景，他们已经制定了一个路线图，计划库未来应该如何开发。</p><h3 id="TensorFlow-与其他库"><a href="#TensorFlow-与其他库" class="headerlink" title="TensorFlow 与其他库"></a><a name="vs-libraries"></a>TensorFlow 与其他库</h3><p>TensorFlow 建立在类似的原理，如使用数学计算图表的 Theano 和 Torch，但是随着分布式计算的额外支持，TensorFlow 更好地解决复杂的问题。 此外，TensorFlow 模型的部署已经被支持，这使得它更容易用于工业目的，打开一些商业的三方库，如 Deeplearning4j ，H2O 和 Turi。 TensorFlow 有用于 Python，C ++ 和 Matlab 的 API 。 最近还出现了对 Ruby 和 R 等其他语言的支持。因此，TensorFlow 试图获得通用语言支持。</p><h3 id="从这里去哪里？"><a href="#从这里去哪里？" class="headerlink" title="从这里去哪里？"></a><a name="Where-to-go-from-here"></a>从这里去哪里？</h3><p>以上你看到了如何用 TensorFlow 构建一个简单的神经网络，这段代码是为了让人们了解如何开始实现 TensorFlow。 要解决更复杂的现实生活中的问题，你必须在这篇文章的基础上在调整一些代码才行。</p><p>许多上述功能可以被抽象为给出无缝的端到端工作流，如果你使用 scikit-learn ，你可能知道一个高级库如何抽象“底层”实现，给终端用户一个更容易的界面。尽管 TensorFlow 已经提取了大多数实现，但是也有更高级的库，如 TF-slim 和 TFlearn。</p><h3 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h3><ul><li><a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">TensorFlow 官方库</a></li><li>Rajat Monga（TensorFlow技术负责人） <a href="https://youtu.be/wmw8Bbb_eIE" target="_blank" rel="noopener">“TensorFlow为大家”</a> 的视频</li><li><a href="https://github.com/jtoy/awesome-tensorflow/#github-projects" target="_blank" rel="noopener">一个专用资源的策划列表</a></li></ul><h3 id="关于原文"><a href="#关于原文" class="headerlink" title="关于原文"></a>关于原文</h3><p>感谢原文作者 <a href="https://www.analyticsvidhya.com/blog/author/jalfaizy/" target="_blank" rel="noopener">Faizan Shaikh</a> 的分享，<br>这篇文章是在 <a href="https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/" target="_blank" rel="noopener">An Introduction to Implementing Neural Networks using TensorFlow</a> 的基础上做的翻译和局部调整，如果发现翻译中有不对或者歧义的的地方欢迎在下面评论里提问，我会加以修正 。</p><p><br><br>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2016/11/neural_networks_using_TensorFlow/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python自动化测试iOS项目</title>
      <link href="/2016/08/04/2016-08-04-PythonTestAutomationiOS/"/>
      <url>/2016/08/04/2016-08-04-PythonTestAutomationiOS/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><p>作为一个开发人员，为了保证自己的代码的健壮，写单元测试是必不可少的环节，然而最痛快的是每天去手动跑一遍所有的case。那么什么能帮我们解决这些繁琐的操作呢，大家应该会想到自动化测试脚本了，是的，我们可以借助脚本来完成全自动化测试，下面是我列的每天脚本自动执行流程：</p><blockquote><ul><li>1、<code>pull</code> git仓库里面的最新代码到本地。</li><li>2、然后打包成<code>App</code>。</li><li>3、安装到模拟器上。</li><li>4、运行App，执行单元测试，生成测试数据并保存到本地。</li><li>5、脚本读取测试数据，邮件发送给相关人员。</li></ul></blockquote><p>当这些全自动化后，可以大大减少开发人员的维护成本，即使每次项目里面有新增模块后，增加测试case就行了，下面会介绍自动测试这5步具体怎么去执行，整个脚本是使用Python写的，代码很少功能也很简单，但这已经可以帮我们完成基本的自动化测试了，这就是脚本的强大之处，选择Pyhton纯属个人喜好，最近也在学习Python，当然了最终使用什么语言看你自己。</p><h3 id="python执行shell命令完成测试"><a href="#python执行shell命令完成测试" class="headerlink" title="python执行shell命令完成测试"></a>python执行shell命令完成测试</h3><p>首先确认本机上安装了<code>git</code> 和 <code>python</code> 。<br>脚本判断本地是否存在项目，不存在则使用命令 <code>git clone ...</code> ，存在则使用命令 <code>git pull ...</code> 。<br>这些在Linux的命令都可以使用脚本来完成的，python的 <code>os.popen()</code> 方法 就是可以在Linux上执行shell命令。<br><strong>例如：</strong> 把下面这段代码添加到一个 test.py 的文件里，然后在终端上执行 <code>python test.py</code> 命令你就会看到，你的当前目录下正在下载我的博客了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">os.popen(&apos;git clone https://github.com/sevenold/sevenold.github.io.git&apos;)</span><br></pre></td></tr></table></figure><p>git pull 。。。 更新代码也是一样的。</p><p>接下来的打包、安装、运行都是使用python执行shell命令</p><p><strong>把iOS项目打包成App，下面的 <code>Demo</code> 是项目的名字</strong></p><blockquote><ul><li>os.popen(‘xcodebuild -project Demo.xcodeproj -target Demo -configuration Debug -sdk iphonesimulator’)</li></ul></blockquote><p>这行脚本运行完成后，你就会发现同会生成一个 <code>build</code> 的文件夹。<br>Debug参数表示现在是Debug模式，如果Xcode里面改成Release了，这里需要改成Release。<br>xcodebuild 命令是 Xcode Command Line Tools 的一部分。通过调用这个命令，可以完成 iOS 工程的编译，打包和签名过程。可以使用 xcodebuild –help 来看看具体有哪些功能。</p><p><strong>打开iOS模拟器，这里运行的是<code>iPhone 6 Plus</code> 你也可以换成其它型号的模拟器</strong></p><blockquote><ul><li>os.popen(‘xcrun instruments -w “iPhone 6 Plus”‘)</li></ul></blockquote><p><strong>把刚才打包生成的App安装到模拟器上</strong><br>在安装之前要先卸载App,不然你运行的永远是最初安装的那个，后来安装的不会覆盖之前的，卸载App</p><blockquote><ul><li>os.popen(‘xcrun simctl uninstall booted com.test.Demo’)</li></ul></blockquote><p>booted 后面接的是 <code>Bundle Identifier</code>，我的是 com.test.Demo，然后再安装App</p><blockquote><ul><li>os.popen(‘xcrun simctl install booted build/Debug-iphonesimulator/Demo.app ‘)</li></ul></blockquote><p>booted 后面接的是.app的路径，我打包的时候的是Debug，所以这个的文件夹名称是Debug-iphonesimulator。</p><p><strong>在模拟器里运行App</strong></p><blockquote><ul><li>os.popen(‘xcrun simctl launch booted com.test.Demo’)</li></ul></blockquote><p>booted 后面接的是 <code>Bundle Identifier</code>，我的是 com.test.Demo。</p><p>到目前为止，你就会发现你的项目已经运行起来了，你可以在项目是Debug模式下一启动就执行单元测试，然后把对应的测试数据保存到本地为data.json。然后在使用python脚本读取测试文件的数据，最终使用邮件发送给相关人员，pyhton读取数据很简单，一行代码就行</p><blockquote><ul><li>data = open(‘data.json’).read()</li></ul></blockquote><p>data里面就是json字符串，为了脚本操作简单，我在存储的时候直接把json格式的转成了字符串类型。</p><h3 id="python发送邮件"><a href="#python发送邮件" class="headerlink" title="python发送邮件"></a>python发送邮件</h3><p>我使用的是SMTP进行邮件发送的，SMTP是发送邮件的协议，Python内置对SMTP的支持，可以发送纯文本邮件、HTML邮件以及带附件的邮件。</p><p>Python对SMTP支持有smtplib和email两个模块，email负责构造邮件，smtplib负责发送邮件，具体代码如下：</p><pre><code>from email import encodersfrom email.header import Headerfrom email.mime.text import MIMETextfrom email.utils import parseaddr, formataddrimport smtplibdef format_addr(self,s):    name, addr = parseaddr(s)    return formataddr(( \        Header(name, &apos;utf-8&apos;).encode(), \        addr.encode(&apos;utf-8&apos;) if isinstance(addr, unicode) else addr))def send_mail(self, mail, message, title):    from_addr = &apos;leopardpan@163.com&apos;    password = &apos;&apos;    to_addr = mail    smtp_server = &apos;smtp.163.com&apos;    msg = MIMEText(message, &apos;plain&apos;, &apos;utf-8&apos;)    msg[&apos;From&apos;] = self.format_addr(u&apos;自动化测试邮件 &lt;%s&gt;&apos; % from_addr)    msg[&apos;To&apos;] = self.format_addr(u&apos;管理员 &lt;%s&gt;&apos; % to_addr)    msg[&apos;Subject&apos;] = Header(title, &apos;utf-8&apos;).encode()    server = smtplib.SMTP(smtp_server, 25)    server.set_debuglevel(1)    server.login(from_addr, password)    server.sendmail(from_addr, [to_addr], msg.as_string())    server.quit()send_mail(&apos;leopardpan@icloud.com&apos;,&apos;正文&apos;,&apos;标题&apos;)</code></pre><p>from_addr是发送方的邮箱地址，password是开通SMTP时输入的密码<br>smtp_server是smtp的服务，如果你的from_addr是gamil.com，那么就要写成smtp_server = ‘smtp.gmail.com’ 了。</p><p>方法 send_mail(self, mail, message, title): 有四个参数，第一个不用传，第二个参数是收信人的邮箱，第三个是邮件的正文，第四个是邮件的标题，方法调用格式： <code>send_mail(&#39;leopardpan@icloud.com&#39;,&#39;正文&#39;,&#39;标题&#39;)</code></p><p>注意：发送方的邮箱必须要开通SMTP功能才行，否则会报错</p><blockquote><ul><li>SMTPSenderRefused: (550, ‘User has no permission’, <a href="mailto:&#39;leopardpan@163.com" target="_blank" rel="noopener">&#39;leopardpan@163.com</a>‘)</li></ul></blockquote><p>163的SMTP开通，需要你登录网易邮箱，然后点击顶部的设置就会出现<code>POP3/SMTP/IMAP</code>，点击之后，勾选选择开启，这个时候需要你输入密码，记住这个密码就是上面代码中的<code>password</code>，如果你都完成的话，你把上面的代码拷贝出现，把邮箱修改成你自己的，使用 pyhton 运行一下吧。</p><p>上面的几个流程结合起来就可以实现一个简单的自动化测试了，如果你有什么建议和意见欢迎讨论。</p><p>参考链接：<br><a href="http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386832745198026a685614e7462fb57dbf733cc9f3ad000" target="_blank" rel="noopener">SMTP发送邮件</a></p><p><br></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2016/08/PythonTestAutomationiOS/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ios </tag>
            
            <tag> 自动化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Git教程</title>
      <link href="/2016/07/13/2016-07-13-GitTutorial/"/>
      <url>/2016/07/13/2016-07-13-GitTutorial/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>Git是做项目的版本管理，你也可以称它们为版本管理工具。假如现在你有一个文件夹，里面可以是项目，也可以是你的个人笔记(如我这个博客)，或者是你的简历、毕业设计等等，都可以使用git来管理。</p><p>目前常用的版本控制器有Git和SVN，即使这两个你没有全用过，至少也会听过，我这里以Git为例，个人比较喜欢Git，你也可以看看这篇文章：<a href="http://www.worldhello.net/2012/04/12/why-git-is-better-than-svn.html" target="_blank" rel="noopener">为什么Git比SVN好</a>。我使用的是Mac，Mac上没自带Git环境，但是作为iOS开发者，我安装Xcode的时候，Xcode里是有自带Git的，所以我不需要考虑怎么去安装Git了。</p><h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><p><strong>在Mac OS X上安装Git</strong></p><p>提供两种方法参考：</p><blockquote><p>1、通过homebrew安装Git，具体方法请参考<a href="http://brew.sh/" target="_blank" rel="noopener">homebrew的文档</a><br>2、直接从AppStore安装Xcode，Xcode集成了Git，不过默认没有安装，你需要运行Xcode。</p></blockquote><p><strong>在Windows上安装Git</strong></p><blockquote><p>从<a href="https://git-for-windows.github.io" target="_blank" rel="noopener">https://git-for-windows.github.io</a> 下载，然后按默认选项安装即可，安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，蹦出一个类似命令行窗口的东西，就说明Git安装成功！</p></blockquote><h3 id="配置Git"><a href="#配置Git" class="headerlink" title="配置Git"></a>配置Git</h3><p>安装完成后，还需要最后一步设置，在命令行输入：</p><blockquote><ul><li>$ git config –global user.name “Your Name”</li><li>$ git config –global user.email “<a href="mailto:email@example.com" target="_blank" rel="noopener">email@example.com</a>“</li></ul></blockquote><p>“Your Name”： 是每次提交时所显示的用户名，因为Git是分布式版本控制系统，当我们push到远端时，就需要区分每个提交记录具体是谁提交的，这个”Your Name”就是最好的区分。</p><p>“<a href="mailto:email@example.com" target="_blank" rel="noopener">email@example.com</a>“： 是你远端仓库的email</p><p>–global：用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然我们也可以对某个仓库指定不同的用户名和Email地址。</p><h3 id="开始使用-建立仓库："><a href="#开始使用-建立仓库：" class="headerlink" title="开始使用-建立仓库："></a>开始使用-建立仓库：</h3><p>你在目标文件夹下使命令：</p><blockquote><ul><li>git init （创建.git文件）</li></ul></blockquote><p>就会创建一个 <code>.git</code> 隐藏文件，相当于已经建立了一个本地仓库。</p><p><strong>添加到暂存区：</strong></p><blockquote><ul><li>git add . （全部添加到暂存区）</li><li>git commit -m ‘ first commit’ （提交暂存区的记录到本地仓库）</li></ul></blockquote><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>git branc 查看时如出现</p><blockquote><ul><li>(HEAD detached at analytics_v2)</li><li>dev</li><li>master</li></ul></blockquote><p>代表现在已经进入一个临时的HEAD，可以使用 <code>git checkout -b temp</code> 创建一个 temp branch，这样临时HEAD上修改的东西就不会被丢掉了。<br>然后切换到 dev 分支上，在使用 git branch merge temp，就可以把 temp 分支上的代码合并到 dev 上了。</p><p><br></p><p>转载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2016/07/GitTutorial/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 开发工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GIT运用 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>树莓派开机配置</title>
      <link href="/2015/08/25/2018-07-20-raspberry-on/"/>
      <url>/2015/08/25/2018-07-20-raspberry-on/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h2 id="树莓派配置系统"><a href="#树莓派配置系统" class="headerlink" title="树莓派配置系统"></a>树莓派配置系统</h2><p><a href="https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=3136800288,1698373933&amp;fm=27&amp;gp=0.jpg" target="_blank" rel="noopener"><img src="https://ss3.bdstatic.com/70cFv8Sh_Q1YnxGkpoWK1HF6hhy/it/u=3136800288,1698373933&amp;fm=27&amp;gp=0.jpg" alt="img"></a></p><h3 id="购买硬件"><a href="#购买硬件" class="headerlink" title="购买硬件"></a>购买硬件</h3><p>直接在某宝搜索入手。必须内容：</p><p>-———————————————————————————————</p><p>树莓派一个（Raspberry Pi 3）</p><p>小usb口电源（5V2A的充电器随便找一个）</p><p>8G或者更大存储空间的SD卡一张（树莓派本身不带存储空间）</p><p>-———————————————————————————————</p><p>以下非必需：</p><p>-———————————————————————————————</p><p>散热器三片（风扇什么的觉得也太夸张了）</p><p>无线网卡（本身有网卡入口，所以不是必须的，但是做项目最好需要）</p><p>SD卡读卡器（安装系统的时候会用到）</p><p>-———————————————————————————————</p><h3 id="下载系统"><a href="#下载系统" class="headerlink" title="下载系统"></a>下载系统</h3><p>树莓派的<a href="http://wiki.nxez.com/rpi:list-of-oses" target="_blank" rel="noopener">系统</a>下载地址（<a href="http://wiki.nxez.com/rpi:list-of-oses）" target="_blank" rel="noopener">http://wiki.nxez.com/rpi:list-of-oses）</a></p><p><strong>注意：</strong>在选择系统时，请选择适合您的树莓派的系统，不适配的系统是用不了的。</p><h3 id="烧录系统"><a href="#烧录系统" class="headerlink" title="烧录系统"></a>烧录系统</h3><p>软件准备：<a href="http://sourceforge.net/projects/win32diskimager/files/Archive/win32diskimager-v0.9-binary.zip/download" target="_blank" rel="noopener">树莓派烧录工具</a>（<a href="http://sourceforge.net/projects/win32diskimager/files/Archive/win32diskimager-v0.9-binary.zip/download）" target="_blank" rel="noopener">http://sourceforge.net/projects/win32diskimager/files/Archive/win32diskimager-v0.9-binary.zip/download）</a></p><p>SD卡格式化工具（<a href="http://www.waveshare.net/w/upload/d/d7/Panasonic_SDFormatter.zip" target="_blank" rel="noopener">SDFormatter.exe</a>）</p><h4 id="一、格式化内存卡"><a href="#一、格式化内存卡" class="headerlink" title="一、格式化内存卡"></a>一、格式化内存卡</h4><p>插上SD 卡到电脑，使用<a href="http://www.waveshare.net/w/upload/d/d7/Panasonic_SDFormatter.zip" target="_blank" rel="noopener">SDFormatter.exe</a>软件格式化SD 卡。</p><h4 id="二、烧写系统"><a href="#二、烧写系统" class="headerlink" title="二、烧写系统"></a>二、烧写系统</h4><p>用<a href="http://www.waveshare.net/w/upload/7/76/Win32DiskImager.zip" target="_blank" rel="noopener">Win32DiskImager.exe</a>烧写镜像。选择要烧写的镜像，点击“Write”进行烧写。</p><p><img src="https://images-blogger-opensocial.googleusercontent.com/gadgets/proxy?url=http%3A%2F%2Fwww.servethehome.com%2Fwp-content%2Fuploads%2F2013%2F03%2FWin32-Disk-Imager-Raspbian-Image-Selection-from-Synology-NAS.png&amp;container=blogger&amp;gadget=a&amp;rewriteMime=image%2F*" alt="img"></p><p>接下来就是等待——-一直到OK，系统就烧录完成了</p><h4 id="三、配置系统"><a href="#三、配置系统" class="headerlink" title="三、配置系统"></a>三、配置系统</h4><p>经过前面两步我们的树莓派已经正常的工作起来了，但是在真正用它开发之前还需要进行一些列的配置以及软件的安装，这样开发起来才会得心应手。</p><p>配置选项</p><p>树莓派第一次使用的时候需要进行一个简单的配置，在命令行模式下运行</p><p>以下<strong>命令</strong>： $sudo raspi-config</p><p>新旧版本的配置界面不太一样，下面列举两种比较常见的：</p><p><strong>旧版本</strong></p><p><img src="https://images-blogger-opensocial.googleusercontent.com/gadgets/proxy?url=http%3A%2F%2Fwiki.jikexueyuan.com%2Fproject%2Fraspberry-pi%2Fimages%2Fold.png&amp;container=blogger&amp;gadget=a&amp;rewriteMime=image%2F*" alt="img"></p><p><strong>expand_rootfs</strong> – 将根分区扩展到整张 SD 卡（树莓派默认不使用 SD 卡的全部空间，有一部分保留，建议选中）</p><p><strong>overscan</strong> – 可以扩充或缩小屏幕（旧版不能自适应屏幕，新版没有这个选项，貌似可以自适应，没仔细研究）</p><p><strong>configure_keyboard</strong> - 键盘配置界面</p><p><strong>change_pass</strong> – 默认的用户名是 pi，密码是 raspberry，用ssh 远程连接或串口登录时要用到这个用户名和密码，这里可以更改密码。</p><p><strong>change_locale</strong> – 更改语言设置。在 Locales to be generated: 中，选择 en_US.UTF-8 和 zh_CN.UTF-8。在 Default locale for the -system environment:中，选择 en_US.UTF-8（等启动完机器，装完中文字体，再改回 zh_CN.UTF-8，否则第一次启动会出现方块）。</p><p><strong>change_timezone</strong> – 因为树莓派没有内部时钟，是通过网络获取的时间，选择 Asia – Shanghai。</p><p><strong>memory_split</strong> – 配置给桌面显示的显存。</p><p><strong>ssh</strong> – 是否激活 sshd 服务。</p><p><strong>boot_behaviour</strong> – 设置启动时启动图形界面，正常肯定是 Yes。</p><p><strong>新版本</strong>（比较新的镜像大部分是这个界面，做了不少改变）</p><p><img src="https://images-blogger-opensocial.googleusercontent.com/gadgets/proxy?url=http%3A%2F%2Fwiki.jikexueyuan.com%2Fproject%2Fraspberry-pi%2Fimages%2Fnew.jpg&amp;container=blogger&amp;gadget=a&amp;rewriteMime=image%2F*" alt="img"></p><p>1.Expand Filesystem 扩展文件系统（同旧版）。</p><p>2.Change User Password 改变默认 pi 用户的密码，按回车后输入 pi 用户的新密码。</p><p>-———————————————————————————————</p><p>3.Enable Boot to Desktop/Scratch 启动时进入的环境选择。</p><p>Console Text console, requiring login(default)</p><p>启动时进入字符控制台，需要进行登录（默认项）。</p><p>Desktop log in as user ‘pi’ at the graphical desktop</p><p>启动时进入 LXDE 图形界面的桌面。</p><p>Scratch Start the Scratch programming environment upon boot</p><p>启动时进入 Scratch 编程环境。</p><p>-———————————————————————————————</p><p>4.Internationalisation Options 国际化选项，可以更改默认语言</p><p>Change Locale：语言和区域设置，建议不要改，默认英文就好。想改中文，最好选安装 中文字体再进行这步，安装中文字体的方法：sudo apt-get update sudo apt-get install ttf-wqy-zenhei ttf-wqy-microhei</p><p>移动到屏幕底部，用空格键选中 zh-CN GB2312,zh-CN GB18030,zh-CN UTF-8,然后按回 车，然后默认语言选中 zh-cn 然后回车。</p><p>Change Timezone：设置时区，如果不进行设置，PI 的时间就显示不正常。选择 Asia（亚洲）再选择 Chongqing（重庆）即可。</p><p>Change Keyboard Layout：改变键盘布局</p><p>-———————————————————————————————</p><p>5.Enable Camera</p><p>启动 PI 的摄像头模块，如果想启用，选择 Enable，禁用选择 Disable就行了。</p><p>6.Add to Rastrack</p><p>把你的 PI 的地理位置添加到一个全世界开启此选项的地图，建议还是不要开了，免得被跟踪。</p><p>-———————————————————————————————</p><p>7.Overclock</p><p>SN描述None 不超频，运行在 700Mhz，核心频率 250Mhz，内存频率 400Mhz，不增加电压</p><p>Modest 适度超频，运行在 800Mhz，核心频率 250Mhz，内存频率 400Mhz，不增加电压</p><p>Medium 中度超频，运行在 900Mhz，核心频率 250Mhz，内存频率 450Mhz，增加电压 2</p><p>High 高度超频，运行在 950Mhz，核心频率 250Mhz，内存频率 450Mhz，增加电压 6</p><p>Turbo 终极超频，运行在 1000Mhz，核心频率 500Mhz，内存频率 600Mhz，增加电压 6</p><p>-———————————————————————————————</p><p>8.Advanced Options 高级设置</p><p>SN标准描述A1 Overscan 是否让屏幕内容全屏显示</p><p>A2 Hostname 在网上邻居或者路由器能看到的主机名称</p><p>A3 Memory Split 内存分配，选择给 GPU 多少内存</p><p>A4 SSH 是否运行 SSH 登录，建议开户此选项，以后操作 PI 方便，有网络就行，不用开屏幕了</p><p>A5 SPI 是否默认启动 SPI 内核驱动，新手就不用管了</p><p>A6 Audio 选择声音默认输出到模拟口还是 HDMI 口</p><p>A7 Update 把 raspi-config 这个工具自动升级到最新版本</p><p>-———————————————————————————————</p><p>A6</p><p>0 Auto 自动选择</p><p>1 Force 3.5mm (‘headphone’) jack强制输出到3.5mm模拟口</p><p>2 Force HDMI 强制输出到HDMI</p><p>-———————————————————————————————</p><p>9.About raspi-config</p><p>关于 raspi-config 的信息。</p><p>-———————————————————————————————</p><p>最开始的简单配置系统就完成了。。</p><p>-———————————————————————————————</p><p>载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2015/08/raspberry-on//" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 树莓派 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 开机 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>树莓派USB声卡配置</title>
      <link href="/2015/08/25/2018-07-20-raspberry-usb/"/>
      <url>/2015/08/25/2018-07-20-raspberry-usb/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h2 id="树莓派USB声卡配置"><a href="#树莓派USB声卡配置" class="headerlink" title="树莓派USB声卡配置"></a>树莓派USB声卡配置</h2><p><a href="https://gd4.alicdn.com/imgextra/i4/2567133025/TB2XTcubHBnpuFjSZFGXXX51pXa_!!2567133025.jpg" target="_blank" rel="noopener"><img src="https://gd4.alicdn.com/imgextra/i4/2567133025/TB2XTcubHBnpuFjSZFGXXX51pXa_!!2567133025.jpg" alt="img"></a></p><p>-————————————————————————————————————————-</p><p>​ 首先在树莓派没有连接的USB声卡时，/ proc / asound / cards显示的只有树莓派默认的音频设备bcm2835 ALSA，连接USB声卡后，可以看到USB音频设备USB Audio Device.ALSA是Advanced Linux Sound Architecture的缩写，现在的版本的Linux的通常都提供对ALSA的支持。</p><p>-————————————————————————————————————————-</p><h3 id="插上USB声卡"><a href="#插上USB声卡" class="headerlink" title="插上USB声卡:"></a>插上USB声卡:</h3><p>查询声卡设备: cat /proc/asound/cards</p><p>-————————————————————————————————————————-</p><p>pi@raspberrypi:~ $ cat /proc/asound/cards</p><p>0 [ALSA ]: bcm2835 - bcm2835 ALSA bcm2835 ALSA</p><p>1 [Device ]: USB-Audio - USB Audio Device C-Media Electronics Inc. USB Audio Device at usb-3f980000.usb-1.5, full speed</p><p>默认的是树莓派自带声卡,而我们外置的USB声卡作为备选项,我们需要让系统默认声卡设置为USB声卡.</p><p>-————————————————————————————————————————-</p><h3 id="切换声卡"><a href="#切换声卡" class="headerlink" title="切换声卡:"></a>切换声卡:</h3><p>编辑配置文件:sudo vim /lib/modprobe.d/aliases.conf</p><p>将 options snd-usb-audio index=-2 这行改为options snd-usb-audio index=0。</p><p><a href="http://image.newnius.com/blog/raspberry-pi-3b-change-usb-audoi-card-order-3.png" target="_blank" rel="noopener"><img src="https://images-blogger-opensocial.googleusercontent.com/gadgets/proxy?url=http%3A%2F%2Fimage.newnius.com%2Fblog%2Fraspberry-pi-3b-change-usb-audoi-card-order-3.png&amp;container=blogger&amp;gadget=a&amp;rewriteMime=image%2F*" alt="img"></a></p><h3 id="重启树莓派"><a href="#重启树莓派" class="headerlink" title="重启树莓派"></a>重启树莓派</h3><p>查看USB声卡是否设为系统首选声卡</p><p>查看命令:cat /proc/asound/cards</p><p><a href="http://image.newnius.com/blog/raspberry-pi-3b-change-usb-audoi-card-order-2.png" target="_blank" rel="noopener"><img src="https://images-blogger-opensocial.googleusercontent.com/gadgets/proxy?url=http%3A%2F%2Fimage.newnius.com%2Fblog%2Fraspberry-pi-3b-change-usb-audoi-card-order-2.png&amp;container=blogger&amp;gadget=a&amp;rewriteMime=image%2F*" alt="img"></a></p><p>此时USB声卡的编号应该和系统声卡成功调换顺序。</p><p>载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2015/08/raspberry-usb/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 树莓派 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 声卡 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>树莓派网络配置</title>
      <link href="/2015/08/25/2018-07-20-raspberry-Internet/"/>
      <url>/2015/08/25/2018-07-20-raspberry-Internet/</url>
      
        <content type="html"><![CDATA[<!-- build time:Wed Dec 12 2018 19:11:20 GMT+0800 (GMT+08:00) --><h2 id="树莓派网络"><a href="#树莓派网络" class="headerlink" title="树莓派网络"></a>树莓派网络</h2><p><a href="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1512209283734&amp;di=654bd155357e718075c66e32e251c20e&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.szthks.com%2Flocalimg%2F687474703a2f2f6777312e616c6963646e2e636f6d2f62616f2f75706c6f616465642f69352f5442317563363151465858585862355846585858585858585858585f2121302d6974656d5f7069632e6a7067.jpg" target="_blank" rel="noopener"><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1512209283734&amp;di=654bd155357e718075c66e32e251c20e&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.szthks.com%2Flocalimg%2F687474703a2f2f6777312e616c6963646e2e636f6d2f62616f2f75706c6f616465642f69352f5442317563363151465858585862355846585858585858585858585f2121302d6974656d5f7069632e6a7067.jpg" alt="img"></a></p><p>-———————————————————————————————————————</p><p>树莓派最常用的三种方式连接网络的方式</p><h3 id="通过有线网络来访问网络："><a href="#通过有线网络来访问网络：" class="headerlink" title="通过有线网络来访问网络："></a>通过有线网络来访问网络：</h3><ol><li><ol><li>自动获取IP:树莓派默认有线网卡是使能的，只需将网线插入树莓派网卡，即可自动获得IP(要求在局域网内)</li><li>手动设定IP:如果是电脑与树莓派直连，不能自动获得IP，可以使用：ifconfig eth0 192.168.1.123设定ip (每次重启都需要设置)</li><li>设置静态IP：如果担心在同网络情况下ip或者不固定，可以讲电脑设置为静态ip，方法如下：</li></ol></li></ol><p>​ 在终端中打开一下文件：sudo nano /etc/network/interfaces</p><p>-———————————————————————————————————————</p><p>auto lo</p><p>iface lo inet loopback</p><p>iface eth0 inet dhcp</p><p>讲以上内容改为：</p><p>auto lo</p><p>iface lo inet loopback</p><p>iface eth0 inet static</p><p>address 192.168.1.1</p><p>netmask 255.255.255.0</p><p>gateway 192.168.1.1</p><p>-———————————————————————————————————————</p><h3 id="通过无线网络来访问网络：-通过内置网卡和外置网卡来访问网络："><a href="#通过无线网络来访问网络：-通过内置网卡和外置网卡来访问网络：" class="headerlink" title="通过无线网络来访问网络：         通过内置网卡和外置网卡来访问网络："></a>通过无线网络来访问网络： 通过内置网卡和外置网卡来访问网络：</h3><p>-———————————————————————————————————————</p><ol><li>系统进入桌面后，点击右上角的wifi图标，系统会自动搜索附近的网络，你直接点击你所需要连接的网络，然后配置密码就可以了。</li><li>配置系统底层文件</li></ol><p>执行命令：打开wpa-supplicant中的配置文件：</p><p>sudo nano /etc/wpa_supplicant/wpa_supplicant.conf</p><p>然后，在文件的末尾添加WIFI网络的名称以及密码，将要连接的wifi名称和密码替换即可。</p><p>-———————————————————————————————————————</p><p>network={</p><p>​ ssid=”The_ESSID_from_earlier” # WiFi名字</p><p>​ psk=”Your_wifi_password” # wifi密码</p><p>​ }</p><p>-———————————————————————————————————————</p><p><strong>输入完毕后按Ctrl + X然后按Y保存文件，最后按Enter键。</strong></p><p>配置完毕，树莓派会自动连接到配置的WIFI网络，如果没有，使用sudo wpa_cli reconfigure命令启动连接或者重启系统。</p><p>###</p><p><strong>最后查询网络ip地址</strong>：ifconfig</p><p>-———————————————————————————————————————</p><h3 id="通过其他设备共享网络：通过GPRS模块访问网络"><a href="#通过其他设备共享网络：通过GPRS模块访问网络" class="headerlink" title="通过其他设备共享网络：通过GPRS模块访问网络"></a>通过其他设备共享网络：通过GPRS模块访问网络</h3><p>载请注明：<a href="http://sevenold.github.io" target="_blank" rel="noopener">Seven的博客</a> » <a href="https://sevenold.github.io/2015/08/raspberry-Internet/" target="_blank" rel="noopener">点击阅读原文</a></p><!-- rebuild by neat -->]]></content>
      
      
      <categories>
          
          <category> 树莓派 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> network </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
